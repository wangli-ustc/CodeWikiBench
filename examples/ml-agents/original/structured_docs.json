{
  "title": "ml-agents",
  "description": "Documentation for ml-agents",
  "content": {},
  "metadata": {
    "type": "root",
    "path": "/home/anhnh/CodeWiki-Benchmarking-System/data/ml-agents/original/docs"
  },
  "subpages": [
    {
      "title": "Api Reference",
      "description": null,
      "content": {
        "API Reference": "Our developer-facing C# classes have been documented to be compatible with\nDoxygen for auto-generating HTML documentation.\n\nTo generate the API reference, download Doxygen and run the following command\nwithin the `docs/` directory:\n\n```\ndoxygen dox-ml-agents.conf\n```\n\n`dox-ml-agents.conf` is a Doxygen configuration file for the ML-Agents Toolkit\nthat includes the classes that have been properly formatted. The generated HTML\nfiles will be placed in the `html/` subdirectory. Open `index.html` within that\nsubdirectory to navigate to the API reference home. Note that `html/` is already\nincluded in the repository's `.gitignore` file.\n\nIn the near future, we aim to expand our documentation to include the Python\nclasses."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 0]"
    },
    {
      "title": "Background: Machine Learning",
      "description": null,
      "content": {
        "Unsupervised Learning": "The goal of\n[unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is\nto group or cluster similar items in a data set. For example, consider the\nplayers of a game. We may want to group the players depending on how engaged\nthey are with the game. This would enable us to target different groups (e.g.\nfor highly-engaged players we might invite them to be beta testers for new\nfeatures, while for unengaged players we might email them helpful tutorials).\nSay that we wish to split our players into two groups. We would first define\nbasic attributes of the players, such as the number of hours played, total money\nspent on in-app purchases and number of levels completed. We can then feed this\ndata set (three attributes for every player) to an unsupervised learning\nalgorithm where we specify the number of groups to be two. The algorithm would\nthen split the data set of players into two groups where the players within each\ngroup would be similar to each other. Given the attributes we used to describe\neach player, in this case, the output would be a split of all the players into\ntwo groups, where one group would semantically represent the engaged players and\nthe second group would semantically represent the unengaged players.\n\nWith unsupervised learning, we did not provide specific examples of which\nplayers are considered engaged and which are considered unengaged. We just\ndefined the appropriate attributes and relied on the algorithm to uncover the\ntwo groups on its own. This type of data set is typically called an unlabeled\ndata set as it is lacking these direct labels. Consequently, unsupervised\nlearning can be helpful in situations where these labels can be expensive or\nhard to produce. In the next paragraph, we overview supervised learning\nalgorithms which accept input labels in addition to attributes.",
        "Supervised Learning": "In [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), we\ndo not want to just group similar items but directly learn a mapping from each\nitem to the group (or class) that it belongs to. Returning to our earlier\nexample of clustering players, let's say we now wish to predict which of our\nplayers are about to churn (that is stop playing the game for the next 30 days).\nWe can look into our historical records and create a data set that contains\nattributes of our players in addition to a label indicating whether they have\nchurned or not. Note that the player attributes we use for this churn prediction\ntask may be different from the ones we used for our earlier clustering task. We\ncan then feed this data set (attributes **and** label for each player) into a\nsupervised learning algorithm which would learn a mapping from the player\nattributes to a label indicating whether that player will churn or not. The\nintuition is that the supervised learning algorithm will learn which values of\nthese attributes typically correspond to players who have churned and not\nchurned (for example, it may learn that players who spend very little and play\nfor very short periods will most likely churn). Now given this learned model, we\ncan provide it the attributes of a new player (one that recently started playing\nthe game) and it would output a _predicted_ label for that player. This\nprediction is the algorithms expectation of whether the player will churn or\nnot. We can now use these predictions to target the players who are expected to\nchurn and entice them to continue playing the game.\n\nAs you may have noticed, for both supervised and unsupervised learning, there\nare two tasks that need to be performed: attribute selection and model\nselection. Attribute selection (also called feature selection) pertains to\nselecting how we wish to represent the entity of interest, in this case, the\nplayer. Model selection, on the other hand, pertains to selecting the algorithm\n(and its parameters) that perform the task well. Both of these tasks are active\nareas of machine learning research and, in practice, require several iterations\nto achieve good performance.\n\nWe now switch to reinforcement learning, the third class of machine learning\nalgorithms, and arguably the one most relevant for the ML-Agents Toolkit.",
        "Reinforcement Learning": "[Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning)\ncan be viewed as a form of learning for sequential decision making that is\ncommonly associated with controlling robots (but is, in fact, much more\ngeneral). Consider an autonomous firefighting robot that is tasked with\nnavigating into an area, finding the fire and neutralizing it. At any given\nmoment, the robot perceives the environment through its sensors (e.g. camera,\nheat, touch), processes this information and produces an action (e.g. move to\nthe left, rotate the water hose, turn on the water). In other words, it is\ncontinuously making decisions about how to interact in this environment given\nits view of the world (i.e. sensors input) and objective (i.e. neutralizing the\nfire). Teaching a robot to be a successful firefighting machine is precisely\nwhat reinforcement learning is designed to do.\n\nMore specifically, the goal of reinforcement learning is to learn a **policy**,\nwhich is essentially a mapping from **observations** to **actions**. An\nobservation is what the robot can measure from its **environment** (in this\ncase, all its sensory inputs) and an action, in its most raw form, is a change\nto the configuration of the robot (e.g. position of its base, position of its\nwater hose and whether the hose is on or off).\n\nThe last remaining piece of the reinforcement learning task is the **reward\nsignal**. The robot is trained to learn a policy that maximizes its overall rewards. When training a robot to be a mean firefighting machine, we provide it\nwith rewards (positive and negative) indicating how well it is doing on\ncompleting the task. Note that the robot does not _know_ how to put out fires\nbefore it is trained. It learns the objective because it receives a large\npositive reward when it puts out the fire and a small negative reward for every\npassing second. The fact that rewards are sparse (i.e. may not be provided at\nevery step, but only when a robot arrives at a success or failure situation), is\na defining characteristic of reinforcement learning and precisely why learning\ngood policies can be difficult (and/or time-consuming) for complex environments.\n\n<div style=\"text-align: center\"><img src=\"../images/rl_cycle.png\" alt=\"The reinforcement learning lifecycle.\"></div>\n\n[Learning a policy](https://blogs.unity3d.com/2017/08/22/unity-ai-reinforcement-learning-with-q-learning/)\nusually requires many trials and iterative policy updates. More specifically,\nthe robot is placed in several fire situations and over time learns an optimal\npolicy which allows it to put out fires more effectively. Obviously, we cannot\nexpect to train a robot repeatedly in the real world, particularly when fires\nare involved. This is precisely why the use of\n[Unity as a simulator](https://blogs.unity3d.com/2018/01/23/designing-safer-cities-through-simulations/)\nserves as the perfect training grounds for learning such behaviors. While our\ndiscussion of reinforcement learning has centered around robots, there are\nstrong parallels between robots and characters in a game. In fact, in many ways,\none can view a non-playable character (NPC) as a virtual robot, with its own\nobservations about the environment, its own set of actions and a specific\nobjective. Thus it is natural to explore how we can train behaviors within Unity\nusing reinforcement learning. This is precisely what the ML-Agents Toolkit\noffers. The video linked below includes a reinforcement learning demo showcasing\ntraining character behaviors using the ML-Agents Toolkit.\n\n<p align=\"center\">\n  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=fiQsmdwEGT8\" target=\"_blank\">\n    <img src=\"http://img.youtube.com/vi/fiQsmdwEGT8/0.jpg\" alt=\"RL Demo\" width=\"400\" border=\"10\" />\n  </a>\n</p>\n\nSimilar to both unsupervised and supervised learning, reinforcement learning\nalso involves two tasks: attribute selection and model selection. Attribute\nselection is defining the set of observations for the robot that best help it\ncomplete its objective, while model selection is defining the form of the policy\n(mapping from observations to actions) and its parameters. In practice, training\nbehaviors is an iterative process that may require changing the attribute and\nmodel choices.",
        "Training and Inference": "One common aspect of all three branches of machine learning is that they all\ninvolve a **training phase** and an **inference phase**. While the details of\nthe training and inference phases are different for each of the three, at a\nhigh-level, the training phase involves building a model using the provided\ndata, while the inference phase involves applying this model to new, previously\nunseen, data. More specifically:\n\n['For our unsupervised learning example, the training phase learns the optimal\\ntwo clusters based on the data describing existing players, while the\\ninference phase assigns a new player to one of these two clusters.', 'For our supervised learning example, the training phase learns the mapping\\nfrom player attributes to player label (whether they churned or not), and the\\ninference phase predicts whether a new player will churn or not based on that\\nlearned mapping.', 'For our reinforcement learning example, the training phase learns the optimal\\npolicy through guided trials, and in the inference phase, the agent observes\\nand takes actions in the wild using its learned policy.']\n\nTo briefly summarize: all three classes of algorithms involve training and\ninference phases in addition to attribute and model selections. What ultimately\nseparates them is the type of data available to learn from. In unsupervised\nlearning our data set was a collection of attributes, in supervised learning our\ndata set was a collection of attribute-label pairs, and, lastly, in\nreinforcement learning our data set was a collection of\nobservation-action-reward tuples.",
        "Deep Learning": "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is a family of\nalgorithms that can be used to address any of the problems introduced above.\nMore specifically, they can be used to solve both attribute and model selection\ntasks. Deep learning has gained popularity in recent years due to its\noutstanding performance on several challenging machine learning tasks. One\nexample is [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo), a\n[computer Go](https://en.wikipedia.org/wiki/Computer_Go) program, that leverages\ndeep learning, that was able to beat Lee Sedol (a Go world champion).\n\nA key characteristic of deep learning algorithms is their ability to learn very\ncomplex functions from large amounts of training data. This makes them a natural\nchoice for reinforcement learning tasks when a large amount of data can be\ngenerated, say through the use of a simulator or engine such as Unity. By\ngenerating hundreds of thousands of simulations of the environment within Unity,\nwe can learn policies for very complex environments (a complex environment is\none where the number of observations an agent perceives and the number of\nactions they can take are large). Many of the algorithms we provide in ML-Agents\nuse some form of deep learning, built on top of the open-source library,\n[PyTorch](Background-PyTorch.md)."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 1]"
    },
    {
      "title": "Background: PyTorch",
      "description": null,
      "content": {
        "PyTorch": "[PyTorch](https://pytorch.org/) is an open source library for\nperforming computations using data flow graphs, the underlying representation of\ndeep learning models. It facilitates training and inference on CPUs and GPUs in\na desktop, server, or mobile device. Within the ML-Agents Toolkit, when you\ntrain the behavior of an agent, the output is a model (.onnx) file that you can\nthen associate with an Agent. Unless you implement a new algorithm, the use of\nPyTorch is mostly abstracted away and behind the scenes.",
        "TensorBoard": "One component of training models with PyTorch is setting the values of\ncertain model attributes (called _hyperparameters_). Finding the right values of\nthese hyperparameters can require a few iterations. Consequently, we leverage a\nvisualization tool called\n[TensorBoard](https://www.tensorflow.org/tensorboard).\nIt allows the visualization of certain agent attributes (e.g. reward) throughout\ntraining which can be helpful in both building intuitions for the different\nhyperparameters and setting the optimal values for your Unity environment. We\nprovide more details on setting the hyperparameters in the\n[Training ML-Agents](Training-ML-Agents.md) page. If you are unfamiliar with\nTensorBoard we recommend our guide on\n[using TensorBoard with ML-Agents](Using-Tensorboard.md) or this\n[tutorial](https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial)."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 2]"
    },
    {
      "title": "Background Unity",
      "description": null,
      "content": {
        "Background: Unity": "If you are not familiar with the [Unity Engine](https://unity3d.com/unity), we\nhighly recommend the [Unity Manual](https://docs.unity3d.com/Manual/index.html)\nand [Tutorials page](https://unity3d.com/learn/tutorials). The\n[Roll-a-ball tutorial](https://learn.unity.com/project/roll-a-ball)\nis a fantastic resource to learn all the basic concepts of Unity to get started\nwith the ML-Agents Toolkit:\n\n['[Editor](https://docs.unity3d.com/Manual/UsingTheEditor.html)', '[Scene](https://docs.unity3d.com/Manual/CreatingScenes.html)', '[GameObject](https://docs.unity3d.com/Manual/GameObjects.html)', '[Rigidbody](https://docs.unity3d.com/ScriptReference/Rigidbody.html)', '[Camera](https://docs.unity3d.com/Manual/Cameras.html)', '[Scripting](https://docs.unity3d.com/Manual/ScriptingSection.html)', '[Physics](https://docs.unity3d.com/Manual/PhysicsSection.html)', '[Ordering of event functions](https://docs.unity3d.com/Manual/ExecutionOrder.html)\\n(e.g. FixedUpdate, Update)', '[Prefabs](https://docs.unity3d.com/Manual/Prefabs.html)']"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 3]"
    },
    {
      "title": "Code Of Conduct",
      "description": null,
      "content": {
        "root": [
          "{!../CODE_OF_CONDUCT.md!}"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 4]"
    },
    {
      "title": "How to Contribute to ML-Agents",
      "description": null,
      "content": {
        "1.Fork the repository": "Fork the ML-Agents repository by clicking on the \"Fork\" button in the top right corner of the GitHub page. This creates a copy of the repository under your GitHub account.",
        "2. Set up your development environment": "Clone the forked repository to your local machine using Git. Install the necessary dependencies and follow the instructions provided in the project's documentation to set up your development environment properly.",
        "3. Choose an issue or feature": "Browse the project's issue tracker or discussions to find an open issue or feature that you would like to contribute to. Read the guidelines and comments associated with the issue to understand the requirements and constraints.",
        "4. Make your changes": "Create a new branch for your changes based on the main branch of the ML-Agents repository. Implement your code changes or add new features as necessary. Ensure that your code follows the project's coding style and conventions.\n\n[\"Example: Let's say you want to add support for a new type of reward function in the ML-Agents framework. You can create a new branch named feature/reward-function to implement this feature.\"]",
        "5. Test your changes": "Run the appropriate tests to ensure your changes work as intended. If necessary, add new tests to cover your code and verify that it doesn't introduce regressions.\n\n['Example: For the reward function feature, you would write tests to check different scenarios and expected outcomes of the new reward function.']",
        "6. Submit a pull request": "Push your branch to your forked repository and submit a pull request (PR) to the ML-Agents main repository. Provide a clear and concise description of your changes, explaining the problem you solved or the feature you added.\n\n['Example: In the pull request description, you would explain how the new reward function works, its benefits, and any relevant implementation details.']",
        "7. Respond to feedback": "Be responsive to any feedback or comments provided by the project maintainers. Address the feedback by making necessary revisions to your code and continue the discussion if required.",
        "8. Continuous integration and code review": "The ML-Agents project utilizes automated continuous integration (CI) systems to run tests on pull requests. Address any issues flagged by the CI system and actively participate in the code review process by addressing comments from reviewers.",
        "9. Merge your changes": "Once your pull request has been approved and meets all the project's requirements, a project maintainer will merge your changes into the main repository. Congratulations, your contribution has been successfully integrated!\n\n**Remember to always adhere to the project's code of conduct, be respectful, and follow any specific contribution guidelines provided by the ML-Agents project. Happy contributing!**"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 5]"
    },
    {
      "title": "Custom Side Channels",
      "description": null,
      "content": {
        "Overview": {
          "Unity side": "The side channel will have to implement the `SideChannel` abstract class and the\nfollowing method.\n\n['`OnMessageReceived(IncomingMessage msg)` : You must implement this method and\\nread the data from IncomingMessage. The data must be read in the order that it\\nwas written.']\n\nThe side channel must also assign a `ChannelId` property in the constructor. The\n`ChannelId` is a Guid (or UUID in Python) used to uniquely identify a side\nchannel. This Guid must be the same on C# and Python. There can only be one side\nchannel of a certain id during communication.\n\nTo send data from C# to Python, create an `OutgoingMessage` instance, add data\nto it, call the `base.QueueMessageToSend(msg)` method inside the side channel,\nand call the `OutgoingMessage.Dispose()` method.\n\nTo register a side channel on the Unity side, call\n`SideChannelManager.RegisterSideChannel` with the side channel as only argument.",
          "Python side": "The side channel will have to implement the `SideChannel` abstract class. You\nmust implement :\n\n['`on_message_received(self, msg: \"IncomingMessage\") -> None` : You must\\nimplement this method and read the data from IncomingMessage. The data must be\\nread in the order that it was written.']\n\nThe side channel must also assign a `channel_id` property in the constructor.\nThe `channel_id` is a UUID (referred in C# as Guid) used to uniquely identify a\nside channel. This number must be the same on C# and Python. There can only be\none side channel of a certain id during communication.\n\nTo assign the `channel_id` call the abstract class constructor with the\nappropriate `channel_id` as follows:\n\n```\nsuper().__init__(my_channel_id)\n```\n\nTo send a byte array from Python to C#, create an `OutgoingMessage` instance,\nadd data to it, and call the `super().queue_message_to_send(msg)` method inside\nthe side channel.\n\nTo register a side channel on the Python side, pass the side channel as argument\nwhen creating the `UnityEnvironment` object. One of the arguments of the\nconstructor (`side_channels`) is a list of side channels."
        },
        "Example implementation": {
          "Example Unity C# code": "The first step is to create the `StringLogSideChannel` class within the Unity\nproject. Here is an implementation of a `StringLogSideChannel` that will listen\nfor messages from python and print them to the Unity debug log, as well as send\nerror messages from Unity to python.\n\n```\nusing UnityEngine;\nusing Unity.MLAgents;\nusing Unity.MLAgents.SideChannels;\nusing System.Text;\nusing System;\n\npublic class StringLogSideChannel : SideChannel\n{\n    public StringLogSideChannel()\n    {\n        ChannelId = new Guid(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\");\n    }\n\n    protected override void OnMessageReceived(IncomingMessage msg)\n    {\n        var receivedString = msg.ReadString();\n        Debug.Log(\"From Python : \" + receivedString);\n    }\n\n    public void SendDebugStatementToPython(string logString, string stackTrace, LogType type)\n    {\n        if (type == LogType.Error)\n        {\n            var stringToSend = type.ToString() + \": \" + logString + \"\\n\" + stackTrace;\n            using (var msgOut = new OutgoingMessage())\n            {\n                msgOut.WriteString(stringToSend);\n                QueueMessageToSend(msgOut);\n            }\n        }\n    }\n}\n```\n\nOnce we have defined our custom side channel class, we need to ensure that it is\ninstantiated and registered. This can typically be done wherever the logic of\nthe side channel makes sense to be associated, for example on a MonoBehaviour\nobject that might need to access data from the side channel. Here we show a\nsimple MonoBehaviour object which instantiates and registers the new side\nchannel. If you have not done it already, make sure that the MonoBehaviour which\nregisters the side channel is attached to a GameObject which will be live in\nyour Unity scene.\n\n```\nusing UnityEngine;\nusing Unity.MLAgents;\n\n\npublic class RegisterStringLogSideChannel : MonoBehaviour\n{\n\n    StringLogSideChannel stringChannel;\n    public void Awake()\n    {\n        // We create the Side Channel\n        stringChannel = new StringLogSideChannel();\n\n        // When a Debug.Log message is created, we send it to the stringChannel\n        Application.logMessageReceived += stringChannel.SendDebugStatementToPython;\n\n        // The channel must be registered with the SideChannelManager class\n        SideChannelManager.RegisterSideChannel(stringChannel);\n    }\n\n    public void OnDestroy()\n    {\n        // De-register the Debug.Log callback\n        Application.logMessageReceived -= stringChannel.SendDebugStatementToPython;\n        if (Academy.IsInitialized){\n            SideChannelManager.UnregisterSideChannel(stringChannel);\n        }\n    }\n\n    public void Update()\n    {\n        // Optional : If the space bar is pressed, raise an error !\n        if (Input.GetKeyDown(KeyCode.Space))\n        {\n            Debug.LogError(\"This is a fake error. Space bar was pressed in Unity.\");\n        }\n    }\n}\n```",
          "Example Python code": "Now that we have created the necessary Unity C# classes, we can create their\nPython counterparts.\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.side_channel.side_channel import (\n    SideChannel,\n    IncomingMessage,\n    OutgoingMessage,\n)\nimport numpy as np\nimport uuid\n\n\n# Create the StringLogChannel class\nclass StringLogChannel(SideChannel):\n\n    def __init__(self) -> None:\n        super().__init__(uuid.UUID(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\"))\n\n    def on_message_received(self, msg: IncomingMessage) -> None:\n        \"\"\"\n        Note: We must implement this method of the SideChannel interface to\n        receive messages from Unity\n        \"\"\"\n        # We simply read a string from the message and print it.\n        print(msg.read_string())\n\n    def send_string(self, data: str) -> None:\n        # Add the string to an OutgoingMessage\n        msg = OutgoingMessage()\n        msg.write_string(data)\n        # We call this method to queue the data we want to send\n        super().queue_message_to_send(msg)\n```\n\nWe can then instantiate the new side channel, launch a `UnityEnvironment` with\nthat side channel active, and send a series of messages to the Unity environment\nfrom Python using it.\n\n```\n# Create the channel\nstring_log = StringLogChannel()\n\n# We start the communication with the Unity Editor and pass the string_log side channel as input\nenv = UnityEnvironment(side_channels=[string_log])\nenv.reset()\nstring_log.send_string(\"The environment was reset\")\n\ngroup_name = list(env.behavior_specs.keys())[0]  # Get the first group_name\ngroup_spec = env.behavior_specs[group_name]\nfor i in range(1000):\n    decision_steps, terminal_steps = env.get_steps(group_name)\n    # We send data to Unity : A string with the number of Agent at each\n    string_log.send_string(\n        f\"Step {i} occurred with {len(decision_steps)} deciding agents and \"\n        f\"{len(terminal_steps)} terminal agents\"\n    )\n    env.step()  # Move the simulation forward\n\nenv.close()\n```\n\nNow, if you run this script and press `Play` the Unity Editor when prompted, the\nconsole in the Unity Editor will display a message at every Python step.\nAdditionally, if you press the Space Bar in the Unity Engine, a message will\nappear in the terminal."
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 6]"
    },
    {
      "title": "ELO Rating System",
      "description": null,
      "content": {
        "What is a zero-sum game?": "A zero-sum game is a game where **each player's gain or loss of utility is exactly balanced by the gain or loss of the utility of the opponent**.\n\nSimply explained, we face a zero-sum game **when one agent gets +1.0, its opponent gets -1.0 reward**.\n\nFor instance, Tennis is a zero-sum game: if you win the point you get +1.0 and your opponent gets -1.0 reward.",
        "How works the ELO Rating System": {
          "The Tennis example": [
            "We start to train our agents.",
            "Both of them have the same skills. So ELO score for each of them that we defined using parameter `initial_elo = 1200.0`."
          ]
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 7]"
    },
    {
      "title": "Frequently Asked Questions",
      "description": null,
      "content": {
        "Installation problems": "",
        "Environment Permission Error": "If you directly import your Unity environment without building it in the editor,\nyou might need to give it additional permissions to execute it.\n\nIf you receive such a permission error on macOS, run:\n\n```\nchmod -R 755 *.app\n```\n\nor on Linux:\n\n```\nchmod -R 755 *.x86_64\n```\n\nOn Windows, you can find\n[instructions](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/cc754344(v=ws.11)).",
        "Environment Connection Timeout": "If you are able to launch the environment from `UnityEnvironment` but then\nreceive a timeout error like this:\n\n```\nUnityAgentsException: The Communicator was unable to connect. Please make sure the External process is ready to accept communication with Unity.\n```\n\nThere may be a number of possible causes:\n\n['_Cause_: There may be no agent in the scene', '_Cause_: On OSX, the firewall may be preventing communication with the\\nenvironment. _Solution_: Add the built environment binary to the list of\\nexceptions on the firewall by following\\n[instructions](https://support.apple.com/en-us/HT201642).', '_Cause_: An error happened in the Unity Environment preventing communication.\\n_Solution_: Look into the\\n[log files](https://docs.unity3d.com/Manual/LogFiles.html) generated by the\\nUnity Environment to figure what error happened.', '_Cause_: You have assigned `HTTP_PROXY` and `HTTPS_PROXY` values in your\\nenvironment variables. _Solution_: Remove these values and try again.', '_Cause_: You are running in a headless environment (e.g. remotely connected\\nto a server). _Solution_: Pass `--no-graphics` to `mlagents-learn`, or\\n`no_graphics=True` to `RemoteRegistryEntry.make()` or the `UnityEnvironment`\\ninitializer. If you need graphics for visual observations, you will need to\\nset up `xvfb` (or equivalent).']",
        "Communication port {} still in use": "If you receive an exception\n`\"Couldn't launch new environment because communication port {} is still in use. \"`,\nyou can change the worker number in the Python script when calling\n\n```\nUnityEnvironment(file_name=filename, worker_id=X)\n```",
        "Mean reward : nan": "If you receive a message `Mean reward : nan` when attempting to train a model\nusing PPO, this is due to the episodes of the Learning Environment not\nterminating. In order to address this, set `Max Steps` for the Agents within the\nScene Inspector to a value greater than 0. Alternatively, it is possible to\nmanually set `done` conditions for episodes from within scripts for custom\nepisode-terminating events.",
        "\"File name\" cannot be opened because the developer cannot be verified.": "If you have downloaded the repository using the github website on macOS 10.15 (Catalina)\nor later, you may see this error when attempting to play scenes in the Unity project.\nWorkarounds include installing the package using the Unity Package Manager (this is\nthe officially supported approach - see [here](Installation.md)), or following the\ninstructions [here](https://support.apple.com/en-us/HT202491) to verify the relevant\nfiles on your machine on a file-by-file basis."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 8]"
    },
    {
      "title": "Getting Started Guide",
      "description": null,
      "content": {
        "Installation": "If you haven't already, follow the [installation instructions](Installation.md).\nAfterwards, open the Unity Project that contains all the example environments:\n\n['Open the Package Manager Window by navigating to `Window -> Package Manager`\\nin the menu.', 'Navigate to the ML-Agents Package and click on it.', 'Find the `3D Ball` sample and click `Import`.', 'In the **Project** window, go to the\\n`Assets/ML-Agents/Examples/3DBall/Scenes` folder and open the `3DBall` scene\\nfile.']",
        "Understanding a Unity Environment": {
          "Agent": {
            "Behavior Parameters : Vector Observation Space": "Before making a decision, an agent collects its observation about its state in\nthe world. The vector observation is a vector of floating point numbers which\ncontain relevant information for the agent to make decisions.\n\nThe Behavior Parameters of the 3D Balance Ball example uses a `Space Size` of 8.\nThis means that the feature vector containing the Agent's observations contains\neight elements: the `x` and `z` components of the agent cube's rotation and the\n`x`, `y`, and `z` components of the ball's relative position and velocity.",
            "Behavior Parameters : Actions": "An Agent is given instructions in the form of actions.\nML-Agents Toolkit classifies actions into two types: continuous and discrete.\nThe 3D Balance Ball example is programmed to use continuous actions, which\nare a vector of floating-point numbers that can vary continuously. More specifically,\nit uses a `Space Size` of 2 to control the amount of `x` and `z` rotations to apply to\nitself to keep the ball balanced on its head."
          }
        },
        "Running a pre-trained model": "We include pre-trained models for our agents (`.onnx` files) and we use the\n[Inference Engine](Inference-Engine.md) to run these models inside\nUnity. In this section, we will use the pre-trained model for the 3D Ball\nexample.\n\n['In the **Project** window, go to the\\n`Assets/ML-Agents/Examples/3DBall/Prefabs` folder. Expand `3DBall` and click\\non the `Agent` prefab. You should see the `Agent` prefab in the **Inspector**\\nwindow.', '**Note**: The platforms in the `3DBall` scene were created using the `3DBall`\\nprefab. Instead of updating all 12 platforms individually, you can update the\\n`3DBall` prefab instead.', '![Platform Prefab](images/platform_prefab.png)', 'In the **Project** window, drag the **3DBall** Model located in\\n`Assets/ML-Agents/Examples/3DBall/TFModels` into the `Model` property under\\n`Behavior Parameters (Script)` component in the Agent GameObject\\n**Inspector** window.', '![3dball learning brain](images/3dball_learning_brain.png)', 'You should notice that each `Agent` under each `3DBall` in the **Hierarchy**\\nwindows now contains **3DBall** as `Model` on the `Behavior Parameters`.\\n**Note** : You can modify multiple game objects in a scene by selecting them\\nall at once using the search bar in the Scene Hierarchy.', 'Set the **Inference Device** to use for this model as `CPU`.', 'Click the **Play** button in the Unity Editor and you will see the platforms\\nbalance the balls using the pre-trained model.']",
        "Training a new model with Reinforcement Learning": {
          "Training the environment": [
            "Open a command or terminal window.",
            "Navigate to the folder where you cloned the `ml-agents` repository. **Note**:\nIf you followed the default [installation](Installation.md), then you should\nbe able to run `mlagents-learn` from any directory.",
            "Run `mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun`.",
            [
              "`config/ppo/3DBall.yaml` is the path to a default training\nconfiguration file that we provide. The `config/ppo` folder includes training configuration\nfiles for all our example environments, including 3DBall.",
              "`run-id` is a unique name for this training session."
            ],
            "When the message _\"Start training by pressing the Play button in the Unity\nEditor\"_ is displayed on the screen, you can press the **Play** button in\nUnity to start training in the Editor."
          ],
          "Observing Training Progress": "Once you start training using `mlagents-learn` in the way described in the\nprevious section, the `ml-agents` directory will contain a `results`\ndirectory. In order to observe the training process in more detail, you can use\nTensorBoard. From the command line run:\n\n```\ntensorboard --logdir results\n```\n\nThen navigate to `localhost:6006` in your browser to view the TensorBoard\nsummary statistics as shown below. For the purposes of this section, the most\nimportant statistic is `Environment/Cumulative Reward` which should increase\nthroughout training, eventually converging close to `100` which is the maximum\nreward the agent can accumulate.\n\n![Example TensorBoard Run](images/mlagents-TensorBoard.png)"
        },
        "Embedding the model into the Unity Environment": "Once the training process completes, and the training process saves the model\n(denoted by the `Saved Model` message) you can add it to the Unity project and\nuse it with compatible Agents (the Agents that generated the model). **Note:**\nDo not just close the Unity Window once the `Saved Model` message appears.\nEither wait for the training process to close the window or press `Ctrl+C` at\nthe command-line prompt. If you close the window manually, the `.onnx` file\ncontaining the trained model is not exported into the ml-agents folder.\n\nIf you've quit the training early using `Ctrl+C` and want to resume training,\nrun the same command again, appending the `--resume` flag:\n\n```\nmlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun --resume\n```\n\nYour trained model will be at `results/<run-identifier>/<behavior_name>.onnx` where\n`<behavior_name>` is the name of the `Behavior Name` of the agents corresponding\nto the model. This file corresponds to your model's latest checkpoint. You can\nnow embed this trained model into your Agents by following the steps below,\nwhich is similar to the steps described [above](#running-a-pre-trained-model).\n\n['Move your model file into\\n`Project/Assets/ML-Agents/Examples/3DBall/TFModels/`.', 'Open the Unity Editor, and select the **3DBall** scene as described above.', 'Select the **3DBall** prefab Agent object.', 'Drag the `<behavior_name>.onnx` file from the Project window of the Editor to\\nthe **Model** placeholder in the **Ball3DAgent** inspector window.', 'Press the **Play** button at the top of the Editor.']",
        "Next Steps": [
          "For more information on the ML-Agents Toolkit, in addition to helpful\nbackground, check out the [ML-Agents Toolkit Overview](ML-Agents-Overview.md)\npage.",
          "For a \"Hello World\" introduction to creating your own Learning Environment,\ncheck out the\n[Making a New Learning Environment](Learning-Environment-Create-New.md) page.",
          "For an overview on the more complex example environments that are provided in\nthis toolkit, check out the\n[Example Environments](Learning-Environment-Examples.md) page.",
          "For more information on the various training options available, check out the\n[Training ML-Agents](Training-ML-Agents.md) page."
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 9]"
    },
    {
      "title": "Glossary",
      "description": null,
      "content": {
        "ML-Agents Toolkit Glossary": [
          "**Academy** - Singleton object which controls timing, reset, and\ntraining/inference settings of the environment.",
          "**Action** - The carrying-out of a decision on the part of an agent within the\nenvironment.",
          "**Agent** - Unity Component which produces observations and takes actions in\nthe environment. Agents actions are determined by decisions produced by a\nPolicy.",
          "**Decision** - The specification produced by a Policy for an action to be\ncarried out given an observation.",
          "**Editor** - The Unity Editor, which may include any pane (e.g. Hierarchy,\nScene, Inspector).",
          "**Environment** - The Unity scene which contains Agents.",
          "**Experience** - Corresponds to a tuple of [Agent observations, actions,\nrewards] of a single Agent obtained after a Step.",
          "**External Coordinator** - ML-Agents class responsible for communication with\noutside processes (in this case, the Python API).",
          "**FixedUpdate** - Unity method called each time the game engine is stepped.\nML-Agents logic should be placed here.",
          "**Frame** - An instance of rendering the main camera for the display.\nCorresponds to each `Update` call of the game engine.",
          "**Observation** - Partial information describing the state of the environment\navailable to a given agent. (e.g. Vector, Visual)",
          "**Policy** - The decision making mechanism for producing decisions from\nobservations, typically a neural network model.",
          "**Reward** - Signal provided at every step used to indicate desirability of an\nagent’s action within the current state of the environment.",
          "**State** - The underlying properties of the environment (including all agents\nwithin it) at a given time.",
          "**Step** - Corresponds to an atomic change of the engine that happens between\nAgent decisions.",
          "**Trainer** - Python class which is responsible for training a given group of\nAgents.",
          "**Update** - Unity function called each time a frame is rendered. ML-Agents\nlogic should not be placed here."
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 10]"
    },
    {
      "title": "The Hugging Face Integration",
      "description": null,
      "content": {
        "Download a model from the Hub": "You can simply download a model from the Hub using `mlagents-load-from-hf`.\n\nYou need to define two parameters:\n\n['`--repo-id`: the name of the Hugging Face repo you want to download.', '`--local-dir`: the path to download the model.']\n\nFor instance, I want to load the model with model-id \"ThomasSimonini/MLAgents-Pyramids\" and put it in the downloads directory:\n\n```\nmlagents-load-from-hf --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --local-dir=\"./downloads\"\n```",
        "Upload a model to the Hub": "You can simply upload a model to the Hub using `mlagents-push-to-hf`\n\nYou need to define four parameters:\n\n['`--run-id`: the name of the training run id.', '`--local-dir`: where the model was saved', '`--repo-id`: the name of the Hugging Face repo you want to create or update. It’s always <your huggingface username>/<the repo name> If the repo does not exist it will be created automatically', '`--commit-message`: since HF repos are git repositories you need to give a commit message.']\n\nFor instance, I want to upload my model trained with run-id \"SnowballTarget1\" to the repo-id: ThomasSimonini/ppo-SnowballTarget:\n\n```\n  mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\"\n```",
        "Visualize an agent playing": "You can watch your agent playing directly in your browser (if the environment is from the [ML-Agents official environments](Learning-Environment-Examples.md))\n\n['Step 1: Go to https://huggingface.co/unity and select the environment demo.', 'Step 2: Find your model_id in the list.', 'Step 3: Select your .nn /.onnx file.', 'Step 4: Click on Watch the agent play']"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 11]"
    },
    {
      "title": "Inference Engine",
      "description": null,
      "content": {
        "Supported devices": "Inference Engine supports [all Unity runtime platforms](https://docs.unity3d.com/Manual/PlatformSpecific.html).\n\nScripting Backends : Inference Engine is generally faster with\n**IL2CPP** than with **Mono** for Standalone builds. In the Editor, It is not\npossible to use Inference Engine with GPU device selected when Editor\nGraphics Emulation is set to **OpenGL(ES) 3.0 or 2.0 emulation**. Also there\nmight be non-fatal build time errors when target platform includes Graphics API\nthat does not support **Unity Compute Shaders**.\n\nIn cases when it is not possible to use compute shaders on the target platform,\ninference can be performed using **CPU** or **GPUPixel** Inference Engine backends.",
        "Using Inference Engine": "When using a model, drag the model file into the **Model** field in the\nInspector of the Agent. Select the **Inference Device**: **Compute Shader**, **Burst** or\n**Pixel Shader** you want to use for inference.\n\n**Note:** For most of the models generated with the ML-Agents Toolkit, CPU inference (**Burst**) will\nbe faster than GPU inference (**Compute Shader** or **Pixel Shader**).\nYou should use GPU inference only if you use the ResNet visual\nencoder or have a large number of agents with visual observations."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 12]"
    },
    {
      "title": "Installing ML-Agents Toolkit for Windows (Deprecated)",
      "description": null,
      "content": {
        "Step 1: Install Python via Anaconda": "[Download](https://www.anaconda.com/download/#windows) and install Anaconda for\nWindows. By using Anaconda, you can manage separate environments for different\ndistributions of Python. Python 3.7.2 or higher is required as we no longer\nsupport Python 2. In this guide, we are using Python version 3.7 and Anaconda\nversion 5.1\n([64-bit](https://repo.continuum.io/archive/Anaconda3-5.1.0-Windows-x86_64.exe)\nor [32-bit](https://repo.continuum.io/archive/Anaconda3-5.1.0-Windows-x86.exe)\ndirect links).\n\n<p align=\"center\">\n  <img src=\"images/anaconda_install.PNG\"\n       alt=\"Anaconda Install\"\n       width=\"500\" border=\"10\" />\n</p>\n\nWe recommend the default _advanced installation options_. However, select the\noptions appropriate for your specific situation.\n\n<p align=\"center\">\n  <img src=\"images/anaconda_default.PNG\" alt=\"Anaconda Install\" width=\"500\" border=\"10\" />\n</p>\n\nAfter installation, you must open **Anaconda Navigator** to finish the setup.\nFrom the Windows search bar, type _anaconda navigator_. You can close Anaconda\nNavigator after it opens.\n\nIf environment variables were not created, you will see error \"conda is not\nrecognized as internal or external command\" when you type `conda` into the\ncommand line. To solve this you will need to set the environment variable\ncorrectly.\n\nType `environment variables` in the search bar (this can be reached by hitting\nthe Windows key or the bottom left Windows button). You should see an option\ncalled **Edit the system environment variables**.\n\n<p align=\"center\">\n  <img src=\"images/edit_env_var.png\"\n       alt=\"edit env variables\"\n       width=\"250\" border=\"10\" />\n</p>\n\nFrom here, click the **Environment Variables** button. Double click \"Path\" under\n**System variable** to edit the \"Path\" variable, click **New** to add the\nfollowing new paths.\n\n```\n%UserProfile%\\Anaconda3\\Scripts\n%UserProfile%\\Anaconda3\\Scripts\\conda.exe\n%UserProfile%\\Anaconda3\n%UserProfile%\\Anaconda3\\python.exe\n```",
        "Step 2: Setup and Activate a New Conda Environment": "You will create a new [Conda environment](https://conda.io/docs/) to be used\nwith the ML-Agents Toolkit. This means that all the packages that you install\nare localized to just this environment. It will not affect any other\ninstallation of Python or other environments. Whenever you want to run\nML-Agents, you will need activate this Conda environment.\n\nTo create a new Conda environment, open a new Anaconda Prompt (_Anaconda Prompt_\nin the search bar) and type in the following command:\n\n```\nconda create -n ml-agents python=3.7\n```\n\nYou may be asked to install new packages. Type `y` and press enter _(make sure\nyou are connected to the Internet)_. You must install these required packages.\nThe new Conda environment is called ml-agents and uses Python version 3.7.\n\n<p align=\"center\">\n  <img src=\"images/conda_new.PNG\" alt=\"Anaconda Install\" width=\"500\" border=\"10\" />\n</p>\n\nTo use this environment, you must activate it. _(To use this environment In the\nfuture, you can run the same command)_. In the same Anaconda Prompt, type in the\nfollowing command:\n\n```\nactivate ml-agents\n```\n\nYou should see `(ml-agents)` prepended on the last line.\n\nNext, install `tensorflow`. Install this package using `pip` - which is a\npackage management system used to install Python packages. Latest versions of\nTensorFlow won't work, so you will need to make sure that you install version\n1.7.1. In the same Anaconda Prompt, type in the following command _(make sure\nyou are connected to the Internet)_:\n\n```\npip install tensorflow==1.7.1\n```",
        "Step 3: Install Required Python Packages": {
          "Installing for Development": "If you intend to make modifications to `ml-agents` or `ml-agents-envs`, you\nshould install the packages from the cloned repo rather than from PyPi. To do\nthis, you will need to install `ml-agents` and `ml-agents-envs` separately.\n\nIn our example, the files are located in `C:\\Downloads`. After you have either\ncloned or downloaded the files, from the Anaconda Prompt, change to the\nml-agents subdirectory inside the ml-agents directory:\n\n```\ncd C:\\Downloads\\ml-agents\n```\n\nFrom the repo's main directory, now run:\n\n```\ncd ml-agents-envs\npip install -e .\ncd ..\ncd ml-agents\npip install -e .\n```\n\nRunning pip with the `-e` flag will let you make changes to the Python files\ndirectly and have those reflected when you run `mlagents-learn`. It is important\nto install these packages in this order as the `mlagents` package depends on\n`mlagents_envs`, and installing it in the other order will download\n`mlagents_envs` from PyPi."
        },
        "(Optional) Step 4: GPU Training using The ML-Agents Toolkit": {
          "Install Nvidia CUDA toolkit": "[Download](https://developer.nvidia.com/cuda-toolkit-archive) and install the\nCUDA toolkit 9.0 from Nvidia's archive. The toolkit includes GPU-accelerated\nlibraries, debugging and optimization tools, a C/C++ (Step Visual Studio 2017)\ncompiler and a runtime library and is needed to run the ML-Agents Toolkit. In\nthis guide, we are using version\n[9.0.176](https://developer.nvidia.com/compute/cuda/9.0/Prod/network_installers/cuda_9.0.176_win10_network-exe)).\n\nBefore installing, please make sure you **close any running instances of Unity\nor Visual Studio**.\n\nRun the installer and select the Express option. Note the directory where you\ninstalled the CUDA toolkit. In this guide, we installed in the directory\n`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0`",
          "Install Nvidia cuDNN library": "[Download](https://developer.nvidia.com/cudnn) and install the cuDNN library\nfrom Nvidia. cuDNN is a GPU-accelerated library of primitives for deep neural\nnetworks. Before you can download, you will need to sign up for free to the\nNvidia Developer Program.\n\n<p align=\"center\">\n  <img src=\"images/cuDNN_membership_required.png\"\n       alt=\"cuDNN membership required\"\n       width=\"500\" border=\"10\" />\n</p>\n\nOnce you've signed up, go back to the cuDNN\n[downloads page](https://developer.nvidia.com/cudnn). You may or may not be\nasked to fill out a short survey. When you get to the list cuDNN releases,\n**make sure you are downloading the right version for the CUDA toolkit you\ninstalled in Step 1.** In this guide, we are using version 7.0.5 for CUDA\ntoolkit version 9.0\n([direct link](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.0.5/prod/9.0_20171129/cudnn-9.0-windows10-x64-v7)).\n\nAfter you have downloaded the cuDNN files, you will need to extract the files\ninto the CUDA toolkit directory. In the cuDNN zip file, there are three folders\ncalled `bin`, `include`, and `lib`.\n\n<p align=\"center\">\n  <img src=\"images/cudnn_zip_files.PNG\"\n       alt=\"cuDNN zip files\"\n       width=\"500\" border=\"10\" />\n</p>\n\nCopy these three folders into the CUDA toolkit directory. The CUDA toolkit\ndirectory is located at\n`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0`\n\n<p align=\"center\">\n  <img src=\"images/cuda_toolkit_directory.PNG\"\n       alt=\"cuda toolkit directory\"\n       width=\"500\" border=\"10\" />\n</p>",
          "Set Environment Variables": "You will need to add one environment variable and two path variables.\n\nTo set the environment variable, type `environment variables` in the search bar\n(this can be reached by hitting the Windows key or the bottom left Windows\nbutton). You should see an option called **Edit the system environment\nvariables**.\n\n<p align=\"center\">\n  <img src=\"images/edit_env_var.png\"\n       alt=\"edit env variables\"\n       width=\"250\" border=\"10\" />\n</p>\n\nFrom here, click the **Environment Variables** button. Click **New** to add a\nnew system variable _(make sure you do this under **System variables** and not\nUser variables_.\n\n<p align=\"center\">\n  <img src=\"images/new_system_variable.PNG\"\n       alt=\"new system variable\"\n       width=\"500\" border=\"10\" />\n</p>\n\nFor **Variable Name**, enter `CUDA_HOME`. For the variable value, put the\ndirectory location for the CUDA toolkit. In this guide, the directory location\nis `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0`. Press **OK** once.\n\n<p align=\"center\">\n  <img src=\"images/system_variable_name_value.PNG\"\n       alt=\"system variable names and values\"\n       width=\"500\" border=\"10\" />\n</p>\n\nTo set the two path variables, inside the same **Environment Variables** window\nand under the second box called **System Variables**, find a variable called\n`Path` and click **Edit**. You will add two directories to the list. For this\nguide, the two entries would look like:\n\n```\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\extras\\CUPTI\\libx64\n```\n\nMake sure to replace the relevant directory location with the one you have\ninstalled. _Please note that case sensitivity matters_.\n\n<p align=\"center\">\n    <img src=\"images/path_variables.PNG\"\n        alt=\"Path variables\"\n        width=\"500\" border=\"10\" />\n</p>",
          "Install TensorFlow GPU": "Next, install `tensorflow-gpu` using `pip`. You'll need version 1.7.1. In an\nAnaconda Prompt with the Conda environment ml-agents activated, type in the\nfollowing command to uninstall TensorFlow for cpu and install TensorFlow for gpu\n_(make sure you are connected to the Internet)_:\n\n```\npip uninstall tensorflow\npip install tensorflow-gpu==1.7.1\n```\n\nLastly, you should test to see if everything installed properly and that\nTensorFlow can identify your GPU. In the same Anaconda Prompt, open Python in\nthe Prompt by calling:\n\n```\npython\n```\n\nAnd then type the following commands:\n\n```\nimport tensorflow as tf\n\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n```\n\nYou should see something similar to:\n\n```\nFound device 0 with properties ...\n```"
        },
        "Acknowledgments": "We would like to thank\n[Jason Weimann](https://unity3d.college/2017/10/25/machine-learning-in-unity3d-setting-up-the-environment-tensorflow-for-agentml-on-windows-10/)\nand\n[Nitish S. Mutha](http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html)\nfor writing the original articles which were used to create this guide."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 13]"
    },
    {
      "title": "Installation",
      "description": null,
      "content": {
        "Next Steps": "The [Getting Started](Getting-Started.md) guide contains several short tutorials\non setting up the ML-Agents Toolkit within Unity, running a pre-trained model,\nin addition to building and training environments.",
        "Help": "If you run into any problems regarding ML-Agents, refer to our [FAQ](FAQ.md) and\nour [Limitations](Limitations.md) pages. If you can't find anything please\n[submit an issue](https://github.com/Unity-Technologies/ml-agents/issues) and\nmake sure to cite relevant information on OS, Python version, and exact error\nmessage (whenever possible)."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 14]"
    },
    {
      "title": "Match-3 with ML-Agents",
      "description": null,
      "content": {
        "Getting started": "The C# code for Match-3 exists inside of the Unity package (`com.unity.ml-agents`).\nThe good first step would be to take a look at how we have implemented the C# code in the example Match-3 scene (located\nunder /Project/Assets/ML-Agents/Examples/match3). Once you have some familiarity, then the next step would be to\nimplement the C# code for Match-3 from the extensions package.\n\nAdditionally, see below for additional technical specifications on the C# code for Match-3. Please note the Match-3 game\nisn't human playable as implemented and can be only played via training.",
        "Technical specifications for Match-3 with ML-Agents": {
          "AbstractBoard class": "The `AbstractBoard` is the bridge between ML-Agents and your game. It allows ML-Agents to\n\n['ask your game what the current and maximum sizes (rows, columns, and potential piece types) of the board are', 'ask your game what the \"color\" of a cell is', 'ask whether the cell is a \"special\" piece type or not', 'ask your game whether a move is allowed', 'request that your game make a move']\n\nThese are handled by implementing the abstract methods of `AbstractBoard`.\n\n`public abstract BoardSize GetMaxBoardSize()`\n\nReturns the largest `BoardSize` that the game can use. This is used to determine the sizes of observations and sensors,\nso don't make it larger than necessary.\n\n`public virtual BoardSize GetCurrentBoardSize()`\n\nReturns the current size of the board. Each field on this BoardSize must be less than or equal to the corresponding\nfield returned by `GetMaxBoardSize()`. This method is optional; if your always use the same size board, you don't\nneed to override it.\n\nIf the current board size is smaller than the maximum board size, `GetCellType()` and `GetSpecialType()` will not be\ncalled for cells outside the current board size, and `IsValidMove` won't be called for moves that would go outside of\nthe current board size.\n\n`public abstract int GetCellType(int row, int col)`\n\nReturns the \"color\" of piece at the given row and column.\nThis should be between 0 and BoardSize.NumCellTypes-1 (inclusive).\nThe actual order of the values doesn't matter.\n\n`public abstract int GetSpecialType(int row, int col)`\n\nReturns the special type of the piece at the given row and column.\nThis should be between 0 and BoardSize.NumSpecialTypes (inclusive).\nThe actual order of the values doesn't matter.\n\n`public abstract bool IsMoveValid(Move m)`\n\nCheck whether the particular `Move` is valid for the game.\nThe actual results will depend on the rules of the game, but we provide the `SimpleIsMoveValid()` method\nthat handles basic match3 rules with no special or immovable pieces.\n\n`public abstract bool MakeMove(Move m)`\n\nInstruct the game to make the given move. Returns true if the move was made.\nNote that during training, a move that was marked as invalid may occasionally still be\nrequested. If this happens, it is safe to do nothing and request another move.",
          "`Move` struct": "The Move struct encapsulates a swap of two adjacent cells. You can get the number of potential moves\nfor a board of a given size with. `Move.NumPotentialMoves(maxBoardSize)`. There are two helper\nfunctions to create a new `Move`:\n\n['`public static Move FromMoveIndex(int moveIndex, BoardSize maxBoardSize)` can be used to\\niterate over all potential moves for the board by looping from 0 to `Move.NumPotentialMoves()`', '`public static Move FromPositionAndDirection(int row, int col, Direction dir, BoardSize maxBoardSize)` creates\\na `Move` from a row, column, and direction (and board size).']",
          "`BoardSize` struct": {
            "`Match3Sensor` and `Match3SensorComponent` classes": "The `Match3Sensor` generates observations about the state using the `AbstractBoard` interface. You can\nchoose whether to use vector or \"visual\" observations; in theory, visual observations should perform\nbetter because they are 2-dimensional like the board, but we need to experiment more on this.\n\nA `Match3SensorComponent` generates `Match3Sensor`s (the exact number of sensors depends on your configuration)\nat runtime, and should be added to the same GameObject as your `Agent` implementation. You do not need to write any\nadditional code to use them.",
            "`Match3Actuator` and `Match3ActuatorComponent` classes": "The `Match3Actuator` converts actions from training or inference into a `Move` that is sent to` AbstractBoard.MakeMove()`\nIt also checks `AbstractBoard.IsMoveValid` for each potential move and uses this to set the action mask for Agent.\n\nA `Match3ActuatorComponent` generates a `Match3Actuator` at runtime, and should be added to the same GameObject\nas your `Agent` implementation.  You do not need to write any additional code to use them."
          },
          "Setting up Match-3 simulation": [
            "Implement the `AbstractBoard` methods to integrate with your game.",
            "Give the `Agent` rewards when it does what you want it to (match multiple pieces in a row, clears pieces of a certain\ntype, etc).",
            "Add the `Agent`, `AbstractBoard` implementation, `Match3SensorComponent`, and `Match3ActuatorComponent` to the same\n`GameObject`.",
            "Call `Agent.RequestDecision()` when you're ready for the `Agent` to make a move on the next `Academy` step. During\nthe next `Academy` step, the `MakeMove()` method on the board will be called."
          ]
        },
        "Implementation Details": {
          "Action Space": "The indexing for actions is the same as described in\n[Human Like Playtesting with Deep Learning](https://www.researchgate.net/publication/328307928_Human-Like_Playtesting_with_Deep_Learning)\n(for example, Figure 2b). The horizontal moves are enumerated first, then the vertical ones.\n<img src=\"images/match3-moves.png\" align=\"center\"/>"
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 15]"
    },
    {
      "title": "Game Integrations",
      "description": null,
      "content": {
        "Match-3": "The [Match-3 integration](Integrations-Match3.md) provides an abstraction of a match-3 game board and moves, along with\na sensor to observe the game state, and an actuator to translate the ML-Agent actions into game moves.",
        "Interested in more game templates?": "Do you have a type of game you are interested for ML-Agents? If so, please post a\n[forum issue](https://forum.unity.com/forums/ml-agents.453/) with `[GAME TEMPLATE]` in the title."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 16]"
    },
    {
      "title": "License",
      "description": null,
      "content": {
        "root": [
          "{!../LICENSE.md!}"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 17]"
    },
    {
      "title": "Making a New Learning Environment",
      "description": null,
      "content": {
        "Overview": "Using the ML-Agents Toolkit in a Unity project involves the following basic\nsteps:\n\n['Create an environment for your agents to live in. An environment can range\\nfrom a simple physical simulation containing a few objects to an entire game\\nor ecosystem.', 'Implement your Agent subclasses. An Agent subclass defines the code an Agent\\nuses to observe its environment, to carry out assigned actions, and to\\ncalculate the rewards used for reinforcement training. You can also implement\\noptional methods to reset the Agent when it has finished or failed its task.', 'Add your Agent subclasses to appropriate GameObjects, typically, the object\\nin the scene that represents the Agent in the simulation.']\n\n**Note:** If you are unfamiliar with Unity, refer to the\n[Unity manual](https://docs.unity3d.com/Manual/index.html)\nif an Editor task isn't explained sufficiently in this tutorial.\n\nIf you haven't already, follow the [installation instructions](Installation.md).",
        "Set Up the Unity Project": "The first task to accomplish is simply creating a new Unity project and\nimporting the ML-Agents assets into it:\n\n['Launch Unity Hub and create a new 3D project named \"RollerBall\".', '[Add the ML-Agents Unity package](Installation.md#install-the-comunityml-agents-unity-package)\\nto your project.']\n\nYour Unity **Project** window should contain the following assets:\n\n![Unity Project Window](images/roller-ball-projects.png){: style=\"width:250px\"}",
        "Create the Environment": {
          "Create the Floor Plane": [
            "Right click in Hierarchy window, select 3D Object > Plane.",
            "Name the GameObject \"Floor\".",
            "Select the Floor Plane to view its properties in the Inspector window.",
            "Set Transform to Position = `(0, 0, 0)`, Rotation = `(0, 0, 0)`, Scale =\n`(1, 1, 1)`."
          ],
          "Add the Target Cube": [
            "Right click in Hierarchy window, select 3D Object > Cube.",
            "Name the GameObject \"Target\".",
            "Select the Target Cube to view its properties in the Inspector window.",
            "Set Transform to Position = `(3, 0.5, 3)`, Rotation = `(0, 0, 0)`, Scale =\n`(1, 1, 1)`."
          ],
          "Add the Agent Sphere": [
            "Right click in Hierarchy window, select 3D Object > Sphere.",
            "Name the GameObject \"RollerAgent\".",
            "Select the RollerAgent Sphere to view its properties in the Inspector window.",
            "Set Transform to Position = `(0, 0.5, 0)`, Rotation = `(0, 0, 0)`, Scale =\n`(1, 1, 1)`.",
            "Click **Add Component**.",
            "Add the `Rigidbody` component to the Sphere."
          ],
          "Group into Training Area": "Group the floor, target and agent under a single, empty, GameObject. This will simplify\nsome of our subsequent steps.\n\nTo do so:\n\n['Right-click on your Project Hierarchy and create a new empty GameObject. Name\\nit TrainingArea.', 'Reset the TrainingArea’s Transform so that it is at `(0,0,0)` with Rotation\\n`(0,0,0)` and Scale `(1,1,1)`.', 'Drag the Floor, Target, and RollerAgent GameObjects in the Hierarchy into the\\nTrainingArea GameObject.']\n\n![Hierarchy window](images/roller-ball-hierarchy.png){: style=\"width:250px\"}"
        },
        "Implement an Agent": {
          "Initialization and Resetting the Agent": "The process of training in the ML-Agents Toolkit involves running episodes where\nthe Agent (Sphere) attempts to solve the task. Each episode lasts until the\nAgents solves the task (i.e. reaches the cube), fails (rolls off the platform)\nor times out (takes too long to solve or fail at the task). At the start of each\nepisode, `OnEpisodeBegin()` is called to set-up the environment for a\nnew episode. Typically the scene is initialized in a random manner to enable the\nagent to learn to solve the task under a variety of conditions.\n\nIn this example, each time the Agent (Sphere) reaches its target (Cube), the\nepisode ends and the target (Cube) is moved to a new random location; and if\nthe Agent rolls off the platform, it will be put back onto the floor.\nThese are all handled in `OnEpisodeBegin()`.\n\nTo move the target (Cube), we need a reference to its Transform (which stores a\nGameObject's position, orientation and scale in the 3D world). To get this\nreference, add a public field of type `Transform` to the RollerAgent class.\nPublic fields of a component in Unity get displayed in the Inspector window,\nallowing you to choose which GameObject to use as the target in the Unity\nEditor.\n\nTo reset the Agent's velocity (and later to apply force to move the agent) we\nneed a reference to the Rigidbody component. A\n[Rigidbody](https://docs.unity3d.com/ScriptReference/Rigidbody.html) is Unity's\nprimary element for physics simulation. (See\n[Physics](https://docs.unity3d.com/Manual/PhysicsSection.html) for full\ndocumentation of Unity physics.) Since the Rigidbody component is on the same\nGameObject as our Agent script, the best way to get this reference is using\n`GameObject.GetComponent<T>()`, which we can call in our script's `Start()`\nmethod.\n\nSo far, our RollerAgent script looks like:\n\n```\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.MLAgents;\nusing Unity.MLAgents.Sensors;\n\npublic class RollerAgent : Agent\n{\n    Rigidbody rBody;\n    void Start () {\n        rBody = GetComponent<Rigidbody>();\n    }\n\n    public Transform Target;\n    public override void OnEpisodeBegin()\n    {\n       // If the Agent fell, zero its momentum\n        if (this.transform.localPosition.y < 0)\n        {\n            this.rBody.angularVelocity = Vector3.zero;\n            this.rBody.velocity = Vector3.zero;\n            this.transform.localPosition = new Vector3( 0, 0.5f, 0);\n        }\n\n        // Move the target to a new spot\n        Target.localPosition = new Vector3(Random.value * 8 - 4,\n                                           0.5f,\n                                           Random.value * 8 - 4);\n    }\n}\n```\n\nNext, let's implement the `Agent.CollectObservations(VectorSensor sensor)`\nmethod.",
          "Observing the Environment": "The Agent sends the information we collect to the Brain, which uses it to make a\ndecision. When you train the Agent (or use a trained model), the data is fed\ninto a neural network as a feature vector. For an Agent to successfully learn a\ntask, we need to provide the correct information. A good rule of thumb for\ndeciding what information to collect is to consider what you would need to\ncalculate an analytical solution to the problem.\n\nIn our case, the information our Agent collects includes the position of the\ntarget, the position of the agent itself, and the velocity of the agent. This\nhelps the Agent learn to control its speed so it doesn't overshoot the target\nand roll off the platform. In total, the agent observation contains 8 values as\nimplemented below:\n\n```\npublic override void CollectObservations(VectorSensor sensor)\n{\n    // Target and Agent positions\n    sensor.AddObservation(Target.localPosition);\n    sensor.AddObservation(this.transform.localPosition);\n\n    // Agent velocity\n    sensor.AddObservation(rBody.velocity.x);\n    sensor.AddObservation(rBody.velocity.z);\n}\n```",
          "Taking Actions and Assigning Rewards": {
            "Actions": "To solve the task of moving towards the target, the Agent (Sphere) needs to be\nable to move in the `x` and `z` directions. As such, the agent needs 2 actions:\nthe first determines the force applied along the x-axis; and the\nsecond determines the force applied along the z-axis. (If we allowed the Agent\nto move in three dimensions, then we would need a third action.)\n\nThe RollerAgent applies the values from the `action[]` array to its Rigidbody\ncomponent `rBody`, using `Rigidbody.AddForce()`:\n\n```\nVector3 controlSignal = Vector3.zero;\ncontrolSignal.x = action[0];\ncontrolSignal.z = action[1];\nrBody.AddForce(controlSignal * forceMultiplier);\n```",
            "Rewards": "Reinforcement learning requires rewards to signal which decisions are good and\nwhich are bad. The learning algorithm uses the rewards to determine whether it\nis giving the Agent the optimal actions. You want to reward an Agent for\ncompleting the assigned task. In this case, the Agent is given a reward of 1.0\nfor reaching the Target cube.\n\nRewards are assigned in `OnActionReceived()`. The RollerAgent\ncalculates the distance to detect when it reaches the target.\nWhen it does, the code calls `Agent.SetReward()` to assign a reward\nof 1.0 and marks the agent as finished by calling `EndEpisode()` on\nthe Agent.\n\n```\nfloat distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);\n// Reached target\nif (distanceToTarget < 1.42f)\n{\n    SetReward(1.0f);\n    EndEpisode();\n}\n```\n\nFinally, if the Agent falls off the platform, end the episode so that it can\nreset itself:\n\n```\n// Fell off platform\nif (this.transform.localPosition.y < 0)\n{\n    EndEpisode();\n}\n```",
            "OnActionReceived()": "With the action and reward logic outlined above, the final version of\n`OnActionReceived()` looks like:\n\n```\npublic float forceMultiplier = 10;\npublic override void OnActionReceived(ActionBuffers actionBuffers)\n{\n    // Actions, size = 2\n    Vector3 controlSignal = Vector3.zero;\n    controlSignal.x = actionBuffers.ContinuousActions[0];\n    controlSignal.z = actionBuffers.ContinuousActions[1];\n    rBody.AddForce(controlSignal * forceMultiplier);\n\n    // Rewards\n    float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);\n\n    // Reached target\n    if (distanceToTarget < 1.42f)\n    {\n        SetReward(1.0f);\n        EndEpisode();\n    }\n\n    // Fell off platform\n    else if (this.transform.localPosition.y < 0)\n    {\n        EndEpisode();\n    }\n}\n```\n\nNote the `forceMultiplier` class variable is defined before the method definition.\nSince `forceMultiplier` is public, you can set the value from the Inspector window."
          }
        },
        "Final Agent Setup in Editor": "Now that all the GameObjects and ML-Agent components are in place, it is time\nto connect everything together in the Unity Editor. This involves adding and\nsetting some of the Agent Component's properties so that they are compatible\nwith our Agent script.\n\n['Select the **RollerAgent** GameObject to show its properties in the Inspector\\nwindow.', 'Drag the Target GameObject in the Hierarchy into the `Target` field in RollerAgent Script.', 'Add a `Decision Requester` script with the **Add Component** button.\\nSet the **Decision Period** to `10`. For more information on decisions,\\nsee [the Agent documentation](Learning-Environment-Design-Agents.md#decisions)', 'Add a `Behavior Parameters` script with the **Add Component** button.\\nSet the Behavior Parameters of the Agent to the following:', ['`Behavior Name`: _RollerBall_', '`Vector Observation` > `Space Size` = 8', '`Actions` > `Continuous Actions` = 2']]\n\nIn the inspector, the `RollerAgent` should look like this now:\n\n![Agent GameObject Inspector window](images/roller-ball-agent.png){: style=\"width:400px\"}\n\nNow you are ready to test the environment before training.",
        "Testing the Environment": "It is always a good idea to first test your environment by controlling the Agent\nusing the keyboard. To do so, you will need to extend the `Heuristic()` method\nin the `RollerAgent` class. For our example, the heuristic will generate an\naction corresponding to the values of the \"Horizontal\" and \"Vertical\" input axis\n(which correspond to the keyboard arrow keys):\n\n```\npublic override void Heuristic(in ActionBuffers actionsOut)\n{\n    var continuousActionsOut = actionsOut.ContinuousActions;\n    continuousActionsOut[0] = Input.GetAxis(\"Horizontal\");\n    continuousActionsOut[1] = Input.GetAxis(\"Vertical\");\n}\n```\n\nIn order for the Agent to use the Heuristic, You will need to set the\n`Behavior Type` to `Heuristic Only` in the `Behavior Parameters` of the\nRollerAgent.\n\nPress **Play** to run the scene and use the arrows keys to move the Agent around\nthe platform. Make sure that there are no errors displayed in the Unity Editor\nConsole window and that the Agent resets when it reaches its target or falls\nfrom the platform.",
        "Training the Environment": "The process is the same as described in the\n[Getting Started Guide](Getting-Started.md).\n\nThe hyperparameters for training are specified in a configuration file that you\npass to the `mlagents-learn` program. Create a new `rollerball_config.yaml` file\nunder `config/` and include the following hyperparameter values:\n\n```\nbehaviors:\n  RollerBall:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 10\n      buffer_size: 100\n      learning_rate: 3.0e-4\n      beta: 5.0e-4\n      epsilon: 0.2\n      lambd: 0.99\n      num_epoch: 3\n      learning_rate_schedule: linear\n      beta_schedule: constant\n      epsilon_schedule: linear\n    network_settings:\n      normalize: false\n      hidden_units: 128\n      num_layers: 2\n    reward_signals:\n      extrinsic:\n        gamma: 0.99\n        strength: 1.0\n    max_steps: 500000\n    time_horizon: 64\n    summary_freq: 10000\n```\n\nHyperparameters are explained in [the training configuration file documentation](Training-Configuration-File.md)\n\nSince this example creates a very simple training environment with only a few\ninputs and outputs, using small batch and buffer sizes speeds up the training\nconsiderably. However, if you add more complexity to the environment or change\nthe reward or observation functions, you might also find that training performs\nbetter with different hyperparameter values. In addition to setting these\nhyperparameter values, the Agent **DecisionFrequency** parameter has a large\neffect on training time and success. A larger value reduces the number of\ndecisions the training algorithm has to consider and, in this simple\nenvironment, speeds up training.\n\nTo train your agent, run the following command before pressing **Play** in the\nEditor:\n\nmlagents-learn config/rollerball_config.yaml --run-id=RollerBall\n\n\nTo monitor the statistics of Agent performance during training, use\n[TensorBoard](Using-Tensorboard.md).\n\n![TensorBoard statistics display](images/mlagents-RollerAgentStats.png)\n\nIn particular, the _cumulative_reward_ and _value_estimate_ statistics show how\nwell the Agent is achieving the task. In this example, the maximum reward an\nAgent can earn is 1.0, so these statistics approach that value when the Agent\nhas successfully _solved_ the problem.",
        "Optional: Multiple Training Areas within the Same Scene": "In many of the [example environments](Learning-Environment-Examples.md), many\ncopies of the training area are instantiated in the scene. This generally speeds\nup training, allowing the environment to gather many experiences in parallel.\nThis can be achieved simply by instantiating many Agents with the same\n`Behavior Name`. Note that we've already simplified our transition to using\nmultiple areas by creating the `TrainingArea` GameObject and relying on local\npositions in `RollerAgent.cs`. Use the following steps to parallelize your\nRollerBall environment:\n\n['Drag the TrainingArea GameObject, along with its attached GameObjects, into\\nyour Assets browser, turning it into a prefab.', 'You can now instantiate copies of the TrainingArea prefab. Drag them into\\nyour scene, positioning them so that they do not overlap.']\n\nAlternatively, you can use the `TrainingAreaReplicator` to replicate training areas. Use the following steps:\n\n['Create a new empty Game Object in the scene.', 'Click on the new object and add a TrainingAreaReplicator component to the empty Game Object through the inspector.', 'Drag the training area to `Base Area` in the Training Area Replicator.', 'Specify the number of areas to replicate and the separation between areas.', 'Hit play and the areas will be replicated automatically!']",
        "Optional: Training Using Concurrent Unity Instances": "Another level of parallelization comes by training using\n[concurrent Unity instances](ML-Agents-Overview.md#additional-features).\nFor example,\n\n```\nmlagents-learn config/rollerball_config.yaml --run-id=RollerBall --num-envs=2\n```\n\nwill start ML Agents training with two environment instances. Combining multiple\ntraining areas within the same scene, with concurrent Unity instances, effectively\ngives you two levels of parallelism to speed up training. The command line option\n`--num-envs=<n>` controls the number of concurrent Unity instances that are\nexecuted in parallel during training."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 18]"
    },
    {
      "title": "Agents",
      "description": null,
      "content": {
        "Decisions": "The observation-decision-action-reward cycle repeats each time the Agent request\na decision. Agents will request a decision when `Agent.RequestDecision()` is\ncalled. If you need the Agent to request decisions on its own at regular\nintervals, add a `Decision Requester` component to the Agent's GameObject.\nMaking decisions at regular step intervals is generally most appropriate for\nphysics-based simulations. For example, an agent in a robotic simulator that\nmust provide fine-control of joint torques should make its decisions every step\nof the simulation. In games such as real-time strategy, where many agents make\ntheir decisions at regular intervals, the decision timing for each agent can be\nstaggered by setting the `DecisionStep` parameter in the `Decision Requester`\ncomponent for each agent. On the other hand, an agent that only needs to make\ndecisions when certain game or simulation events occur, such as in a turn-based\ngame, should call `Agent.RequestDecision()` manually.",
        "Observations and Sensors": {
          "Generating Observations": {
            "Agent.CollectObservations()": "Agent.CollectObservations() is best used for aspects of the environment which are\nnumerical and non-visual. The Policy class calls the\n`CollectObservations(VectorSensor sensor)` method of each Agent. Your\nimplementation of this function must call `VectorSensor.AddObservation` to add\nvector observations.\n\nThe `VectorSensor.AddObservation` method provides a number of overloads for\nadding common types of data to your observation vector. You can add Integers and\nbooleans directly to the observation vector, as well as some common Unity data\ntypes such as `Vector2`, `Vector3`, and `Quaternion`.\n\nFor examples of various state observation functions, you can look at the\n[example environments](Learning-Environment-Examples.md) included in the\nML-Agents SDK. For instance, the 3DBall example uses the rotation of the\nplatform, the relative position of the ball, and the velocity of the ball as its\nstate observation.\n\n```\npublic GameObject ball;\n\npublic override void CollectObservations(VectorSensor sensor)\n{\n    // Orientation of the cube (2 floats)\n    sensor.AddObservation(gameObject.transform.rotation.z);\n    sensor.AddObservation(gameObject.transform.rotation.x);\n    // Relative position of the ball to the cube (3 floats)\n    sensor.AddObservation(ball.transform.position - gameObject.transform.position);\n    // Velocity of the ball (3 floats)\n    sensor.AddObservation(m_BallRb.velocity);\n    // 8 floats total\n}\n```\n\nAs an experiment, you can remove the velocity components from\nthe observation and retrain the 3DBall agent. While it will learn to balance the\nball reasonably well, the performance of the agent without using velocity is\nnoticeably worse.\n\nThe observations passed to `VectorSensor.AddObservation()` must always contain\nthe same number of elements must always be in the same order. If the number\nof observed entities in an environment can vary, you can pad the calls\nwith zeros for any missing entities in a specific observation, or you can limit\nan agent's observations to a fixed subset. For example, instead of observing\nevery enemy in an environment, you could only observe the closest five.\n\nAdditionally, when you set up an Agent's `Behavior Parameters` in the Unity\nEditor, you must set the **Vector Observations > Space Size**\nto equal the number of floats that are written by `CollectObservations()`.",
            "Observable Fields and Properties": "Another approach is to define the relevant observations as fields or properties\non your Agent class, and annotate them with an `ObservableAttribute`. For\nexample, in the Ball3DHardAgent, the difference between positions could be observed\nby adding a property to the Agent:\n\n```\nusing Unity.MLAgents.Sensors.Reflection;\n\npublic class Ball3DHardAgent : Agent {\n\n    [Observable(numStackedObservations: 9)]\n    Vector3 PositionDelta\n    {\n        get\n        {\n            return ball.transform.position - gameObject.transform.position;\n        }\n    }\n}\n```\n\n`ObservableAttribute` currently supports most basic types (e.g. floats, ints,\nbools), as well as `Vector2`, `Vector3`, `Vector4`, `Quaternion`, and enums.\n\nThe behavior of `ObservableAttribute`s are controlled by the \"Observable Attribute\nHandling\" in the Agent's `Behavior Parameters`. The possible values for this are:\n\n['**Ignore** (default) - All ObservableAttributes on the Agent will be ignored.\\nIf there are no ObservableAttributes on the Agent, this will result in the\\nfastest  initialization time.', '**Exclude Inherited** - Only members on the declared class will be examined;\\nmembers that are inherited are ignored. This is a reasonable tradeoff between\\nperformance and flexibility.', '**Examine All** All members on the class will be examined. This can lead to\\nslower startup times.']\n\n\"Exclude Inherited\" is generally sufficient, but if your Agent inherits from\nanother Agent implementation that has Observable members, you will need to use\n\"Examine All\".\n\nInternally, ObservableAttribute uses reflection to determine which members of\nthe Agent have ObservableAttributes, and also uses reflection to access the\nfields or invoke the properties at runtime. This may be slower than using\nCollectObservations or an ISensor, although this might not be enough to\nnoticeably affect performance.\n\n**NOTE**: you do not need to adjust the Space Size in the Agent's\n`Behavior Parameters` when you add `[Observable]` fields or properties to an\nAgent, since their size can be computed before they are used.",
            "ISensor interface and SensorComponents": "The `ISensor` interface is generally intended for advanced users. The `Write()`\nmethod is used to actually generate the observation, but some other methods\nsuch as returning the shape of the observations must also be implemented.\n\nThe `SensorComponent` abstract class is used to create the actual `ISensor` at\nruntime. It must be attached to the same `GameObject` as the `Agent`, or to a\nchild `GameObject`.\n\nThere are several SensorComponents provided in the API, including:\n\n['`CameraSensorComponent` - Uses images from a `Camera` as observations.', '`RenderTextureSensorComponent` - Uses the content of a `RenderTexture` as\\nobservations.', '`RayPerceptionSensorComponent` - Uses the information from set of ray casts\\nas observations.', '`Match3SensorComponent` - Uses the board of a [Match-3 game](Integrations-Match3.md)\\nas observations.', '`GridSensorComponent` - Uses a set of box queries in a grid shape as\\nobservations.']\n\n**NOTE**: you do not need to adjust the Space Size in the Agent's\n`Behavior Parameters` when using `SensorComponents`s.\n\nInternally, both `Agent.CollectObservations` and `[Observable]` attribute use an\nISensors to write observations, although this is mostly abstracted from the user."
          },
          "Vector Observations": {
            "One-hot encoding categorical information": "Type enumerations should be encoded in the _one-hot_ style. That is, add an\nelement to the feature vector for each element of enumeration, setting the\nelement representing the observed member to one and set the rest to zero. For\nexample, if your enumeration contains \\[Sword, Shield, Bow\\] and the agent\nobserves that the current item is a Bow, you would add the elements: 0, 0, 1 to\nthe feature vector. The following code example illustrates how to add.\n\n```\nenum ItemType { Sword, Shield, Bow, LastItem }\npublic override void CollectObservations(VectorSensor sensor)\n{\n    for (int ci = 0; ci < (int)ItemType.LastItem; ci++)\n    {\n        sensor.AddObservation((int)currentItem == ci ? 1.0f : 0.0f);\n    }\n}\n```\n\n`VectorSensor` also provides a two-argument function `AddOneHotObservation()` as\na shortcut for _one-hot_ style observations. The following example is identical\nto the previous one.\n\n```\nenum ItemType { Sword, Shield, Bow, LastItem }\nconst int NUM_ITEM_TYPES = (int)ItemType.LastItem + 1;\n\npublic override void CollectObservations(VectorSensor sensor)\n{\n    // The first argument is the selection index; the second is the\n    // number of possibilities\n    sensor.AddOneHotObservation((int)currentItem, NUM_ITEM_TYPES);\n}\n```\n\n`ObservableAttribute` has built-in support for enums. Note that you don't need\nthe `LastItem` placeholder in this case:\n\n```\nenum ItemType { Sword, Shield, Bow }\n\npublic class HeroAgent : Agent\n{\n    [Observable]\n    ItemType m_CurrentItem;\n}\n```",
            "Normalization": "For the best results when training, you should normalize the components of your\nfeature vector to the range [-1, +1] or [0, 1]. When you normalize the values,\nthe PPO neural network can often converge to a solution faster. Note that it\nisn't always necessary to normalize to these recommended ranges, but it is\nconsidered a best practice when using neural networks. The greater the variation\nin ranges between the components of your observation, the more likely that\ntraining will be affected.\n\nTo normalize a value to [0, 1], you can use the following formula:\n\n```\nnormalizedValue = (currentValue - minValue)/(maxValue - minValue)\n```\n\n:warning: For vectors, you should apply the above formula to each component (x,\ny, and z). Note that this is _not_ the same as using the `Vector3.normalized`\nproperty or `Vector3.Normalize()` method in Unity (and similar for `Vector2`).\n\nRotations and angles should also be normalized. For angles between 0 and 360\ndegrees, you can use the following formulas:\n\n```\nQuaternion rotation = transform.rotation;\nVector3 normalized = rotation.eulerAngles / 180.0f - Vector3.one;  // [-1,1]\nVector3 normalized = rotation.eulerAngles / 360.0f;  // [0,1]\n```\n\nFor angles that can be outside the range [0,360], you can either reduce the\nangle, or, if the number of turns is significant, increase the maximum value\nused in your normalization formula.",
            "Stacking": "Stacking refers to repeating observations from previous steps as part of a\nlarger observation. For example, consider an Agent that generates these\nobservations in four steps\n\n```\nstep 1: [0.1]\nstep 2: [0.2]\nstep 3: [0.3]\nstep 4: [0.4]\n```\n\nIf we use a stack size of 3, the observations would instead be:\n\n```\nstep 1: [0.1, 0.0, 0.0]\nstep 2: [0.2, 0.1, 0.0]\nstep 3: [0.3, 0.2, 0.1]\nstep 4: [0.4, 0.3, 0.2]\n```\n\n(The observations are padded with zeroes for the first `stackSize-1` steps).\nThis is a simple way to give an Agent limited \"memory\" without the complexity\nof adding a recurrent neural network (RNN).\n\nThe steps for enabling stacking depends on how you generate observations:\n\n['For Agent.CollectObservations(), set \"Stacked Vectors\" on the Agent\\'s\\n`Behavior Parameters` to a value greater than 1.', 'For ObservableAttribute, set the `numStackedObservations` parameter in the\\nconstructor, e.g. `[Observable(numStackedObservations: 2)]`.', 'For `ISensor`s, wrap them in a `StackingSensor` (which is also an `ISensor`).\\nGenerally, this should happen in the `CreateSensor()` method of your\\n`SensorComponent`.']",
            "Vector Observation Summary & Best Practices": [
              "Vector Observations should include all variables relevant for allowing the\nagent to take the optimally informed decision, and ideally no extraneous\ninformation.",
              "In cases where Vector Observations need to be remembered or compared over\ntime, either an RNN should be used in the model, or the `Stacked Vectors`\nvalue in the agent GameObject's `Behavior Parameters` should be changed.",
              "Categorical variables such as type of object (Sword, Shield, Bow) should be\nencoded in one-hot fashion (i.e. `3` -> `0, 0, 1`). This can be done\nautomatically using the `AddOneHotObservation()` method of the `VectorSensor`,\nor using `[Observable]` on an enum field or property of the Agent.",
              "In general, all inputs should be normalized to be in the range 0 to +1 (or -1\nto 1). For example, the `x` position information of an agent where the maximum\npossible value is `maxValue` should be recorded as\n`VectorSensor.AddObservation(transform.position.x / maxValue);` rather than\n`VectorSensor.AddObservation(transform.position.x);`.",
              "Positional information of relevant GameObjects should be encoded in relative\ncoordinates wherever possible. This is often relative to the agent position."
            ]
          },
          "Visual Observations": {
            "Visual Observation Summary & Best Practices": [
              "To collect visual observations, attach `CameraSensor` or `RenderTextureSensor`\ncomponents to the agent GameObject.",
              "Visual observations should generally only be used when vector observations are\nnot sufficient.",
              "Image size should be kept as small as possible, without the loss of needed\ndetails for decision making.",
              "Images should be made grayscale in situations where color information is not\nneeded for making informed decisions."
            ]
          },
          "Raycast Observations": {
            "RayCast Observation Summary & Best Practices": [
              "Attach `RayPerceptionSensorComponent3D` or `RayPerceptionSensorComponent2D` to\nuse.",
              "This observation type is best used when there is relevant spatial information\nfor the agent that doesn't require a fully rendered image to convey.",
              "Use as few rays and tags as necessary to solve the problem in order to improve\nlearning stability and agent performance.",
              "If you run into performance issues, try using batched raycasts by enabling the _Use Batched Raycast_ setting.\n(Only available for 3D ray perception sensors.)"
            ]
          },
          "Grid Observations": {
            "Grid Observation Summary & Best Practices": [
              "Attach `GridSensorComponent` to use.",
              "This observation type is best used when there is relevant non-visual spatial information that\ncan be best captured in 2D representations.",
              "Use as small grid size and as few tags as necessary to solve the problem in order to improve\nlearning stability and agent performance.",
              "Do not use `GridSensor` in a 2D game."
            ]
          },
          "Variable Length Observations": {
            "Variable Length Observation Summary & Best Practices": [
              "Attach `BufferSensorComponent` to use.",
              "Call `BufferSensorComponent.AppendObservation()` in the\nAgent.CollectObservations() methodto add the observations\nof an entity to the `BufferSensor`.",
              "Normalize the entities observations before feeding them into the `BufferSensor`."
            ]
          },
          "Goal Signal": {
            "Goal Signal Summary & Best Practices": [
              "Attach a `VectorSensorComponent` or `CameraSensorComponent` to an agent and\nset the observation type to goal to use the feature.",
              "Set the goal_conditioning_type parameter in the training configuration.",
              "Reduce the number of hidden units in the network when using the HyperNetwork\nconditioning type."
            ]
          }
        },
        "Actions and Actuators": {
          "Continuous Actions": "When an Agent's Policy has **Continuous** actions, the\n`ActionBuffers.ContinuousActions` passed to the Agent's `OnActionReceived()` function\nis an array with length equal to the `Continuous Action Size` property value. The\nindividual values in the array have whatever meanings that you ascribe to them.\nIf you assign an element in the array as the speed of an Agent, for example, the\ntraining process learns to control the speed of the Agent through this\nparameter.\n\nThe [3DBall example](Learning-Environment-Examples.md#3dball-3d-balance-ball) uses\ncontinuous actions with two control values.\n\n![3DBall](images/balance.png)\n\nThese control values are applied as rotation to the cube:\n\n```\n    public override void OnActionReceived(ActionBuffers actionBuffers)\n    {\n        var actionZ = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[0], -1f, 1f);\n        var actionX = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[1], -1f, 1f);\n\n        gameObject.transform.Rotate(new Vector3(0, 0, 1), actionZ);\n        gameObject.transform.Rotate(new Vector3(1, 0, 0), actionX);\n    }\n```\n\nBy default the output from our provided PPO algorithm pre-clamps the values of\n`ActionBuffers.ContinuousActions` into the [-1, 1] range. It is a best practice to manually clip\nthese as well, if you plan to use a 3rd party algorithm with your environment.\nAs shown above, you can scale the control values as needed after clamping them.",
          "Discrete Actions": {
            "Masking Discrete Actions": "When using Discrete Actions, it is possible to specify that some actions are\nimpossible for the next decision. When the Agent is controlled by a neural\nnetwork, the Agent will be unable to perform the specified action. Note that\nwhen the Agent is controlled by its Heuristic, the Agent will still be able to\ndecide to perform the masked action. In order to disallow an action, override\nthe `Agent.WriteDiscreteActionMask()` virtual method, and call\n`SetActionEnabled()` on the provided `IDiscreteActionMask`:\n\n```\npublic override void WriteDiscreteActionMask(IDiscreteActionMask actionMask)\n{\n    actionMask.SetActionEnabled(branch, actionIndex, isEnabled);\n}\n```\n\nWhere:\n\n['`branch` is the index (starting at 0) of the branch on which you want to\\nallow or disallow the action', '`actionIndex` is the index of the action that you want to allow or disallow.', '`isEnabled` is a bool indicating whether the action should be allowed or now.']\n\nFor example, if you have an Agent with 2 branches and on the first branch\n(branch 0) there are 4 possible actions : _\"do nothing\"_, _\"jump\"_, _\"shoot\"_\nand _\"change weapon\"_. Then with the code bellow, the Agent will either _\"do\nnothing\"_ or _\"change weapon\"_ for their next decision (since action index 1 and 2\nare masked)\n\n```\nactionMask.SetActionEnabled(0, 1, false);\nactionMask.SetActionEnabled(0, 2, false);\n```\n\nNotes:\n\n['You can call `SetActionEnabled` multiple times if you want to put masks on multiple\\nbranches.', 'At each step, the state of an action is reset and enabled by default.', 'You cannot mask all the actions of a branch.', 'You cannot mask actions in continuous control.']"
          },
          "IActuator interface and ActuatorComponents": "The Actuator API allows users to abstract behavior out of Agents and in to\ncomponents (similar to the ISensor API).  The `IActuator` interface and `Agent`\nclass both implement the `IActionReceiver` interface to allow for backward compatibility\nwith the current `Agent.OnActionReceived`.\nThis means you will not have to change your code until you decide to use the `IActuator` API.\n\nLike the `ISensor` interface, the `IActuator` interface is intended for advanced users.\n\nThe `ActuatorComponent` abstract class is used to create the actual `IActuator` at\nruntime. It must be attached to the same `GameObject` as the `Agent`, or to a\nchild `GameObject`.  Actuators and all of their data structures are initialized\nduring `Agent.Initialize`.  This was done to prevent an unexpected allocations at runtime.\n\nYou can find an example of an `IActuator` implementation in the `Basic` example scene.\n**NOTE**: you do not need to adjust the Actions in the Agent's\n`Behavior Parameters` when using an `IActuator` and `ActuatorComponents`.\n\nInternally, `Agent.OnActionReceived` uses an `IActuator` to send actions to the Agent,\nalthough this is mostly abstracted from the user.",
          "Actions Summary & Best Practices": [
            "Agents can use `Discrete` and/or `Continuous` actions.",
            "Discrete actions can have multiple action branches, and it's possible to mask\ncertain actions so that they won't be taken.",
            "In general, fewer actions will make for easier learning.",
            "Be sure to set the Continuous Action Size and Discrete Branch Size to the desired\nnumber for each type of action, and not greater, as doing the latter can interfere with the\nefficiency of the training process.",
            "Continuous action values should be clipped to an\nappropriate range. The provided PPO model automatically clips these values\nbetween -1 and 1, but third party training systems may not do so."
          ]
        },
        "Rewards": {
          "Examples": "You can examine the `OnActionReceived()` functions defined in the\n[example environments](Learning-Environment-Examples.md) to see how those\nprojects allocate rewards.\n\nThe `GridAgent` class in the\n[GridWorld example](Learning-Environment-Examples.md#gridworld) uses a very\nsimple reward system:\n\n```\nCollider[] hitObjects = Physics.OverlapBox(trueAgent.transform.position,\n                                           new Vector3(0.3f, 0.3f, 0.3f));\nif (hitObjects.Where(col => col.gameObject.tag == \"goal\").ToArray().Length == 1)\n{\n    AddReward(1.0f);\n    EndEpisode();\n}\nelse if (hitObjects.Where(col => col.gameObject.tag == \"pit\").ToArray().Length == 1)\n{\n    AddReward(-1f);\n    EndEpisode();\n}\n```\n\nThe agent receives a positive reward when it reaches the goal and a negative\nreward when it falls into the pit. Otherwise, it gets no rewards. This is an\nexample of a _sparse_ reward system. The agent must explore a lot to find the\ninfrequent reward.\n\nIn contrast, the `AreaAgent` in the\n[Area example](Learning-Environment-Examples.md#push-block) gets a small\nnegative reward every step. In order to get the maximum reward, the agent must\nfinish its task of reaching the goal square as quickly as possible:\n\n```\nAddReward( -0.005f);\nMoveAgent(act);\n\nif (gameObject.transform.position.y < 0.0f ||\n    Mathf.Abs(gameObject.transform.position.x - area.transform.position.x) > 8f ||\n    Mathf.Abs(gameObject.transform.position.z + 5 - area.transform.position.z) > 8)\n{\n    AddReward(-1f);\n    EndEpisode();\n}\n```\n\nThe agent also gets a larger negative penalty if it falls off the playing\nsurface.\n\nThe `Ball3DAgent` in the\n[3DBall](Learning-Environment-Examples.md#3dball-3d-balance-ball) takes a\nsimilar approach, but allocates a small positive reward as long as the agent\nbalances the ball. The agent can maximize its rewards by keeping the ball on the\nplatform:\n\n```\n\nSetReward(0.1f);\n\n// When ball falls mark Agent as finished and give a negative penalty\nif ((ball.transform.position.y - gameObject.transform.position.y) < -2f ||\n    Mathf.Abs(ball.transform.position.x - gameObject.transform.position.x) > 3f ||\n    Mathf.Abs(ball.transform.position.z - gameObject.transform.position.z) > 3f)\n{\n    SetReward(-1f);\n    EndEpisode();\n\n}\n```\n\nThe `Ball3DAgent` also assigns a negative penalty when the ball falls off the\nplatform.\n\nNote that all of these environments make use of the `EndEpisode()` method, which\nmanually terminates an episode when a termination condition is reached. This can\nbe called independently of the `Max Step` property.",
          "Rewards Summary & Best Practices": [
            "Use `AddReward()` to accumulate rewards between decisions. Use `SetReward()`\nto overwrite any previous rewards accumulate between decisions.",
            "The magnitude of any given reward should typically not be greater than 1.0 in\norder to ensure a more stable learning process.",
            "Positive rewards are often more helpful to shaping the desired behavior of an\nagent than negative rewards. Excessive negative rewards can result in the\nagent failing to learn any meaningful behavior.",
            "For locomotion tasks, a small positive reward (+0.1) for forward velocity is\ntypically used.",
            "If you want the agent to finish a task quickly, it is often helpful to provide\na small penalty every step (-0.05) that the agent does not complete the task.\nIn this case completion of the task should also coincide with the end of the\nepisode by calling `EndEpisode()` on the agent when it has accomplished its\ngoal."
          ]
        },
        "Agent Properties": "![Agent Inspector](images/3dball_learning_brain.png)\n\n['`Behavior Parameters` - The parameters dictating what Policy the Agent will\\nreceive.', ['`Behavior Name` - The identifier for the behavior. Agents with the same\\nbehavior name will learn the same policy.', '`Vector Observation`', ['`Space Size` - Length of vector observation for the Agent.', '`Stacked Vectors` - The number of previous vector observations that will\\nbe stacked and used collectively for decision making. This results in the\\neffective size of the vector observation being passed to the Policy being:\\n_Space Size_ x _Stacked Vectors_.'], '`Actions`', ['`Continuous Actions` - The number of concurrent continuous actions that\\nthe Agent can take.', '`Discrete Branches` - An array of integers, defines multiple concurrent\\ndiscrete actions. The values in the `Discrete Branches` array correspond\\nto the number of possible discrete values for each action branch.'], '`Model` - The neural network model used for inference (obtained after\\ntraining)', '`Inference Device` - Whether to use CPU or GPU to run the model during\\ninference', '`Behavior Type` - Determines whether the Agent will do training, inference,\\nor use its Heuristic() method:', ['`Default` - the Agent will train if they connect to a python trainer,\\notherwise they will perform inference.', '`Heuristic Only` - the Agent will always use the `Heuristic()` method.', '`Inference Only` - the Agent will always perform inference.'], '`Team ID` - Used to define the team for self-play', '`Use Child Sensors` - Whether to use all Sensor components attached to child\\nGameObjects of this Agent.'], '`Max Step` - The per-agent maximum number of steps. Once this number is\\nreached, the Agent will be reset.']",
        "Destroying an Agent": "You can destroy an Agent GameObject during the simulation. Make sure that there\nis always at least one Agent training at all times by either spawning a new\nAgent every time one is destroyed or by re-spawning new Agents when the whole\nenvironment resets.",
        "Defining Multi-agent Scenarios": {
          "Teams for Adversarial Scenarios": "Self-play is triggered by including the self-play hyperparameter hierarchy in\nthe [trainer configuration](Training-ML-Agents.md#training-configurations). To\ndistinguish opposing agents, set the team ID to different integer values in the\nbehavior parameters script on the agent prefab.\n\n<p align=\"center\">\n  <img src=\"images/team_id.png\"\n       alt=\"Team ID\"\n       width=\"375\" border=\"10\" />\n</p>\n\n**_Team ID must be 0 or an integer greater than 0._**\n\nIn symmetric games, since all agents (even on opposing teams) will share the\nsame policy, they should have the same 'Behavior Name' in their Behavior\nParameters Script. In asymmetric games, they should have a different Behavior\nName in their Behavior Parameters script. Note, in asymmetric games, the agents\nmust have both different Behavior Names _and_ different team IDs!\n\nFor examples of how to use this feature, you can see the trainer configurations\nand agent prefabs for our Tennis and Soccer environments. Tennis and Soccer\nprovide examples of symmetric games. To train an asymmetric game, specify\ntrainer configurations for each of your behavior names and include the self-play\nhyperparameter hierarchy in both.",
          "Groups for Cooperative Scenarios": {
            "Cooperative Behaviors Notes and Best Practices": [
              "An agent can only be registered to one MultiAgentGroup at a time. If you want to re-assign an\nagent from one group to another, you have to unregister it from the current group first.",
              "Agents with different behavior names in the same group are not supported.",
              "Agents within groups should always set the `Max Steps` parameter in the Agent script to 0.\nInstead, handle Max Steps using the MultiAgentGroup by ending the episode for the entire\nGroup using `GroupEpisodeInterrupted()`.",
              "`EndGroupEpisode` and `GroupEpisodeInterrupted` do the same job in the game, but has\nslightly different effect on the training. If the episode is completed, you would want to call\n`EndGroupEpisode`. But if the episode is not over but it has been running for enough steps, i.e.\nreaching max step, you would call `GroupEpisodeInterrupted`.",
              "If an agent finished earlier, e.g. completed tasks/be removed/be killed in the game, do not call\n`EndEpisode()` on the Agent. Instead, disable the agent and re-enable it when the next episode starts,\nor destroy the agent entirely. This is because calling `EndEpisode()` will call `OnEpisodeBegin()`, which\nwill reset the agent immediately. While it is possible to call `EndEpisode()` in this way, it is usually not the\ndesired behavior when training groups of agents.",
              "If an agent that was disabled in a scene needs to be re-enabled, it must be re-registered to the MultiAgentGroup.",
              "Group rewards are meant to reinforce agents to act in the group's best interest instead of\nindividual ones, and are treated differently than individual agent rewards during\ntraining. So calling `AddGroupReward()` is not equivalent to calling agent.AddReward() on each agent\nin the group.",
              "You can still add incremental rewards to agents using `Agent.AddReward()` if they are\nin a Group. These rewards will only be given to those agents and are received when the\nAgent is active.",
              "Environments which use Multi Agent Groups can be trained using PPO or SAC, but agents will\nnot be able to learn from group rewards after deactivation/removal, nor will they behave as cooperatively."
            ]
          }
        },
        "Recording Demonstrations": "In order to record demonstrations from an agent, add the\n`Demonstration Recorder` component to a GameObject in the scene which contains\nan `Agent` component. Once added, it is possible to name the demonstration that\nwill be recorded from the agent.\n\n<p align=\"center\">\n  <img src=\"images/demo_component.png\"\n       alt=\"Demonstration Recorder\"\n       width=\"650\" border=\"10\" />\n</p>\n\nWhen `Record` is checked, a demonstration will be created whenever the scene is\nplayed from the Editor. Depending on the complexity of the task, anywhere from a\nfew minutes or a few hours of demonstration data may be necessary to be useful\nfor imitation learning. To specify an exact number of steps you want to record\nuse the `Num Steps To Record` field and the editor will end your play session\nautomatically once that many steps are recorded. If you set `Num Steps To Record`\nto `0` then recording will continue until you manually end the play session. Once\nthe play session ends a `.demo` file will be created in the `Assets/Demonstrations`\nfolder (by default). This file contains the demonstrations. Clicking on the file will\nprovide metadata about the demonstration in the inspector.\n\n<p align=\"center\">\n  <img src=\"images/demo_inspector.png\"\n       alt=\"Demonstration Inspector\"\n       width=\"375\" border=\"10\" />\n</p>\n\nYou can then specify the path to this file in your\n[training configurations](Training-Configuration-File.md#behavioral-cloning)."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 19]"
    },
    {
      "title": "Designing a Learning Environment",
      "description": null,
      "content": {
        "The Simulation and Training Process": "Training and simulation proceed in steps orchestrated by the ML-Agents Academy\nclass. The Academy works with Agent objects in the scene to step through the\nsimulation.\n\nDuring training, the external Python training process communicates with the\nAcademy to run a series of episodes while it collects data and optimizes its\nneural network model. When training is completed successfully, you can add the\ntrained model file to your Unity project for later use.\n\nThe ML-Agents Academy class orchestrates the agent simulation loop as follows:\n\n[\"Calls your Academy's `OnEnvironmentReset` delegate.\", 'Calls the `OnEpisodeBegin()` function for each Agent in the scene.', 'Gathers information about the scene. This is done by calling the\\n`CollectObservations(VectorSensor sensor)` function for each Agent in the\\nscene, as well as updating their sensor and collecting the resulting\\nobservations.', \"Uses each Agent's Policy to decide on the Agent's next action.\", \"Calls the `OnActionReceived()` function for each Agent in the scene, passing\\nin the action chosen by the Agent's Policy.\", \"Calls the Agent's `OnEpisodeBegin()` function if the Agent has reached its\\n`Max Step` count or has otherwise marked itself as `EndEpisode()`.\"]\n\nTo create a training environment, extend the Agent class to implement the above\nmethods whether you need to implement them or not depends on your specific\nscenario.",
        "Organizing the Unity Scene": {
          "Academy": {
            "Academy resetting": "To alter the environment at the start of each episode, add your method to the\nAcademy's OnEnvironmentReset action.\n\n```\npublic class MySceneBehavior : MonoBehaviour\n{\n    public void Awake()\n    {\n        Academy.Instance.OnEnvironmentReset += EnvironmentReset;\n    }\n\n    void EnvironmentReset()\n    {\n        // Reset the scene here\n    }\n}\n```\n\nFor example, you might want to reset an Agent to its starting position or move a\ngoal to a random position. An environment resets when the `reset()` method is\ncalled on the Python `UnityEnvironment`.\n\nWhen you reset an environment, consider the factors that should change so that\ntraining is generalizable to different conditions. For example, if you were\ntraining a maze-solving agent, you would probably want to change the maze itself\nfor each training episode. Otherwise, the agent would probably on learn to solve\none, particular maze, not mazes in general."
          },
          "Multiple Areas": "In many of the example environments, many copies of the training area are\ninstantiated in the scene. This generally speeds up training, allowing the\nenvironment to gather many experiences in parallel. This can be achieved simply\nby instantiating many Agents with the same Behavior Name. If possible, consider\ndesigning your scene to support multiple areas.\n\nCheck out our example environments to see examples of multiple areas.\nAdditionally, the\n[Making a New Learning Environment](Learning-Environment-Create-New.md#optional-multiple-training-areas-within-the-same-scene)\nguide demonstrates this option."
        },
        "Environments": "When you create a training environment in Unity, you must set up the scene so\nthat it can be controlled by the external training process. Considerations\ninclude:\n\n['The training scene must start automatically when your Unity application is\\nlaunched by the training process.', 'The Academy must reset the scene to a valid starting point for each episode of\\ntraining.', 'A training episode must have a definite end — either using `Max Steps` or by\\neach Agent ending its episode manually with `EndEpisode()`.']",
        "Environment Parameters": "Curriculum learning and environment parameter randomization are two training\nmethods that control specific parameters in your environment. As such, it is\nimportant to ensure that your environment parameters are updated at each step to\nthe correct values. To enable this, we expose a `EnvironmentParameters` C# class\nthat you can use to retrieve the values of the parameters defined in the\ntraining configurations for both of those features. Please see our\n[documentation](Training-ML-Agents.md#environment-parameters)\nfor curriculum learning and environment parameter randomization for details.\n\nWe recommend modifying the environment from the Agent's `OnEpisodeBegin()`\nfunction by leveraging `Academy.Instance.EnvironmentParameters`. See the\nWallJump example environment for a sample usage (specifically,\n[WallJumpAgent.cs](../Project/Assets/ML-Agents/Examples/WallJump/Scripts/WallJumpAgent.cs)\n).",
        "Agent": "The Agent class represents an actor in the scene that collects observations and\ncarries out actions. The Agent class is typically attached to the GameObject in\nthe scene that otherwise represents the actor — for example, to a player object\nin a football game or a car object in a vehicle simulation. Every Agent must\nhave appropriate `Behavior Parameters`.\n\nGenerally, when creating an Agent, you should extend the Agent class and implement\nthe `CollectObservations(VectorSensor sensor)` and `OnActionReceived()` methods:\n\n[\"`CollectObservations(VectorSensor sensor)` — Collects the Agent's observation\\nof its environment.\", \"`OnActionReceived()` — Carries out the action chosen by the Agent's Policy and\\nassigns a reward to the current state.\"]\n\nYour implementations of these functions determine how the Behavior Parameters\nassigned to this Agent must be set.\n\nYou must also determine how an Agent finishes its task or times out. You can\nmanually terminate an Agent episode in your `OnActionReceived()` function when\nthe Agent has finished (or irrevocably failed) its task by calling the\n`EndEpisode()` function. You can also set the Agent's `Max Steps` property to a\npositive value and the Agent will consider the episode over after it has taken\nthat many steps. You can use the `Agent.OnEpisodeBegin()` function to prepare\nthe Agent to start again.\n\nSee [Agents](Learning-Environment-Design-Agents.md) for detailed information\nabout programming your own Agents.",
        "Recording Statistics": "We offer developers a mechanism to record statistics from within their Unity\nenvironments. These statistics are aggregated and generated during the training\nprocess. To record statistics, see the `StatsRecorder` C# class.\n\nSee the FoodCollector example environment for a sample usage (specifically,\n[FoodCollectorSettings.cs](../Project/Assets/ML-Agents/Examples/FoodCollector/Scripts/FoodCollectorSettings.cs)\n)."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 20]"
    },
    {
      "title": "Example Learning Environments",
      "description": null,
      "content": {
        "Basic": "![Basic](images/basic.png)\n\n['Set-up: A linear movement task where the agent must move left or right to\\nrewarding states.', 'Goal: Move to the most reward state.', 'Agents: The environment contains one agent.', 'Agent Reward Function:', ['-0.01 at each step', '+0.1 for arriving at suboptimal state.', '+1.0 for arriving at optimal state.'], 'Behavior Parameters:', ['Vector Observation space: One variable corresponding to current state.', 'Actions: 1 discrete action branch with 3 actions (Move left, do nothing, move\\nright).', 'Visual Observations: None'], 'Float Properties: None', 'Benchmark Mean Reward: 0.93']",
        "3DBall: 3D Balance Ball": "![3D Balance Ball](images/balance.png)\n\n[\"Set-up: A balance-ball task, where the agent balances the ball on it's head.\", \"Goal: The agent must balance the ball on it's head for as long as possible.\", 'Agents: The environment contains 12 agents of the same kind, all using the\\nsame Behavior Parameters.', 'Agent Reward Function:', [\"+0.1 for every step the ball remains on it's head.\", '-1.0 if the ball falls off.'], 'Behavior Parameters:', ['Vector Observation space: 8 variables corresponding to rotation of the agent\\ncube, and position and velocity of ball.', 'Vector Observation space (Hard Version): 5 variables corresponding to\\nrotation of the agent cube and position of ball.', 'Actions: 2 continuous actions, with one value corresponding to\\nX-rotation, and the other to Z-rotation.', 'Visual Observations: Third-person view from the upper-front of the agent. Use\\n`Visual3DBall` scene.'], 'Float Properties: Three', ['scale: Specifies the scale of the ball in the 3 dimensions (equal across the\\nthree dimensions)', ['Default: 1', 'Recommended Minimum: 0.2', 'Recommended Maximum: 5'], 'gravity: Magnitude of gravity', ['Default: 9.81', 'Recommended Minimum: 4', 'Recommended Maximum: 105'], 'mass: Specifies mass of the ball', ['Default: 1', 'Recommended Minimum: 0.1', 'Recommended Maximum: 20']], 'Benchmark Mean Reward: 100']",
        "GridWorld": "![GridWorld](images/gridworld.png)\n\n['Set-up: A multi-goal version of the grid-world task. Scene contains agent, goal,\\nand obstacles.', 'Goal: The agent must navigate the grid to the appropriate goal while\\navoiding the obstacles.', 'Agents: The environment contains nine agents with the same Behavior\\nParameters.', 'Agent Reward Function:', ['-0.01 for every step.', '+1.0 if the agent navigates to the correct goal (episode ends).', '-1.0 if the agent navigates to an incorrect goal (episode ends).'], 'Behavior Parameters:', ['Vector Observation space: None', 'Actions: 1 discrete action branch with 5 actions, corresponding to movement in\\ncardinal directions or not moving. Note that for this environment,\\n[action masking](Learning-Environment-Design-Agents.md#masking-discrete-actions)\\nis turned on by default (this option can be toggled using the `Mask Actions`\\ncheckbox within the `trueAgent` GameObject). The trained model file provided\\nwas generated with action masking turned on.', 'Visual Observations: One corresponding to top-down view of GridWorld.', 'Goal Signal : A one hot vector corresponding to which color is the correct goal\\nfor the Agent'], 'Float Properties: Three, corresponding to grid size, number of green goals, and\\nnumber of red goals.', 'Benchmark Mean Reward: 0.8']",
        "Push Block": "![Push](images/push.png)\n\n['Set-up: A platforming environment where the agent can push a block around.', 'Goal: The agent must push the block to the goal.', 'Agents: The environment contains one agent.', 'Agent Reward Function:', ['-0.0025 for every step.', '+1.0 if the block touches the goal.'], 'Behavior Parameters:', ['Vector Observation space: (Continuous) 70 variables corresponding to 14\\nray-casts each detecting one of three possible objects (wall, goal, or\\nblock).', 'Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise\\nand counterclockwise, move along four different face directions, or do nothing.'], 'Float Properties: Four', ['block_scale: Scale of the block along the x and z dimensions', ['Default: 2', 'Recommended Minimum: 0.5', 'Recommended Maximum: 4'], 'dynamic_friction: Coefficient of friction for the ground material acting on\\nmoving objects', ['Default: 0', 'Recommended Minimum: 0', 'Recommended Maximum: 1'], 'static_friction: Coefficient of friction for the ground material acting on\\nstationary objects', ['Default: 0', 'Recommended Minimum: 0', 'Recommended Maximum: 1'], 'block_drag: Effect of air resistance on block', ['Default: 0.5', 'Recommended Minimum: 0', 'Recommended Maximum: 2000']], 'Benchmark Mean Reward: 4.5']",
        "Wall Jump": "![Wall](images/wall.png)\n\n['Set-up: A platforming environment where the agent can jump over a wall.', 'Goal: The agent must use the block to scale the wall and reach the goal.', 'Agents: The environment contains one agent linked to two different Models. The\\nPolicy the agent is linked to changes depending on the height of the wall. The\\nchange of Policy is done in the WallJumpAgent class.', 'Agent Reward Function:', ['-0.0005 for every step.', '+1.0 if the agent touches the goal.', '-1.0 if the agent falls off the platform.'], 'Behavior Parameters:', ['Vector Observation space: Size of 74, corresponding to 14 ray casts each\\ndetecting 4 possible objects. plus the global position of the agent and\\nwhether or not the agent is grounded.', 'Actions: 4 discrete action branches:', ['Forward Motion (3 possible actions: Forward, Backwards, No Action)', 'Rotation (3 possible actions: Rotate Left, Rotate Right, No Action)', 'Side Motion (3 possible actions: Left, Right, No Action)', 'Jump (2 possible actions: Jump, No Action)'], 'Visual Observations: None'], 'Float Properties: Four', 'Benchmark Mean Reward (Big & Small Wall): 0.8']",
        "Crawler": "![Crawler](images/crawler.png)\n\n['Set-up: A creature with 4 arms and 4 forearms.', 'Goal: The agents must move its body toward the goal direction without falling.', 'Agents: The environment contains 10 agents with same Behavior Parameters.', 'Agent Reward Function (independent):\\nThe reward function is now geometric meaning the reward each step is a product\\nof all the rewards instead of a sum, this helps the agent try to maximize all\\nrewards instead of the easiest rewards.', ['Body velocity matches goal velocity. (normalized between (0,1))', 'Head direction alignment with goal direction. (normalized between (0,1))'], 'Behavior Parameters:', ['Vector Observation space: 172 variables corresponding to position, rotation,\\nvelocity, and angular velocities of each limb plus the acceleration and\\nangular acceleration of the body.', 'Actions: 20 continuous actions, corresponding to target\\nrotations for joints.', 'Visual Observations: None'], 'Float Properties: None', 'Benchmark Mean Reward: 3000']",
        "Worm": "![Worm](images/worm.png)\n\n['Set-up: A worm with a head and 3 body segments.', 'Goal: The agents must move its body toward the goal direction.', 'Agents: The environment contains 10 agents with same Behavior Parameters.', 'Agent Reward Function (independent):\\nThe reward function is now geometric meaning the reward each step is a product\\nof all the rewards instead of a sum, this helps the agent try to maximize all\\nrewards instead of the easiest rewards.', ['Body velocity matches goal velocity. (normalized between (0,1))', 'Body direction alignment with goal direction. (normalized between (0,1))'], 'Behavior Parameters:', ['Vector Observation space: 64 variables corresponding to position, rotation,\\nvelocity, and angular velocities of each limb plus the acceleration and\\nangular acceleration of the body.', 'Actions: 9 continuous actions, corresponding to target\\nrotations for joints.', 'Visual Observations: None'], 'Float Properties: None', 'Benchmark Mean Reward: 800']",
        "Food Collector": "![Collector](images/foodCollector.png)\n\n['Set-up: A multi-agent environment where agents compete to collect food.', 'Goal: The agents must learn to collect as many green food spheres as possible\\nwhile avoiding red spheres.', 'Agents: The environment contains 5 agents with same Behavior Parameters.', 'Agent Reward Function (independent):', ['+1 for interaction with green spheres', '-1 for interaction with red spheres'], 'Behavior Parameters:', [\"Vector Observation space: 53 corresponding to velocity of agent (2), whether\\nagent is frozen and/or shot its laser (2), plus grid based perception of\\nobjects around agent's forward direction (40 by 40 with 6 different categories).\", 'Actions:', ['3 continuous actions correspond to Forward Motion, Side Motion and Rotation', '1 discrete action branch for Laser with 2 possible actions corresponding to\\nShoot Laser or No Action'], 'Visual Observations (Optional): First-person camera per-agent, plus one vector\\nflag representing the frozen state of the agent. This scene uses a combination\\nof vector and visual observations and the training will not succeed without\\nthe frozen vector flag. Use `VisualFoodCollector` scene.'], 'Float Properties: Two', ['laser_length: Length of the laser used by the agent', ['Default: 1', 'Recommended Minimum: 0.2', 'Recommended Maximum: 7'], 'agent_scale: Specifies the scale of the agent in the 3 dimensions (equal\\nacross the three dimensions)', ['Default: 1', 'Recommended Minimum: 0.5', 'Recommended Maximum: 5']], 'Benchmark Mean Reward: 10']",
        "Hallway": "![Hallway](images/hallway.png)\n\n['Set-up: Environment where the agent needs to find information in a room,\\nremember it, and use it to move to the correct goal.', 'Goal: Move to the goal which corresponds to the color of the block in the\\nroom.', 'Agents: The environment contains one agent.', 'Agent Reward Function (independent):', ['+1 For moving to correct goal.', '-0.1 For moving to incorrect goal.', '-0.0003 Existential penalty.'], 'Behavior Parameters:', ['Vector Observation space: 30 corresponding to local ray-casts detecting\\nobjects, goals, and walls.', 'Actions: 1 discrete action Branch, with 4 actions corresponding to agent\\nrotation and forward/backward movement.'], 'Float Properties: None', 'Benchmark Mean Reward: 0.7', ['To train this environment, you can enable curiosity by adding the `curiosity` reward signal\\nin `config/ppo/Hallway.yaml`']]",
        "Soccer Twos": "![SoccerTwos](images/soccer.png)\n\n['Set-up: Environment where four agents compete in a 2 vs 2 toy soccer game.', 'Goal:', [\"Get the ball into the opponent's goal while preventing the ball from\\nentering own goal.\"], 'Agents: The environment contains two different Multi Agent Groups with two agents in each.\\nParameters : SoccerTwos.', 'Agent Reward Function (dependent):', [\"(1 - `accumulated time penalty`) When ball enters opponent's goal\\n`accumulated time penalty` is incremented by (1 / `MaxStep`) every fixed\\nupdate and is reset to 0 at the beginning of an episode.\", \"-1 When ball enters team's goal.\"], 'Behavior Parameters:', [\"Vector Observation space: 336 corresponding to 11 ray-casts forward\\ndistributed over 120 degrees and 3 ray-casts backward distributed over 90\\ndegrees each detecting 6 possible object types, along with the object's\\ndistance. The forward ray-casts contribute 264 state dimensions and backward\\n72 state dimensions over three observation stacks.\", 'Actions: 3 discrete branched actions corresponding to\\nforward, backward, sideways movement, as well as rotation.', 'Visual Observations: None'], 'Float Properties: Two', ['ball_scale: Specifies the scale of the ball in the 3 dimensions (equal\\nacross the three dimensions)', ['Default: 7.5', 'Recommended minimum: 4', 'Recommended maximum: 10'], 'gravity: Magnitude of the gravity', ['Default: 9.81', 'Recommended minimum: 6', 'Recommended maximum: 20']]]",
        "Strikers Vs. Goalie": "![StrikersVsGoalie](images/strikersvsgoalie.png)\n\n['Set-up: Environment where two agents compete in a 2 vs 1 soccer variant.', 'Goal:', [\"Striker: Get the ball into the opponent's goal.\", 'Goalie: Keep the ball out of the goal.'], 'Agents: The environment contains two different Multi Agent Groups. One with two Strikers and the other one Goalie.\\nBehavior Parameters : Striker, Goalie.', 'Striker Agent Reward Function (dependent):', [\"+1 When ball enters opponent's goal.\", '-0.001 Existential penalty.'], 'Goalie Agent Reward Function (dependent):', ['-1 When ball enters goal.', '0.001 Existential bonus.'], 'Behavior Parameters:', [\"Striker Vector Observation space: 294 corresponding to 11 ray-casts forward\\ndistributed over 120 degrees and 3 ray-casts backward distributed over 90\\ndegrees each detecting 5 possible object types, along with the object's\\ndistance. The forward ray-casts contribute 231 state dimensions and backward\\n63 state dimensions over three observation stacks.\", 'Striker Actions: 3 discrete branched actions corresponding\\nto forward, backward, sideways movement, as well as rotation.', \"Goalie Vector Observation space: 738 corresponding to 41 ray-casts\\ndistributed over 360 degrees each detecting 4 possible object types, along\\nwith the object's distance and 3 observation stacks.\", 'Goalie Actions: 3 discrete branched actions corresponding\\nto forward, backward, sideways movement, as well as rotation.', 'Visual Observations: None'], 'Float Properties: Two', ['ball_scale: Specifies the scale of the ball in the 3 dimensions (equal\\nacross the three dimensions)', ['Default: 7.5', 'Recommended minimum: 4', 'Recommended maximum: 10'], 'gravity: Magnitude of the gravity', ['Default: 9.81', 'Recommended minimum: 6', 'Recommended maximum: 20']]]",
        "Walker": "![Walker](images/walker.png)\n\n['Set-up: Physics-based Humanoid agents with 26 degrees of freedom. These DOFs\\ncorrespond to articulation of the following body-parts: hips, chest, spine,\\nhead, thighs, shins, feet, arms, forearms and hands.', 'Goal: The agents must move its body toward the goal direction without falling.', 'Agents: The environment contains 10 independent agents with same Behavior\\nParameters.', 'Agent Reward Function (independent):\\nThe reward function is now geometric meaning the reward each step is a product\\nof all the rewards instead of a sum, this helps the agent try to maximize all\\nrewards instead of the easiest rewards.', ['Body velocity matches goal velocity. (normalized between (0,1))', 'Head direction alignment with goal direction. (normalized between (0,1))'], 'Behavior Parameters:', ['Vector Observation space: 243 variables corresponding to position, rotation,\\nvelocity, and angular velocities of each limb, along with goal direction.', 'Actions: 39 continuous actions, corresponding to target\\nrotations and strength applicable to the joints.', 'Visual Observations: None'], 'Float Properties: Four', ['gravity: Magnitude of gravity', ['Default: 9.81', 'Recommended Minimum:', 'Recommended Maximum:'], 'hip_mass: Mass of the hip component of the walker', ['Default: 8', 'Recommended Minimum: 7', 'Recommended Maximum: 28'], 'chest_mass: Mass of the chest component of the walker', ['Default: 8', 'Recommended Minimum: 3', 'Recommended Maximum: 20'], 'spine_mass: Mass of the spine component of the walker', ['Default: 8', 'Recommended Minimum: 3', 'Recommended Maximum: 20']], 'Benchmark Mean Reward : 2500']",
        "Pyramids": "![Pyramids](images/pyramids.png)\n\n['Set-up: Environment where the agent needs to press a button to spawn a\\npyramid, then navigate to the pyramid, knock it over, and move to the gold\\nbrick at the top.', 'Goal: Move to the golden brick on top of the spawned pyramid.', 'Agents: The environment contains one agent.', 'Agent Reward Function (independent):', ['+2 For moving to golden brick (minus 0.001 per step).'], 'Behavior Parameters:', ['Vector Observation space: 148 corresponding to local ray-casts detecting\\nswitch, bricks, golden brick, and walls, plus variable indicating switch\\nstate.', 'Actions: 1 discrete action branch, with 4 actions corresponding to agent rotation and\\nforward/backward movement.'], 'Float Properties: None', 'Benchmark Mean Reward: 1.75']",
        "Match 3": "![Match 3](images/match3.png)\n\n['Set-up: Simple match-3 game. Matched pieces are removed, and remaining pieces\\ndrop down. New pieces are spawned randomly at the top, with a chance of being\\n\"special\".', 'Goal: Maximize score from matching pieces.', 'Agents: The environment contains several independent Agents.', 'Agent Reward Function (independent):', ['.01 for each normal piece cleared. Special pieces are worth 2x or 3x.'], 'Behavior Parameters:', ['None', 'Observations and actions are defined with a sensor and actuator respectively.'], 'Float Properties: None', 'Benchmark Mean Reward:', ['39.5 for visual observations', '38.5 for vector observations', '34.2 for simple heuristic (pick a random valid move)', '37.0 for greedy heuristic (pick the highest-scoring valid move)']]",
        "Sorter": "![Sorter](images/sorter.png)\n\n['Set-up: The Agent is in a circular room with numbered tiles. The values of the\\ntiles are random between 1 and 20. The tiles present in the room are randomized\\nat each episode. When the Agent visits a tile, it turns green.', 'Goal: Visit all the tiles in ascending order.', 'Agents: The environment contains a single Agent', 'Agent Reward Function:', '-.0002 Existential penalty.', '+1 For visiting the right tile', '-1 For visiting the wrong tile', 'BehaviorParameters:', 'Vector Observations : 4 : 2 floats for Position and 2 floats for orientation', 'Variable Length Observations : Between 1 and 20 entities (one for each tile)\\neach with 22 observations, the first 20 are one hot encoding of the value of the tile,\\nthe 21st and 22nd represent the position of the tile relative to the Agent and the 23rd\\nis `1` if the tile was visited and `0` otherwise.', 'Actions: 3 discrete branched actions corresponding to forward, backward,\\nsideways movement, as well as rotation.', 'Float Properties: One', ['num_tiles: The maximum number of tiles to sample.', ['Default: 2', 'Recommended Minimum: 1', 'Recommended Maximum: 20']], 'Benchmark Mean Reward: Depends on the number of tiles.']",
        "Cooperative Push Block": "![CoopPushBlock](images/cooperative_pushblock.png)\n\n['Set-up: Similar to Push Block, the agents are in an area with blocks that need\\nto be pushed into a goal. Small blocks can be pushed by one agents and are worth\\n+1 value, medium blocks require two agents to push in and are worth +2, and large\\nblocks require all 3 agents to push and are worth +3.', 'Goal: Push all blocks into the goal.', 'Agents: The environment contains three Agents in a Multi Agent Group.', 'Agent Reward Function:', ['-0.0001 Existential penalty, as a group reward.', '+1, +2, or +3 for pushing in a block, added as a group reward.'], 'Behavior Parameters:', ['Observation space: A single Grid Sensor with separate tags for each block size,\\nthe goal, the walls, and other agents.', 'Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise\\nand counterclockwise, move along four different face directions, or do nothing.'], 'Float Properties: None', 'Benchmark Mean Reward: 11 (Group Reward)']",
        "Dungeon Escape": "![DungeonEscape](images/dungeon_escape.png)\n\n['Set-up: Agents are trapped in a dungeon with a dragon, and must work together to escape.\\nTo retrieve the key, one of the agents must find and slay the dragon, sacrificing itself\\nto do so. The dragon will drop a key for the others to use. The other agents can then pick\\nup this key and unlock the dungeon door. If the agents take too long, the dragon will escape\\nthrough a portal and the environment resets.', 'Goal: Unlock the dungeon door and leave.', 'Agents: The environment contains three Agents in a Multi Agent Group and one Dragon, which\\nmoves in a predetermined pattern.', 'Agent Reward Function:', ['+1 group reward if any agent successfully unlocks the door and leaves the dungeon.'], 'Behavior Parameters:', [\"Observation space: A Ray Perception Sensor with separate tags for the walls, other agents,\\nthe door, key, the dragon, and the dragon's portal. A single Vector Observation which indicates\\nwhether the agent is holding a key.\", 'Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise\\nand counterclockwise, move along four different face directions, or do nothing.'], 'Float Properties: None', 'Benchmark Mean Reward: 1.0 (Group Reward)']"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 21]"
    },
    {
      "title": "Using an Environment Executable",
      "description": null,
      "content": {
        "Building the 3DBall environment": "The first step is to open the Unity scene containing the 3D Balance Ball\nenvironment:\n\n['Launch Unity.', 'On the Projects dialog, choose the **Open** option at the top of the window.', 'Using the file dialog that opens, locate the `Project` folder within the\\nML-Agents project and click **Open**.', 'In the **Project** window, navigate to the folder\\n`Assets/ML-Agents/Examples/3DBall/Scenes/`.', 'Double-click the `3DBall` file to load the scene containing the Balance Ball\\nenvironment.']\n\n![3DBall Scene](images/mlagents-Open3DBall.png)\n\nNext, we want the set up scene to play correctly when the training process\nlaunches our environment executable. This means:\n\n['The environment application runs in the background.', 'No dialogs require interaction.', 'The correct scene loads automatically.']\n\n['Open Player Settings (menu: **Edit** > **Project Settings** > **Player**).', 'Under **Resolution and Presentation**:', ['Ensure that **Run in Background** is Checked.', 'Ensure that **Display Resolution Dialog** is set to Disabled. (Note: this\\nsetting may not be available in newer versions of the editor.)'], 'Open the Build Settings window (menu:**File** > **Build Settings**).', 'Choose your target platform.', ['(optional) Select “Development Build” to\\n[log debug messages](https://docs.unity3d.com/Manual/LogFiles.html).'], 'If any scenes are shown in the **Scenes in Build** list, make sure that the\\n3DBall Scene is the only one checked. (If the list is empty, then only the\\ncurrent scene is included in the build).', 'Click **Build**:', ['In the File dialog, navigate to your ML-Agents directory.', 'Assign a file name and click **Save**.', \"(For Windows）With Unity 2018.1, it will ask you to select a folder instead\\nof a file name. Create a subfolder within the root directory and select\\nthat folder to build. In the following steps you will refer to this\\nsubfolder's name as `env_name`. You cannot create builds in the Assets\\nfolder\"]]\n\n![Build Window](images/mlagents-BuildWindow.png)\n\nNow that we have a Unity executable containing the simulation environment, we\ncan interact with it.",
        "Interacting with the Environment": "If you want to use the [Python API](Python-LLAPI.md) to interact with your\nexecutable, you can pass the name of the executable with the argument\n'file_name' of the `UnityEnvironment`. For instance:\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\nenv = UnityEnvironment(file_name=<env_name>)\n```",
        "Training the Environment": [
          "Open a command or terminal window.",
          "Navigate to the folder where you installed the ML-Agents Toolkit. If you\nfollowed the default [installation](Installation.md), then navigate to the\n`ml-agents/` folder.",
          "Run\n`mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier>`\nWhere:",
          [
            "`<trainer-config-file>` is the file path of the trainer configuration yaml",
            "`<env_name>` is the name and path to the executable you exported from Unity\n(without extension)",
            "`<run-identifier>` is a string used to separate the results of different\ntraining runs"
          ]
        ],
        "Training on Headless Server": "To run training on headless server with no graphics rendering support, you need to turn off\ngraphics display in the Unity executable. There are two ways to achieve this:\n\n[\"Pass `--no-graphics` option to mlagents-learn training command. This is equivalent to\\nadding `-nographics -batchmode` to the Unity executable's commandline.\", 'Build your Unity executable with **Server Build**. You can find this setting in Build Settings\\nin the Unity Editor.']\n\nIf you want to train with graphics (for example, using camera and visual observations), you'll\nneed to set up display rendering support (e.g. xvfb) on you server machine. In our\n[Colab Notebook Tutorials](ML-Agents-Toolkit-Documentation.md#python-tutorial-with-google-colab), the Setup section has\nexamples of setting up xvfb on servers."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 22]"
    },
    {
      "title": "Limitations",
      "description": null,
      "content": {
        "Limitations": "See the package-specific Limitations pages:\n\n['[`com.unity.mlagents` Unity package](https://docs.unity3d.com/Packages/com.unity.ml-agents@2.3/manual/index.html#known-limitations)', '[`mlagents` Python package](../ml-agents/)', '[`mlagents_envs` Python package](../ml-agents-envs/)']"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 23]"
    },
    {
      "title": "Ml Agents Envs Readme",
      "description": null,
      "content": {
        "root": [
          "{!../ml-agents-envs/README.md!}"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 24]"
    },
    {
      "title": "ML-Agents Toolkit Overview",
      "description": null,
      "content": {
        "Running Example: Training NPC Behaviors": "To help explain the material and terminology in this page, we'll use a\nhypothetical, running example throughout. We will explore the problem of\ntraining the behavior of a non-playable character (NPC) in a game. (An NPC is a\ngame character that is never controlled by a human player and its behavior is\npre-defined by the game developer.) More specifically, let's assume we're\nbuilding a multi-player, war-themed game in which players control the soldiers.\nIn this game, we have a single NPC who serves as a medic, finding and reviving\nwounded players. Lastly, let us assume that there are two teams, each with five\nplayers and one NPC medic.\n\nThe behavior of a medic is quite complex. It first needs to avoid getting\ninjured, which requires detecting when it is in danger and moving to a safe\nlocation. Second, it needs to be aware of which of its team members are injured\nand require assistance. In the case of multiple injuries, it needs to assess the\ndegree of injury and decide who to help first. Lastly, a good medic will always\nplace itself in a position where it can quickly help its team members. Factoring\nin all of these traits means that at every instance, the medic needs to measure\nseveral attributes of the environment (e.g. position of team members, position\nof enemies, which of its team members are injured and to what degree) and then\ndecide on an action (e.g. hide from enemy fire, move to help one of its\nmembers). Given the large number of settings of the environment and the large\nnumber of actions that the medic can take, defining and implementing such\ncomplex behaviors by hand is challenging and prone to errors.\n\nWith ML-Agents, it is possible to _train_ the behaviors of such NPCs (called\n**Agents**) using a variety of methods. The basic idea is quite simple. We need\nto define three entities at every moment of the game (called **environment**):\n\n[\"**Observations** - what the medic perceives about the environment.\\nObservations can be numeric and/or visual. Numeric observations measure\\nattributes of the environment from the point of view of the agent. For our\\nmedic this would be attributes of the battlefield that are visible to it. For\\nmost interesting environments, an agent will require several continuous\\nnumeric observations. Visual observations, on the other hand, are images\\ngenerated from the cameras attached to the agent and represent what the agent\\nis seeing at that point in time. It is common to confuse an agent's\\nobservation with the environment (or game) **state**. The environment state\\nrepresents information about the entire scene containing all the game\\ncharacters. The agents observation, however, only contains information that\\nthe agent is aware of and is typically a subset of the environment state. For\\nexample, the medic observation cannot include information about an enemy in\\nhiding that the medic is unaware of.\", '**Actions** - what actions the medic can take. Similar to observations,\\nactions can either be continuous or discrete depending on the complexity of\\nthe environment and agent. In the case of the medic, if the environment is a\\nsimple grid world where only their location matters, then a discrete action\\ntaking on one of four values (north, south, east, west) suffices. However, if\\nthe environment is more complex and the medic can move freely then using two\\ncontinuous actions (one for direction and another for speed) is more\\nappropriate.', '**Reward signals** - a scalar value indicating how well the medic is doing.\\nNote that the reward signal need not be provided at every moment, but only\\nwhen the medic performs an action that is good or bad. For example, it can\\nreceive a large negative reward if it dies, a modest positive reward whenever\\nit revives a wounded team member, and a modest negative reward when a wounded\\nteam member dies due to lack of assistance. Note that the reward signal is how\\nthe objectives of the task are communicated to the agent, so they need to be\\nset up in a manner where maximizing reward generates the desired optimal\\nbehavior.']\n\nAfter defining these three entities (the building blocks of a **reinforcement\nlearning task**), we can now _train_ the medic's behavior. This is achieved by\nsimulating the environment for many trials where the medic, over time, learns\nwhat is the optimal action to take for every observation it measures by\nmaximizing its future reward. The key is that by learning the actions that\nmaximize its reward, the medic is learning the behaviors that make it a good\nmedic (i.e. one who saves the most number of lives). In **reinforcement\nlearning** terminology, the behavior that is learned is called a **policy**,\nwhich is essentially a (optimal) mapping from observations to actions. Note that\nthe process of learning a policy through running simulations is called the\n**training phase**, while playing the game with an NPC that is using its learned\npolicy is called the **inference phase**.\n\nThe ML-Agents Toolkit provides all the necessary tools for using Unity as the\nsimulation engine for learning the policies of different objects in a Unity\nenvironment. In the next few sections, we discuss how the ML-Agents Toolkit\nachieves this and what features it provides.",
        "Key Components": "The ML-Agents Toolkit contains five high-level components:\n\n['**Learning Environment** - which contains the Unity scene and all the game\\ncharacters. The Unity scene provides the environment in which agents observe,\\nact, and learn. How you set up the Unity scene to serve as a learning\\nenvironment really depends on your goal. You may be trying to solve a specific\\nreinforcement learning problem of limited scope, in which case you can use the\\nsame scene for both training and for testing trained agents. Or, you may be\\ntraining agents to operate in a complex game or simulation. In this case, it\\nmight be more efficient and practical to create a purpose-built training\\nscene. The ML-Agents Toolkit includes an ML-Agents Unity SDK\\n(`com.unity.ml-agents` package) that enables you to transform any Unity scene\\ninto a learning environment by defining the agents and their behaviors.', '**Python Low-Level API** - which contains a low-level Python interface for\\ninteracting and manipulating a learning environment. Note that, unlike the\\nLearning Environment, the Python API is not part of Unity, but lives outside\\nand communicates with Unity through the Communicator. This API is contained in\\na dedicated `mlagents_envs` Python package and is used by the Python training\\nprocess to communicate with and control the Academy during training. However,\\nit can be used for other purposes as well. For example, you could use the API\\nto use Unity as the simulation engine for your own machine learning\\nalgorithms. See [Python API](Python-LLAPI.md) for more information.', '**External Communicator** - which connects the Learning Environment with the\\nPython Low-Level API. It lives within the Learning Environment.', '**Python Trainers** which contains all the machine learning algorithms that\\nenable training agents. The algorithms are implemented in Python and are part\\nof their own `mlagents` Python package. The package exposes a single\\ncommand-line utility `mlagents-learn` that supports all the training methods\\nand options outlined in this document. The Python Trainers interface solely\\nwith the Python Low-Level API.', '**Gym Wrapper** (not pictured). A common way in which machine learning\\nresearchers interact with simulation environments is via a wrapper provided by\\nOpenAI called [gym](https://github.com/openai/gym). We provide a gym wrapper\\nin the `ml-agents-envs` package and [instructions](Python-Gym-API.md) for using\\nit with existing machine learning algorithms which utilize gym.', '**PettingZoo Wrapper** (not pictured) PettingZoo is python API for\\ninteracting with multi-agent simulation environments that provides a\\ngym-like interface. We provide a PettingZoo wrapper for Unity ML-Agents\\nenvironments in the `ml-agents-envs` package and\\n[instructions](Python-PettingZoo-API.md) for using it with machine learning\\nalgorithms.']\n\n<p align=\"center\">\n  <img src=\"../images/learning_environment_basic.png\"\n       alt=\"Simplified ML-Agents Scene Block Diagram\"\n       width=\"600\"\n       border=\"10\" />\n</p>\n\n_Simplified block diagram of ML-Agents._\n\nThe Learning Environment contains two Unity Components that help organize the\nUnity scene:\n\n['**Agents** - which is attached to a Unity GameObject (any character within a\\nscene) and handles generating its observations, performing the actions it\\nreceives and assigning a reward (positive / negative) when appropriate. Each\\nAgent is linked to a Behavior.', '**Behavior** - defines specific attributes of the agent such as the number of\\nactions that agent can take. Each Behavior is uniquely identified by a\\n`Behavior Name` field. A Behavior can be thought as a function that receives\\nobservations and rewards from the Agent and returns actions. A Behavior can be\\nof one of three types: Learning, Heuristic or Inference. A Learning Behavior\\nis one that is not, yet, defined but about to be trained. A Heuristic Behavior\\nis one that is defined by a hard-coded set of rules implemented in code. An\\nInference Behavior is one that includes a trained Neural Network file. In\\nessence, after a Learning Behavior is trained, it becomes an Inference\\nBehavior.']\n\nEvery Learning Environment will always have one Agent for every character in the\nscene. While each Agent must be linked to a Behavior, it is possible for Agents\nthat have similar observations and actions to have the same Behavior. In our\nsample game, we have two teams each with their own medic. Thus we will have two\nAgents in our Learning Environment, one for each medic, but both of these medics\ncan have the same Behavior. This does not mean that at each instance they will\nhave identical observation and action _values_.\n\n<p align=\"center\">\n  <img src=\"../images/learning_environment_example.png\"\n       alt=\"Example ML-Agents Scene Block Diagram\"\n       width=\"700\"\n       border=\"10\" />\n</p>\n\n_Example block diagram of ML-Agents Toolkit for our sample game._\n\nNote that in a single environment, there can be multiple Agents and multiple\nBehaviors at the same time. For example, if we expanded our game to include tank\ndriver NPCs, then the Agent attached to those characters cannot share its\nBehavior with the Agent linked to the medics (medics and drivers have different\nactions). The Learning Environment through the Academy (not represented in the\ndiagram) ensures that all the Agents are in sync in addition to controlling\nenvironment-wide settings.\n\nLastly, it is possible to exchange data between Unity and Python outside of the\nmachine learning loop through _Side Channels_. One example of using _Side\nChannels_ is to exchange data with Python about _Environment Parameters_. The\nfollowing diagram illustrates the above.\n\n<p align=\"center\">\n  <img src=\"../images/learning_environment_full.png\"\n       alt=\"More Complete Example ML-Agents Scene Block Diagram\"\n       border=\"10\" />\n</p>",
        "Training Modes": {
          "Built-in Training and Inference": {
            "Cross-Platform Inference": "It is important to note that the ML-Agents Toolkit leverages the\n[Inference Engine](Inference-Engine.md) to run the models within a\nUnity scene such that an agent can take the _optimal_ action at each step. Given\nthat Inference Engine supports all Unity runtime platforms, this\nmeans that any model you train with the ML-Agents Toolkit can be embedded into\nyour Unity application that runs on any platform. See our\n[dedicated blog post](https://blogs.unity3d.com/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/)\nfor additional information."
          },
          "Custom Training and Inference": "In the previous mode, the Agents were used for training to generate a PyTorch\nmodel that the Agents can later use. However, any user of the ML-Agents Toolkit\ncan leverage their own algorithms for training. In this case, the behaviors of\nall the Agents in the scene will be controlled within Python. You can even turn\nyour environment into a [gym.](Python-Gym-API.md)\n\nWe do not currently have a tutorial highlighting this mode, but you can learn\nmore about the Python API [here](Python-LLAPI.md)."
        },
        "Flexible Training Scenarios": "While the discussion so-far has mostly focused on training a single agent, with\nML-Agents, several training scenarios are possible. We are excited to see what\nkinds of novel and fun environments the community creates. For those new to\ntraining intelligent agents, below are a few examples that can serve as\ninspiration:\n\n['Single-Agent. A single agent, with its own reward signal. The traditional way\\nof training an agent. An example is any single-player game, such as Chicken.', 'Simultaneous Single-Agent. Multiple independent agents with independent reward\\nsignals with same `Behavior Parameters`. A parallelized version of the\\ntraditional training scenario, which can speed-up and stabilize the training\\nprocess. Helpful when you have multiple versions of the same character in an\\nenvironment who should learn similar behaviors. An example might be training a\\ndozen robot-arms to each open a door simultaneously.', 'Adversarial Self-Play. Two interacting agents with inverse reward signals. In\\ntwo-player games, adversarial self-play can allow an agent to become\\nincreasingly more skilled, while always having the perfectly matched opponent:\\nitself. This was the strategy employed when training AlphaGo, and more\\nrecently used by OpenAI to train a human-beating 1-vs-1 Dota 2 agent.', 'Cooperative Multi-Agent. Multiple interacting agents with a shared reward\\nsignal with same or different `Behavior Parameters`. In this scenario, all\\nagents must work together to accomplish a task that cannot be done alone.\\nExamples include environments where each agent only has access to partial\\ninformation, which needs to be shared in order to accomplish the task or\\ncollaboratively solve a puzzle.', 'Competitive Multi-Agent. Multiple interacting agents with inverse reward\\nsignals with same or different `Behavior Parameters`. In this scenario, agents\\nmust compete with one another to either win a competition, or obtain some\\nlimited set of resources. All team sports fall into this scenario.', 'Ecosystem. Multiple interacting agents with independent reward signals with\\nsame or different `Behavior Parameters`. This scenario can be thought of as\\ncreating a small world in which animals with different goals all interact,\\nsuch as a savanna in which there might be zebras, elephants and giraffes, or\\nan autonomous driving simulation within an urban environment.']",
        "Training Methods: Environment-agnostic": {
          "Deep Reinforcement Learning": {
            "Curiosity for Sparse-reward Environments": "In environments where the agent receives rare or infrequent rewards (i.e.\nsparse-reward), an agent may never receive a reward signal on which to bootstrap\nits training process. This is a scenario where the use of an intrinsic reward\nsignals can be valuable. Curiosity is one such signal which can help the agent\nexplore when extrinsic rewards are sparse.\n\nThe `curiosity` Reward Signal enables the Intrinsic Curiosity Module. This is an\nimplementation of the approach described in\n[Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/)\nby Pathak, et al. It trains two networks:\n\n['an inverse model, which takes the current and next observation of the agent,\\nencodes them, and uses the encoding to predict the action that was taken\\nbetween the observations', 'a forward model, which takes the encoded current observation and action, and\\npredicts the next encoded observation.']\n\nThe loss of the forward model (the difference between the predicted and actual\nencoded observations) is used as the intrinsic reward, so the more surprised the\nmodel is, the larger the reward will be.\n\nFor more information, see our dedicated\n[blog post on the Curiosity module](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/).",
            "RND for Sparse-reward Environments": "Similarly to Curiosity, Random Network Distillation (RND) is useful in sparse or rare\nreward environments as it helps the Agent explore. The RND Module is implemented following\nthe paper [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894).\nRND uses two networks:\n\n['The first is a network with fixed random weights that takes observations as inputs and\\ngenerates an encoding', 'The second is a network with similar architecture that is trained to predict the\\noutputs of the first network and uses the observations the Agent collects as training data.']\n\nThe loss (the squared difference between the predicted and actual encoded observations)\nof the trained model is used as intrinsic reward. The more an Agent visits a state, the\nmore accurate the predictions and the lower the rewards which encourages the Agent to\nexplore new states with higher prediction errors."
          },
          "Imitation Learning": {
            "GAIL (Generative Adversarial Imitation Learning)": "GAIL, or\n[Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476),\nuses an adversarial approach to reward your Agent for behaving similar to a set\nof demonstrations. GAIL can be used with or without environment rewards, and\nworks well when there are a limited number of demonstrations. In this framework,\na second neural network, the discriminator, is taught to distinguish whether an\nobservation/action is from a demonstration or produced by the agent. This\ndiscriminator can then examine a new observation/action and provide it a reward\nbased on how close it believes this new observation/action is to the provided\ndemonstrations.\n\nAt each training step, the agent tries to learn how to maximize this reward.\nThen, the discriminator is trained to better distinguish between demonstrations\nand agent state/actions. In this way, while the agent gets better and better at\nmimicking the demonstrations, the discriminator keeps getting stricter and\nstricter and the agent must try harder to \"fool\" it.\n\nThis approach learns a _policy_ that produces states and actions similar to the\ndemonstrations, requiring fewer demonstrations than direct cloning of the\nactions. In addition to learning purely from demonstrations, the GAIL reward\nsignal can be mixed with an extrinsic reward signal to guide the learning\nprocess.",
            "Behavioral Cloning (BC)": "BC trains the Agent's policy to exactly mimic the actions shown in a set of\ndemonstrations. The BC feature can be enabled on the PPO or SAC trainers. As BC\ncannot generalize past the examples shown in the demonstrations, BC tends to\nwork best when there exists demonstrations for nearly all of the states that the\nagent can experience, or in conjunction with GAIL and/or an extrinsic reward.",
            "Recording Demonstrations": "Demonstrations of agent behavior can be recorded from the Unity Editor or build,\nand saved as assets. These demonstrations contain information on the\nobservations, actions, and rewards for a given agent during the recording\nsession. They can be managed in the Editor, as well as used for training with BC\nand GAIL. See the\n[Designing Agents](Learning-Environment-Design-Agents.md#recording-demonstrations)\npage for more information on how to record demonstrations for your agent."
          },
          "Summary": "To summarize, we provide 3 training methods: BC, GAIL and RL (PPO or SAC) that\ncan be used independently or together:\n\n['BC can be used on its own or as a pre-training step before GAIL and/or RL', 'GAIL can be used with or without extrinsic rewards', 'RL can be used on its own (either PPO or SAC) or in conjunction with BC and/or\\nGAIL.']\n\nLeveraging either BC or GAIL requires recording demonstrations to be provided as\ninput to the training algorithms."
        },
        "Training Methods: Environment-specific": {
          "Training in Competitive Multi-Agent Environments with Self-Play": "ML-Agents provides the functionality to train both symmetric and asymmetric\nadversarial games with\n[Self-Play](https://openai.com/research/competitive-self-play). A symmetric game is\none in which opposing agents are equal in form, function and objective. Examples\nof symmetric games are our Tennis and Soccer example environments. In\nreinforcement learning, this means both agents have the same observation and\nactions and learn from the same reward function and so _they can share the\nsame policy_. In asymmetric games, this is not the case. An example of an\nasymmetric games are Hide and Seek. Agents in these types of games do not always\nhave the same observation or actions and so sharing policy networks is not\nnecessarily ideal.\n\nWith self-play, an agent learns in adversarial games by competing against fixed,\npast versions of its opponent (which could be itself as in symmetric games) to\nprovide a more stable, stationary learning environment. This is compared to\ncompeting against the current, best opponent in every episode, which is\nconstantly changing (because it's learning).\n\nSelf-play can be used with our implementations of both Proximal Policy\nOptimization (PPO) and Soft Actor-Critic (SAC). However, from the perspective of\nan individual agent, these scenarios appear to have non-stationary dynamics\nbecause the opponent is often changing. This can cause significant issues in the\nexperience replay mechanism used by SAC. Thus, we recommend that users use PPO.\nFor further reading on this issue in particular, see the paper\n[Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1702.08887.pdf).\n\nSee our\n[Designing Agents](Learning-Environment-Design-Agents.md#defining-teams-for-multi-agent-scenarios)\npage for more information on setting up teams in your Unity scene. Also, read\nour\n[blog post on self-play](https://blogs.unity3d.com/2020/02/28/training-intelligent-adversaries-using-self-play-with-ml-agents/)\nfor additional information. Additionally, check [ELO Rating System](ELO-Rating-System.md) the method we use to calculate\nthe relative skill level between two players.",
          "Training In Cooperative Multi-Agent Environments with MA-POCA": "![PushBlock with Agents Working Together](images/cooperative_pushblock.png)\n\nML-Agents provides the functionality for training cooperative behaviors - i.e.,\ngroups of agents working towards a common goal, where the success of the individual\nis linked to the success of the whole group. In such a scenario, agents typically receive\nrewards as a group. For instance, if a team of agents wins a game against an opposing\nteam, everyone is rewarded - even agents who did not directly contribute to the win. This\nmakes learning what to do as an individual difficult - you may get a win\nfor doing nothing, and a loss for doing your best.\n\nIn ML-Agents, we provide MA-POCA (MultiAgent POsthumous Credit Assignment), which\nis a novel multi-agent trainer that trains a _centralized critic_, a neural network\nthat acts as a \"coach\" for a whole group of agents. You can then give rewards to the team\nas a whole, and the agents will learn how best to contribute to achieving that reward.\nAgents can _also_ be given rewards individually, and the team will work together to help the\nindividual achieve those goals. During an episode, agents can be added or removed from the group,\nsuch as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die\nor are removed from the game), they will still learn whether their actions contributed\nto the team winning later, enabling agents to take group-beneficial actions even if\nthey result in the individual being removed from the game (i.e., self-sacrifice).\nMA-POCA can also be combined with self-play to train teams of agents to play against each other.\n\nTo learn more about enabling cooperative behaviors for agents in an ML-Agents environment,\ncheck out [this page](Learning-Environment-Design-Agents.md#groups-for-cooperative-scenarios).\n\nTo learn more about MA-POCA, please see our paper\n[On the Use and Misuse of Absorbing States in Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2111.05992.pdf).\nFor further reading, MA-POCA builds on previous work in multi-agent cooperative learning\n([Lowe et al.](https://arxiv.org/abs/1706.02275), [Foerster et al.](https://arxiv.org/pdf/1705.08926.pdf),\namong others) to enable the above use-cases.",
          "Solving Complex Tasks using Curriculum Learning": "Curriculum learning is a way of training a machine learning model where more\ndifficult aspects of a problem are gradually introduced in such a way that the\nmodel is always optimally challenged. This idea has been around for a long time,\nand it is how we humans typically learn. If you imagine any childhood primary\nschool education, there is an ordering of classes and topics. Arithmetic is\ntaught before algebra, for example. Likewise, algebra is taught before calculus.\nThe skills and knowledge learned in the earlier subjects provide a scaffolding\nfor later lessons. The same principle can be applied to machine learning, where\ntraining on easier tasks can provide a scaffolding for harder tasks in the\nfuture.\n\nImagine training the medic to scale a wall to arrive at a wounded team\nmember. The starting point when training a medic to accomplish this task will be\na random policy. That starting policy will have the medic running in circles,\nand will likely never, or very rarely scale the wall properly to revive their\nteam member (and achieve the reward). If we start with a simpler task, such as\nmoving toward an unobstructed team member, then the medic can easily learn to\naccomplish the task. From there, we can slowly add to the difficulty of the task\nby increasing the size of the wall until the medic can complete the initially\nnear-impossible task of scaling the wall. We have included an environment to\ndemonstrate this with ML-Agents, called\n[Wall Jump](Learning-Environment-Examples.md#wall-jump).\n\n![Wall](images/curriculum.png)\n\n_Demonstration of a hypothetical curriculum training scenario in which a\nprogressively taller wall obstructs the path to the goal._\n\n_[**Note**: The example provided above is for instructional purposes, and was\nbased on an early version of the\n[Wall Jump example environment](Learning-Environment-Examples.md). As such, it\nis not possible to directly replicate the results here using that environment.]_\n\nThe ML-Agents Toolkit supports modifying custom environment parameters during\nthe training process to aid in learning. This allows elements of the environment\nrelated to difficulty or complexity to be dynamically adjusted based on training\nprogress. The [Training ML-Agents](Training-ML-Agents.md#curriculum-learning)\npage has more information on defining training curriculums.",
          "Training Robust Agents using Environment Parameter Randomization": "An agent trained on a specific environment, may be unable to generalize to any\ntweaks or variations in the environment (in machine learning this is referred to\nas overfitting). This becomes problematic in cases where environments are\ninstantiated with varying objects or properties. One mechanism to alleviate this\nand train more robust agents that can generalize to unseen variations of the\nenvironment is to expose them to these variations during training. Similar to\nCurriculum Learning, where environments become more difficult as the agent\nlearns, the ML-Agents Toolkit provides a way to randomly sample parameters of\nthe environment during training. We refer to this approach as **Environment\nParameter Randomization**. For those familiar with Reinforcement Learning\nresearch, this approach is based on the concept of\n[Domain Randomization](https://arxiv.org/abs/1703.06907). By using\n[parameter randomization during training](Training-ML-Agents.md#environment-parameter-randomization),\nthe agent can be better suited to adapt (with higher performance) to future\nunseen variations of the environment.\n\n|      Ball scale of 0.5       |      Ball scale of 4       |\n| :--------------------------: | :------------------------: |\n| ![](images/3dball_small.png) | ![](images/3dball_big.png) |\n\n_Example of variations of the 3D Ball environment. The environment parameters\nare `gravity`, `ball_mass` and `ball_scale`._"
        },
        "Model Types": {
          "Learning from Vector Observations": "Whether an agent's observations are ray cast or vector, the ML-Agents Toolkit\nprovides a fully connected neural network model to learn from those\nobservations. At training time you can configure different aspects of this model\nsuch as the number of hidden units and number of layers.",
          "Learning from Cameras using Convolutional Neural Networks": "Unlike other platforms, where the agent’s observation might be limited to a\nsingle vector or image, the ML-Agents Toolkit allows multiple cameras to be used\nfor observations per agent. This enables agents to learn to integrate\ninformation from multiple visual streams. This can be helpful in several\nscenarios such as training a self-driving car which requires multiple cameras\nwith different viewpoints, or a navigational agent which might need to integrate\naerial and first-person visuals. You can learn more about adding visual\nobservations to an agent\n[here](Learning-Environment-Design-Agents.md#multiple-visual-observations).\n\nWhen visual observations are utilized, the ML-Agents Toolkit leverages\nconvolutional neural networks (CNN) to learn from the input images. We offer\nthree network architectures:\n\n['a simple encoder which consists of two convolutional layers', 'the implementation proposed by\\n[Mnih et al.](https://www.nature.com/articles/nature14236), consisting of\\nthree convolutional layers,', 'the [IMPALA Resnet](https://arxiv.org/abs/1802.01561) consisting of three\\nstacked layers, each with two residual blocks, making a much larger network\\nthan the other two.']\n\nThe choice of the architecture depends on the visual complexity of the scene and\nthe available computational resources.",
          "Learning from Variable Length Observations using Attention": "Using the ML-Agents Toolkit, it is possible to have agents learn from a\nvarying number of inputs. To do so, each agent can keep track of a buffer\nof vector observations. At each step, the agent will go through all the\nelements in the buffer and extract information but the elements\nin the buffer can change at every step.\nThis can be useful in scenarios in which the agents must keep track of\na varying number of elements throughout the episode. For example in a game\nwhere an agent must learn to avoid projectiles, but the projectiles can vary in\nnumbers.\n\n![Variable Length Observations Illustrated](images/variable-length-observation-illustrated.png)\n\nYou can learn more about variable length observations\n[here](Learning-Environment-Design-Agents.md#variable-length-observations).\nWhen variable length observations are utilized, the ML-Agents Toolkit\nleverages attention networks to learn from a varying number of entities.\nAgents using attention will ignore entities that are deemed not relevant\nand pay special attention to entities relevant to the current situation\nbased on context.",
          "Memory-enhanced Agents using Recurrent Neural Networks": "Have you ever entered a room to get something and immediately forgot what you\nwere looking for? Don't let that happen to your agents.\n\n![Inspector](images/ml-agents-LSTM.png)\n\nIn some scenarios, agents must learn to remember the past in order to take the\nbest decision. When an agent only has partial observability of the environment,\nkeeping track of past observations can help the agent learn. Deciding what the\nagents should remember in order to solve a task is not easy to do by hand, but\nour training algorithms can learn to keep track of what is important to remember\nwith [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)."
        },
        "Additional Features": "Beyond the flexible training scenarios available, the ML-Agents Toolkit includes\nadditional features which improve the flexibility and interpretability of the\ntraining process.\n\n['**Concurrent Unity Instances** - We enable developers to run concurrent,\\nparallel instances of the Unity executable during training. For certain\\nscenarios, this should speed up training. Check out our dedicated page on\\n[creating a Unity executable](Learning-Environment-Executable.md) and the\\n[Training ML-Agents](Training-ML-Agents.md#training-using-concurrent-unity-instances)\\npage for instructions on how to set the number of concurrent instances.', '**Recording Statistics from Unity** - We enable developers to\\n[record statistics](Learning-Environment-Design.md#recording-statistics) from\\nwithin their Unity environments. These statistics are aggregated and generated\\nduring the training process.', '**Custom Side Channels** - We enable developers to\\n[create custom side channels](Custom-SideChannels.md) to manage data transfer\\nbetween Unity and Python that is unique to their training workflow and/or\\nenvironment.', '**Custom Samplers** - We enable developers to\\n[create custom sampling methods](Training-ML-Agents.md#defining-a-new-sampler-type)\\nfor Environment Parameter Randomization. This enables users to customize this\\ntraining method for their particular environment.']",
        "Summary and Next Steps": "To briefly summarize: The ML-Agents Toolkit enables games and simulations built\nin Unity to serve as the platform for training intelligent agents. It is\ndesigned to enable a large variety of training modes and scenarios and comes\npacked with several features to enable researchers and developers to leverage\n(and enhance) machine learning within Unity.\n\nIn terms of next steps:\n\n['For a walkthrough of running ML-Agents with a simple scene, check out the\\n[Getting Started](Getting-Started.md) guide.', 'For a \"Hello World\" introduction to creating your own Learning Environment,\\ncheck out the\\n[Making a New Learning Environment](Learning-Environment-Create-New.md) page.', 'For an overview on the more complex example environments that are provided in\\nthis toolkit, check out the\\n[Example Environments](Learning-Environment-Examples.md) page.', 'For more information on the various training options available, check out the\\n[Training ML-Agents](Training-ML-Agents.md) page.']"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 25]"
    },
    {
      "title": "Ml Agents Readme",
      "description": null,
      "content": {
        "root": [
          "{!../ml-agents/README.md!}"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 26]"
    },
    {
      "title": "Unity ML-Agents Toolkit Documentation",
      "description": null,
      "content": {
        "Installation & Set-up": [
          "[Installation](Installation.md)",
          [
            "[Using Virtual Environment](Using-Virtual-Environment.md)"
          ]
        ],
        "Getting Started": [
          "[Getting Started Guide](Getting-Started.md)",
          "[ML-Agents Toolkit Overview](ML-Agents-Overview.md)",
          [
            "[Background: Unity](Background-Unity.md)",
            "[Background: Machine Learning](Background-Machine-Learning.md)",
            "[Background: PyTorch](Background-PyTorch.md)"
          ],
          "[Example Environments](Learning-Environment-Examples.md)"
        ],
        "Creating Learning Environments": [
          "[Making a New Learning Environment](Learning-Environment-Create-New.md)",
          "[Designing a Learning Environment](Learning-Environment-Design.md)",
          [
            "[Designing Agents](Learning-Environment-Design-Agents.md)"
          ],
          "[Using an Executable Environment](Learning-Environment-Executable.md)",
          "[ML-Agents Package Settings](Package-Settings.md)"
        ],
        "Training & Inference": [
          "[Training ML-Agents](Training-ML-Agents.md)",
          [
            "[Training Configuration File](Training-Configuration-File.md)",
            "[Using TensorBoard to Observe Training](Using-Tensorboard.md)",
            "[Profiling Trainers](Profiling-Python.md)"
          ],
          "[Inference Engine](Inference-Engine.md)"
        ],
        "Extending ML-Agents": [
          "[Creating Custom Side Channels](Custom-SideChannels.md)",
          "[Creating Custom Samplers for Environment Parameter Randomization](Training-ML-Agents.md#defining-a-new-sampler-type)"
        ],
        "Hugging Face Integration": [
          "[Using Hugging Face to download and upload trained models](Hugging-Face-Integration.md)"
        ],
        "Python Tutorial with Google Colab": [
          "[Using a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_22_docs/colab/Colab_UnityEnvironment_1_Run.ipynb)",
          "[Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_22_docs/colab/Colab_UnityEnvironment_2_Train.ipynb)",
          "[Using Side Channels on a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_22_docs/colab/Colab_UnityEnvironment_3_SideChannel.ipynb)"
        ],
        "Help": [
          "[Migrating from earlier versions of ML-Agents](Migrating.md)",
          "[Frequently Asked Questions](FAQ.md)",
          "[ML-Agents Glossary](Glossary.md)",
          "[Limitations](Limitations.md)"
        ],
        "API Docs": [
          "[API Reference](API-Reference.md)",
          "[Python API Documentation](Python-LLAPI-Documentation.md)",
          "[How to use the Python API](Python-LLAPI.md)",
          "[How to use the Unity Environment Registry](Unity-Environment-Registry.md)",
          "[Wrapping Learning Environment as a Gym (+Baselines/Dopamine Integration)](Python-Gym-API.md)"
        ],
        "Translations": "To make the Unity ML-Agents Toolkit accessible to the global research and Unity\ndeveloper communities, we're attempting to create and maintain translations of\nour documentation. We've started with translating a subset of the documentation\nto one language (Chinese), but we hope to continue translating more pages and to\nother languages. Consequently, we welcome any enhancements and improvements from\nthe community.\n\n['[Chinese](../localized_docs/zh-CN/)', '[Korean](../localized_docs/KR/)']",
        "Deprecated Docs": "We no longer use them ourselves and so they may not be up-to-date. We've decided\nto keep them up just in case they are helpful to you.\n\n['[Windows Anaconda Installation](Installation-Anaconda-Windows.md)', '[Using Docker](Using-Docker.md)', '[Training on the Cloud with Amazon Web Services](Training-on-Amazon-Web-Service.md)', '[Training on the Cloud with Microsoft Azure](Training-on-Microsoft-Azure.md)', '[Using the Video Recorder](https://github.com/Unity-Technologies/video-recorder)']"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 27]"
    },
    {
      "title": "Migrating",
      "description": null,
      "content": {
        "Migrating to Release 11": {
          "Agent virtual method deprecation": [
            "`Agent.CollectDiscreteActionMasks()` was deprecated and should be replaced with `Agent.WriteDiscreteActionMask()`",
            "`Agent.Heuristic(float[])` was deprecated and should be replaced with `Agent.Heuristic(ActionBuffers)`.",
            "`Agent.OnActionReceived(float[])` was deprecated and should be replaced with `Agent.OnActionReceived(ActionBuffers)`.",
            "`Agent.GetAction()` was deprecated and should be replaced with `Agent.GetStoredActionBuffers()`."
          ],
          "BrainParameters field and method deprecation": [
            "`BrainParameters.VectorActionSize` was deprecated; you can now set `BrainParameters.ActionSpec.NumContinuousActions`\nor `BrainParameters.ActionSpec.BranchSizes` instead.",
            "`BrainParameters.VectorActionSpaceType` was deprecated, since both continuous and discrete actions can now be used.",
            "`BrainParameters.NumActions()` was deprecated. Use  `BrainParameters.ActionSpec.NumContinuousActions` and\n`BrainParameters.ActionSpec.NumDiscreteActions` instead."
          ]
        },
        "Migrating from Release 7 to latest": {
          "Important changes": [
            "Some trainer files were moved. If you were using the `TrainerFactory` class, it was moved to\nthe `trainers/trainer` folder.",
            "The `components` folder containing `bc` and `reward_signals` code was moved to the `trainers/tf`\nfolder"
          ],
          "Steps to Migrate": [
            "Replace calls to `from mlagents.trainers.trainer_util import TrainerFactory` to `from mlagents.trainers.trainer import TrainerFactory`",
            "Replace calls to `from mlagents.trainers.trainer_util import handle_existing_directories` to `from mlagents.trainers.directory_utils import validate_existing_directories`",
            "Replace `mlagents.trainers.components` with `mlagents.trainers.tf.components` in your import statements."
          ]
        },
        "Migrating from Release 3 to Release 7": {
          "Important changes": [
            "The Parameter Randomization feature has been merged with the Curriculum feature. It is now possible to specify a sampler\nin the lesson of a Curriculum. Curriculum has been refactored and is now specified at the level of the parameter, not the\nbehavior. More information\n[here](https://github.com/Unity-Technologies/ml-agents/blob/release_22_docs/docs/Training-ML-Agents.md).(#4160)"
          ],
          "Steps to Migrate": [
            "The configuration format for curriculum and parameter randomization has changed. To upgrade your configuration files,\nan upgrade script has been provided. Run `python -m mlagents.trainers.upgrade_config -h` to see the script usage. Note that you will have had to upgrade to/install the current version of ML-Agents before running the script. To update manually:",
            [
              "If your config file used a `parameter_randomization` section, rename that section to `environment_parameters`",
              "If your config file used a `curriculum` section, you will need to rewrite your curriculum with this [format](Training-ML-Agents.md#curriculum)."
            ]
          ]
        },
        "Migrating from Release 1 to Release 3": {
          "Important changes": [
            "Training artifacts (trained models, summaries) are now found under `results/`\ninstead of `summaries/` and `models/`.",
            "Trainer configuration, curriculum configuration, and parameter randomization\nconfiguration have all been moved to a single YAML file. (#3791)",
            "Trainer configuration format has changed, and using a \"default\" behavior name has\nbeen deprecated. (#3936)",
            "`max_step` in the `TerminalStep` and `TerminalSteps` objects was renamed `interrupted`.",
            "On the UnityEnvironment API, `get_behavior_names()` and `get_behavior_specs()` methods were combined into the property `behavior_specs` that contains a mapping from behavior names to behavior spec.",
            "`use_visual` and `allow_multiple_visual_obs` in the `UnityToGymWrapper` constructor\nwere replaced by `allow_multiple_obs` which allows one or more visual observations and\nvector observations to be used simultaneously.",
            "`--save-freq` has been removed from the CLI and is now configurable in the trainer configuration\nfile.",
            "`--lesson` has been removed from the CLI. Lessons will resume when using `--resume`.\nTo start at a different lesson, modify your Curriculum configuration."
          ],
          "Steps to Migrate": [
            "To upgrade your configuration files, an upgrade script has been provided. Run\n`python -m mlagents.trainers.upgrade_config -h` to see the script usage. Note that you will have\nhad to upgrade to/install the current version of ML-Agents before running the script.",
            "To do it manually, copy your `<BehaviorName>` sections from `trainer_config.yaml` into a separate trainer configuration file, under a `behaviors` section.\nThe `default` section is no longer needed. This new file should be specific to your environment, and not contain\nconfigurations for multiple environments (unless they have the same Behavior Names).",
            [
              "You will need to reformat your trainer settings as per the [example](Training-ML-Agents.md).",
              "If your training uses [curriculum](Training-ML-Agents.md#curriculum-learning), move those configurations under a `curriculum` section.",
              "If your training uses [parameter randomization](Training-ML-Agents.md#environment-parameter-randomization), move\nthe contents of the sampler config to `parameter_randomization` in the main trainer configuration."
            ],
            "If you are using `UnityEnvironment` directly, replace `max_step` with `interrupted`\nin the `TerminalStep` and `TerminalSteps` objects.",
            "Replace usage of `get_behavior_names()` and `get_behavior_specs()` in UnityEnvironment with `behavior_specs`.",
            "If you use the `UnityToGymWrapper`, remove `use_visual` and `allow_multiple_visual_obs`\nfrom the constructor and add `allow_multiple_obs = True` if the environment contains either\nboth visual and vector observations or multiple visual observations.",
            "If you were setting `--save-freq` in the CLI, add a `checkpoint_interval` value in your\ntrainer configuration, and set it equal to `save-freq * n_agents_in_scene`."
          ]
        },
        "Migrating from 0.15 to Release 1": {
          "Important changes": [
            "The `MLAgents` C# namespace was renamed to `Unity.MLAgents`, and other nested\nnamespaces were similarly renamed (#3843).",
            "The `--load` and `--train` command-line flags have been deprecated and\nreplaced with `--resume` and `--inference`.",
            "Running with the same `--run-id` twice will now throw an error.",
            "The `play_against_current_self_ratio` self-play trainer hyperparameter has\nbeen renamed to `play_against_latest_model_ratio`",
            "Removed the multi-agent gym option from the gym wrapper. For multi-agent\nscenarios, use the [Low Level Python API](Python-LLAPI.md).",
            "The low level Python API has changed. You can look at the document\n[Low Level Python API documentation](Python-LLAPI.md) for more information. If\nyou use `mlagents-learn` for training, this should be a transparent change.",
            "The obsolete `Agent` methods `GiveModel`, `Done`, `InitializeAgent`,\n`AgentAction` and `AgentReset` have been removed.",
            "The signature of `Agent.Heuristic()` was changed to take a `float[]` as a\nparameter, instead of returning the array. This was done to prevent a common\nsource of error where users would return arrays of the wrong size.",
            "The SideChannel API has changed (#3833, #3660) :",
            [
              "Introduced the `SideChannelManager` to register, unregister and access side\nchannels.",
              "`EnvironmentParameters` replaces the default `FloatProperties`. You can\naccess the `EnvironmentParameters` with\n`Academy.Instance.EnvironmentParameters` on C#. If you were previously\ncreating a `UnityEnvironment` in python and passing it a\n`FloatPropertiesChannel`, create an `EnvironmentParametersChannel` instead.",
              "`SideChannel.OnMessageReceived` is now a protected method (was public)",
              "SideChannel IncomingMessages methods now take an optional default argument,\nwhich is used when trying to read more data than the message contains.",
              "Added a feature to allow sending stats from C# environments to TensorBoard\n(and other python StatsWriters). To do this from your code, use\n`Academy.Instance.StatsRecorder.Add(key, value)`(#3660)"
            ],
            "`num_updates` and `train_interval` for SAC have been replaced with\n`steps_per_update`.",
            "The `UnityEnv` class from the `gym-unity` package was renamed\n`UnityToGymWrapper` and no longer creates the `UnityEnvironment`. Instead, the\n`UnityEnvironment` must be passed as input to the constructor of\n`UnityToGymWrapper`",
            "Public fields and properties on several classes were renamed to follow Unity's\nC# style conventions. All public fields and properties now use \"PascalCase\"\ninstead of \"camelCase\"; for example, `Agent.maxStep` was renamed to\n`Agent.MaxStep`. For a full list of changes, see the pull request. (#3828)",
            "`WriteAdapter` was renamed to `ObservationWriter`. (#3834)"
          ],
          "Steps to Migrate": [
            "In C# code, replace `using MLAgents` with `using Unity.MLAgents`. Replace\nother nested namespaces such as `using MLAgents.Sensors` with\n`using Unity.MLAgents.Sensors`",
            "Replace the `--load` flag with `--resume` when calling `mlagents-learn`, and\ndon't use the `--train` flag as training will happen by default. To run\nwithout training, use `--inference`.",
            "To force-overwrite files from a pre-existing run, add the `--force`\ncommand-line flag.",
            "The Jupyter notebooks have been removed from the repository.",
            "If your Agent class overrides `Heuristic()`, change the signature to\n`public override void Heuristic(float[] actionsOut)` and assign values to\n`actionsOut` instead of returning an array.",
            "If you used `SideChannels` you must:",
            [
              "Replace `Academy.FloatProperties` with\n`Academy.Instance.EnvironmentParameters`.",
              "`Academy.RegisterSideChannel` and `Academy.UnregisterSideChannel` were\nremoved. Use `SideChannelManager.RegisterSideChannel` and\n`SideChannelManager.UnregisterSideChannel` instead."
            ],
            "Set `steps_per_update` to be around equal to the number of agents in your\nenvironment, times `num_updates` and divided by `train_interval`.",
            "Replace `UnityEnv` with `UnityToGymWrapper` in your code. The constructor no\nlonger takes a file name as input but a fully constructed `UnityEnvironment`\ninstead.",
            "Update uses of \"camelCase\" fields and properties to \"PascalCase\"."
          ]
        },
        "Migrating from 0.14 to 0.15": {
          "Important changes": [
            "The `Agent.CollectObservations()` virtual method now takes as input a\n`VectorSensor` sensor as argument. The `Agent.AddVectorObs()` methods were\nremoved.",
            "The `SetMask` was renamed to `SetMask` method must now be called on the\n`DiscreteActionMasker` argument of the `CollectDiscreteActionMasks` virtual\nmethod.",
            "We consolidated our API for `DiscreteActionMasker`. `SetMask` takes two\narguments : the branch index and the list of masked actions for that branch.",
            "The `Monitor` class has been moved to the Examples Project. (It was prone to\nerrors during testing)",
            "The `MLAgents.Sensors` namespace has been introduced. All sensors classes are\npart of the `MLAgents.Sensors` namespace.",
            "The `MLAgents.SideChannels` namespace has been introduced. All side channel\nclasses are part of the `MLAgents.SideChannels` namespace.",
            "The interface for `RayPerceptionSensor.PerceiveStatic()` was changed to take\nan input class and write to an output class, and the method was renamed to\n`Perceive()`.",
            "The `SetMask` method must now be called on the `DiscreteActionMasker` argument\nof the `CollectDiscreteActionMasks` method.",
            "The method `GetStepCount()` on the Agent class has been replaced with the\nproperty getter `StepCount`",
            "The `--multi-gpu` option has been removed temporarily.",
            "`AgentInfo.actionMasks` has been renamed to `AgentInfo.discreteActionMasks`.",
            "`BrainParameters` and `SpaceType` have been removed from the public API",
            "`BehaviorParameters` have been removed from the public API.",
            "`DecisionRequester` has been made internal (you can still use the\nDecisionRequesterComponent from the inspector). `RepeatAction` was renamed\n`TakeActionsBetweenDecisions` for clarity.",
            "The following methods in the `Agent` class have been renamed. The original\nmethod names will be removed in a later release:",
            [
              "`InitializeAgent()` was renamed to `Initialize()`",
              "`AgentAction()` was renamed to `OnActionReceived()`",
              "`AgentReset()` was renamed to `OnEpisodeBegin()`",
              "`Done()` was renamed to `EndEpisode()`",
              "`GiveModel()` was renamed to `SetModel()`"
            ],
            "The `IFloatProperties` interface has been removed.",
            "The interface for SideChannels was changed:",
            [
              "In C#, `OnMessageReceived` now takes a `IncomingMessage` argument, and\n`QueueMessageToSend` takes an `OutgoingMessage` argument.",
              "In python, `on_message_received` now takes a `IncomingMessage` argument, and\n`queue_message_to_send` takes an `OutgoingMessage` argument.",
              "Automatic stepping for Academy is now controlled from the\nAutomaticSteppingEnabled property."
            ]
          ],
          "Steps to Migrate": [
            "Add the `using MLAgents.Sensors;` in addition to `using MLAgents;` on top of\nyour Agent's script.",
            "Replace your Agent's implementation of `CollectObservations()` with\n`CollectObservations(VectorSensor sensor)`. In addition, replace all calls to\n`AddVectorObs()` with `sensor.AddObservation()` or\n`sensor.AddOneHotObservation()` on the `VectorSensor` passed as argument.",
            "Replace your calls to `SetActionMask` on your Agent to\n`DiscreteActionMasker.SetActionMask` in `CollectDiscreteActionMasks`.",
            "If you call `RayPerceptionSensor.PerceiveStatic()` manually, add your inputs\nto a `RayPerceptionInput`. To get the previous float array output, iterate\nthrough `RayPerceptionOutput.rayOutputs` and call\n`RayPerceptionOutput.RayOutput.ToFloatArray()`.",
            "Replace all calls to `Agent.GetStepCount()` with `Agent.StepCount`",
            "We strongly recommend replacing the following methods with their new\nequivalent as they will be removed in a later release:",
            [
              "`InitializeAgent()` to `Initialize()`",
              "`AgentAction()` to `OnActionReceived()`",
              "`AgentReset()` to `OnEpisodeBegin()`",
              "`Done()` to `EndEpisode()`",
              "`GiveModel()` to `SetModel()`"
            ],
            "Replace `IFloatProperties` variables with `FloatPropertiesChannel` variables.",
            "If you implemented custom `SideChannels`, update the signatures of your\nmethods, and add your data to the `OutgoingMessage` or read it from the\n`IncomingMessage`.",
            "Replace calls to Academy.EnableAutomaticStepping()/DisableAutomaticStepping()\nwith Academy.AutomaticSteppingEnabled = true/false."
          ]
        },
        "Migrating from 0.13 to 0.14": {
          "Important changes": [
            "The `UnitySDK` folder has been split into a Unity Package\n(`com.unity.ml-agents`) and an examples project (`Project`). Please follow the\n[Installation Guide](Installation.md) to get up and running with this new repo\nstructure.",
            "Several changes were made to how agents are reset and marked as done:",
            [
              "Calling `Done()` on the Agent will now reset it immediately and call the\n`AgentReset` virtual method. (This is to simplify the previous logic in\nwhich the Agent had to wait for the next `EnvironmentStep` to reset)",
              "The \"Reset on Done\" setting in AgentParameters was removed; this is now\neffectively always true. `AgentOnDone` virtual method on the Agent has been\nremoved."
            ],
            "The `Decision Period` and `On Demand decision` checkbox have been removed from\nthe Agent. On demand decision is now the default (calling `RequestDecision` on\nthe Agent manually.)",
            "The Academy class was changed to a singleton, and its virtual methods were\nremoved.",
            "Trainer steps are now counted per-Agent, not per-environment as in previous\nversions. For instance, if you have 10 Agents in the scene, 20 environment\nsteps now corresponds to 200 steps as printed in the terminal and in\nTensorboard.",
            "Curriculum config files are now YAML formatted and all curricula for a\ntraining run are combined into a single file.",
            "The `--num-runs` command-line option has been removed from `mlagents-learn`.",
            "Several fields on the Agent were removed or made private in order to simplify\nthe interface.",
            [
              "The `agentParameters` field of the Agent has been removed. (Contained only\n`maxStep` information)",
              "`maxStep` is now a public field on the Agent. (Was moved from\n`agentParameters`)",
              "The `Info` field of the Agent has been made private. (Was only used\ninternally and not meant to be modified outside of the Agent)",
              "The `GetReward()` method on the Agent has been removed. (It was being\nconfused with `GetCumulativeReward()`)",
              "The `AgentAction` struct no longer contains a `value` field. (Value\nestimates were not set during inference)",
              "The `GetValueEstimate()` method on the Agent has been removed.",
              "The `UpdateValueAction()` method on the Agent has been removed."
            ],
            "The deprecated `RayPerception3D` and `RayPerception2D` classes were removed,\nand the `legacyHitFractionBehavior` argument was removed from\n`RayPerceptionSensor.PerceiveStatic()`.",
            "RayPerceptionSensor was inconsistent in how it handle scale on the Agent's\ntransform. It now scales the ray length and sphere size for casting as the\ntransform's scale changes."
          ],
          "Steps to Migrate": [
            "Follow the instructions on how to install the `com.unity.ml-agents` package\ninto your project in the [Installation Guide](Installation.md).",
            "If your Agent implemented `AgentOnDone` and did not have the checkbox\n`Reset On Done` checked in the inspector, you must call the code that was in\n`AgentOnDone` manually.",
            "If you give your Agent a reward or penalty at the end of an episode (e.g. for\nreaching a goal or falling off of a platform), make sure you call\n`AddReward()` or `SetReward()` _before_ calling `Done()`. Previously, the\norder didn't matter.",
            "If you were not using `On Demand Decision` for your Agent, you **must** add a\n`DecisionRequester` component to your Agent GameObject and set its\n`Decision Period` field to the old `Decision Period` of the Agent.",
            "If you have a class that inherits from Academy:",
            [
              "If the class didn't override any of the virtual methods and didn't store any\nadditional data, you can just remove the old script from the scene.",
              "If the class had additional data, create a new MonoBehaviour and store the\ndata in the new MonoBehaviour instead.",
              "If the class overrode the virtual methods, create a new MonoBehaviour and\nmove the logic to it:",
              [
                "Move the InitializeAcademy code to MonoBehaviour.Awake",
                "Move the AcademyStep code to MonoBehaviour.FixedUpdate",
                "Move the OnDestroy code to MonoBehaviour.OnDestroy.",
                "Move the AcademyReset code to a new method and add it to the\nAcademy.OnEnvironmentReset action."
              ]
            ],
            "Multiply `max_steps` and `summary_freq` in your `trainer_config.yaml` by the\nnumber of Agents in the scene.",
            "Combine curriculum configs into a single file. See\n[the WallJump curricula](https://github.com/Unity-Technologies/ml-agents/blob/0.14.1/config/curricula/wall_jump.yaml) for an example of\nthe new curriculum config format. A tool like https://www.json2yaml.com may be\nuseful to help with the conversion.",
            "If you have a model trained which uses RayPerceptionSensor and has non-1.0\nscale in the Agent's transform, it must be retrained."
          ]
        },
        "Migrating from ML-Agents Toolkit v0.12.0 to v0.13.0": {
          "Important changes": [
            "The low level Python API has changed. You can look at the document\n[Low Level Python API documentation](Python-LLAPI.md) for more information. This\nshould only affect you if you're writing a custom trainer; if you use\n`mlagents-learn` for training, this should be a transparent change.",
            [
              "`reset()` on the Low-Level Python API no longer takes a `train_mode`\nargument. To modify the performance/speed of the engine, you must use an\n`EngineConfigurationChannel`",
              "`reset()` on the Low-Level Python API no longer takes a `config` argument.\n`UnityEnvironment` no longer has a `reset_parameters` field. To modify float\nproperties in the environment, you must use a `FloatPropertiesChannel`. For\nmore information, refer to the\n[Low Level Python API documentation](Python-LLAPI.md)"
            ],
            "`CustomResetParameters` are now removed.",
            "The Academy no longer has a `Training Configuration` nor\n`Inference Configuration` field in the inspector. To modify the configuration\nfrom the Low-Level Python API, use an `EngineConfigurationChannel`. To modify\nit during training, use the new command line arguments `--width`, `--height`,\n`--quality-level`, `--time-scale` and `--target-frame-rate` in\n`mlagents-learn`.",
            "The Academy no longer has a `Default Reset Parameters` field in the inspector.\nThe Academy class no longer has a `ResetParameters`. To access shared float\nproperties with Python, use the new `FloatProperties` field on the Academy.",
            "Offline Behavioral Cloning has been removed. To learn from demonstrations, use\nthe GAIL and Behavioral Cloning features with either PPO or SAC.",
            "`mlagents.envs` was renamed to `mlagents_envs`. The previous repo layout\ndepended on [PEP420](https://www.python.org/dev/peps/pep-0420/), which caused\nproblems with some of our tooling such as mypy and pylint.",
            "The official version of Unity ML-Agents supports is now 2022.3 LTS. If you run\ninto issues, please consider deleting your library folder and reopening your\nprojects. You will need to install the Sentis package into your project in\norder to ML-Agents to compile correctly."
          ],
          "Steps to Migrate": [
            "If you had a custom `Training Configuration` in the Academy inspector, you\nwill need to pass your custom configuration at every training run using the\nnew command line arguments `--width`, `--height`, `--quality-level`,\n`--time-scale` and `--target-frame-rate`.",
            "If you were using `--slow` in `mlagents-learn`, you will need to pass your old\n`Inference Configuration` of the Academy inspector with the new command line\narguments `--width`, `--height`, `--quality-level`, `--time-scale` and\n`--target-frame-rate` instead.",
            "Any imports from `mlagents.envs` should be replaced with `mlagents_envs`."
          ]
        },
        "Migrating from ML-Agents Toolkit v0.11.0 to v0.12.0": {
          "Important Changes": [
            "Text actions and observations, and custom action and observation protos have\nbeen removed.",
            "RayPerception3D and RayPerception2D are marked deprecated, and will be removed\nin a future release. They can be replaced by RayPerceptionSensorComponent3D\nand RayPerceptionSensorComponent2D.",
            "The `Use Heuristic` checkbox in Behavior Parameters has been replaced with a\n`Behavior Type` dropdown menu. This has the following options:",
            [
              "`Default` corresponds to the previous unchecked behavior, meaning that\nAgents will train if they connect to a python trainer, otherwise they will\nperform inference.",
              "`Heuristic Only` means the Agent will always use the `Heuristic()` method.\nThis corresponds to having \"Use Heuristic\" selected in 0.11.0.",
              "`Inference Only` means the Agent will always perform inference."
            ],
            "ML-Agents was upgraded to use Sentis 1.2.0-exp.2 and is installed via the package manager."
          ],
          "Steps to Migrate": [
            "We [fixed a bug](https://github.com/Unity-Technologies/ml-agents/pull/2823) in\n`RayPerception3d.Perceive()` that was causing the `endOffset` to be used\nincorrectly. However this may produce different behavior from previous\nversions if you use a non-zero `startOffset`. To reproduce the old behavior,\nyou should increase the value of `endOffset` by `startOffset`. You can\nverify your raycasts are performing as expected in scene view using the debug\nrays.",
            "If you use RayPerception3D, replace it with RayPerceptionSensorComponent3D\n(and similarly for 2D). The settings, such as ray angles and detectable tags,\nare configured on the component now. RayPerception3D would contribute\n`(# of rays) * (# of tags + 2)` to the State Size in Behavior Parameters, but\nthis is no longer necessary, so you should reduce the State Size by this\namount. Making this change will require retraining your model, since the\nobservations that RayPerceptionSensorComponent3D produces are different from\nthe old behavior.",
            "If you see messages such as\n`The type or namespace 'Sentis' could not be found` or\n`The type or namespace 'Google' could not be found`, you will need to\n[install the Sentis preview package](Installation.md#package-installation)."
          ]
        },
        "Migrating from ML-Agents Toolkit v0.10 to v0.11.0": {
          "Important Changes": {
            "Steps to Migrate": [
              "In order to be able to train, make sure both your ML-Agents Python package and\nUnitySDK code come from the v0.11 release. Training will not work, for\nexample, if you update the ML-Agents Python package, and only update the API\nVersion in UnitySDK.",
              "If your Agents used visual observations, you must add a CameraSensorComponent\ncorresponding to each old Camera in the Agent's camera list (and similarly for\nRenderTextures).",
              "Since Brain ScriptableObjects have been removed, you will need to delete all\nthe Brain ScriptableObjects from your `Assets` folder. Then, add a\n`Behavior Parameters` component to each `Agent` GameObject. You will then need\nto complete the fields on the new `Behavior Parameters` component with the\nBrainParameters of the old Brain."
            ]
          }
        },
        "Migrating from ML-Agents Toolkit v0.9 to v0.10": {
          "Important Changes": {
            "Steps to Migrate": [
              "`UnitySDK/Assets/ML-Agents/Scripts/Communicator.cs` and its class\n`Communicator` have been renamed to\n`UnitySDK/Assets/ML-Agents/Scripts/ICommunicator.cs` and `ICommunicator`\nrespectively.",
              "The `SpaceType` Enums `discrete`, and `continuous` have been renamed to\n`Discrete` and `Continuous`.",
              "We have removed the `Done` call as well as the capacity to set `Max Steps` on\nthe Academy. Therefore an AcademyReset will never be triggered from C# (only\nfrom Python). If you want to reset the simulation after a fixed number of\nsteps, or when an event in the simulation occurs, we recommend looking at our\nmulti-agent example environments (such as FoodCollector). In our examples,\ngroups of Agents can be reset through an \"Area\" that can reset groups of\nAgents.",
              "The import for `mlagents.envs.UnityEnvironment` was removed. If you are using\nthe Python API, change `from mlagents_envs import UnityEnvironment` to\n`from mlagents_envs.environment import UnityEnvironment`."
            ]
          }
        },
        "Migrating from ML-Agents Toolkit v0.8 to v0.9": {
          "Important Changes": {
            "Steps to Migrate": [
              "If you were overriding any of these following parameters in your config file,\nremove them from the top-level config and follow the steps below:",
              [
                "`gamma`: Define a new `extrinsic` reward signal and set it's `gamma` to your\nnew gamma.",
                "`use_curiosity`, `curiosity_strength`, `curiosity_enc_size`: Define a\n`curiosity` reward signal and set its `strength` to `curiosity_strength`,\nand `encoding_size` to `curiosity_enc_size`. Give it the same `gamma` as\nyour `extrinsic` signal to mimic previous behavior."
              ],
              "TensorBoards generated when running multiple environments in v0.8 are not\ncomparable to those generated in v0.9 in terms of step count. Multiply your\nv0.8 step count by `num_envs` for an approximate comparison. You may need to\nchange `max_steps` in your config as appropriate as well."
            ]
          }
        },
        "Migrating from ML-Agents Toolkit v0.7 to v0.8": {
          "Important Changes": {
            "Steps to Migrate": [
              "If you are installing via PyPI, there is no change.",
              "If you intend to make modifications to `ml-agents` or `ml-agents-envs` please\ncheck the Installing for Development in the\n[Installation documentation](Installation.md)."
            ]
          }
        },
        "Migrating from ML-Agents Toolkit v0.6 to v0.7": {
          "Important Changes": {
            "Steps to Migrate": [
              "Make sure to remove the `ENABLE_TENSORFLOW` flag in your Unity Project\nsettings"
            ]
          }
        },
        "Migrating from ML-Agents Toolkit v0.5 to v0.6": {
          "Important Changes": {
            "Steps to Migrate": [
              "To update a scene from v0.5 to v0.6, you must:",
              [
                "Remove the `Brain` GameObjects in the scene. (Delete all of the Brain\nGameObjects under Academy in the scene.)",
                "Create new `Brain` Scriptable Objects using `Assets -> Create -> ML-Agents`\nfor each type of the Brain you plan to use, and put the created files under\na folder called Brains within your project.",
                "Edit their `Brain Parameters` to be the same as the parameters used in the\n`Brain` GameObjects.",
                "Agents have a `Brain` field in the Inspector, you need to drag the\nappropriate Brain ScriptableObject in it.",
                "The Academy has a `Broadcast Hub` field in the inspector, which is list of\nbrains used in the scene. To train or control your Brain from the\n`mlagents-learn` Python script, you need to drag the relevant\n`LearningBrain` ScriptableObjects used in your scene into entries into this\nlist."
              ]
            ]
          }
        },
        "Migrating from ML-Agents Toolkit v0.4 to v0.5": {
          "Important": [
            "The Unity project `unity-environment` has been renamed `UnitySDK`.",
            "The `python` folder has been renamed to `ml-agents`. It now contains two\npackages, `mlagents.env` and `mlagents.trainers`. `mlagents.env` can be used\nto interact directly with a Unity environment, while `mlagents.trainers`\ncontains the classes for training agents.",
            "The supported Unity version has changed from `2017.1 or later` to\n`2017.4 or later`. 2017.4 is an LTS (Long Term Support) version that helps us\nmaintain good quality and support. Earlier versions of Unity might still work,\nbut you may encounter an\n[error](FAQ.md#instance-of-corebraininternal-couldnt-be-created) listed here."
          ],
          "Unity API": [
            "Discrete Actions now use [branches](https://arxiv.org/abs/1711.08946). You can\nnow specify concurrent discrete actions. You will need to update the Brain\nParameters in the Brain Inspector in all your environments that use discrete\nactions. Refer to the\n[discrete action documentation](Learning-Environment-Design-Agents.md#discrete-action-space)\nfor more information."
          ],
          "Python API": [
            "In order to run a training session, you can now use the command\n`mlagents-learn` instead of `python3 learn.py` after installing the `mlagents`\npackages. This change is documented\n[here](Training-ML-Agents.md#training-with-mlagents-learn). For example, if we\npreviously ran",
            "```\npython3 learn.py 3DBall --train\n```",
            "from the `python` subdirectory (which is changed to `ml-agents` subdirectory\nin v0.5), we now run",
            "```\nmlagents-learn config/trainer_config.yaml --env=3DBall --train\n```",
            "from the root directory where we installed the ML-Agents Toolkit.",
            "It is now required to specify the path to the yaml trainer configuration file\nwhen running `mlagents-learn`. For an example trainer configuration file, see\n[trainer_config.yaml](https://github.com/Unity-Technologies/ml-agents/blob/0.5.0a/config/trainer_config.yaml). An example of passing a\ntrainer configuration to `mlagents-learn` is shown above.",
            "The environment name is now passed through the `--env` option.",
            "Curriculum learning has been changed. In summary:",
            [
              "Curriculum files for the same environment must now be placed into a folder.\nEach curriculum file should be named after the Brain whose curriculum it\nspecifies.",
              "`min_lesson_length` now specifies the minimum number of episodes in a lesson\nand affects reward thresholding.",
              "It is no longer necessary to specify the `Max Steps` of the Academy to use\ncurriculum learning."
            ]
          ]
        },
        "Migrating from ML-Agents Toolkit v0.3 to v0.4": {
          "Unity API": [
            "`using MLAgents;` needs to be added in all of the C# scripts that use\nML-Agents."
          ],
          "Python API": [
            "We've changed some of the Python packages dependencies in requirement.txt\nfile. Make sure to run `pip3 install -e .` within your `ml-agents/python`\nfolder to update your Python packages."
          ]
        },
        "Migrating from ML-Agents Toolkit v0.2 to v0.3": {
          "Important": [
            "The ML-Agents Toolkit is no longer compatible with Python 2."
          ],
          "Python Training": [
            "The training script `ppo.py` and `PPO.ipynb` Python notebook have been\nreplaced with a single `learn.py` script as the launching point for training\nwith ML-Agents. For more information on using `learn.py`, see\n[here](Training-ML-Agents.md#training-with-mlagents-learn).",
            "Hyperparameters for training Brains are now stored in the\n`trainer_config.yaml` file. For more information on using this file, see\n[here](Training-ML-Agents.md#training-configurations)."
          ],
          "Unity API": [
            "Modifications to an Agent's rewards must now be done using either\n`AddReward()` or `SetReward()`.",
            "Setting an Agent to done now requires the use of the `Done()` method.",
            "`CollectStates()` has been replaced by `CollectObservations()`, which now no\nlonger returns a list of floats.",
            "To collect observations, call `AddVectorObs()` within `CollectObservations()`.\nNote that you can call `AddVectorObs()` with floats, integers, lists and\narrays of floats, Vector3 and Quaternions.",
            "`AgentStep()` has been replaced by `AgentAction()`.",
            "`WaitTime()` has been removed.",
            "The `Frame Skip` field of the Academy is replaced by the Agent's\n`Decision Frequency` field, enabling the Agent to make decisions at different\nfrequencies.",
            "The names of the inputs in the Internal Brain have been changed. You must\nreplace `state` with `vector_observation` and `observation` with\n`visual_observation`. In addition, you must remove the `epsilon` placeholder."
          ],
          "Semantics": "In order to more closely align with the terminology used in the Reinforcement\nLearning field, and to be more descriptive, we have changed the names of some of\nthe concepts used in ML-Agents. The changes are highlighted in the table below.\n\n| Old - v0.2 and earlier | New - v0.3 and later |\n| ---------------------- | -------------------- |\n| State                  | Vector Observation   |\n| Observation            | Visual Observation   |\n| Action                 | Vector Action        |\n| N/A                    | Text Observation     |\n| N/A                    | Text Action          |"
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 28]"
    },
    {
      "title": "ML-Agents Package Settings",
      "description": null,
      "content": {
        "Create Custom Settings": "In order to to use your own settings for your project, you'll need to create a settings asset.\n\nYou can do this by clicking the `Create Settings Asset` button or clicking the gear on the top right and select `New Settings Asset...`.\nThe asset file can be placed anywhere in the `Asset/` folder in your project.\nAfter Creating the settings asset, you'll be able to modify the settings for your project and your settings will be saved in the asset.\n\n![Package Settings](images/package-settings.png)",
        "Multiple Custom Settings for Different Scenarios": "You can create multiple settings assets in one project.\n\nBy clicking the gear on the top right you'll see all available settings listed in the drop-down menu to choose from.\n\nThis allows you to create different settings for different scenarios. For example, you can create two\nseparate settings for training and inference, and specify which one you want to use according to what you're currently running.\n\n![Multiple Settings](images/multiple-settings.png)"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 29]"
    },
    {
      "title": "Profiling in Python",
      "description": null,
      "content": {
        "Adding Profiling": "There are two ways to indicate code should be included in profiling. The\nsimplest way is to add the `@timed` decorator to a function or method of\ninterested.\n\n```\nclass TrainerController:\n    # ....\n    @timed\n    def advance(self, env: EnvManager) -> int:\n        # do stuff\n```\n\nYou can also used the `hierarchical_timer` context manager.\n\n```\nwith hierarchical_timer(\"communicator.exchange\"):\n    outputs = self.communicator.exchange(step_input)\n```\n\nThe context manager may be easier than the `@timed` decorator for profiling\ndifferent parts of a large function, or profiling calls to abstract methods that\nmight not use decorator.",
        "Output": {
          "Parallel execution": {
            "Subprocesses": "For code that executes in multiple processes (for example,\nSubprocessEnvManager), we periodically send the timer information back to the\n\"main\" process, aggregate the timers there, and flush them in the subprocess.\nNote that (depending on the number of processes) this can result in timers where\nthe total time may exceed the parent's total time. This is analogous to the\ndifference between \"real\" and \"user\" values reported from the unix `time`\ncommand. In the timer output, blocks that were run in parallel are indicated by\nthe `is_parallel` flag.",
            "Threads": "Timers currently use `time.perf_counter()` to track time spent, which may not\ngive accurate results for multiple threads. If this is problematic, set\n`threaded: false` in your trainer configuration."
          }
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 30]"
    },
    {
      "title": "Unity Ml-Agents Custom trainers Plugin",
      "description": null,
      "content": {
        "Overview": "Model-free RL algorithms generally fall into two broad categories: on-policy and off-policy. On-policy algorithms perform updates based on data gathered from the current policy. Off-policy algorithms learn a Q function from a buffer of previous data, then use this Q function to make decisions. Off-policy algorithms have three key benefits in the context of ML-Agents: They tend to use fewer samples than on-policy as they can pull and re-use data from the buffer many times. They allow player demonstrations to be inserted in-line with RL data into the buffer, enabling new ways of doing imitation learning by streaming player data.\n\nTo add new custom trainers to ML-agents, you would need to create a new python package.\nTo give you an idea of how to structure your package, we have created a [mlagents_trainer_plugin](../ml-agents-trainer-plugin) package ourselves as an\nexample, with implementation of `A2c` and `DQN` algorithms. You would need a `setup.py` file to list extra requirements and\nregister the new RL algorithm in ml-agents ecosystem and be able to call `mlagents-learn` CLI with your customized\nconfiguration.\n\n```\n├── mlagents_trainer_plugin\n│    ├── __init__.py\n│    ├── a2c\n│    │    ├── __init__.py\n│    │    ├── a2c_3DBall.yaml\n│    │    ├── a2c_optimizer.py\n│    │    └── a2c_trainer.py\n│    └── dqn\n│        ├── __init__.py\n│        ├── dqn_basic.yaml\n│        ├── dqn_optimizer.py\n│        └── dqn_trainer.py\n└── setup.py\n```",
        "Installation and Execution": "If you haven't already, follow the [installation instructions](Installation.md). Once you have the `ml-agents-env` and `ml-agents` packages you can install the plugin package. From the repository's root directory install `ml-agents-trainer-plugin` (or replace with the name of your plugin folder).\n\n```\npip3 install -e <./ml-agents-trainer-plugin>\n```\n\nFollowing the previous installations your package is added as an entrypoint and you can use a config file with new\ntrainers:\n\n```\nmlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id <run-id-name>\n--env <env-executable>\n```",
        "Tutorial": "Here’s a step-by-step [tutorial](Tutorial-Custom-Trainer-Plugin.md) on how to write a setup file and extend ml-agents trainers, optimizers, and\nhyperparameter settings.To extend ML-agents classes see references on\n[trainers](Python-On-Off-Policy-Trainer-Documentation.md) and [Optimizer](Python-Optimizer-Documentation.md)."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 31]"
    },
    {
      "title": "mlagents\\_envs.envs.unity\\_gym\\_env",
      "description": null,
      "content": {
        "UnityGymException Objects": "```\nclass UnityGymException(error.Error)\n```\n\nAny error related to the gym wrapper of ml-agents.\n\n<a name=\"mlagents_envs.envs.unity_gym_env.UnityToGymWrapper\"></a>",
        "UnityToGymWrapper Objects": "```\nclass UnityToGymWrapper(gym.Env)\n```\n\nProvides Gym wrapper for Unity Learning Environments.\n\n<a name=\"mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.__init__\"></a>\n\n\\_\\_init\\_\\_\n\n```\n | __init__(unity_env: BaseEnv, uint8_visual: bool = False, flatten_branched: bool = False, allow_multiple_obs: bool = False, action_space_seed: Optional[int] = None)\n```\n\nEnvironment initialization\n\n**Arguments**:\n\n['`unity_env`: The Unity BaseEnv to be wrapped in the gym. Will be closed when the UnityToGymWrapper closes.', '`uint8_visual`: Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0).', '`flatten_branched`: If True, turn branched discrete action spaces into a Discrete space rather than\\nMultiDiscrete.', '`allow_multiple_obs`: If True, return a list of np.ndarrays as observations with the first elements\\ncontaining the visual observations and the last element containing the array of vector observations.\\nIf False, returns a single np.ndarray containing either only a single visual observation or the array of\\nvector observations.', '`action_space_seed`: If non-None, will be used to set the random seed on created gym.Space instances.']\n\n<a name=\"mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.reset\"></a>\n\nreset\n\n```\n | reset() -> Union[List[np.ndarray], np.ndarray]\n```\n\nResets the state of the environment and returns an initial observation.\nReturns: observation (object/list): the initial observation of the\nspace.\n\n<a name=\"mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.step\"></a>\n\nstep\n\n```\n | step(action: List[Any]) -> GymStepResult\n```\n\nRun one timestep of the environment's dynamics. When end of\nepisode is reached, you are responsible for calling `reset()`\nto reset this environment's state.\nAccepts an action and returns a tuple (observation, reward, done, info).\n\n**Arguments**:\n\n['`action` _object/list_ - an action provided by the environment']\n\n**Returns**:\n\n[\"`observation` _object/list_ - agent's observation of the current environment\\nreward (float/list) : amount of reward returned after previous action\", '`done` _boolean/list_ - whether the episode has ended.', '`info` _dict_ - contains auxiliary diagnostic information.']\n\n<a name=\"mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.render\"></a>\n\nrender\n\n```\n | render(mode=\"rgb_array\")\n```\n\nReturn the latest visual observations.\nNote that it will not render a new frame of the environment.\n\n<a name=\"mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.close\"></a>\n\nclose\n\n```\n | close() -> None\n```\n\nOverride _close in your subclass to perform any necessary cleanup.\nEnvironments will automatically close() themselves when\ngarbage collected or when the program exits.\n\n<a name=\"mlagents_envs.envs.unity_gym_env.UnityToGymWrapper.seed\"></a>\n\nseed\n\n```\n | seed(seed: Any = None) -> None\n```\n\nSets the seed for this env's random number generator(s).\nCurrently not implemented.\n\n<a name=\"mlagents_envs.envs.unity_gym_env.ActionFlattener\"></a>",
        "ActionFlattener Objects": "```\nclass ActionFlattener()\n```\n\nFlattens branched discrete action spaces into single-branch discrete action spaces.\n\n<a name=\"mlagents_envs.envs.unity_gym_env.ActionFlattener.__init__\"></a>\n\n\\_\\_init\\_\\_\n\n```\n | __init__(branched_action_space)\n```\n\nInitialize the flattener.\n\n**Arguments**:\n\n['`branched_action_space`: A List containing the sizes of each branch of the action\\nspace, e.g. [2,3,3] for three branches with size 2, 3, and 3 respectively.']\n\n<a name=\"mlagents_envs.envs.unity_gym_env.ActionFlattener.lookup_action\"></a>\n\nlookup\\_action\n\n```\n | lookup_action(action)\n```\n\nConvert a scalar discrete action into a unique set of branched actions.\n\n**Arguments**:\n\n['`action`: A scalar value representing one of the discrete actions.']\n\n**Returns**:\n\nThe List containing the branched actions."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 32]"
    },
    {
      "title": "Unity ML-Agents Gym Wrapper",
      "description": null,
      "content": {
        "Installation": "The gym wrapper is part of the `mlagents_envs` package. Please refer to the\n[mlagents_envs installation instructions](ML-Agents-Envs-README.md).",
        "Using the Gym Wrapper": "The gym interface is available from `gym_unity.envs`. To launch an environment\nfrom the root of the project repository use:\n\n```\nfrom mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n\nenv = UnityToGymWrapper(unity_env, uint8_visual, flatten_branched, allow_multiple_obs)\n```\n\n['`unity_env` refers to the Unity environment to be wrapped.', '`uint8_visual` refers to whether to output visual observations as `uint8`\\nvalues (0-255). Many common Gym environments (e.g. Atari) do this. By default\\nthey will be floats (0.0-1.0). Defaults to `False`.', '`flatten_branched` will flatten a branched discrete action space into a Gym\\nDiscrete. Otherwise, it will be converted into a MultiDiscrete. Defaults to\\n`False`.', '`allow_multiple_obs` will return a list of observations. The first elements\\ncontain the visual observations and the last element contains the array of\\nvector observations. If False the environment returns a single array (containing\\na single visual observations, if present, otherwise the vector observation).\\nDefaults to `False`.', '`action_space_seed` is the optional seed for action sampling. If non-None, will\\nbe used to set the random seed on created gym.Space instances.']\n\nThe returned environment `env` will function as a gym.",
        "Limitations": [
          "It is only possible to use an environment with a **single** Agent.",
          "By default, the first visual observation is provided as the `observation`, if\npresent. Otherwise, vector observations are provided. You can receive all\nvisual and vector observations by using the `allow_multiple_obs=True` option in\nthe gym parameters. If set to `True`, you will receive a list of `observation`\ninstead of only one.",
          "The `TerminalSteps` or `DecisionSteps` output from the environment can still\nbe accessed from the `info` provided by `env.step(action)`.",
          "Stacked vector observations are not supported.",
          "Environment registration for use with `gym.make()` is currently not supported.",
          "Calling env.render() will not render a new frame of the environment. It will\nreturn the latest visual observation if using visual observations."
        ],
        "Running OpenAI Baselines Algorithms": {
          "Example - DQN Baseline": "In order to train an agent to play the `GridWorld` environment using the\nBaselines DQN algorithm, you first need to install the baselines package using\npip:\n\n```\npip install git+git://github.com/openai/baselines\n```\n\nNext, create a file called `train_unity.py`. Then create an `/envs/` directory\nand build the environment to that directory. For more information on\nbuilding Unity environments, see\n[here](../docs/Learning-Environment-Executable.md). Note that because of\nlimitations of the DQN baseline, the environment must have a single visual\nobservation, a single discrete action and a single Agent in the scene.\nAdd the following code to the `train_unity.py` file:\n\n```\nimport gym\n\nfrom baselines import deepq\nfrom baselines import logger\n\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n\n\ndef main():\n  unity_env = UnityEnvironment( < path - to - environment >)\n  env = UnityToGymWrapper(unity_env, uint8_visual=True)\n  logger.configure('./logs')  # Change to log in a different directory\n  act = deepq.learn(\n    env,\n    \"cnn\",  # For visual inputs\n    lr=2.5e-4,\n    total_timesteps=1000000,\n    buffer_size=50000,\n    exploration_fraction=0.05,\n    exploration_final_eps=0.1,\n    print_freq=20,\n    train_freq=5,\n    learning_starts=20000,\n    target_network_update_freq=50,\n    gamma=0.99,\n    prioritized_replay=False,\n    checkpoint_freq=1000,\n    checkpoint_path='./logs',  # Change to save model in a different directory\n    dueling=True\n  )\n  print(\"Saving model to unity_model.pkl\")\n  act.save(\"unity_model.pkl\")\n\n\nif __name__ == '__main__':\n  main()\n```\n\nTo start the training process, run the following from the directory containing\n`train_unity.py`:\n\n```\npython -m train_unity\n```",
          "Other Algorithms": "Other algorithms in the Baselines repository can be run using scripts similar to\nthe examples from the baselines package. In most cases, the primary changes\nneeded to use a Unity environment are to import `UnityToGymWrapper`, and to\nreplace the environment creation code, typically `gym.make()`, with a call to\n`UnityToGymWrapper(unity_environment)` passing the environment as input.\n\nA typical rule of thumb is that for vision-based environments, modification\nshould be done to Atari training scripts, and for vector observation\nenvironments, modification should be done to Mujoco scripts.\n\nSome algorithms will make use of `make_env()` or `make_mujoco_env()` functions.\nYou can define a similar function for Unity environments. An example of such a\nmethod using the PPO2 baseline:\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs import UnityToGymWrapper\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom baselines.bench import Monitor\nfrom baselines import logger\nimport baselines.ppo2.ppo2 as ppo2\n\nimport os\n\ntry:\n  from mpi4py import MPI\nexcept ImportError:\n  MPI = None\n\n\ndef make_unity_env(env_directory, num_env, visual, start_index=0):\n  \"\"\"\n  Create a wrapped, monitored Unity environment.\n  \"\"\"\n\n  def make_env(rank, use_visual=True):  # pylint: disable=C0111\n    def _thunk():\n      unity_env = UnityEnvironment(env_directory, base_port=5000 + rank)\n      env = UnityToGymWrapper(unity_env, uint8_visual=True)\n      env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))\n      return env\n\n    return _thunk\n\n  if visual:\n    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])\n  else:\n    rank = MPI.COMM_WORLD.Get_rank() if MPI else 0\n    return DummyVecEnv([make_env(rank, use_visual=False)])\n\n\ndef main():\n  env = make_unity_env( < path - to - environment >, 4, True)\n  ppo2.learn(\n    network=\"mlp\",\n    env=env,\n    total_timesteps=100000,\n    lr=1e-3,\n  )\n\n\nif __name__ == '__main__':\n  main()\n```"
        },
        "Run Google Dopamine Algorithms": {
          "Adapting Dopamine's Scripts": "First, open `dopamine/atari/run_experiment.py`. Alternatively, copy the entire\n`atari` folder, and name it something else (e.g. `unity`). If you choose the\ncopy approach, be sure to change the package names in the import statements in\n`train.py` to your new directory.\n\nWithin `run_experiment.py`, we will need to make changes to which environment is\ninstantiated, just as in the Baselines example. At the top of the file, insert\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs import UnityToGymWrapper\n```\n\nto import the Gym Wrapper. Navigate to the `create_atari_environment` method in\nthe same file, and switch to instantiating a Unity environment by replacing the\nmethod with the following code.\n\n```\n    game_version = 'v0' if sticky_actions else 'v4'\n    full_game_name = '{}NoFrameskip-{}'.format(game_name, game_version)\n    unity_env = UnityEnvironment(<path-to-environment>)\n    env = UnityToGymWrapper(unity_env, uint8_visual=True)\n    return env\n```\n\n`<path-to-environment>` is the path to your built Unity executable. For more\ninformation on building Unity environments, see\n[here](../docs/Learning-Environment-Executable.md), and note the Limitations\nsection below.\n\nNote that we are not using the preprocessor from Dopamine, as it uses many\nAtari-specific calls. Furthermore, frame-skipping can be done from within Unity,\nrather than on the Python side.",
          "Limitations": "Since Dopamine is designed around variants of DQN, it is only compatible with\ndiscrete action spaces, and specifically the Discrete Gym space. For\nenvironments that use branched discrete action spaces, you can enable the\n`flatten_branched` parameter in `UnityToGymWrapper`, which treats each\ncombination of branched actions as separate actions.\n\nFurthermore, when building your environments, ensure that your Agent is using\nvisual observations with greyscale enabled, and that the dimensions of the\nvisual observations is 84 by 84 (matches the parameter found in `dqn_agent.py`\nand `rainbow_agent.py`). Dopamine's agents currently do not automatically adapt\nto the observation dimensions or number of channels.",
          "Hyperparameters": "The hyperparameters provided by Dopamine are tailored to the Atari games, and\nyou will likely need to adjust them for ML-Agents environments. Here is a sample\n`dopamine/agents/rainbow/configs/rainbow.gin` file that is known to work with\na simple GridWorld.\n\n```\nimport dopamine.agents.rainbow.rainbow_agent\nimport dopamine.unity.run_experiment\nimport dopamine.replay_memory.prioritized_replay_buffer\nimport gin.tf.external_configurables\n\nRainbowAgent.num_atoms = 51\nRainbowAgent.stack_size = 1\nRainbowAgent.vmax = 10.\nRainbowAgent.gamma = 0.99\nRainbowAgent.update_horizon = 3\nRainbowAgent.min_replay_history = 20000  # agent steps\nRainbowAgent.update_period = 5\nRainbowAgent.target_update_period = 50  # agent steps\nRainbowAgent.epsilon_train = 0.1\nRainbowAgent.epsilon_eval = 0.01\nRainbowAgent.epsilon_decay_period = 50000  # agent steps\nRainbowAgent.replay_scheme = 'prioritized'\nRainbowAgent.tf_device = '/cpu:0'  # use '/cpu:*' for non-GPU version\nRainbowAgent.optimizer = @tf.train.AdamOptimizer()\n\ntf.train.AdamOptimizer.learning_rate = 0.00025\ntf.train.AdamOptimizer.epsilon = 0.0003125\n\nRunner.game_name = \"Unity\" # any name can be used here\nRunner.sticky_actions = False\nRunner.num_iterations = 200\nRunner.training_steps = 10000  # agent steps\nRunner.evaluation_steps = 500  # agent steps\nRunner.max_steps_per_episode = 27000  # agent steps\n\nWrappedPrioritizedReplayBuffer.replay_capacity = 1000000\nWrappedPrioritizedReplayBuffer.batch_size = 32\n```\n\nThis example assumed you copied `atari` to a separate folder named `unity`.\nReplace `unity` in `import dopamine.unity.run_experiment` with the folder you\ncopied your `run_experiment.py` and `trainer.py` files to. If you directly\nmodified the existing files, then use `atari` here.",
          "Starting a Run": "You can now run Dopamine as you would normally:\n\n```\npython -um dopamine.unity.train \\\n  --agent_name=rainbow \\\n  --base_dir=/tmp/dopamine \\\n  --gin_files='dopamine/agents/rainbow/configs/rainbow.gin'\n```\n\nAgain, we assume that you've copied `atari` into a separate folder. Remember to\nreplace `unity` with the directory you copied your files into. If you edited the\nAtari files directly, this should be `atari`.",
          "Example: GridWorld": "As a baseline, here are rewards over time for the three algorithms provided with\nDopamine as run on the GridWorld example environment. All Dopamine (DQN,\nRainbow, C51) runs were done with the same epsilon, epsilon decay, replay\nhistory, training steps, and buffer settings as specified above. Note that the\nfirst 20000 steps are used to pre-fill the training buffer, and no learning\nhappens.\n\nWe provide results from our PPO implementation and the DQN from Baselines as\nreference. Note that all runs used the same greyscale GridWorld as Dopamine. For\nPPO, `num_layers` was set to 2, and all other hyperparameters are the default\nfor GridWorld in `config/ppo/GridWorld.yaml`. For Baselines DQN, the provided\nhyperparameters in the previous section are used. Note that Baselines implements\ncertain features (e.g. dueling-Q) that are not enabled in Dopamine DQN.\n\n![Dopamine on GridWorld](images/dopamine_gridworld_plot.png)"
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 33]"
    },
    {
      "title": "mlagents\\_envs.base\\_env",
      "description": null,
      "content": {
        "DecisionStep Objects": "```\nclass DecisionStep(NamedTuple)\n```\n\nContains the data a single Agent collected since the last\nsimulation step.\n\n['obs is a list of numpy arrays observations collected by the agent.', 'reward is a float. Corresponds to the rewards collected by the agent\\nsince the last simulation step.', 'agent_id is an int and an unique identifier for the corresponding Agent.', 'action_mask is an optional list of one dimensional array of booleans.\\nOnly available when using multi-discrete actions.\\nEach array corresponds to an action branch. Each array contains a mask\\nfor each action of the branch. If true, the action is not available for\\nthe agent during this simulation step.']\n\n<a name=\"mlagents_envs.base_env.DecisionSteps\"></a>",
        "DecisionSteps Objects": "```\nclass DecisionSteps(Mapping)\n```\n\nContains the data a batch of similar Agents collected since the last\nsimulation step. Note that all Agents do not necessarily have new\ninformation to send at each simulation step. Therefore, the ordering of\nagents and the batch size of the DecisionSteps are not fixed across\nsimulation steps.\n\n['obs is a list of numpy arrays observations collected by the batch of\\nagent. Each obs has one extra dimension compared to DecisionStep: the\\nfirst dimension of the array corresponds to the batch size of the batch.', 'reward is a float vector of length batch size. Corresponds to the\\nrewards collected by each agent since the last simulation step.', 'agent_id is an int vector of length batch size containing unique\\nidentifier for the corresponding Agent. This is used to track Agents\\nacross simulation steps.', 'action_mask is an optional list of two dimensional array of booleans.\\nOnly available when using multi-discrete actions.\\nEach array corresponds to an action branch. The first dimension of each\\narray is the batch size and the second contains a mask for each action of\\nthe branch. If true, the action is not available for the agent during\\nthis simulation step.']\n\n<a name=\"mlagents_envs.base_env.DecisionSteps.agent_id_to_index\"></a>\n\nagent\\_id\\_to\\_index\n\n```\n | @property\n | agent_id_to_index() -> Dict[AgentId, int]\n```\n\n**Returns**:\n\nA Dict that maps agent_id to the index of those agents in\nthis DecisionSteps.\n\n<a name=\"mlagents_envs.base_env.DecisionSteps.__getitem__\"></a>\n\n\\_\\_getitem\\_\\_\n\n```\n | __getitem__(agent_id: AgentId) -> DecisionStep\n```\n\nreturns the DecisionStep for a specific agent.\n\n**Arguments**:\n\n['`agent_id`: The id of the agent']\n\n**Returns**:\n\nThe DecisionStep\n\n<a name=\"mlagents_envs.base_env.DecisionSteps.empty\"></a>\n\nempty\n\n```\n | @staticmethod\n | empty(spec: \"BehaviorSpec\") -> \"DecisionSteps\"\n```\n\nReturns an empty DecisionSteps.\n\n**Arguments**:\n\n['`spec`: The BehaviorSpec for the DecisionSteps']\n\n<a name=\"mlagents_envs.base_env.TerminalStep\"></a>",
        "TerminalStep Objects": "```\nclass TerminalStep(NamedTuple)\n```\n\nContains the data a single Agent collected when its episode ended.\n\n['obs is a list of numpy arrays observations collected by the agent.', 'reward is a float. Corresponds to the rewards collected by the agent\\nsince the last simulation step.', 'interrupted is a bool. Is true if the Agent was interrupted since the last\\ndecision step. For example, if the Agent reached the maximum number of steps for\\nthe episode.', 'agent_id is an int and an unique identifier for the corresponding Agent.']\n\n<a name=\"mlagents_envs.base_env.TerminalSteps\"></a>",
        "TerminalSteps Objects": "```\nclass TerminalSteps(Mapping)\n```\n\nContains the data a batch of Agents collected when their episode\nterminated. All Agents present in the TerminalSteps have ended their\nepisode.\n\n['obs is a list of numpy arrays observations collected by the batch of\\nagent. Each obs has one extra dimension compared to DecisionStep: the\\nfirst dimension of the array corresponds to the batch size of the batch.', 'reward is a float vector of length batch size. Corresponds to the\\nrewards collected by each agent since the last simulation step.', 'interrupted is an array of booleans of length batch size. Is true if the\\nassociated Agent was interrupted since the last decision step. For example, if the\\nAgent reached the maximum number of steps for the episode.', 'agent_id is an int vector of length batch size containing unique\\nidentifier for the corresponding Agent. This is used to track Agents\\nacross simulation steps.']\n\n<a name=\"mlagents_envs.base_env.TerminalSteps.agent_id_to_index\"></a>\n\nagent\\_id\\_to\\_index\n\n```\n | @property\n | agent_id_to_index() -> Dict[AgentId, int]\n```\n\n**Returns**:\n\nA Dict that maps agent_id to the index of those agents in\nthis TerminalSteps.\n\n<a name=\"mlagents_envs.base_env.TerminalSteps.__getitem__\"></a>\n\n\\_\\_getitem\\_\\_\n\n```\n | __getitem__(agent_id: AgentId) -> TerminalStep\n```\n\nreturns the TerminalStep for a specific agent.\n\n**Arguments**:\n\n['`agent_id`: The id of the agent']\n\n**Returns**:\n\nobs, reward, done, agent_id and optional action mask for a\nspecific agent\n\n<a name=\"mlagents_envs.base_env.TerminalSteps.empty\"></a>\n\nempty\n\n```\n | @staticmethod\n | empty(spec: \"BehaviorSpec\") -> \"TerminalSteps\"\n```\n\nReturns an empty TerminalSteps.\n\n**Arguments**:\n\n['`spec`: The BehaviorSpec for the TerminalSteps']\n\n<a name=\"mlagents_envs.base_env.ActionTuple\"></a>",
        "ActionTuple Objects": "```\nclass ActionTuple(_ActionTupleBase)\n```\n\nAn object whose fields correspond to actions of different types.\nContinuous and discrete actions are numpy arrays of type float32 and\nint32, respectively and are type checked on construction.\nDimensions are of (n_agents, continuous_size) and (n_agents, discrete_size),\nrespectively. Note, this also holds when continuous or discrete size is\nzero.\n\n<a name=\"mlagents_envs.base_env.ActionTuple.discrete_dtype\"></a>\n\ndiscrete\\_dtype\n\n```\n | @property\n | discrete_dtype() -> np.dtype\n```\n\nThe dtype of a discrete action.\n\n<a name=\"mlagents_envs.base_env.ActionSpec\"></a>",
        "ActionSpec Objects": "```\nclass ActionSpec(NamedTuple)\n```\n\nA NamedTuple containing utility functions and information about the action spaces\nfor a group of Agents under the same behavior.\n\n['num_continuous_actions is an int corresponding to the number of floats which\\nconstitute the action.', 'discrete_branch_sizes is a Tuple of int where each int corresponds to\\nthe number of discrete actions available to the agent on an independent action branch.']\n\n<a name=\"mlagents_envs.base_env.ActionSpec.is_discrete\"></a>\n\nis\\_discrete\n\n```\n | is_discrete() -> bool\n```\n\nReturns true if this Behavior uses discrete actions\n\n<a name=\"mlagents_envs.base_env.ActionSpec.is_continuous\"></a>\n\nis\\_continuous\n\n```\n | is_continuous() -> bool\n```\n\nReturns true if this Behavior uses continuous actions\n\n<a name=\"mlagents_envs.base_env.ActionSpec.discrete_size\"></a>\n\ndiscrete\\_size\n\n```\n | @property\n | discrete_size() -> int\n```\n\nReturns a an int corresponding to the number of discrete branches.\n\n<a name=\"mlagents_envs.base_env.ActionSpec.empty_action\"></a>\n\nempty\\_action\n\n```\n | empty_action(n_agents: int) -> ActionTuple\n```\n\nGenerates ActionTuple corresponding to an empty action (all zeros)\nfor a number of agents.\n\n**Arguments**:\n\n['`n_agents`: The number of agents that will have actions generated']\n\n<a name=\"mlagents_envs.base_env.ActionSpec.random_action\"></a>\n\nrandom\\_action\n\n```\n | random_action(n_agents: int) -> ActionTuple\n```\n\nGenerates ActionTuple corresponding to a random action (either discrete\nor continuous) for a number of agents.\n\n**Arguments**:\n\n['`n_agents`: The number of agents that will have actions generated']\n\n<a name=\"mlagents_envs.base_env.ActionSpec.create_continuous\"></a>\n\ncreate\\_continuous\n\n```\n | @staticmethod\n | create_continuous(continuous_size: int) -> \"ActionSpec\"\n```\n\nCreates an ActionSpec that is homogenously continuous\n\n<a name=\"mlagents_envs.base_env.ActionSpec.create_discrete\"></a>\n\ncreate\\_discrete\n\n```\n | @staticmethod\n | create_discrete(discrete_branches: Tuple[int]) -> \"ActionSpec\"\n```\n\nCreates an ActionSpec that is homogenously discrete\n\n<a name=\"mlagents_envs.base_env.ActionSpec.create_hybrid\"></a>\n\ncreate\\_hybrid\n\n```\n | @staticmethod\n | create_hybrid(continuous_size: int, discrete_branches: Tuple[int]) -> \"ActionSpec\"\n```\n\nCreates a hybrid ActionSpace\n\n<a name=\"mlagents_envs.base_env.DimensionProperty\"></a>",
        "DimensionProperty Objects": "```\nclass DimensionProperty(IntFlag)\n```\n\nThe dimension property of a dimension of an observation.\n\n<a name=\"mlagents_envs.base_env.DimensionProperty.UNSPECIFIED\"></a>\n\nUNSPECIFIED\n\nNo properties specified.\n\n<a name=\"mlagents_envs.base_env.DimensionProperty.NONE\"></a>\n\nNONE\n\nNo Property of the observation in that dimension. Observation can be processed with\nFully connected networks.\n\n<a name=\"mlagents_envs.base_env.DimensionProperty.TRANSLATIONAL_EQUIVARIANCE\"></a>\n\nTRANSLATIONAL\\_EQUIVARIANCE\n\nMeans it is suitable to do a convolution in this dimension.\n\n<a name=\"mlagents_envs.base_env.DimensionProperty.VARIABLE_SIZE\"></a>\n\nVARIABLE\\_SIZE\n\nMeans that there can be a variable number of observations in this dimension.\nThe observations are unordered.\n\n<a name=\"mlagents_envs.base_env.ObservationType\"></a>",
        "ObservationType Objects": "```\nclass ObservationType(Enum)\n```\n\nAn Enum which defines the type of information carried in the observation\nof the agent.\n\n<a name=\"mlagents_envs.base_env.ObservationType.DEFAULT\"></a>\n\nDEFAULT\n\nObservation information is generic.\n\n<a name=\"mlagents_envs.base_env.ObservationType.GOAL_SIGNAL\"></a>\n\nGOAL\\_SIGNAL\n\nObservation contains goal information for current task.\n\n<a name=\"mlagents_envs.base_env.ObservationSpec\"></a>",
        "ObservationSpec Objects": "```\nclass ObservationSpec(NamedTuple)\n```\n\nA NamedTuple containing information about the observation of Agents.\n\n[\"shape is a Tuple of int : It corresponds to the shape of\\nan observation's dimensions.\", 'dimension_property is a Tuple of DimensionProperties flag, one flag for each\\ndimension.', 'observation_type is an enum of ObservationType.']\n\n<a name=\"mlagents_envs.base_env.BehaviorSpec\"></a>",
        "BehaviorSpec Objects": "```\nclass BehaviorSpec(NamedTuple)\n```\n\nA NamedTuple containing information about the observation and action\nspaces for a group of Agents under the same behavior.\n\n[\"observation_specs is a List of ObservationSpec NamedTuple containing\\ninformation about the information of the Agent's observations such as their shapes.\\nThe order of the ObservationSpec is the same as the order of the observations of an\\nagent.\", 'action_spec is an ActionSpec NamedTuple.']\n\n<a name=\"mlagents_envs.base_env.BaseEnv\"></a>",
        "BaseEnv Objects": "```\nclass BaseEnv(ABC)\n```\n\n<a name=\"mlagents_envs.base_env.BaseEnv.step\"></a>\n\nstep\n\n```\n | @abstractmethod\n | step() -> None\n```\n\nSignals the environment that it must move the simulation forward\nby one step.\n\n<a name=\"mlagents_envs.base_env.BaseEnv.reset\"></a>\n\nreset\n\n```\n | @abstractmethod\n | reset() -> None\n```\n\nSignals the environment that it must reset the simulation.\n\n<a name=\"mlagents_envs.base_env.BaseEnv.close\"></a>\n\nclose\n\n```\n | @abstractmethod\n | close() -> None\n```\n\nSignals the environment that it must close.\n\n<a name=\"mlagents_envs.base_env.BaseEnv.behavior_specs\"></a>\n\nbehavior\\_specs\n\n```\n | @property\n | @abstractmethod\n | behavior_specs() -> MappingType[str, BehaviorSpec]\n```\n\nReturns a Mapping from behavior names to behavior specs.\nAgents grouped under the same behavior name have the same action and\nobservation specs, and are expected to behave similarly in the\nenvironment.\nNote that new keys can be added to this mapping as new policies are instantiated.\n\n<a name=\"mlagents_envs.base_env.BaseEnv.set_actions\"></a>\n\nset\\_actions\n\n```\n | @abstractmethod\n | set_actions(behavior_name: BehaviorName, action: ActionTuple) -> None\n```\n\nSets the action for all of the agents in the simulation for the next\nstep. The Actions must be in the same order as the order received in\nthe DecisionSteps.\n\n**Arguments**:\n\n['`behavior_name`: The name of the behavior the agents are part of', '`action`: ActionTuple tuple of continuous and/or discrete action.\\nActions are np.arrays with dimensions  (n_agents, continuous_size) and\\n(n_agents, discrete_size), respectively.']\n\n<a name=\"mlagents_envs.base_env.BaseEnv.set_action_for_agent\"></a>\n\nset\\_action\\_for\\_agent\n\n```\n | @abstractmethod\n | set_action_for_agent(behavior_name: BehaviorName, agent_id: AgentId, action: ActionTuple) -> None\n```\n\nSets the action for one of the agents in the simulation for the next\nstep.\n\n**Arguments**:\n\n['`behavior_name`: The name of the behavior the agent is part of', '`agent_id`: The id of the agent the action is set for', '`action`: ActionTuple tuple of continuous and/or discrete action\\nActions are np.arrays with dimensions  (1, continuous_size) and\\n(1, discrete_size), respectively. Note, this initial dimensions of 1 is because\\nthis action is meant for a single agent.']\n\n<a name=\"mlagents_envs.base_env.BaseEnv.get_steps\"></a>\n\nget\\_steps\n\n```\n | @abstractmethod\n | get_steps(behavior_name: BehaviorName) -> Tuple[DecisionSteps, TerminalSteps]\n```\n\nRetrieves the steps of the agents that requested a step in the\nsimulation.\n\n**Arguments**:\n\n['`behavior_name`: The name of the behavior the agents are part of']\n\n**Returns**:\n\nA tuple containing :\n\n['A DecisionSteps NamedTuple containing the observations,\\nthe rewards, the agent ids and the action masks for the Agents\\nof the specified behavior. These Agents need an action this step.', 'A TerminalSteps NamedTuple containing the observations,\\nrewards, agent ids and interrupted flags of the agents that had their\\nepisode terminated last step.']\n\n<a name=\"mlagents_envs.environment\"></a>"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 34]"
    },
    {
      "title": "Unity ML-Agents Python Low Level API",
      "description": null,
      "content": {
        "mlagents_envs": "The ML-Agents Toolkit Low Level API is a Python API for controlling the\nsimulation loop of an environment or game built with Unity. This API is used by\nthe training algorithms inside the ML-Agent Toolkit, but you can also write your\nown Python programs using this API.\n\nThe key objects in the Python API include:\n\n['**UnityEnvironment** — the main interface between the Unity application and\\nyour code. Use UnityEnvironment to start and control a simulation or training\\nsession.', '**BehaviorName** - is a string that identifies a behavior in the simulation.', '**AgentId** - is an `int` that serves as unique identifier for Agents in the\\nsimulation.', '**DecisionSteps** — contains the data from Agents belonging to the same\\n\"Behavior\" in the simulation, such as observations and rewards. Only Agents\\nthat requested a decision since the last call to `env.step()` are in the\\nDecisionSteps object.', '**TerminalSteps** — contains the data from Agents belonging to the same\\n\"Behavior\" in the simulation, such as observations and rewards. Only Agents\\nwhose episode ended since the last call to `env.step()` are in the\\nTerminalSteps object.', '**BehaviorSpec** — describes the shape of the observation data inside\\nDecisionSteps and TerminalSteps as well as the expected action shapes.']\n\nThese classes are all defined in the\n[base_env](../ml-agents-envs/mlagents_envs/base_env.py) script.\n\nAn Agent \"Behavior\" is a group of Agents identified by a `BehaviorName` that\nshare the same observations and action types (described in their\n`BehaviorSpec`). You can think about Agent Behavior as a group of agents that\nwill share the same policy. All Agents with the same behavior have the same goal\nand reward signals.\n\nTo communicate with an Agent in a Unity environment from a Python program, the\nAgent in the simulation must have `Behavior Parameters` set to communicate. You\nmust set the `Behavior Type` to `Default` and give it a `Behavior Name`.\n\n_Notice: Currently communication between Unity and Python takes place over an\nopen socket without authentication. As such, please make sure that the network\nwhere training takes place is secure. This will be addressed in a future\nrelease._",
        "Loading a Unity Environment": {
          "Interacting with a Unity Environment": {
            "The BaseEnv interface": "A `BaseEnv` has the following methods:\n\n['**Reset : `env.reset()`** Sends a signal to reset the environment. Returns\\nNone.', '**Step : `env.step()`** Sends a signal to step the environment. Returns None.\\nNote that a \"step\" for Python does not correspond to either Unity `Update` nor\\n`FixedUpdate`. When `step()` or `reset()` is called, the Unity simulation will\\nmove forward until an Agent in the simulation needs a input from Python to\\nact.', '**Close : `env.close()`** Sends a shutdown signal to the environment and\\nterminates the communication.', '**Behavior Specs : `env.behavior_specs`** Returns a Mapping of\\n`BehaviorName` to `BehaviorSpec` objects (read only).\\nA `BehaviorSpec` contains the observation shapes and the\\n`ActionSpec` (which defines the action shape). Note that\\nthe `BehaviorSpec` for a specific group is fixed throughout the simulation.\\nThe number of entries in the Mapping can change over time in the simulation\\nif new Agent behaviors are created in the simulation.', '**Get Steps : `env.get_steps(behavior_name: str)`** Returns a tuple\\n`DecisionSteps, TerminalSteps` corresponding to the behavior_name given as\\ninput. The `DecisionSteps` contains information about the state of the agents\\n**that need an action this step** and have the behavior behavior_name. The\\n`TerminalSteps` contains information about the state of the agents **whose\\nepisode ended** and have the behavior behavior_name. Both `DecisionSteps` and\\n`TerminalSteps` contain information such as the observations, the rewards and\\nthe agent identifiers. `DecisionSteps` also contains action masks for the next\\naction while `TerminalSteps` contains the reason for termination (did the\\nAgent reach its maximum step and was interrupted). The data is in `np.array`\\nof which the first dimension is always the number of agents note that the\\nnumber of agents is not guaranteed to remain constant during the simulation\\nand it is not unusual to have either `DecisionSteps` or `TerminalSteps`\\ncontain no Agents at all.', '**Set Actions :`env.set_actions(behavior_name: str, action: ActionTuple)`** Sets\\nthe actions for a whole agent group. `action` is an `ActionTuple`, which\\nis made up of a 2D `np.array` of `dtype=np.int32` for discrete actions, and\\n`dtype=np.float32` for continuous actions. The first dimension of `np.array`\\nin the tuple is the number of agents that requested a decision since the\\nlast call to `env.step()`. The second dimension is the number of discrete or\\ncontinuous actions for the corresponding array.', '**Set Action for Agent :\\n`env.set_action_for_agent(agent_group: str, agent_id: int, action: ActionTuple)`**\\nSets the action for a specific Agent in an agent group. `agent_group` is the\\nname of the group the Agent belongs to and `agent_id` is the integer\\nidentifier of the Agent. `action` is an `ActionTuple` as described above.\\n**Note:** If no action is provided for an agent group between two calls to\\n`env.step()` then the default action will be all zeros.']",
            "DecisionSteps and DecisionStep": "`DecisionSteps` (with `s`) contains information about a whole batch of Agents\nwhile `DecisionStep` (no `s`) only contains information about a single Agent.\n\nA `DecisionSteps` has the following fields :\n\n['`obs` is a list of numpy arrays observations collected by the group of agent.\\nThe first dimension of the array corresponds to the batch size of the group\\n(number of agents requesting a decision since the last call to `env.step()`).', '`reward` is a float vector of length batch size. Corresponds to the rewards\\ncollected by each agent since the last simulation step.', '`agent_id` is an int vector of length batch size containing unique identifier\\nfor the corresponding Agent. This is used to track Agents across simulation\\nsteps.', '`action_mask` is an optional list of two dimensional arrays of booleans which is only\\navailable when using multi-discrete actions. Each array corresponds to an\\naction branch. The first dimension of each array is the batch size and the\\nsecond contains a mask for each action of the branch. If true, the action is\\nnot available for the agent during this simulation step.']\n\nIt also has the two following methods:\n\n['`len(DecisionSteps)` Returns the number of agents requesting a decision since\\nthe last call to `env.step()`.', '`DecisionSteps[agent_id]` Returns a `DecisionStep` for the Agent with the\\n`agent_id` unique identifier.']\n\nA `DecisionStep` has the following fields:\n\n['`obs` is a list of numpy arrays observations collected by the agent. (Each\\narray has one less dimension than the arrays in `DecisionSteps`)', '`reward` is a float. Corresponds to the rewards collected by the agent since\\nthe last simulation step.', '`agent_id` is an int and an unique identifier for the corresponding Agent.', '`action_mask` is an optional list of one dimensional arrays of booleans which is only\\navailable when using multi-discrete actions. Each array corresponds to an\\naction branch. Each array contains a mask for each action of the branch. If\\ntrue, the action is not available for the agent during this simulation step.']",
            "TerminalSteps and TerminalStep": "Similarly to `DecisionSteps` and `DecisionStep`, `TerminalSteps` (with `s`)\ncontains information about a whole batch of Agents while `TerminalStep` (no `s`)\nonly contains information about a single Agent.\n\nA `TerminalSteps` has the following fields :\n\n['`obs` is a list of numpy arrays observations collected by the group of agent.\\nThe first dimension of the array corresponds to the batch size of the group\\n(number of agents requesting a decision since the last call to `env.step()`).', '`reward` is a float vector of length batch size. Corresponds to the rewards\\ncollected by each agent since the last simulation step.', '`agent_id` is an int vector of length batch size containing unique identifier\\nfor the corresponding Agent. This is used to track Agents across simulation\\nsteps.', '`interrupted` is an array of booleans of length batch size. Is true if the\\nassociated Agent was interrupted since the last decision step. For example,\\nif the Agent reached the maximum number of steps for the episode.']\n\nIt also has the two following methods:\n\n['`len(TerminalSteps)` Returns the number of agents requesting a decision since\\nthe last call to `env.step()`.', '`TerminalSteps[agent_id]` Returns a `TerminalStep` for the Agent with the\\n`agent_id` unique identifier.']\n\nA `TerminalStep` has the following fields:\n\n['`obs` is a list of numpy arrays observations collected by the agent. (Each\\narray has one less dimension than the arrays in `TerminalSteps`)', '`reward` is a float. Corresponds to the rewards collected by the agent since\\nthe last simulation step.', '`agent_id` is an int and an unique identifier for the corresponding Agent.', '`interrupted` is a bool. Is true if the Agent was interrupted since the last\\ndecision step. For example, if the Agent reached the maximum number of steps for\\nthe episode.']",
            "BehaviorSpec": "A `BehaviorSpec` has the following fields :\n\n[\"`observation_specs` is a List of `ObservationSpec` objects : Each `ObservationSpec`\\ncorresponds to an observation's properties: `shape` is a tuple of ints that\\ncorresponds to the shape of the observation (without the number of agents dimension).\\n`dimension_property` is a tuple of flags containing extra information about how the\\ndata should be processed in the corresponding dimension. `observation_type` is an enum\\ncorresponding to what type of observation is generating the data (i.e., default, goal,\\netc). Note that the `ObservationSpec` have the same ordering as the ordering of observations\\nin the DecisionSteps, DecisionStep, TerminalSteps and TerminalStep.\", '`action_spec` is an `ActionSpec` namedtuple that defines the number and types\\nof actions for the Agent.']\n\nAn `ActionSpec` has the following fields and properties:\n\n['`continuous_size` is the number of floats that constitute the continuous actions.', '`discrete_size` is the number of branches (the number of independent actions) that\\nconstitute the multi-discrete actions.', '`discrete_branches` is a Tuple of ints. Each int corresponds to the number of\\ndifferent options for each branch of the action. For example:\\nIn a game direction input (no movement, left, right) and\\njump input (no jump, jump) there will be two branches (direction and jump),\\nthe first one with 3 options and the second with 2 options. (`discrete_size = 2`\\nand `discrete_action_branches = (3,2,)`)']"
          },
          "Communicating additional information with the Environment": {
            "EngineConfigurationChannel": "The `EngineConfiguration` side channel allows you to modify the time-scale,\nresolution, and graphics quality of the environment. This can be useful for\nadjusting the environment to perform better during training, or be more\ninterpretable during inference.\n\n`EngineConfigurationChannel` has two methods :\n\n['`set_configuration_parameters` which takes the following arguments:', ['`width`: Defines the width of the display. (Must be set alongside height)', '`height`: Defines the height of the display. (Must be set alongside width)', '`quality_level`: Defines the quality level of the simulation.', '`time_scale`: Defines the multiplier for the deltatime in the simulation. If\\nset to a higher value, time will pass faster in the simulation but the\\nphysics may perform unpredictably.', '`target_frame_rate`: Instructs simulation to try to render at a specified\\nframe rate.', '`capture_frame_rate` Instructs the simulation to consider time between\\nupdates to always be constant, regardless of the actual frame rate.'], '`set_configuration` with argument config which is an `EngineConfig` NamedTuple\\nobject.']\n\nFor example, the following code would adjust the time-scale of the simulation to\nbe 2x realtime.\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n\nchannel = EngineConfigurationChannel()\n\nenv = UnityEnvironment(side_channels=[channel])\n\nchannel.set_configuration_parameters(time_scale = 2.0)\n\ni = env.reset()\n...\n```",
            "EnvironmentParameters": "The `EnvironmentParameters` will allow you to get and set pre-defined numerical\nvalues in the environment. This can be useful for adjusting environment-specific\nsettings, or for reading non-agent related information from the environment. You\ncan call `get_property` and `set_property` on the side channel to read and write\nproperties.\n\n`EnvironmentParametersChannel` has one methods:\n\n['`set_float_parameter` Sets a float parameter in the Unity Environment.', ['key: The string identifier of the property.', 'value: The float value of the property.']]\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel\n\nchannel = EnvironmentParametersChannel()\n\nenv = UnityEnvironment(side_channels=[channel])\n\nchannel.set_float_parameter(\"parameter_1\", 2.0)\n\ni = env.reset()\n...\n```\n\nOnce a property has been modified in Python, you can access it in C# after the\nnext call to `step` as follows:\n\n```\nvar envParameters = Academy.Instance.EnvironmentParameters;\nfloat property1 = envParameters.GetWithDefault(\"parameter_1\", 0.0f);\n```",
            "Custom side channels": "For information on how to make custom side channels for sending additional data\ntypes, see the documentation [here](Custom-SideChannels.md)."
          }
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 35]"
    },
    {
      "title": "mlagents.trainers.trainer.on\\_policy\\_trainer",
      "description": null,
      "content": {
        "OnPolicyTrainer Objects": "```\nclass OnPolicyTrainer(RLTrainer)\n```\n\nThe PPOTrainer is an implementation of the PPO algorithm.\n\n<a name=\"mlagents.trainers.trainer.on_policy_trainer.OnPolicyTrainer.__init__\"></a>\n\n\\_\\_init\\_\\_\n\n```\n | __init__(behavior_name: str, reward_buff_cap: int, trainer_settings: TrainerSettings, training: bool, load: bool, seed: int, artifact_path: str)\n```\n\nResponsible for collecting experiences and training an on-policy model.\n\n**Arguments**:\n\n['`behavior_name`: The name of the behavior associated with trainer config', '`reward_buff_cap`: Max reward history to track in the reward buffer', '`trainer_settings`: The parameters for the trainer.', '`training`: Whether the trainer is set for training.', '`load`: Whether the model should be loaded.', '`seed`: The seed the model will be initialized with', '`artifact_path`: The directory within which to store artifacts from this trainer.']\n\n<a name=\"mlagents.trainers.trainer.on_policy_trainer.OnPolicyTrainer.add_policy\"></a>\n\nadd\\_policy\n\n```\n | add_policy(parsed_behavior_id: BehaviorIdentifiers, policy: Policy) -> None\n```\n\nAdds policy to trainer.\n\n**Arguments**:\n\n['`parsed_behavior_id`: Behavior identifiers that the policy should belong to.', '`policy`: Policy to associate with name_behavior_id.']\n\n<a name=\"mlagents.trainers.trainer.off_policy_trainer\"></a>"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 36]"
    },
    {
      "title": "mlagents.trainers.optimizer.torch\\_optimizer",
      "description": null,
      "content": {
        "TorchOptimizer Objects": "```\nclass TorchOptimizer(Optimizer)\n```\n\n<a name=\"mlagents.trainers.optimizer.torch_optimizer.TorchOptimizer.create_reward_signals\"></a>\n\ncreate\\_reward\\_signals\n\n```\n | create_reward_signals(reward_signal_configs: Dict[RewardSignalType, RewardSignalSettings]) -> None\n```\n\nCreate reward signals\n\n**Arguments**:\n\n['`reward_signal_configs`: Reward signal config.']\n\n<a name=\"mlagents.trainers.optimizer.torch_optimizer.TorchOptimizer.get_trajectory_value_estimates\"></a>\n\nget\\_trajectory\\_value\\_estimates\n\n```\n | get_trajectory_value_estimates(batch: AgentBuffer, next_obs: List[np.ndarray], done: bool, agent_id: str = \"\") -> Tuple[Dict[str, np.ndarray], Dict[str, float], Optional[AgentBufferField]]\n```\n\nGet value estimates and memories for a trajectory, in batch form.\n\n**Arguments**:\n\n['`batch`: An AgentBuffer that consists of a trajectory.', '`next_obs`: the next observation (after the trajectory). Used for bootstrapping\\nif this is not a terminal trajectory.', '`done`: Set true if this is a terminal trajectory.', '`agent_id`: Agent ID of the agent that this trajectory belongs to.']\n\n**Returns**:\n\nA Tuple of the Value Estimates as a Dict of [name, np.ndarray(trajectory_len)],\nthe final value estimate as a Dict of [name, float], and optionally (if using memories)\nan AgentBufferField of initial critic memories to be used during update.\n\n<a name=\"mlagents.trainers.optimizer.optimizer\"></a>"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 37]"
    },
    {
      "title": "mlagents\\_envs.envs.pettingzoo\\_env\\_factory",
      "description": null,
      "content": {
        "PettingZooEnvFactory Objects": "```\nclass PettingZooEnvFactory()\n```\n\n<a name=\"mlagents_envs.envs.pettingzoo_env_factory.PettingZooEnvFactory.env\"></a>\n\nenv\n\n```\n | env(seed: Optional[int] = None, **kwargs: Union[List, int, bool, None]) -> UnityAECEnv\n```\n\nCreates the environment with env_id from unity's default_registry and wraps it in a UnityToPettingZooWrapper\n\n**Arguments**:\n\n['`seed`: The seed for the action spaces of the agents.', '`kwargs`: Any argument accepted by `UnityEnvironment`class except file_name']\n\n<a name=\"mlagents_envs.envs.unity_aec_env\"></a>"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 38]"
    },
    {
      "title": "Unity ML-Agents PettingZoo Wrapper",
      "description": null,
      "content": {
        "Installation and Examples": "The PettingZoo wrapper is part of the `mlagents_envs` package. Please refer to the\n[mlagents_envs installation instructions](ML-Agents-Envs-README.md).\n\n[[Colab] PettingZoo Wrapper Example](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/develop-python-api-ga/ml-agents-envs/colabs/Colab_PettingZoo.ipynb)\n\nThis colab notebook demonstrates the example usage of the wrapper, including installation,\nbasic usages, and an example with our\n[Striker vs Goalie environment](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#strikers-vs-goalie)\nwhich is a multi-agents environment with multiple different behavior names.",
        "API interface": "This wrapper is compatible with PettingZoo API. Please check out\n[PettingZoo API page](https://pettingzoo.farama.org/api/aec/) for more details.\nHere's an example of interacting with wrapped environment:\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\nfrom mlagents_envs.envs import UnityToPettingZooWrapper\n\nunity_env = UnityEnvironment(\"StrikersVsGoalie\")\nenv = UnityToPettingZooWrapper(unity_env)\nenv.reset()\nfor agent in env.agent_iter():\n    observation, reward, done, info = env.last()\n    action = policy(observation, agent)\n    env.step(action)\n```",
        "Notes": [
          "There is support for both [AEC](https://pettingzoo.farama.org/api/aec/)\nand [Parallel](https://pettingzoo.farama.org/api/parallel/) PettingZoo APIs.",
          "The AEC wrapper is compatible with PettingZoo (PZ) API interface but works in a slightly\ndifferent way under the hood. For the AEC API, Instead of stepping the environment in every `env.step(action)`,\nthe PZ wrapper will store the action, and will only perform environment stepping when all the\nagents requesting for actions in the current step have been assigned an action. This is for\nperformance, considering that the communication between Unity and python is more efficient\nwhen data are sent in batches.",
          "Since the actions for the AEC wrapper are stored without applying them to the environment until\nall the actions are queued, some components of the API might behave in unexpected way. For example, a call\nto `env.reward` should return the instantaneous reward for that particular step, but the true\nreward would only be available when an actual environment step is performed. It's recommended that\nyou follow the API definition for training (access rewards from `env.last()` instead of\n`env.reward`) and the underlying mechanism shouldn't affect training results.",
          "The environments will automatically reset when it's done, so `env.agent_iter(max_step)` will\nkeep going on until the specified max step is reached (default: `2**63`). There is no need to\ncall `env.reset()` except for the very beginning of instantiating an environment."
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 39]"
    },
    {
      "title": "Unity ML-Agents Toolkit",
      "description": null,
      "content": {
        "Features": [
          "17+ [example Unity environments](Learning-Environment-Examples.md)",
          "Support for multiple environment configurations and training scenarios",
          "Flexible Unity SDK that can be integrated into your game or custom Unity scene",
          "Support for training single-agent, multi-agent cooperative, and multi-agent\ncompetitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play).",
          "Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL).",
          "Quickly and easily add your own [custom training algorithm](Python-Custom-Trainer-Plugin.md) and/or components.",
          "Easily definable Curriculum Learning scenarios for complex tasks",
          "Train robust agents using environment randomization",
          "Flexible agent control with On Demand Decision Making",
          "Train using multiple concurrent Unity environment instances",
          "Utilizes the [Inference Engine](Inference-Engine.md) to\nprovide native cross-platform support",
          "Unity environment [control from Python](Python-LLAPI.md)",
          "Wrap Unity learning environments as a [gym](Python-Gym-API.md) environment",
          "Wrap Unity learning environments as a [PettingZoo](Python-PettingZoo-API.md) environment"
        ],
        "Releases & Documentation": "**Our latest, stable release is `Release 22`. Click\n[here](Getting-Started.md)\nto get started with the latest release of ML-Agents.**\n\n**You can also check out our new [web docs](https://unity-technologies.github.io/ml-agents/)!**\n\nThe table below lists all our releases, including our `main` branch which is\nunder active development and may be unstable. A few helpful guidelines:\n\n['The [Versioning page](Versioning.md) overviews how we manage our GitHub\\nreleases and the versioning process for each of the ML-Agents components.', 'The [Releases page](https://github.com/Unity-Technologies/ml-agents/releases)\\ncontains details of the changes between releases.', 'The [Migration page](Migrating.md) contains details on how to upgrade\\nfrom earlier releases of the ML-Agents Toolkit.', \"The **Documentation** links in the table below include installation and usage\\ninstructions specific to each release. Remember to always use the\\ndocumentation that corresponds to the release version you're using.\", 'The `com.unity.ml-agents` package is [verified](https://docs.unity3d.com/2020.1/Documentation/Manual/pack-safe.html)\\nfor Unity 2020.1 and later. Verified packages releases are numbered 1.0.x.']\n\n|        **Version**         | **Release Date** | **Source** | **Documentation** | **Download** | **Python Package** | **Unity Package** |\n|:--------------------------:|:------:|:-------------:|:-------:|:------------:|:------------:|:------------:|\n| **Release 22** | **October 5, 2024** | **[source](https://github.com/Unity-Technologies/ml-agents/tree/release_22)** | **[docs](https://unity-technologies.github.io/ml-agents/)** | **[download](https://github.com/Unity-Technologies/ml-agents/archive/release_22.zip)** | **[1.1.0](https://pypi.org/project/mlagents/1.1.0/)** | **[3.0.0](https://docs.unity3d.com/Packages/com.unity.ml-agents@3.0/manual/index.html)** |\n| **develop (unstable)** | -- | [source](https://github.com/Unity-Technologies/ml-agents/tree/develop) | [docs](https://unity-technologies.github.io/ml-agents/) | [download](https://github.com/Unity-Technologies/ml-agents/archive/develop.zip) | -- | -- |\n\nIf you are a researcher interested in a discussion of Unity as an AI platform,\nsee a pre-print of our\n[reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627).\n\nIf you use Unity or the ML-Agents Toolkit to conduct research, we ask that you\ncite the following paper as a reference:\n\n```\n@article{juliani2020,\n  title={Unity: A general platform for intelligent agents},\n  author={Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},\n  journal={arXiv preprint arXiv:1809.02627},\n  url={https://arxiv.org/pdf/1809.02627.pdf},\n  year={2020}\n}\n```\n\nAdditionally, if you use the MA-POCA trainer in your research, we ask that you\ncite the following paper as a reference:\n\n```\n@article{cohen2022,\n  title={On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning},\n  author={Cohen, Andrew and Teng, Ervin and Berges, Vincent-Pierre and Dong, Ruo-Ping and Henry, Hunter and Mattar, Marwan and Zook, Alexander and Ganguly, Sujoy},\n  journal={RL in Games Workshop AAAI 2022},\n  url={http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf},\n  year={2022}\n}\n```",
        "Additional Resources": {
          "More from Unity": [
            "[Unity Inference Engine](https://unity.com/products/sentis)",
            "[Introducing Unity Muse and Sentis](https://blog.unity.com/engine-platform/introducing-unity-muse-and-unity-sentis-ai)"
          ]
        },
        "Community and Feedback": "The ML-Agents Toolkit is an open-source project and we encourage and welcome\ncontributions. If you wish to contribute, be sure to review our\n[contribution guidelines](CONTRIBUTING.md) and\n[code of conduct](CODE_OF_CONDUCT.md).\n\nFor problems with the installation and setup of the ML-Agents Toolkit, or\ndiscussions about how to best setup or train your agents, please create a new\nthread on the\n[Unity ML-Agents forum](https://forum.unity.com/forums/ml-agents.453/) and make\nsure to include as much detail as possible. If you run into any other problems\nusing the ML-Agents Toolkit or have a specific feature request, please\n[submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues).\n\nPlease tell us which samples you would like to see shipped with the ML-Agents Unity\npackage by replying to\n[this forum thread](https://forum.unity.com/threads/feedback-wanted-shipping-sample-s-with-the-ml-agents-package.1073468/).\n\nYour opinion matters a great deal to us. Only by hearing your thoughts on the\nUnity ML-Agents Toolkit can we continue to improve and grow. Please take a few\nminutes to\n[let us know about it](https://unitysoftware.co1.qualtrics.com/jfe/form/SV_55pQKCZ578t0kbc).\n\nFor any other questions or feedback, connect directly with the ML-Agents team at\nml-agents@unity3d.com.",
        "Privacy": "In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics.\nPlease refer to \"Information that is passively collected by Unity\" in the\n[Unity Privacy Policy](https://unity3d.com/legal/privacy-policy)."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 40]"
    },
    {
      "title": "Training Configuration File",
      "description": null,
      "content": {
        "Common Trainer Configurations": "One of the first decisions you need to make regarding your training run is which\ntrainer to use: PPO, SAC, or POCA. There are some training configurations that are\ncommon to both trainers (which we review now) and others that depend on the\nchoice of the trainer (which we review on subsequent sections).\n\n| **Setting**              | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| :----------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `trainer_type`                | (default = `ppo`) The type of trainer to use: `ppo`,  `sac`, or `poca`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| `summary_freq`           | (default = `50000`) Number of experiences that needs to be collected before generating and displaying training statistics. This determines the granularity of the graphs in Tensorboard.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| `time_horizon`           | (default = `64`) How many steps of experience to collect per-agent before adding it to the experience buffer. When this limit is reached before the end of an episode, a value estimate is used to predict the overall expected reward from the agent's current state. As such, this parameter trades off between a less biased, but higher variance estimate (long time horizon) and more biased, but less varied estimate (short time horizon). In cases where there are frequent rewards within an episode, or episodes are prohibitively large, a smaller number can be more ideal. This number should be large enough to capture all the important behavior within a sequence of an agent's actions. <br><br> Typical range: `32` - `2048` |\n| `max_steps`              | (default = `500000`) Total number of steps (i.e., observation collected and action taken) that must be taken in the environment (or across all environments if using multiple in parallel) before ending the training process. If you have multiple agents with the same behavior name within your environment, all steps taken by those agents will contribute to the same `max_steps` count. <br><br>Typical range: `5e5` - `1e7`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| `keep_checkpoints`         | (default = `5`) The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option. Once the maximum number of checkpoints has been reached, the oldest checkpoint is deleted when saving a new checkpoint. |\n| `even_checkpoints`         | (default = `false`) If set to true, ignores `checkpoint_interval` and evenly distributes checkpoints throughout training based on `keep_checkpoints`and `max_steps`, i.e. `checkpoint_interval = max_steps / keep_checkpoints`. Useful for cataloging agent behavior throughout training. |\n| `checkpoint_interval`         | (default = `500000`) The number of experiences collected between each checkpoint by the trainer. A maximum of `keep_checkpoints` checkpoints are saved before old ones are deleted. Each checkpoint saves the `.onnx` files in `results/` folder.|\n| `init_path`              | (default = None) Initialize trainer from a previously saved model. Note that the prior run should have used the same trainer configurations as the current run, and have been saved with the same version of ML-Agents. <br><br>You can provide either the file name or the full path to the checkpoint, e.g. `{checkpoint_name.pt}` or `./models/{run-id}/{behavior_name}/{checkpoint_name.pt}`. This option is provided in case you want to initialize different behaviors from different runs or initialize from an older checkpoint; in most cases, it is sufficient to use the `--initialize-from` CLI parameter to initialize all models from the same run.                                                                                                                                  |\n| `threaded`               | (default = `false`) Allow environments to step while updating the model. This might result in a training speedup, especially when using SAC. For best performance, leave setting to `false` when using self-play.                                                                                                                                                                                                                      |\n| `hyperparameters -> learning_rate`          | (default = `3e-4`) Initial learning rate for gradient descent. Corresponds to the strength of each gradient descent update step. This should typically be decreased if training is unstable, and the reward does not consistently increase. <br><br>Typical range: `1e-5` - `1e-3`                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| `hyperparameters -> batch_size`             | Number of experiences in each iteration of gradient descent. **This should always be multiple times smaller than `buffer_size`**. If you are using continuous actions, this value should be large (on the order of 1000s). If you are using only discrete actions, this value should be smaller (on the order of 10s). <br><br> Typical range: (Continuous - PPO): `512` - `5120`; (Continuous - SAC): `128` - `1024`; (Discrete, PPO & SAC): `32` - `512`.                                                                                                                                                                                                                                                               |\n| `hyperparameters -> buffer_size`            | (default = `10240` for PPO and `50000` for SAC)<br> **PPO:** Number of experiences to collect before updating the policy model. Corresponds to how many experiences should be collected before we do any learning or updating of the model. **This should be multiple times larger than `batch_size`**. Typically a larger `buffer_size` corresponds to more stable training updates. <br> **SAC:** The max size of the experience buffer - on the order of thousands of times longer than your episodes, so that SAC can learn from old as well as new experiences. <br><br>Typical range: PPO: `2048` - `409600`; SAC: `50000` - `1000000`                                                                                                                                                      |\n| `hyperparameters -> learning_rate_schedule` | (default = `linear` for PPO and `constant` for SAC) Determines how learning rate changes over time. For PPO, we recommend decaying learning rate until max_steps so learning converges more stably. However, for some cases (e.g. training for an unknown amount of time) this feature can be disabled. For SAC, we recommend holding learning rate constant so that the agent can continue to learn until its Q function converges naturally. <br><br>`linear` decays the learning_rate linearly, reaching 0 at max_steps, while `constant` keeps the learning rate constant for the entire training run.                                                                                                           |\n| `network_settings -> hidden_units`           | (default = `128`) Number of units in the hidden layers of the neural network. Correspond to how many units are in each fully connected layer of the neural network. For simple problems where the correct action is a straightforward combination of the observation inputs, this should be small. For problems where the action is a very complex interaction between the observation variables, this should be larger. <br><br> Typical range: `32` - `512`                                                                                                                                                                                                                                                                                    |\n| `network_settings -> num_layers`             | (default = `2`) The number of hidden layers in the neural network. Corresponds to how many hidden layers are present after the observation input, or after the CNN encoding of the visual observation. For simple problems, fewer layers are likely to train faster and more efficiently. More layers may be necessary for more complex control problems. <br><br> Typical range: `1` - `3`                                                                                                                                                                                                                                                                                                                                                    |\n| `network_settings -> normalize`              | (default = `false`) Whether normalization is applied to the vector observation inputs. This normalization is based on the running average and variance of the vector observation. Normalization can be helpful in cases with complex continuous control problems, but may be harmful with simpler discrete control problems.                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `network_settings -> vis_encode_type`       | (default = `simple`) Encoder type for encoding visual observations. <br><br> `simple` (default) uses a simple encoder which consists of two convolutional layers, `nature_cnn` uses the CNN implementation proposed by [Mnih et al.](https://www.nature.com/articles/nature14236), consisting of three convolutional layers, and `resnet` uses the [IMPALA Resnet](https://arxiv.org/abs/1802.01561) consisting of three stacked layers, each with two residual blocks, making a much larger network than the other two. `match3` is a smaller CNN ([Gudmundsoon et al.](https://www.researchgate.net/publication/328307928_Human-Like_Playtesting_with_Deep_Learning)) that can capture more granular spatial relationships and is optimized for board games. `fully_connected` uses a single fully connected dense layer as encoder without any convolutional layers. <br><br> Due to the size of convolution kernel, there is a minimum observation size limitation that each encoder type can handle - `simple`: 20x20, `nature_cnn`: 36x36, `resnet`: 15 x 15, `match3`: 5x5.  `fully_connected` doesn't have convolutional layers and thus no size limits, but since it has less representation power it should be reserved for very small inputs. Note that using the `match3` CNN with very large visual input might result in a huge observation encoding and thus potentially slow down training or cause memory issues.                                                                                                                                                                                              |\n| `network_settings -> goal_conditioning_type`       | (default = `hyper`) Conditioning type for the policy using goal observations. <br><br> `none` treats the goal observations as regular observations, `hyper` (default) uses a HyperNetwork with goal observations as input to generate some of the weights of the policy. Note that when using `hyper` the number of parameters of the network increases greatly. Therefore, it is recommended to reduce the number of `hidden_units` when using this `goal_conditioning_type`",
        "Trainer-specific Configurations": {
          "PPO-specific Configurations": "| **Setting** | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| :---------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `hyperparameters -> beta`      | (default = `5.0e-3`) Strength of the entropy regularization, which makes the policy \"more random.\" This ensures that agents properly explore the action space during training. Increasing this will ensure more random actions are taken. This should be adjusted such that the entropy (measurable from TensorBoard) slowly decreases alongside increases in reward. If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease `beta`. <br><br>Typical range: `1e-4` - `1e-2`                                                                                                                                                                     |\n| `hyperparameters -> epsilon`   | (default = `0.2`) Influences how rapidly the policy can evolve during training. Corresponds to the acceptable threshold of divergence between the old and new policies during gradient descent updating. Setting this value small will result in more stable updates, but will also slow the training process. <br><br>Typical range: `0.1` - `0.3`                                                                                                                                                                                                                                                                                                                      |\n| `hyperparameters -> beta_schedule` | (default = `learning_rate_schedule`) Determines how beta changes over time. <br><br>`linear` decays beta linearly, reaching 0 at max_steps, while `constant` keeps beta constant for the entire training run. If not explicitly set, the default beta schedule will be set to `hyperparameters -> learning_rate_schedule`.                                                                                                           |\n| `hyperparameters -> epsilon_schedule` | (default = `learning_rate_schedule `) Determines how epsilon changes over time (PPO only). <br><br>`linear` decays epsilon linearly, reaching 0 at max_steps, while `constant` keeps the epsilon constant for the entire training run. If not explicitly set, the default epsilon schedule will be set to `hyperparameters -> learning_rate_schedule`.\n| `hyperparameters -> lambd`     | (default = `0.95`) Regularization parameter (lambda) used when calculating the Generalized Advantage Estimate ([GAE](https://arxiv.org/abs/1506.02438)). This can be thought of as how much the agent relies on its current value estimate when calculating an updated value estimate. Low values correspond to relying more on the current value estimate (which can be high bias), and high values correspond to relying more on the actual rewards received in the environment (which can be high variance). The parameter provides a trade-off between the two, and the right value can lead to a more stable training process. <br><br>Typical range: `0.9` - `0.95` |\n| `hyperparameters -> num_epoch` | (default = `3`) Number of passes to make through the experience buffer when performing gradient descent optimization.The larger the batch_size, the larger it is acceptable to make this. Decreasing this will ensure more stable updates, at the cost of slower learning. <br><br>Typical range: `3` - `10`                                                                                                                                                                                                                                                                                                                                                           |\n| `hyperparameters -> shared_critic` | (default = `False`) Whether or not the policy and value function networks share a backbone. It may be useful to use a shared backbone when learning from image observations.",
          "SAC-specific Configurations": "| **Setting**          | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| :------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `hyperparameters -> buffer_init_steps`  | (default = `0`) Number of experiences to collect into the buffer before updating the policy model. As the untrained policy is fairly random, pre-filling the buffer with random actions is useful for exploration. Typically, at least several episodes of experiences should be pre-filled. <br><br>Typical range: `1000` - `10000`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| `hyperparameters -> init_entcoef` | (default = `1.0`) How much the agent should explore in the beginning of training. Corresponds to the initial entropy coefficient set at the beginning of training. In SAC, the agent is incentivized to make its actions entropic to facilitate better exploration. The entropy coefficient weighs the true reward with a bonus entropy reward. The entropy coefficient is [automatically adjusted](https://arxiv.org/abs/1812.05905) to a preset target entropy, so the `init_entcoef` only corresponds to the starting value of the entropy bonus. Increase init_entcoef to explore more in the beginning, decrease to converge to a solution faster. <br><br>Typical range: (Continuous): `0.5` - `1.0`; (Discrete): `0.05` - `0.5`                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| `hyperparameters -> save_replay_buffer` | (default = `false`) Whether to save and load the experience replay buffer as well as the model when quitting and re-starting training. This may help resumes go more smoothly, as the experiences collected won't be wiped. Note that replay buffers can be very large, and will take up a considerable amount of disk space. For that reason, we disable this feature by default.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| `hyperparameters -> tau` | (default = `0.005`) How aggressively to update the target network used for bootstrapping value estimation in SAC. Corresponds to the magnitude of the target Q update during the SAC model update. In SAC, there are two neural networks: the target and the policy. The target network is used to bootstrap the policy's estimate of the future rewards at a given state, and is fixed while the policy is being updated. This target is then slowly updated according to tau. Typically, this value should be left at 0.005. For simple problems, increasing tau to 0.01 might reduce the time it takes to learn, at the cost of stability. <br><br>Typical range: `0.005` - `0.01`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| `hyperparameters -> steps_per_update` | (default = `1`) Average ratio of agent steps (actions) taken to updates made of the agent's policy. In SAC, a single \"update\" corresponds to grabbing a batch of size `batch_size` from the experience replay buffer, and using this mini batch to update the models. Note that it is not guaranteed that after exactly `steps_per_update` steps an update will be made, only that the ratio will hold true over many steps. Typically, `steps_per_update` should be greater than or equal to 1. Note that setting `steps_per_update` lower will improve sample efficiency (reduce the number of steps required to train) but increase the CPU time spent performing updates. For most environments where steps are fairly fast (e.g. our example environments) `steps_per_update` equal to the number of agents in the scene is a good balance. For slow environments (steps take 0.1 seconds or more) reducing `steps_per_update` may improve training speed. We can also change `steps_per_update` to lower than 1 to update more often than once per step, though this will usually result in a slowdown unless the environment is very slow. <br><br>Typical range: `1` - `20` |\n| `hyperparameters -> reward_signal_num_update` | (default = `steps_per_update`) Number of steps per mini batch sampled and used for updating the reward signals. By default, we update the reward signals once every time the main policy is updated. However, to imitate the training procedure in certain imitation learning papers (e.g. [Kostrikov et. al](http://arxiv.org/abs/1809.02925), [Blondé et. al](http://arxiv.org/abs/1809.02064)), we may want to update the reward signal (GAIL) M times for every update of the policy. We can change `steps_per_update` of SAC to N, as well as `reward_signal_steps_per_update` under `reward_signals` to N / M to accomplish this. By default, `reward_signal_steps_per_update` is set to `steps_per_update`. |",
          "MA-POCA-specific Configurations": "MA-POCA uses the same configurations as PPO, and there are no additional POCA-specific parameters.\n\n**NOTE**: Reward signals other than Extrinsic Rewards have not been extensively tested with MA-POCA,\nthough they can still be added and used for training on a your-mileage-may-vary basis."
        },
        "Reward Signals": {
          "Extrinsic Rewards": "Enable these settings to ensure that your training run incorporates your\nenvironment-based reward signal:\n\n| **Setting**             | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| :---------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `extrinsic -> strength` | (default = `1.0`) Factor by which to multiply the reward given by the environment. Typical ranges will vary depending on the reward signal. <br><br>Typical range: `1.00`                                                                                                                                                                                                                                                                                              |\n| `extrinsic -> gamma`    | (default = `0.99`) Discount factor for future rewards coming from the environment. This can be thought of as how far into the future the agent should care about possible rewards. In situations when the agent should be acting in the present in order to prepare for rewards in the distant future, this value should be large. In cases when rewards are more immediate, it can be smaller. Must be strictly smaller than 1. <br><br>Typical range: `0.8` - `0.995` |",
          "Curiosity Intrinsic Reward": "To enable curiosity, provide these settings:\n\n| **Setting**                  | **Description**                                                                                                                                                                                                                                                                                                                       |\n| :--------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `curiosity -> strength`      | (default = `1.0`) Magnitude of the curiosity reward generated by the intrinsic curiosity module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. <br><br>Typical range: `0.001` - `0.1` |\n| `curiosity -> gamma`         | (default = `0.99`) Discount factor for future rewards. <br><br>Typical range: `0.8` - `0.995`                                                                                                                                                                                                                                                            |\n| `curiosity -> network_settings` | Please see the documentation for `network_settings` under [Common Trainer Configurations](#common-trainer-configurations). The network specs used by the intrinsic curiosity model. The value should of `hidden_units` should be small enough to encourage the ICM to compress the original observation, but also not too small to prevent it from learning to differentiate between expected and actual observations. <br><br>Typical range: `64` - `256` |\n| `curiosity -> learning_rate` | (default = `3e-4`) Learning rate used to update the intrinsic curiosity module. This should typically be decreased if training is unstable, and the curiosity loss is unstable. <br><br>Typical range: `1e-5` - `1e-3`                                                                                                      |",
          "GAIL Intrinsic Reward": "To enable GAIL (assuming you have recorded demonstrations), provide these\nsettings:\n\n| **Setting**             | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| :---------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `gail -> strength`      | (default = `1.0`) Factor by which to multiply the raw reward. Note that when using GAIL with an Extrinsic Signal, this value should be set lower if your demonstrations are suboptimal (e.g. from a human), so that a trained agent will focus on receiving extrinsic rewards instead of exactly copying the demonstrations. Keep the strength below about 0.1 in those cases. <br><br>Typical range: `0.01` - `1.0`                                                                              |\n| `gail -> gamma`         | (default = `0.99`) Discount factor for future rewards. <br><br>Typical range: `0.8` - `0.9`                                                                                                                                                                                                                                                                                                                                                                                                        |\n| `gail -> demo_path`     | (Required, no default) The path to your .demo file or directory of .demo files.                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| `gail -> network_settings` | Please see the documentation for `network_settings` under [Common Trainer Configurations](#common-trainer-configurations). The network specs for the GAIL discriminator. The value of `hidden_units` should be small enough to encourage the discriminator to compress the original observation, but also not too small to prevent it from learning to differentiate between demonstrated and actual behavior. Dramatically increasing this size will also negatively affect training times. <br><br>Typical range: `64` - `256`                                                           |\n| `gail -> learning_rate` | (Optional, default = `3e-4`) Learning rate used to update the discriminator. This should typically be decreased if training is unstable, and the GAIL loss is unstable. <br><br>Typical range: `1e-5` - `1e-3`                                                                                                                                                                                                                                                                  |\n| `gail -> use_actions`   | (default = `false`) Determines whether the discriminator should discriminate based on both observations and actions, or just observations. Set to True if you want the agent to mimic the actions from the demonstrations, and False if you'd rather have the agent visit the same states as in the demonstrations but with possibly different actions. Setting to False is more likely to be stable, especially with imperfect demonstrations, but may learn slower. |\n| `gail -> use_vail`      | (default = `false`) Enables a variational bottleneck within the GAIL discriminator. This forces the discriminator to learn a more general representation and reduces its tendency to be \"too good\" at discriminating, making learning more stable. However, it does increase training time. Enable this if you notice your imitation learning is unstable, or unable to learn the task at hand.                                                                       |",
          "RND Intrinsic Reward": "Random Network Distillation (RND) is only available for the PyTorch trainers.\nTo enable RND, provide these settings:\n\n| **Setting**                  | **Description**                                                                                                                                                                                                                                                                                                                       |\n| :--------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `rnd -> strength`      | (default = `1.0`) Magnitude of the curiosity reward generated by the intrinsic rnd module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. <br><br>Typical range: `0.001` - `0.01` |\n| `rnd -> gamma`         | (default = `0.99`) Discount factor for future rewards. <br><br>Typical range: `0.8` - `0.995`                                                                                                                                                                                                                                                            |\n| `rnd -> network_settings` | Please see the documentation for `network_settings` under [Common Trainer Configurations](#common-trainer-configurations). The network specs for the RND model. |\n| `curiosity -> learning_rate` | (default = `3e-4`) Learning rate used to update the RND module. This should be large enough for the RND module to quickly learn the state representation, but small enough to allow for stable learning. <br><br>Typical range: `1e-5` - `1e-3`"
        },
        "Behavioral Cloning": "To enable Behavioral Cloning as a pre-training option (assuming you have\nrecorded demonstrations), provide the following configurations under the\n`behavioral_cloning` section:\n\n| **Setting**          | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| :------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `demo_path`          | (Required, no default) The path to your .demo file or directory of .demo files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| `strength`           | (default = `1.0`) Learning rate of the imitation relative to the learning rate of PPO, and roughly corresponds to how strongly we allow BC to influence the policy. <br><br>Typical range: `0.1` - `0.5`                                                                                                                                                                                                                                                                                                                                                                     |\n| `steps`              | (default = `0`) During BC, it is often desirable to stop using demonstrations after the agent has \"seen\" rewards, and allow it to optimize past the available demonstrations and/or generalize outside of the provided demonstrations. steps corresponds to the training steps over which BC is active. The learning rate of BC will anneal over the steps. Set the steps to 0 for constant imitation over the entire training run.                                                                                                                                        |\n| `batch_size`         | (default = `batch_size` of trainer) Number of demonstration experiences used for one iteration of a gradient descent update. If not specified, it will default to the `batch_size` of the trainer. <br><br>Typical range: (Continuous): `512` - `5120`; (Discrete): `32` - `512`                                                                                                                                                                                                                                                                                                                              |\n| `num_epoch`          | (default = `num_epoch` of trainer) Number of passes through the experience buffer during gradient descent. If not specified, it will default to the number of epochs set for PPO. <br><br>Typical range: `3` - `10`                                                                                                                                                                                                                                                                                                                                                                           |\n| `samples_per_update` | (default = `0`) Maximum number of samples to use during each imitation update. You may want to lower this if your demonstration dataset is very large to avoid overfitting the policy on demonstrations. Set to 0 to train over all of the demonstrations at each update step. <br><br>Typical range: `buffer_size`",
        "Memory-enhanced Agents using Recurrent Neural Networks": "You can enable your agents to use memory by adding a `memory` section under `network_settings`,\nand setting `memory_size` and `sequence_length`:\n\n| **Setting**       | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| :---------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `network_settings -> memory -> memory_size` | (default = `128`) Size of the memory an agent must keep. In order to use a LSTM, training requires a sequence of experiences instead of single experiences. Corresponds to the size of the array of floating point numbers used to store the hidden state of the recurrent neural network of the policy. This value must be a multiple of 2, and should scale with the amount of information you expect the agent will need to remember in order to successfully complete the task. <br><br>Typical range: `32` - `256` |\n| `network_settings -> memory -> sequence_length` | (default = `64`) Defines how long the sequences of experiences must be while training. Note that if this number is too small, the agent will not be able to remember things over longer periods of time. If this number is too large, the neural network will take longer to train. <br><br>Typical range: `4` - `128`                                                                                                                                                                                                 |\n\nA few considerations when deciding to use memory:\n\n['LSTM does not work well with continuous actions. Please use\\ndiscrete actions for better results.', 'Adding a recurrent layer increases the complexity of the neural network, it is\\nrecommended to decrease `num_layers` when using recurrent.', 'It is required that `memory_size` be divisible by 2.']",
        "Self-Play": {
          "Note on Reward Signals": "We make the assumption that the final reward in a trajectory corresponds to the\noutcome of an episode. A final reward of +1 indicates winning, -1 indicates\nlosing and 0 indicates a draw. The ELO calculation (discussed below) depends on\nthis final reward being either +1, 0, -1.\n\nThe reward signal should still be used as described in the documentation for the\nother trainers. However, we encourage users to be a bit more conservative when\nshaping reward functions due to the instability and non-stationarity of learning\nin adversarial games. Specifically, we encourage users to begin with the\nsimplest possible reward function (+1 winning, -1 losing) and to allow for more\niterations of training to compensate for the sparsity of reward.",
          "Note on Swap Steps": "As an example, in a 2v1 scenario, if we want the swap to occur x=4 times during\nteam-change=200000 steps, the swap_steps for the team of one agent is:\n\nswap_steps = (1 / 2) \\* (200000 / 4) = 25000 The swap_steps for the team of two\nagents is:\n\nswap_steps = (2 / 1) \\* (200000 / 4) = 100000 Note, with equal team sizes, the\nfirst term is equal to 1 and swap_steps can be calculated by just dividing the\ntotal steps by the desired number of swaps.\n\nA larger value of swap_steps means that an agent will play against the same\nfixed opponent for a longer number of training iterations. This results in a\nmore stable training scenario, but leaves the agent open to the risk of\noverfitting it's behavior for this particular opponent. Thus, when a new\nopponent is swapped, the agent may lose more often than expected."
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 41]"
    },
    {
      "title": "Training ML-Agents",
      "description": null,
      "content": {
        "Training with mlagents-learn": {
          "Starting Training": {
            "Observing Training": "Regardless of which training methods, configurations or hyperparameters you\nprovide, the training process will always generate three artifacts, all found\nin the `results/<run-identifier>` folder:\n\n['Summaries: these are training metrics that\\nare updated throughout the training process. They are helpful to monitor your\\ntraining performance and may help inform how to update your hyperparameter\\nvalues. See [Using TensorBoard](Using-Tensorboard.md) for more details on how\\nto visualize the training metrics.', 'Models: these contain the model checkpoints that\\nare updated throughout training and the final model file (`.onnx`). This final\\nmodel file is generated once either when training completes or is\\ninterrupted.', 'Timers file (under `results/<run-identifier>/run_logs`): this contains aggregated\\nmetrics on your training process, including time spent on specific code\\nblocks. See [Profiling in Python](Profiling-Python.md) for more information\\non the timers generated.']\n\nThese artifacts are updated throughout the training\nprocess and finalized when training is completed or is interrupted.",
            "Stopping and Resuming Training": "To interrupt training and save the current progress, hit `Ctrl+C` once and wait\nfor the model(s) to be saved out.\n\nTo resume a previously interrupted or completed training run, use the `--resume`\nflag and make sure to specify the previously used run ID.\n\nIf you would like to re-run a previously interrupted or completed training run\nand re-use the same run ID (in this case, overwriting the previously generated\nartifacts), then use the `--force` flag.",
            "Loading an Existing Model": "You can also use this mode to run inference of an already-trained model in\nPython by using both the `--resume` and `--inference` flags. Note that if you\nwant to run inference in Unity, you should use the\n[Inference Engine](Getting-Started.md#running-a-pre-trained-model).\n\nAdditionally, if the network architecture changes, you may still load an existing model,\nbut ML-Agents will only load the parts of the model it can load and ignore all others. For instance,\nif you add a new reward signal, the existing model will load but the new reward signal\nwill be initialized from scratch. If you have a model with a visual encoder (CNN) but\nchange the `hidden_units`, the CNN will be loaded but the body of the network will be\ninitialized from scratch.\n\nAlternatively, you might want to start a new training run but _initialize_ it\nusing an already-trained model. You may want to do this, for instance, if your\nenvironment changed and you want a new model, but the old behavior is still\nbetter than random. You can do this by specifying\n`--initialize-from=<run-identifier>`, where `<run-identifier>` is the old run\nID."
          }
        },
        "Training Configurations": {
          "Adding CLI Arguments to the Training Configuration file": {
            "Environment settings": "```\nenv_settings:\n  env_path: FoodCollector\n  env_args: null\n  base_port: 5005\n  num_envs: 1\n  timeout_wait: 10\n  seed: -1\n  max_lifetime_restarts: 10\n  restarts_rate_limit_n: 1\n  restarts_rate_limit_period_s: 60\n```",
            "Engine settings": "```\nengine_settings:\n  width: 84\n  height: 84\n  quality_level: 5\n  time_scale: 20\n  target_frame_rate: -1\n  capture_frame_rate: 60\n  no_graphics: false\n```",
            "Checkpoint settings": "```\ncheckpoint_settings:\n  run_id: foodtorch\n  initialize_from: null\n  load_model: false\n  resume: false\n  force: true\n  train_model: false\n  inference: false\n```",
            "Torch settings:": "```\ntorch_settings:\n  device: cpu\n```"
          },
          "Behavior Configurations": "The primary section of the trainer config file is a\nset of configurations for each Behavior in your scene. These are defined under\nthe sub-section `behaviors` in your trainer config file. Some of the\nconfigurations are required while others are optional. To help us get started,\nbelow is a sample file that includes all the possible settings if we're using a\nPPO trainer with all the possible training functionalities enabled (memory,\nbehavioral cloning, curiosity, GAIL and self-play). You will notice that\ncurriculum and environment parameter randomization settings are not part of the `behaviors`\nconfiguration, but in their own section called `environment_parameters`.\n\n```\nbehaviors:\n  BehaviorPPO:\n    trainer_type: ppo\n\n    hyperparameters:\n      # Hyperparameters common to PPO and SAC\n      batch_size: 1024\n      buffer_size: 10240\n      learning_rate: 3.0e-4\n      learning_rate_schedule: linear\n\n      # PPO-specific hyperparameters\n      beta: 5.0e-3\n      beta_schedule: constant\n      epsilon: 0.2\n      epsilon_schedule: linear\n      lambd: 0.95\n      num_epoch: 3\n      shared_critic: False\n\n    # Configuration of the neural network (common to PPO/SAC)\n    network_settings:\n      vis_encode_type: simple\n      normalize: false\n      hidden_units: 128\n      num_layers: 2\n      # memory\n      memory:\n        sequence_length: 64\n        memory_size: 256\n\n    # Trainer configurations common to all trainers\n    max_steps: 5.0e5\n    time_horizon: 64\n    summary_freq: 10000\n    keep_checkpoints: 5\n    checkpoint_interval: 50000\n    threaded: false\n    init_path: null\n\n    # behavior cloning\n    behavioral_cloning:\n      demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo\n      strength: 0.5\n      steps: 150000\n      batch_size: 512\n      num_epoch: 3\n      samples_per_update: 0\n\n    reward_signals:\n      # environment reward (default)\n      extrinsic:\n        strength: 1.0\n        gamma: 0.99\n\n      # curiosity module\n      curiosity:\n        strength: 0.02\n        gamma: 0.99\n        encoding_size: 256\n        learning_rate: 3.0e-4\n\n      # GAIL\n      gail:\n        strength: 0.01\n        gamma: 0.99\n        encoding_size: 128\n        demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo\n        learning_rate: 3.0e-4\n        use_actions: false\n        use_vail: false\n\n    # self-play\n    self_play:\n      window: 10\n      play_against_latest_model_ratio: 0.5\n      save_steps: 50000\n      swap_steps: 2000\n      team_change: 100000\n```\n\nHere is an equivalent file if we use an SAC trainer instead. Notice that the\nconfigurations for the additional functionalities (memory, behavioral cloning,\ncuriosity and self-play) remain unchanged.\n\n```\nbehaviors:\n  BehaviorSAC:\n    trainer_type: sac\n\n    # Trainer configs common to PPO/SAC (excluding reward signals)\n    # same as PPO config\n\n    # SAC-specific configs (replaces the hyperparameters section above)\n    hyperparameters:\n      # Hyperparameters common to PPO and SAC\n      # Same as PPO config\n\n      # SAC-specific hyperparameters\n      # Replaces the \"PPO-specific hyperparameters\" section above\n      buffer_init_steps: 0\n      tau: 0.005\n      steps_per_update: 10.0\n      save_replay_buffer: false\n      init_entcoef: 0.5\n      reward_signal_steps_per_update: 10.0\n\n    # Configuration of the neural network (common to PPO/SAC)\n    network_settings:\n      # Same as PPO config\n\n    # Trainer configurations common to all trainers\n      # <Same as PPO config>\n\n    # pre-training using behavior cloning\n    behavioral_cloning:\n      # same as PPO config\n\n    reward_signals:\n      # environment reward\n      extrinsic:\n        # same as PPO config\n\n      # curiosity module\n      curiosity:\n        # same as PPO config\n\n      # GAIL\n      gail:\n        # same as PPO config\n\n    # self-play\n    self_play:\n      # same as PPO config\n```\n\nWe now break apart the components of the configuration file and describe what\neach of these parameters mean and provide guidelines on how to set them. See\n[Training Configuration File](Training-Configuration-File.md) for a detailed\ndescription of all the configurations listed above, along with their defaults.\nUnless otherwise specified, omitting a configuration will revert it to its default.",
          "Default Behavior Settings": "In some cases, you may want to specify a set of default configurations for your Behaviors.\nThis may be useful, for instance, if your Behavior names are generated procedurally by\nthe environment and not known before runtime, or if you have many Behaviors with very similar\nsettings. To specify a default configuration, insert a `default_settings` section in your YAML.\nThis section should be formatted exactly like a configuration for a Behavior.\n\n```\ndefault_settings:\n  # < Same as Behavior configuration >\nbehaviors:\n  # < Same as above >\n```\n\nBehaviors found in the environment that aren't specified in the YAML will now use the `default_settings`,\nand unspecified settings in behavior configurations will default to the values in `default_settings` if\nspecified there.",
          "Environment Parameters": {
            "Environment Parameter Randomization": {
              "Supported Sampler Types": "Below is a list of the `sampler_type` values supported by the toolkit.\n\n['`uniform` - Uniform sampler', ['Uniformly samples a single float value from a range with a given minimum\\nand maximum value (inclusive).', '**parameters** - `min_value`, `max_value`'], '`gaussian` - Gaussian sampler', ['Samples a single float value from a normal distribution with a given mean\\nand standard deviation.', '**parameters** - `mean`, `st_dev`'], '`multirange_uniform` - Multirange uniform sampler', ['First, samples an interval from a set of intervals in proportion to relative\\nlength of the intervals. Then, uniformly samples a single float value from the\\nsampled interval (inclusive). This sampler can take an arbitrary number of\\nintervals in a list in the following format:\\n[[`interval_1_min`, `interval_1_max`], [`interval_2_min`,\\n`interval_2_max`], ...]', '**parameters** - `intervals`']]\n\nThe implementation of the samplers can be found in the\n[Samplers.cs file](https://github.com/Unity-Technologies/ml-agents/blob/main/com.unity.ml-agents/Runtime/Sampler.cs).",
              "Training with Environment Parameter Randomization": "After the sampler configuration is defined, we proceed by launching `mlagents-learn`\nand specify trainer configuration with  parameter randomization enabled. For example,\nif we wanted to train the 3D ball agent with parameter randomization, we would run\n\n```\nmlagents-learn config/ppo/3DBall_randomize.yaml --run-id=3D-Ball-randomize\n```\n\nWe can observe progress and metrics via TensorBoard."
            },
            "Curriculum": {
              "Training with a Curriculum": "Once we have specified our metacurriculum and curricula, we can launch\n`mlagents-learn` to point to the config file containing\nour curricula and PPO will train using Curriculum Learning. For example, to\ntrain agents in the Wall Jump environment with curriculum learning, we can run:\n\n```\nmlagents-learn config/ppo/WallJump_curriculum.yaml --run-id=wall-jump-curriculum\n```\n\nWe can then keep track of the current lessons and progresses via TensorBoard. If you've terminated\nthe run, you can resume it using `--resume` and lesson progress will start off where it\nended."
            }
          },
          "Training Using Concurrent Unity Instances": "In order to run concurrent Unity instances during training, set the number of\nenvironment instances using the command line option `--num-envs=<n>` when you\ninvoke `mlagents-learn`. Optionally, you can also set the `--base-port`, which\nis the starting port used for the concurrent Unity instances.\n\nSome considerations:\n\n['**Buffer Size** - If you are having trouble getting an agent to train, even\\nwith multiple concurrent Unity instances, you could increase `buffer_size` in\\nthe trainer config file. A common practice is to multiply\\n`buffer_size` by `num-envs`.', '**Resource Constraints** - Invoking concurrent Unity instances is constrained\\nby the resources on the machine. Please use discretion when setting\\n`--num-envs=<n>`.', '**Result Variation Using Concurrent Unity Instances** - If you keep all the\\nhyperparameters the same, but change `--num-envs=<n>`, the results and model\\nwould likely change.']"
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 42]"
    },
    {
      "title": "Customizing Training via Plugins",
      "description": null,
      "content": {
        "How to Write Your Own Plugin": {
          "setup.py": "If you don't already have a `setup.py` file for your python code, you'll need to add one. `ml-agents-plugin-examples`\nhas a [minimal example](../ml-agents-plugin-examples/setup.py) of this.\n\nIn the call to `setup()`, you'll need to add to the `entry_points` dictionary for each plugin interface that you\nimplement. The form of this is `{entry point name}={plugin module}:{plugin function}`. For example, in\n`ml-agents-plugin-examples`:\n\n```\nentry_points={\n    ML_AGENTS_STATS_WRITER: [\n        \"example=mlagents_plugin_examples.example_stats_writer:get_example_stats_writer\"\n    ]\n}\n```\n\n['`ML_AGENTS_STATS_WRITER` (which is a string constant, `mlagents.stats_writer`) is the name of the plugin interface.\\nThis must be one of the provided interfaces ([see below](#plugin-interfaces)).', '`example` is the plugin implementation name. This can be anything.', '`mlagents_plugin_examples.example_stats_writer` is the plugin module. This points to the module where the\\nplugin registration function is defined.', '`get_example_stats_writer` is the plugin registration function. This is called when running `mlagents-learn`. The\\narguments and expected return type for this are different for each plugin interface.']",
          "Local Installation": "Once you've defined `entry_points` in your `setup.py`, you will need to run\n\n```\npip install -e [path to your plugin code]\n```\n\nin the same python virtual environment that you have `mlagents` installed."
        },
        "Plugin Interfaces": {
          "StatsWriter": {
            "Interface": "The `StatsWriter.write_stats()` method must be implemented in any derived classes. It takes a \"category\" parameter,\nwhich typically is the behavior name of the Agents being trained, and a dictionary of `StatSummary` values with\nstring keys. Additionally, `StatsWriter.on_add_stat()` may be extended to register a callback handler for each stat\nemission.",
            "Registration": "The `StatsWriter` registration function takes a `RunOptions` argument and returns a list of `StatsWriter`s. An\nexample implementation is provided in [`mlagents_plugin_examples`](../ml-agents-plugin-examples/mlagents_plugin_examples/example_stats_writer.py)"
          }
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 43]"
    },
    {
      "title": "Training on Amazon Web Service",
      "description": null,
      "content": {
        "Pre-configured AMI": "We've prepared a pre-configured AMI for you with the ID: `ami-016ff5559334f8619`\nin the `us-east-1` region. It was created as a modification of\n[Deep Learning AMI (Ubuntu)](https://aws.amazon.com/marketplace/pp/B077GCH38C).\nThe AMI has been tested with p2.xlarge instance. Furthermore, if you want to\ntrain without headless mode, you need to enable X Server.\n\nAfter launching your EC2 instance using the ami and ssh into it, run the\nfollowing commands to enable it:\n\n```\n# Start the X Server, press Enter to come to the command line\n$ sudo /usr/bin/X :0 &\n\n# Check if Xorg process is running\n# You will have a list of processes running on the GPU, Xorg should be in the\n# list, as shown below\n$ nvidia-smi\n\n# Thu Jun 14 20:27:26 2018\n# +-----------------------------------------------------------------------------+\n# | NVIDIA-SMI 390.67                 Driver Version: 390.67                    |\n# |-------------------------------+----------------------+----------------------+\n# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n# |===============================+======================+======================|\n# |   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n# | N/A   35C    P8    31W / 149W |      9MiB / 11441MiB |      0%      Default |\n# +-------------------------------+----------------------+----------------------+\n#\n# +-----------------------------------------------------------------------------+\n# | Processes:                                                       GPU Memory |\n# |  GPU       PID   Type   Process name                             Usage      |\n# |=============================================================================|\n# |    0      2331      G   /usr/lib/xorg/Xorg                             8MiB |\n# +-----------------------------------------------------------------------------+\n\n# Make the ubuntu use X Server for display\n$ export DISPLAY=:0\n```",
        "Configuring your own instance": {
          "Installing the ML-Agents Toolkit on the instance": "After launching your EC2 instance using the ami and ssh into it:\n\n['Activate the python3 environment', '```\\nsource activate python3\\n```', 'Clone the ML-Agents repo and install the required Python packages', '```\\ngit clone --branch release_22 https://github.com/Unity-Technologies/ml-agents.git\\ncd ml-agents/ml-agents/\\npip3 install -e .\\n```']",
          "Setting up X Server (optional)": {
            "Install and setup Xorg:": "```sh\n# Install Xorg\n$ sudo apt-get update\n$ sudo apt-get install -y xserver-xorg mesa-utils\n$ sudo nvidia-xconfig -a --use-display-device=None --virtual=1280x1024\n\n# Get the BusID information\n$ nvidia-xconfig --query-gpu-info\n\n# Add the BusID information to your /etc/X11/xorg.conf file\n$ sudo sed -i 's/    BoardName      \"Tesla K80\"/    BoardName      \"Tesla K80\"\\n    BusID          \"0:30:0\"/g' /etc/X11/xorg.conf\n\n# Remove the Section \"Files\" from the /etc/X11/xorg.conf file\n# And remove two lines that contain Section \"Files\" and EndSection\n$ sudo vim /etc/X11/xorg.conf\n```\n",
            "Update and setup Nvidia driver:": "```sh\n# Download and install the latest Nvidia driver for ubuntu\n# Please refer to http://download.nvidia.com/XFree86/Linux-#x86_64/latest.txt\n$ wget http://download.nvidia.com/XFree86/Linux-x86_64/390.87/NVIDIA-Linux-x86_64-390.87.run\n$ sudo /bin/bash ./NVIDIA-Linux-x86_64-390.87.run --accept-license --no-questions --ui=none\n\n# Disable Nouveau as it will clash with the Nvidia driver\n$ sudo echo 'blacklist nouveau'  | sudo tee -a /etc/modprobe.d/blacklist.conf\n$ sudo echo 'options nouveau modeset=0'  | sudo tee -a /etc/modprobe.d/blacklist.conf\n$ sudo echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf\n$ sudo update-initramfs -u\n```\n",
            "Restart the EC2 instance:": "```sh\nsudo reboot now\n```\n",
            "Make sure there are no Xorg processes running:": "```\n# Kill any possible running Xorg processes\n# Note that you might have to run this command multiple times depending on\n# how Xorg is configured.\n$ sudo killall Xorg\n\n# Check if there is any Xorg process left\n# You will have a list of processes running on the GPU, Xorg should not be in\n# the list, as shown below.\n$ nvidia-smi\n\n# Thu Jun 14 20:21:11 2018\n# +-----------------------------------------------------------------------------+\n# | NVIDIA-SMI 390.67                 Driver Version: 390.67                    |\n# |-------------------------------+----------------------+----------------------+\n# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n# |===============================+======================+======================|\n# |   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n# | N/A   37C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n# +-------------------------------+----------------------+----------------------+\n#\n# +-----------------------------------------------------------------------------+\n# | Processes:                                                       GPU Memory |\n# |  GPU       PID   Type   Process name                             Usage      |\n# |=============================================================================|\n# |  No running processes found                                                 |\n# +-----------------------------------------------------------------------------+\n\n```",
            "Start X Server and make the ubuntu use X Server for display:": "```console\n# Start the X Server, press Enter to come back to the command line\n$ sudo /usr/bin/X :0 &\n\n# Check if Xorg process is running\n# You will have a list of processes running on the GPU, Xorg should be in the list.\n$ nvidia-smi\n\n# Make the ubuntu use X Server for display\n$ export DISPLAY=:0\n```\n",
            "Ensure the Xorg is correctly configured:": "```sh\n# For more information on glxgears, see ftp://www.x.org/pub/X11R6.8.1/doc/glxgears.1.html.\n$ glxgears\n# If Xorg is configured correctly, you should see the following message\n\n# Running synchronized to the vertical refresh.  The framerate should be\n# approximately the same as the monitor refresh rate.\n# 137296 frames in 5.0 seconds = 27459.053 FPS\n# 141674 frames in 5.0 seconds = 28334.779 FPS\n# 141490 frames in 5.0 seconds = 28297.875 FPS\n\n```\n"
          }
        },
        "Training on EC2 instance": [
          "In the Unity Editor, load a project containing an ML-Agents environment (you\ncan use one of the example environments if you have not created your own).",
          "Open the Build Settings window (menu: File > Build Settings).",
          "Select Linux as the Target Platform, and x86_64 as the target architecture\n(the default x86 currently does not work).",
          "Check Headless Mode if you have not setup the X Server. (If you do not use\nHeadless Mode, you have to setup the X Server to enable training.)",
          "Click Build to build the Unity environment executable.",
          "Upload the executable to your EC2 instance within `ml-agents` folder.",
          "Change the permissions of the executable.",
          "```\nchmod +x <your_env>.x86_64\n```",
          "(Without Headless Mode) Start X Server and use it for display:",
          "```\n# Start the X Server, press Enter to come back to the command line\n$ sudo /usr/bin/X :0 &\n\n# Check if Xorg process is running\n# You will have a list of processes running on the GPU, Xorg should be in the list.\n$ nvidia-smi\n\n# Make the ubuntu use X Server for display\n$ export DISPLAY=:0\n```",
          "Test the instance setup from Python using:",
          "```\nfrom mlagents_envs.environment import UnityEnvironment\n\nenv = UnityEnvironment(<your_env>)\n```",
          "Where `<your_env>` corresponds to the path to your environment executable.",
          "You should receive a message confirming that the environment was loaded\nsuccessfully.",
          "Train your models",
          "```\nmlagents-learn <trainer-config-file> --env=<your_env> --train\n```"
        ],
        "FAQ": {
          "The <Executable_Name>\\_Data folder hasn't been copied cover": "If you've built your Linux executable, but forget to copy over the corresponding\n<Executable_Name>\\_Data folder, you will see error message like the following:\n\n```\nSet current directory to /home/ubuntu/ml-agents/ml-agents\nFound path: /home/ubuntu/ml-agents/ml-agents/3dball_linux.x86_64\nno boot config - using default values\n\n(Filename:  Line: 403)\n\nThere is no data folder\n```",
          "Unity Environment not responding": "If you didn't setup X Server or hasn't launched it properly, or your environment\nsomehow crashes, or you haven't `chmod +x` your Unity Environment, all of these\nwill cause connection between Unity and Python to fail. Then you will see\nsomething like this:\n\n```\nLogging to /home/ubuntu/.config/unity3d/<Some_Path>/Player.log\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 63, in __init__\n    aca_params = self.send_academy_parameters(rl_init_parameters_in)\n  File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 489, in send_academy_parameters\n    return self.communicator.initialize(inputs).rl_initialization_output\n  File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/rpc_communicator.py\", line 60, in initialize\nmlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :\n         The environment does not need user interaction to launch\n         The environment and the Python interface have compatible versions.\n```\n\nIt would be also really helpful to check your\n/home/ubuntu/.config/unity3d/<Some_Path>/Player.log to see what happens with\nyour Unity environment.",
          "Could not launch X Server": "When you execute:\n\n```\nsudo /usr/bin/X :0 &\n```\n\nYou might see something like:\n\n```\nX.Org X Server 1.18.4\n...\n(==) Log file: \"/var/log/Xorg.0.log\", Time: Thu Oct 11 21:10:38 2018\n(==) Using config file: \"/etc/X11/xorg.conf\"\n(==) Using system config directory \"/usr/share/X11/xorg.conf.d\"\n(EE)\nFatal server error:\n(EE) no screens found(EE)\n(EE)\nPlease consult the X.Org Foundation support\n         at http://wiki.x.org\n for help.\n(EE) Please also check the log file at \"/var/log/Xorg.0.log\" for additional information.\n(EE)\n(EE) Server terminated with error (1). Closing log file.\n```\n\nAnd when you execute:\n\n```\nnvidia-smi\n```\n\nYou might see something like:\n\n```\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\nThis means the NVIDIA's driver needs to be updated. Refer to\n[this section](Training-on-Amazon-Web-Service.md#update-and-setup-nvidia-driver)\nfor more information."
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 44]"
    },
    {
      "title": "Training on Microsoft Azure (works with ML-Agents Toolkit v0.3)",
      "description": null,
      "content": {
        "Pre-Configured Azure Virtual Machine": "A pre-configured virtual machine image is available in the Azure Marketplace and\nis nearly completely ready for training. You can start by deploying the\n[Data Science Virtual Machine for Linux (Ubuntu)](https://learn.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/dsvm-ubuntu-intro?view=azureml-api-2)\ninto your Azure subscription.\n\nNote that, if you choose to deploy the image to an\n[N-Series GPU optimized VM](https://docs.microsoft.com/azure/virtual-machines/linux/sizes-gpu),\ntraining will, by default, run on the GPU. If you choose any other type of VM,\ntraining will run on the CPU.",
        "Configuring your own Instance": "Setting up your own instance requires a number of package installations. Please\nview the documentation for doing so [here](#custom-instances).",
        "Installing ML-Agents": [
          "[Move](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/copy-files-to-linux-vm-using-scp)\nthe `ml-agents` sub-folder of this ml-agents repo to the remote Azure\ninstance, and set it as the working directory.",
          "Install the required packages:\nTorch: `pip3 install torch==1.7.0 -f https://download.pytorch.org/whl/torch_stable.html` and\nMLAgents: `python -m pip install mlagents==1.1.0`"
        ],
        "Testing": "To verify that all steps worked correctly:\n\n['In the Unity Editor, load a project containing an ML-Agents environment (you\\ncan use one of the example environments if you have not created your own).', 'Open the Build Settings window (menu: File > Build Settings).', 'Select Linux as the Target Platform, and x86_64 as the target architecture.', 'Check Headless Mode.', 'Click Build to build the Unity environment executable.', 'Upload the resulting files to your Azure instance.', 'Test the instance setup from Python using:']\n\n```\nfrom mlagents_envs.environment import UnityEnvironment\n\nenv = UnityEnvironment(file_name=\"<your_env>\", seed=1, side_channels=[])\n```\n\nWhere `<your_env>` corresponds to the path to your environment executable (i.e. `/home/UserName/Build/yourFile`).\n\nYou should receive a message confirming that the environment was loaded\nsuccessfully.\n\n**Note:** When running your environment in headless mode, you must append `--no-graphics` to your mlagents-learn command, as it won't train otherwise.\nYou can test this simply by aborting a training and check if it says \"Model Saved\" or \"Aborted\", or see if it generated the .onnx in the result folder.",
        "Running Training on your Virtual Machine": "To run your training on the VM:\n\n['[Move](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/copy-files-to-linux-vm-using-scp)\\nyour built Unity application to your Virtual Machine.', 'Set the directory where the ML-Agents Toolkit was installed to your working\\ndirectory.', 'Run the following command:']\n\n```\nmlagents-learn <trainer_config> --env=<your_app> --run-id=<run_id> --train\n```\n\nWhere `<your_app>` is the path to your app (i.e.\n`~/unity-volume/3DBallHeadless`) and `<run_id>` is an identifier you would like\nto identify your training run with.\n\nIf you've selected to run on a N-Series VM with GPU support, you can verify that\nthe GPU is being used by running `nvidia-smi` from the command line.",
        "Monitoring your Training Run with TensorBoard": "Once you have started training, you can\n[use TensorBoard to observe the training](Using-Tensorboard.md).\n\n['Start by\\n[opening the appropriate port for web traffic to connect to your VM](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/nsg-quickstart-portal).', [\"Note that you don't need to generate a new `Network Security Group` but\\ninstead, go to the **Networking** tab under **Settings** for your VM.\", 'As an example, you could use the following settings to open the Port with\\nthe following Inbound Rule settings:', ['Source: Any', 'Source Port Ranges: \\\\*', 'Destination: Any', 'Destination Port Ranges: 6006', 'Protocol: Any', 'Action: Allow', 'Priority: (Leave as default)']], 'Unless you started the training as a background process, connect to your VM\\nfrom another terminal instance.', 'Run the following command from your terminal\\n`tensorboard --logdir results --host 0.0.0.0`', 'You should now be able to open a browser and navigate to\\n`<Your_VM_IP_Address>:6060` to view the TensorBoard report.']",
        "Running on Azure Container Instances": "[Azure Container Instances](https://azure.microsoft.com/en-us/products/container-instances/)\nallow you to spin up a container, on demand, that will run your training and\nthen be shut down. This ensures you aren't leaving a billable VM running when it\nisn't needed. Using ACI enables you to offload training of your models without\nneeding to install Python and TensorFlow on your own computer.",
        "Custom Instances": "This page contains instructions for setting up a custom Virtual Machine on\nMicrosoft Azure so you can running ML-Agents training in the cloud.\n\n['Start by\\n[deploying an Azure VM](https://docs.microsoft.com/azure/virtual-machines/linux/quick-create-portal)\\nwith Ubuntu Linux (tests were done with 16.04 LTS). To use GPU support, use a\\nN-Series VM.', 'SSH into your VM.', 'Start with the following commands to install the Nvidia driver:', '```\\nwget http://us.download.nvidia.com/tesla/375.66/nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb\\n\\nsudo dpkg -i nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb\\n\\nsudo apt-get update\\n\\nsudo apt-get install cuda-drivers\\n\\nsudo reboot\\n```', 'After a minute you should be able to reconnect to your VM and install the\\nCUDA toolkit:', '```\\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\\n\\nsudo dpkg -i cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\\n\\nsudo apt-get update\\n\\nsudo apt-get install cuda-8-0\\n```', \"You'll next need to download cuDNN from the Nvidia developer site. This\\nrequires a registered account.\", 'Navigate to [http://developer.nvidia.com](http://developer.nvidia.com) and\\ncreate an account and verify it.', 'Download (to your own computer) cuDNN from\\n[this url](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v6/prod/8.0_20170307/Ubuntu16_04_x64/libcudnn6_6.0.20-1+cuda8.0_amd64-deb).', 'Copy the deb package to your VM:', '```\\nscp libcudnn6_6.0.21-1+cuda8.0_amd64.deb <VMUserName>@<VMIPAddress>:libcudnn6_6.0.21-1+cuda8.0_amd64.deb\\n```', 'SSH back to your VM and execute the following:', '```\\nsudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb\\n\\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/lib/x86_64-linux-gnu/:$LD_LIBRARY_PATH\\n. ~/.profile\\n\\nsudo reboot\\n```', 'After a minute, you should be able to SSH back into your VM. After doing so,\\nrun the following:', '```\\nsudo apt install python-pip\\nsudo apt install python3-pip\\n```', 'At this point, you need to install TensorFlow. The version you install\\nshould be tied to if you are using GPU to train:', '```\\npip3 install tensorflow-gpu==1.4.0 keras==2.0.6\\n```', 'Or CPU to train:', '```\\npip3 install tensorflow==1.4.0 keras==2.0.6\\n```', \"You'll then need to install additional dependencies:\", '```\\npip3 install pillow\\npip3 install numpy\\n```']"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 45]"
    },
    {
      "title": "Custom Trainer Plugin",
      "description": null,
      "content": {
        "How to write a custom trainer plugin": {
          "Step 1: Write your custom trainer class": "Before you start writing your code, make sure to use your favorite environment management tool(e.g. `venv` or `conda`) to create and activate a Python virtual environment. The following command uses `conda`, but other tools work similarly:\n\n```\nconda create -n trainer-env python=3.10.12\nconda activate trainer-env\n```\n\nUsers of the plug-in system are responsible for implementing the trainer class subject to the API standard. Let us follow an example by implementing a custom trainer named \"YourCustomTrainer\". You can either extend `OnPolicyTrainer` or `OffPolicyTrainer` classes depending on the training strategies you choose.\n\nPlease refer to the internal [PPO implementation](../ml-agents/mlagents/trainers/ppo/trainer.py) for a complete code example. We will not provide a workable code in the document. The purpose of the tutorial is to introduce you to the core components and interfaces of our plugin framework. We use code snippets and patterns to demonstrate the control and data flow.\n\nYour custom trainers are responsible for collecting experiences and training the models. Your custom trainer class acts like a coordinator to the policy and optimizer. To start implementing methods in the class, create a policy class objects from method `create_policy`:\n\n```\ndef create_policy(\n    self, parsed_behavior_id: BehaviorIdentifiers, behavior_spec: BehaviorSpec\n) -> TorchPolicy:\n\n    actor_cls: Union[Type[SimpleActor], Type[SharedActorCritic]] = SimpleActor\n    actor_kwargs: Dict[str, Any] = {\n        \"conditional_sigma\": False,\n        \"tanh_squash\": False,\n    }\n    if self.shared_critic:\n        reward_signal_configs = self.trainer_settings.reward_signals\n        reward_signal_names = [\n            key.value for key, _ in reward_signal_configs.items()\n        ]\n        actor_cls = SharedActorCritic\n        actor_kwargs.update({\"stream_names\": reward_signal_names})\n\n    policy = TorchPolicy(\n        self.seed,\n        behavior_spec,\n        self.trainer_settings.network_settings,\n        actor_cls,\n        actor_kwargs,\n    )\n    return policy\n\n```\n\nDepending on whether you use shared or separate network architecture for your policy, we provide `SimpleActor` and `SharedActorCritic` from `mlagents.trainers.torch_entities.networks` that you can choose from. In our example above, we use a `SimpleActor`.\n\nNext, create an optimizer class object from `create_optimizer` method and connect it to the policy object you created above:\n\n```\ndef create_optimizer(self) -> TorchOptimizer:\n    return TorchPPOOptimizer(  # type: ignore\n        cast(TorchPolicy, self.policy), self.trainer_settings  # type: ignore\n    )  # type: ignore\n\n```\n\nThere are a couple of abstract methods(`_process_trajectory` and `_update_policy`) inherited from `RLTrainer` that you need to implement in your custom trainer class. `_process_trajectory` takes a trajectory and processes it, putting it into the update buffer. Processing involves calculating value and advantage targets for the model updating step. Given input `trajectory: Trajectory`, users are responsible for processing the data in the trajectory and append `agent_buffer_trajectory` to the back of the update buffer by calling `self._append_to_update_buffer(agent_buffer_trajectory)`, whose output will be used in updating the model in `optimizer` class.\n\nA typical `_process_trajectory` function(incomplete) will convert a trajectory object to an agent buffer then get all value estimates from the trajectory by calling `self.optimizer.get_trajectory_value_estimates`. From the returned dictionary of value estimates we extract reward signals keyed by their names:\n\n```\ndef _process_trajectory(self, trajectory: Trajectory) -> None:\n    super()._process_trajectory(trajectory)\n    agent_id = trajectory.agent_id  # All the agents should have the same ID\n\n    agent_buffer_trajectory = trajectory.to_agentbuffer()\n\n    # Get all value estimates\n    (\n        value_estimates,\n        value_next,\n        value_memories,\n    ) =  self.optimizer.get_trajectory_value_estimates(\n        agent_buffer_trajectory,\n        trajectory.next_obs,\n        trajectory.done_reached and not trajectory.interrupted,\n    )\n\n    for name, v in value_estimates.items():\n        agent_buffer_trajectory[RewardSignalUtil.value_estimates_key(name)].extend(\n            v\n        )\n        self._stats_reporter.add_stat(\n            f\"Policy/{self.optimizer.reward_signals[name].name.capitalize()} Value Estimate\",\n            np.mean(v),\n        )\n\n    # Evaluate all reward functions\n    self.collected_rewards[\"environment\"][agent_id] += np.sum(\n        agent_buffer_trajectory[BufferKey.ENVIRONMENT_REWARDS]\n    )\n    for name, reward_signal in self.optimizer.reward_signals.items():\n        evaluate_result = (\n            reward_signal.evaluate(agent_buffer_trajectory) * reward_signal.strength\n        )\n        agent_buffer_trajectory[RewardSignalUtil.rewards_key(name)].extend(\n            evaluate_result\n        )\n        # Report the reward signals\n        self.collected_rewards[name][agent_id] += np.sum(evaluate_result)\n\n    self._append_to_update_buffer(agent_buffer_trajectory)\n\n```\n\nA trajectory will be a list of dictionaries of strings mapped to `Anything`. When calling `forward` on a policy, the argument will include an “experience” dictionary from the last step. The `forward` method will generate an action and the next “experience” dictionary. Examples of fields in the “experience” dictionary include observation, action, reward, done status, group_reward, LSTM memory state, etc.",
          "Step 2: implement your custom optimizer for the trainer.": "We will show you an example we implemented - `class TorchPPOOptimizer(TorchOptimizer)`, which takes a Policy and a Dict of trainer parameters and creates an Optimizer that connects to the policy. Your optimizer should include a value estimator and a loss function in the `update` method.\n\nBefore writing your optimizer class, first define setting class `class PPOSettings(OnPolicyHyperparamSettings)` for your custom optimizer:\n\n```\nclass PPOSettings(OnPolicyHyperparamSettings):\n    beta: float = 5.0e-3\n    epsilon: float = 0.2\n    lambd: float = 0.95\n    num_epoch: int = 3\n    shared_critic: bool = False\n    learning_rate_schedule: ScheduleType = ScheduleType.LINEAR\n    beta_schedule: ScheduleType = ScheduleType.LINEAR\n    epsilon_schedule: ScheduleType = ScheduleType.LINEAR\n\n```\n\nYou should implement `update` function following interface:\n\n```\ndef update(self, batch: AgentBuffer, num_sequences: int) -> Dict[str, float]:\n\n```\n\nIn which losses and other metrics are calculated from an `AgentBuffer` that is generated from your trainer class, depending on which model you choose to implement the loss functions will be different. In our case we calculate value loss from critic and trust region policy loss. A typical pattern(incomplete) of the calculations will look like the following:\n\n```\nrun_out = self.policy.actor.get_stats(\n    current_obs,\n    actions,\n    masks=act_masks,\n    memories=memories,\n    sequence_length=self.policy.sequence_length,\n)\n\nlog_probs = run_out[\"log_probs\"]\nentropy = run_out[\"entropy\"]\n\nvalues, _ = self.critic.critic_pass(\n    current_obs,\n    memories=value_memories,\n    sequence_length=self.policy.sequence_length,\n)\npolicy_loss = ModelUtils.trust_region_policy_loss(\n    ModelUtils.list_to_tensor(batch[BufferKey.ADVANTAGES]),\n    log_probs,\n    old_log_probs,\n    loss_masks,\n    decay_eps,\n)\nloss = (\n    policy_loss\n    + 0.5 * value_loss\n    - decay_bet * ModelUtils.masked_mean(entropy, loss_masks)\n)\n\n```\n\nFinally update the model and return the a dictionary including calculated losses and updated decay learning rate:\n\n```\nModelUtils.update_learning_rate(self.optimizer, decay_lr)\nself.optimizer.zero_grad()\nloss.backward()\n\nself.optimizer.step()\nupdate_stats = {\n    \"Losses/Policy Loss\": torch.abs(policy_loss).item(),\n    \"Losses/Value Loss\": value_loss.item(),\n    \"Policy/Learning Rate\": decay_lr,\n    \"Policy/Epsilon\": decay_eps,\n    \"Policy/Beta\": decay_bet,\n}\n\n```",
          "Step 3: Integrate your custom trainer into the plugin system": "By integrating a custom trainer into the plugin system, a user can use their published packages which have their implementations. To do that, you need to add a setup.py file. In the call to setup(), you'll need to add to the entry_points dictionary for each plugin interface that you implement. The form of this is {entry point name}={plugin module}:{plugin function}. For example:\n\n```\nentry_points={\n        ML_AGENTS_TRAINER_TYPE: [\n            \"your_trainer_type=your_package.your_custom_trainer:get_type_and_setting\"\n        ]\n    },\n```\n\nSome key elements in the code:\n\n```\nML_AGENTS_TRAINER_TYPE: a string constant for trainer type\nyour_trainer_type: name your trainer type, used in configuration file\nyour_package: your pip installable package containing custom trainer implementation\n```\n\nAlso define `get_type_and_setting` method in `YourCustomTrainer` class:\n\n```\ndef get_type_and_setting():\n    return {YourCustomTrainer.get_trainer_name(): YourCustomTrainer}, {\n        YourCustomTrainer.get_trainer_name(): YourCustomSetting\n    }\n\n```\n\nFinally, specify trainer type in the config file:\n\n```\nbehaviors:\n  3DBall:\n    trainer_type: your_trainer_type\n...\n```",
          "Step 4: Install your custom trainer and run training:": "Before installing your custom trainer package, make sure you have `ml-agents-env` and `ml-agents` installed\n\n```\npip3 install -e ./ml-agents-envs && pip3 install -e ./ml-agents\n```\n\nInstall your custom trainer package(if your package is pip installable):\n\n```\npip3 install your_custom_package\n```\n\nOr follow our internal implementations:\n\n```\npip3 install -e ./ml-agents-trainer-plugin\n```\n\nFollowing the previous installations your package is added as an entrypoint and you can use a config file with new\ntrainers:\n\n```\nmlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id <run-id-name>\n--env <env-executable>\n```",
          "Validate your implementations:": "Create a clean Python environment with Python 3.10.12 and activate it before you start, if you haven't done so already:\n\n```\nconda create -n trainer-env python=3.10.12\nconda activate trainer-env\n```\n\nMake sure you follow previous steps and install all required packages. We are testing internal implementations in this tutorial, but ML-Agents users can run similar validations once they have their own implementations installed:\n\n```\npip3 install -e ./ml-agents-envs && pip3 install -e ./ml-agents\npip3 install -e ./ml-agents-trainer-plugin\n```\n\nOnce your package is added as an `entrypoint`, you can add to the config file the new trainer type. Check if trainer type is specified in the config file `a2c_3DBall.yaml`:\n\n```\ntrainer_type: a2c\n```\n\nTest if custom trainer package is installed by running:\n\n```\nmlagents-learn ml-agents-trainer-plugin/mlagents_trainer_plugin/a2c/a2c_3DBall.yaml --run-id test-trainer\n```\n\nYou can also list all trainers installed in the registry. Type `python` in your shell to open a REPL session. Run the python code below, you should be able to see all trainer types currently installed:\n\n```\n>>> import pkg_resources\n>>> for entry in pkg_resources.iter_entry_points('mlagents.trainer_type'):\n...     print(entry)\n...\ndefault = mlagents.plugins.trainer_type:get_default_trainer_types\na2c = mlagents_trainer_plugin.a2c.a2c_trainer:get_type_and_setting\ndqn = mlagents_trainer_plugin.dqn.dqn_trainer:get_type_and_setting\n```\n\nIf it is properly installed, you will see Unity logo and message indicating training will start:\n\n```\n[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n```\n\nIf you see the following error message, it could be due to trainer type is wrong or the trainer type specified is not installed:\n\n```\nmlagents.trainers.exception.TrainerConfigError: Invalid trainer type a2c was found\n```"
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 46]"
    },
    {
      "title": "Unity Environment Registry [Experimental]",
      "description": null,
      "content": {
        "Loading an Environment from the Registry": "To get started, you can access the default registry we provide with our [Example Environments](Learning-Environment-Examples.md). The Unity Environment Registry implements a _Mapping_, therefore, you can access an entry with its identifier with the square brackets `[ ]`. Use the following code to list all of the environment identifiers present in the default registry:\n\n```\nfrom mlagents_envs.registry import default_registry\n\nenvironment_names = list(default_registry.keys())\nfor name in environment_names:\n   print(name)\n```\n\nThe `make()` method on a registry value will return a `UnityEnvironment` ready to be used. All arguments passed to the make method will be passed to the constructor of the `UnityEnvironment` as well. Refer to the documentation on the [Python-API](Python-LLAPI.md) for more information about the arguments of the `UnityEnvironment` constructor. For example, the following code will create the environment under the identifier `\"my-env\"`, reset it, perform a few steps and finally close it:\n\n```\nfrom mlagents_envs.registry import default_registry\n\nenv = default_registry[\"my-env\"].make()\nenv.reset()\nfor _ in range(10):\n  env.step()\nenv.close()\n```",
        "Create and share your own registry": "In order to share the `UnityEnvironment` you created, you must:\n\n['[Create a Unity executable](Learning-Environment-Executable.md) of your environment for each platform (Linux, OSX and/or Windows)', 'Place each executable in a `zip` compressed folder', 'Upload each zip file online to your preferred hosting platform', 'Create a `yaml` file that will contain the description and path to your environment', 'Upload the `yaml` file online\\nThe `yaml` file must have the following format :']\n\n```\nenvironments:\n  - <environment-identifier>:\n     expected_reward: <expected-reward-float>\n     description: <description-of-the-environment>\n     linux_url: <url-to-the-linux-zip-folder>\n     darwin_url: <url-to-the-osx-zip-folder>\n     win_url: <url-to-the-windows-zip-folder>\n     additional_args:\n      - <an-optional-list-of-command-line-arguments-for-the-executable>\n      - ...\n```\n\nYour users can now use your environment with the following code :\n\n```\nfrom mlagents_envs.registry import UnityEnvRegistry\n\nregistry = UnityEnvRegistry()\nregistry.register_from_yaml(\"url-or-path-to-your-yaml-file\")\n```\n\n__Note__: The `\"url-or-path-to-your-yaml-file\"` can be either a url or a local path."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 47]"
    },
    {
      "title": "Using Docker For ML-Agents (Deprecated)",
      "description": null,
      "content": {
        "Requirements": [
          "[Docker](https://www.docker.com)",
          "Unity _Linux Build Support_ Component. Make sure to select the _Linux Build\nSupport_ component when installing Unity."
        ],
        "Setup": [
          "[Download](https://unity3d.com/get-unity/download) the Unity Installer and add\nthe _Linux Build Support_ Component",
          "[Download](https://www.docker.com/community-edition#/download) and install\nDocker if you don't have it setup on your machine.",
          "Since Docker runs a container in an environment that is isolated from the host\nmachine, a mounted directory in your host machine is used to share data, e.g.\nthe trainer configuration file, Unity executable and\nTensorFlow graph. For convenience, we created an empty `unity-volume`\ndirectory at the root of the repository for this purpose, but feel free to use\nany other directory. The remainder of this guide assumes that the\n`unity-volume` directory is the one used."
        ],
        "Usage": {
          "Build the Environment (Optional)": "_If you want to used the Editor to perform training, you can skip this step._\n\nSince Docker typically runs a container sharing a (linux) kernel with the host\nmachine, the Unity environment **has** to be built for the **linux platform**.\nWhen building a Unity environment, please select the following options from the\nthe Build Settings window:\n\n['Set the _Target Platform_ to `Linux`', 'Set the _Architecture_ to `x86_64`']\n\nThen click `Build`, pick an environment name (e.g. `3DBall`) and set the output\ndirectory to `unity-volume`. After building, ensure that the file\n`<environment-name>.x86_64` and subdirectory `<environment-name>_Data/` are\ncreated under `unity-volume`.\n\n![Build Settings For Docker](images/docker_build_settings.png)",
          "Build the Docker Container": "First, make sure the Docker engine is running on your machine. Then build the\nDocker container by calling the following command at the top-level of the\nrepository:\n\n```\ndocker build -t <image-name> .\n```\n\nReplace `<image-name>` with a name for the Docker image, e.g.\n`balance.ball.v0.1`.",
          "Run the Docker Container": "Run the Docker container by calling the following command at the top-level of\nthe repository:\n\n```\ndocker run -it --name <container-name> \\\n           --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\\n           -p 5005:5005 \\\n           -p 6006:6006 \\\n           <image-name>:latest \\\n           <trainer-config-file> \\\n           --env=<environment-name> \\\n           --train \\\n           --run-id=<run-id>\n```\n\nNotes on argument values:\n\n['`<container-name>` is used to identify the container (in case you want to\\ninterrupt and terminate it). This is optional and Docker will generate a\\nrandom name if this is not set. _Note that this must be unique for every run\\nof a Docker image._', '`<image-name>` references the image name used when building the container.', '`<environment-name>` **(Optional)**: If you are training with a linux\\nexecutable, this is the name of the executable. If you are training in the\\nEditor, do not pass a `<environment-name>` argument and press the **Play**\\nbutton in Unity when the message _\"Start training by pressing the Play button\\nin the Unity Editor\"_ is displayed on the screen.', '`source`: Reference to the path in your host OS where you will store the Unity\\nexecutable.', '`target`: Tells Docker to mount the `source` path as a disk with this name.', '`trainer-config-file`, `train`, `run-id`: ML-Agents arguments passed to\\n`mlagents-learn`. `trainer-config-file` is the filename of the trainer config\\nfile, `train` trains the algorithm, and `run-id` is used to tag each\\nexperiment with a unique identifier. We recommend placing the trainer-config\\nfile inside `unity-volume` so that the container has access to the file.']\n\nTo train with a `3DBall` environment executable, the command would be:\n\n```\ndocker run -it --name 3DBallContainer.first.trial \\\n           --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\\n           -p 5005:5005 \\\n           -p 6006:6006 \\\n           balance.ball.v0.1:latest 3DBall \\\n           /unity-volume/trainer_config.yaml \\\n           --env=/unity-volume/3DBall \\\n           --train \\\n           --run-id=3dball_first_trial\n```\n\nFor more detail on Docker mounts, check out\n[these](https://docs.docker.com/storage/bind-mounts/) docs from Docker.\n\n**NOTE** If you are training using docker for environments that use visual\nobservations, you may need to increase the default memory that Docker allocates\nfor the container. For example, see\n[here](https://docs.docker.com/docker-for-mac/#advanced) for instructions for\nDocker for Mac.",
          "Running Tensorboard": "You can run Tensorboard to monitor your training instance on\nhttp://localhost:6006:\n\n```\ndocker exec -it <container-name> tensorboard --logdir /unity-volume/results --host 0.0.0.0\n```\n\nWith our previous 3DBall example, this command would look like this:\n\n```\ndocker exec -it 3DBallContainer.first.trial tensorboard --logdir /unity-volume/results --host 0.0.0.0\n```\n\nFor more details on Tensorboard, check out the documentation about\n[Using Tensorboard](Using-Tensorboard.md).",
          "Stopping Container and Saving State": "If you are satisfied with the training progress, you can stop the Docker\ncontainer while saving state by either using `Ctrl+C` or `⌘+C` (Mac) or by using\nthe following command:\n\n```\ndocker kill --signal=SIGINT <container-name>\n```\n\n`<container-name>` is the name of the container specified in the earlier\n`docker run` command. If you didn't specify one, you can find the randomly\ngenerated identifier by running `docker container ls`."
        }
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 48]"
    },
    {
      "title": "Using TensorBoard to Observe Training",
      "description": null,
      "content": {
        "The ML-Agents Toolkit training statistics": {
          "Environment Statistics": [
            "`Environment/Lesson` - Plots the progress from lesson to lesson. Only\ninteresting when performing curriculum training.",
            "`Environment/Cumulative Reward` - The mean cumulative episode reward over all\nagents. Should increase during a successful training session.",
            "`Environment/Episode Length` - The mean length of each episode in the\nenvironment for all agents."
          ],
          "Is Training": [
            "`Is Training` - A boolean indicating if the agent is updating its model."
          ],
          "Policy Statistics": [
            "`Policy/Entropy` (PPO; SAC) - How random the decisions of the model are.\nShould slowly decrease during a successful training process. If it decreases\ntoo quickly, the `beta` hyperparameter should be increased.",
            "`Policy/Learning Rate` (PPO; SAC) - How large a step the training algorithm\ntakes as it searches for the optimal policy. Should decrease over time.",
            "`Policy/Entropy Coefficient` (SAC) - Determines the relative importance of the\nentropy term. This value is adjusted automatically so that the agent retains\nsome amount of randomness during training.",
            "`Policy/Extrinsic Reward` (PPO; SAC) - This corresponds to the mean cumulative\nreward received from the environment per-episode.",
            "`Policy/Value Estimate` (PPO; SAC) - The mean value estimate for all states\nvisited by the agent. Should increase during a successful training session.",
            "`Policy/Curiosity Reward` (PPO/SAC+Curiosity) - This corresponds to the mean\ncumulative intrinsic reward generated per-episode.",
            "`Policy/Curiosity Value Estimate` (PPO/SAC+Curiosity) - The agent's value\nestimate for the curiosity reward.",
            "`Policy/GAIL Reward` (PPO/SAC+GAIL) - This corresponds to the mean cumulative\ndiscriminator-based reward generated per-episode.",
            "`Policy/GAIL Value Estimate` (PPO/SAC+GAIL) - The agent's value estimate for\nthe GAIL reward.",
            "`Policy/GAIL Policy Estimate` (PPO/SAC+GAIL) - The discriminator's estimate\nfor states and actions generated by the policy.",
            "`Policy/GAIL Expert Estimate` (PPO/SAC+GAIL) - The discriminator's estimate\nfor states and actions drawn from expert demonstrations."
          ],
          "Learning Loss Functions": [
            "`Losses/Policy Loss` (PPO; SAC) - The mean magnitude of policy loss function.\nCorrelates to how much the policy (process for deciding actions) is changing.\nThe magnitude of this should decrease during a successful training session.",
            "`Losses/Value Loss` (PPO; SAC) - The mean loss of the value function update.\nCorrelates to how well the model is able to predict the value of each state.\nThis should increase while the agent is learning, and then decrease once the\nreward stabilizes.",
            "`Losses/Forward Loss` (PPO/SAC+Curiosity) - The mean magnitude of the forward\nmodel loss function. Corresponds to how well the model is able to predict the\nnew observation encoding.",
            "`Losses/Inverse Loss` (PPO/SAC+Curiosity) - The mean magnitude of the inverse\nmodel loss function. Corresponds to how well the model is able to predict the\naction taken between two observations.",
            "`Losses/Pretraining Loss` (BC) - The mean magnitude of the behavioral cloning\nloss. Corresponds to how well the model imitates the demonstration data.",
            "`Losses/GAIL Loss` (GAIL) - The mean magnitude of the GAIL discriminator loss.\nCorresponds to how well the model imitates the demonstration data."
          ],
          "Self-Play": [
            "`Self-Play/ELO` (Self-Play) -\n[ELO](https://en.wikipedia.org/wiki/Elo_rating_system) measures the relative\nskill level between two players. In a proper training run, the ELO of the\nagent should steadily increase."
          ]
        },
        "Exporting Data from TensorBoard": "To export timeseries data in CSV or JSON format, check the \"Show data download\nlinks\" in the upper left. This will enable download links below each chart.\n\n![Example TensorBoard Run](images/TensorBoard-download.png)",
        "Custom Metrics from Unity": "To get custom metrics from a C# environment into TensorBoard, you can use the\n`StatsRecorder`:\n\n```\nvar statsRecorder = Academy.Instance.StatsRecorder;\nstatsRecorder.Add(\"MyMetric\", 1.0);\n```"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 49]"
    },
    {
      "title": "Using Virtual Environment",
      "description": null,
      "content": {
        "What is a Virtual Environment?": "A Virtual Environment is a self contained directory tree that contains a Python\ninstallation for a particular version of Python, plus a number of additional\npackages. To learn more about Virtual Environments see\n[here](https://docs.python.org/3/library/venv.html).",
        "Why should I use a Virtual Environment?": "A Virtual Environment keeps all dependencies for the Python project separate\nfrom dependencies of other projects. This has a few advantages:\n\n['It makes dependency management for the project easy.', 'It enables using and testing of different library versions by quickly\\nspinning up a new environment and verifying the compatibility of the code\\nwith the different version.']",
        "Python Version Requirement (Required)": "This guide has been tested with Python 3.10.12. Newer versions might not\nhave support for the dependent libraries, so are not recommended.",
        "Use Conda (or Mamba)": "While there are many options for setting up virtual environments for python, by far the most common and simpler approach is by using Anaconda (aka Conda). You can read the documentation on how to get started with Conda [here](https://learning.anaconda.cloud/get-started-with-anaconda).",
        "Installing Pip (Required)": [
          "Download the `get-pip.py` file using the command\n`curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py`",
          "Run the following `python3 get-pip.py`",
          "Check pip version using `pip3 -V`"
        ],
        "Mac OS X Setup": [
          "Create a folder where the virtual environments will reside\n`$ mkdir ~/python-envs`",
          "To create a new environment named `sample-env` execute\n`$ python3 -m venv ~/python-envs/sample-env`",
          "To activate the environment execute\n`$ source ~/python-envs/sample-env/bin/activate`",
          "Upgrade to the latest pip version using `$ pip3 install --upgrade pip`",
          "Upgrade to the latest setuptools version using\n`$ pip3 install --upgrade setuptools`",
          "To deactivate the environment execute `$ deactivate` (you can reactivate the\nenvironment using the same `activate` command listed above)"
        ],
        "Ubuntu Setup": [
          "Install the python3-venv package using `$ sudo apt-get install python3-venv`",
          "Follow the steps in the Mac OS X installation."
        ],
        "Windows Setup": [
          "Create a folder where the virtual environments will reside `md python-envs`",
          "To create a new environment named `sample-env` execute\n`python -m venv python-envs\\sample-env`",
          "To activate the environment execute `python-envs\\sample-env\\Scripts\\activate`",
          "Upgrade to the latest pip version using `pip install --upgrade pip`",
          "To deactivate the environment execute `deactivate` (you can reactivate the\nenvironment using the same `activate` command listed above)"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 50]"
    },
    {
      "title": "ML-Agents Versioning",
      "description": null,
      "content": {
        "Context": "As the ML-Agents project evolves into a more mature product, we want to communicate the process\nwe use to version our packages and the data that flows into, through, and out of them clearly.\nOur project now has four packages (1 Unity, 3 Python) along with artifacts that are produced as\nwell as consumed.  This document covers the versioning for these packages and artifacts.",
        "GitHub Releases": "Up until now, all packages were in lockstep in-terms of versioning. As a result, the GitHub releases\nwere tagged with the version of all those packages (e.g. v0.15.0, v0.15.1) and labeled accordingly.\nWith the decoupling of package versions, we now need to revisit our GitHub release tagging.\nThe proposal is that we move towards an integer release numbering for our repo and each such\nrelease will call out specific version upgrades of each package. For instance, with\n[the April 30th release](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_1),\nwe will have:\n\n['GitHub Release 1 (branch name: *release_1_branch*)', ['com.unity.ml-agents release 1.0.0', 'ml-agents release 0.16.0', 'ml-agents-envs release 0.16.0', 'gym-unity release 0.16.0']]\n\nOur release cadence will not be affected by these versioning changes.  We will keep having\nmonthly releases to fix bugs and release new features.",
        "Packages": {
          "Unity package": "Package name: com.unity.ml-agents\n\n['Versioned following [Semantic Versioning Guidelines](https://www.semver.org)', 'This package consumes an artifact of the training process: the `.nn` file.  These files\\nare integer versioned and currently at version 2. The com.unity.ml-agents package\\nwill need to support the version of `.nn` files which existed at its 1.0.0 release.\\nFor example, consider that com.unity.ml-agents is at version 1.0.0 and the NN files\\nare at version 2.  If the NN files change to version 3, the next release of\\ncom.unity.ml-agents at version 1.1.0 guarantees it will be able to read both of these\\nformats.  If the NN files were to change to version 4 and com.unity.ml-agents to\\nversion 2.0.0, support for NN versions 2 and 3 could be dropped for com.unity.ml-agents\\nversion 2.0.0.', 'This package produces one artifact, the `.demo` files.  These files will have integer\\nversioning. This means their version will increment by 1 at each change.  The\\ncom.unity.ml-agents package must be backward compatible with version changes\\nthat occur between minor versions.', 'To summarize, the artifacts produced and consumed by com.unity.ml-agents are guaranteed\\nto be supported for 1.x.x versions of com.unity.ml-agents.  We intend to provide stability\\nfor our users by moving to a 1.0.0 release of com.unity.ml-agents.']",
          "Python Packages": "Package names: ml-agents / ml-agents-envs / gym-unity\n\n['The python packages remain in \"Beta.\"  This means that breaking changes to the public\\nAPI of the python packages can change without having to have a major version bump.\\nHistorically, the python and C# packages were in version lockstep.  This is no longer\\nthe case.  The python packages will remain in lockstep with each other for now, while the\\nC# package will follow its own versioning as is appropriate.  However, the python package\\nversions may diverge in the future.', 'While the python packages will remain in Beta for now, we acknowledge that the most\\nheavily used portion of our python interface is the `mlagents-learn` CLI and strive\\nto make this part of our API backward compatible. We are actively working on this and\\nexpect to have a stable CLI in the next few weeks.']"
        },
        "Communicator": "Packages which communicate: com.unity.ml-agents / ml-agents-envs\n\nAnother entity of the ML-Agents Toolkit that requires versioning is the communication layer\nbetween C# and Python, which will follow also semantic versioning.  This guarantees a level of\nbackward compatibility between different versions of C# and Python packages which communicate.\nAny Communicator version 1.x.x of the Unity package should be compatible with any 1.x.x\nCommunicator Version in Python.\n\nAn RLCapabilities struct keeps track of which features exist. This struct is passed from C# to\nPython, and another from Python to C#.  With this feature level granularity, we can notify users\nmore specifically about feature limitations based on what's available in both C# and Python.\nThese notifications will be logged to the python terminal, or to the Unity Editor Console.",
        "Side Channels": "The communicator is what manages data transfer between Unity and Python for the core\ntraining loop. Side Channels are another means of data transfer between Unity and Python.\nSide Channels are not versioned, but have been designed to support backward compatibility\nfor what they are. As of today, we provide 4 side channels:\n\n['FloatProperties: shared float data between Unity - Python (bidirectional)', 'RawBytes: raw data that can be sent Unity - Python (bidirectional)', 'EngineConfig: a set of numeric fields in a pre-defined order sent from Python to Unity', 'Stats: (name, value, agg) messages sent from Unity to Python']\n\nAside from the specific implementations of side channels we provide (and use ourselves),\nthe Side Channel interface is made available for users to create their own custom side\nchannels. As such, we guarantee that the built in SideChannel interface between Unity and\nPython is backward compatible in packages that share the same major version."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 51]"
    },
    {
      "title": "Com.Unity.Ml Agents",
      "description": null,
      "content": {
        "root": [
          "{!../com.unity.ml-agents/Documentation~/com.unity.ml-agents.md!}"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 52]"
    },
    {
      "title": "Index",
      "description": null,
      "content": {
        "⚠️ Documentation Moved ⚠️": "**This documentation is deprecated and no longer maintained. Visit the [Unity Package Documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest) for the latest ML-Agents documentation. This site remains for legacy reference only.**\n\n[]\n\n<img src=\"images/image-banner.png\" align=\"middle\" width=\"3000\"/>\n{!README.md!}"
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 53]"
    },
    {
      "title": "Ml Agents Envs",
      "description": null,
      "content": {
        "root": [
          "{!../ml-agents-envs/README.md!}"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 54]"
    },
    {
      "title": "Ml Agents",
      "description": null,
      "content": {
        "root": [
          "{!../ml-agents/README.md!}"
        ]
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 55]"
    },
    {
      "title": "Doxygen",
      "description": "Documentation section: doxygen",
      "content": {},
      "metadata": {
        "type": "directory",
        "path": "/home/anhnh/CodeWiki-Benchmarking-System/data/ml-agents/original/docs/doxygen"
      },
      "subpages": [
        {
          "title": "Readme",
          "description": null,
          "content": {
            "Doxygen files": "To generate the API reference as HTML files, run:\n\ndoxygen dox-ml-agents.conf"
          },
          "metadata": {},
          "subpages": [],
          "path": "[\"subpages\", 56, \"subpages\", 0]"
        }
      ],
      "path": "[\"subpages\", 56]"
    }
  ]
}