{
  "title": "rasa",
  "description": "Documentation for rasa",
  "content": {},
  "metadata": {
    "type": "root",
    "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs"
  },
  "subpages": [
    {
      "title": "Docs",
      "description": null,
      "content": {
        "Useful commands": {
          "Installation": "Firstly, install python dependencies for Rasa:\n\n```\n$ make install\n```\n\nThen, install doc dependencies:\n\n```\n$ make install-docs\n```",
          "Local Development": "In order to build the docs, run:\n\n```\n$ make docs\n```\n\nThen, start doc server in watch mode:\n\n```\n$ make livedocs\n```\n\nThis command starts a local development server and open up a browser window. Most changes are reflected live without having to restart the server.",
          "Build": "```\n$ yarn build\n```\n\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.",
          "Deployment": "Deployment is handled by Netlify: it is setup for listening to changes on the `documentation` branch."
        },
        "Manual steps after a new version": "When a new docs version has been released, we'll need to do the following manual steps:\n\n['Remove all the callouts from previous versions, with the exception of experimental features. You can find\\nthose using `:::info` or `:::caution` in all the docs files.', 'Update the wording of the top banner, configured in `docusaurus.config.js` in `announcementBar`: update the Rasa versions\\nthat are mentioned and link to the now previous major version documentation.', 'Update Netlify redirects in `netlify.toml`, under `# Redirects for latest version permalinks`, by adjusting the\\nversion number to the now new major version.']",
        "Handling deadlinks after removal of deprecated features": "When removing deprecated features, it will happen that some links become dead because they now link to\nparts of the docs that no longer exist. This usually happens in the CHANGELOG or migration links,\nand thankfully we do have CI checks that alert for dead links.\n\nThe trick here is to make these links point to _previous_ versions of the docs. For instance, if the feature\nyou removed was documented at `./policies#mapping-policy` and the current latest version for the docs is `2.x`\n(this also means that the next version is `3.x`), then you can update the link to `https://rasa.com/docs/rasa/2.x/policies#mapping-policy`."
      },
      "metadata": {},
      "subpages": [],
      "path": "[\"subpages\", 0]"
    },
    {
      "title": "Docs",
      "description": "Documentation section: docs",
      "content": {},
      "metadata": {
        "type": "directory",
        "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs"
      },
      "subpages": [
        {
          "title": "Actions",
          "description": null,
          "content": {
            "Responses": "A [response](./responses.mdx) is a message the assistant will send back to the user. This is\nthe action you will use most often, when you want the assistant to send text, images, buttons\nor similar to the user.",
            "Custom Actions": "A [custom action](./custom-actions.mdx) is an action that can run any code you want. This can be used to make an\nAPI call, or to query a database for example.",
            "Forms": "[Forms](./forms.mdx) are a special type of custom action, designed to handle business logic. If you have\nany conversation designs where you expect the assistant to ask for a specific set of\ninformation, you should use forms.",
            "Default Actions": "[Default actions](./default-actions.mdx) are actions that are built into the dialogue manager by default. Most of\nthese are automatically predicted based on certain conversation situations. You may want to\ncustomize these to personalize your assistant.",
            "Slot Validation Actions": "A [slot validation action](./slot-validation-actions.mdx) is a special type of custom action, designed to handle custom extraction and/or validation of slot values.\nThis can be used to validate slots with predefined mappings or extract slots with custom mappings."
          },
          "metadata": {
            "id": "actions",
            "sidebar_label": "Overview",
            "title": "Actions",
            "abstract": "After each user message, the model will predict an action that the assistant should perform next. This page gives you an overview of the different types of actions you can use."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 0]"
        },
        {
          "title": "Rasa Architecture Overview",
          "description": null,
          "content": {
            "root": [
              "import useBaseUrl from '@docusaurus/useBaseUrl';",
              "The diagram below provides an overview of the Rasa architecture. The two primary\ncomponents are Natural Language Understanding (NLU) and dialogue management.",
              "NLU is the part that handles intent classification, entity extraction, and response retrieval.\nIt's shown below as the *NLU Pipeline* because it processes\nuser utterances using an NLU model that is generated by the trained pipeline.",
              "The dialogue management component decides the next action in a conversation based on the\ncontext. This is displayed as the *Dialogue Policies* in the diagram.\n<img alt=\"image\" src={useBaseUrl(\"/img/architecture.png\")} />"
            ]
          },
          "metadata": {
            "id": "arch-overview",
            "sidebar_label": "Overview",
            "title": "Rasa Architecture Overview",
            "abstract": "Rasa has a scalable architecture. Read about the key components of the Rasa architecture."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 1]"
        },
        {
          "title": "Rasa Architecture",
          "description": "Check the architecture to understand how Rasa uses machine learning, context and state of the conversation to predict the next action of the AI Assistant.",
          "content": {
            "Message Handling": "This diagram shows the basic steps of how an assistant built with Rasa\nresponds to a message:\n\nimport archImage from './architecture-img.png';\n\n<Image img={archImage} caption=\"Rasa architecture\" alt=\"A visual representation of the steps that follow.\"/>\n\nThe steps are:\n\n['The message is received and passed to an `Interpreter`, which\\nconverts it into a dictionary including the original text, the intent,\\nand any entities that were found. This part is handled by NLU.', 'The message is passed from the `Interpreter` to the `Tracker`.\\nThe `Tracker` is the object which keeps track of conversation state.', 'The current state of the tracker is sent to each policy.', 'Each policy chooses which action to take next.', 'The chosen action is logged by the tracker.', 'A response is sent to the user.']"
          },
          "metadata": {
            "id": "architecture",
            "sidebar_label": "Rasa Architecture",
            "title": "Rasa Architecture",
            "description": "Check the architecture to understand how Rasa uses machine learning, context and state of the conversation to predict the next action of the AI Assistant."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 2]"
        },
        {
          "title": "Handling Business Logic",
          "description": null,
          "content": {
            "Step-by-step Guide on Using Forms to Handle Business Logic": {
              "1. Defining the form": {
                "Slot Mappings": {
                  "Slot Mappings with Conditions": "The `outdoor_seating` slot is\nfilled based on the user's intent: If it is `affirm`, it'll be `true`, if it is\n`deny`, it'll be `false`. \n\nHowever, the slot should only be set to `true` or `false` if the user was responding to the question, `Do you want to sit outside?`.\nTo enforce this condition, the `conditions` for the `outdoor_seating` slot requires that `restaurant_form` is active and that the requested slot is `outdoor_seating`. \nIf there were no conditions and the user had sent a message with the `affirm` or `deny` intent earlier in the conversation,\nthe `outdoor_seating` slot would already be filled when the form was activated. Therefore the form would not prompt the user for their outdoor seating preference. See [mapping conditions](./domain.mdx#mapping-conditions) for more information."
                },
                "Validating Slots": "Often, you'll want to validate the user's input before accepting it,\nfor example by checking if the given cuisine is in your assistant's database\nof available cuisines.\nSee the docs on [validating form input](forms.mdx#validating-form-input) for more information\nabout validation actions.",
                "Requesting Slots": "To specify how the bot should ask for the required information,\nyou define [`responses`](domain.mdx#responses) called `utter_ask_{slotname}` in your domain:\n\n```\nresponses:\n  utter_ask_cuisine:\n    - text: \"What cuisine?\"\n  utter_ask_num_people:\n    - text: \"How many people?\"\n  utter_ask_outdoor_seating:\n    - text: \"Do you want to sit outside?\"\n```"
              },
              "2. Updating the configuration": "A form's [happy path](glossary.mdx#happy--unhappy-paths) should be defined as a [rule](rules.mdx) which means you'll need to add the [RulePolicy](policies.mdx#rule-policy)\nto your policies:\n\n```\npolicies:\n  - name: RulePolicy\n```",
              "3. Creating rules": "The form itself takes care of the logic around asking the user for all\nthe required information, so\nyou need only two rules for a form's happy path:\nOne that defines when it starts, and one that defines what happens when it has been filled.\nFor the restaurant search example, in real life the assistant would look up\na restaurant based on the user's preferences.\nIn this case, the bot will utter a response with the details\nthat would be used for a search.\n\n```\nrules:\n  - rule: activate restaurant form\n    steps:\n      - intent: request_restaurant   # intent that triggers form activation\n      - action: restaurant_form      # run the form\n      - active_loop: restaurant_form # this form is active\n\n  - rule: submit form\n    condition:\n    - active_loop: restaurant_form   # this form must be active\n    steps:\n      - action: restaurant_form      # run the form\n      - active_loop: null            # the form is no longer active because it has been filled\n      - action: utter_submit         # action to take after the form is complete\n      - action: utter_slots_values   # action to take after the form is complete\n```\n\nBy splitting up the activation and submission of the form,\nthe rules will still apply if the user provides\n[unexpected input](unexpected-input.mdx) or interrupts\nthe form with [chitchat](chitchat-faqs.mdx).",
              "4. Updating the NLU training data": {
                "Form Activation Intent(s)": "You need to provide training examples for the\nintent(s) that should activate the form.\nAdd examples for the intent `request_restaurant`:\n\n```\nnlu:\n- intent: request_restaurant\n  examples: |\n    - im looking for a restaurant\n    - can i get [swedish](cuisine) food in any area\n    - a restaurant that serves [caribbean](cuisine) food\n    - id like a restaurant\n    - im looking for a restaurant that serves [mediterranean](cuisine) food\n    - can i find a restaurant that serves [chinese](cuisine)\n```\n\nSlots filled with `from_entity` can by default be filled by any user utterance, regardless of the intent, as\nlong as the correct entity is extracted. That means that if the user provides the `cuisine` entity as part of\ntheir first message, the slot will be filled at the beginning of the form and the bot will not\nask them for the cuisine again.",
                "Form Filling Intent(s)": "While the form is filling slots, it will not pay attention to which intent was predicted\nunless a slot mapping explicitly requires or excludes an intent.\n\nFor the restaurant search example, the `outdoor_seating` slot is mapped to two intents,\nso you need to add training data for these intents.\n\nFor the `cuisine` and `number` slots, no intent is specified, so you can add examples to a generic `inform` intent. You need\nto annotate the `cuisine` entity so that DIETClassifier can\nlearn to extract it. You don't need to annotate the `number` entity since DucklingEntityExtractor is a rule-based extractors\nthat isn't trained on your training data. Only a few examples are shown for each intent;\nfor your bot to work well, you should add more training data than is shown here:\n\n```\nnlu:\n- intent: affirm\n  examples: |\n    - Yes\n    - yes, please\n    - yup\n- intent: deny\n  examples: |\n    - no don't\n    - no\n    - no I don't want that\n\n- intent: inform\n  examples: |\n    - [afghan](cuisine) food\n    - how bout [asian oriental](cuisine)\n    - what about [indian](cuisine) food\n    - uh how about [turkish](cuisine) type of food\n    - um [english](cuisine)\n    - im looking for [tuscan](cuisine) food\n    - id like [moroccan](cuisine) food\n    - for ten people\n    - 2 people\n    - for three people\n    - just one person\n    - book for seven people\n    - 2 please\n    - nine people\n```\n\nUpdate your domain to include these intents:\n\n```\nintents:\n  - request_restaurant\n  - affirm\n  - deny\n  - inform\n```"
              },
              "5. Defining the responses": "Add the responses that are sent after the form has been submitted:\n\n```\nresponses:\n  utter_submit:\n  - text: \"All done!\"\n  utter_slots_values:\n  - text: \"I am going to run a restaurant search using the following parameters:\\n\n            - cuisine: {cuisine}\\n\n            - num_people: {num_people}\\n\n            - outdoor_seating: {outdoor_seating}\"\n```"
            },
            "Summary": "Forms can simplify the logic of collecting user\ninformation. To define a minimal form like the restaurant search\nexample above, this is a summary of what you'll need to do:\n\n['[ ] Add the RulePolicy to `config.yml`', '[ ] Define the form with required slots in the domain', '[ ] Add slot mappings for all required slots in the domain', '[ ] Add rules for activating and submitting the form', '[ ] Add examples for the intent(s) to activate your form', '[ ] Add examples for the intent(s) to fill the required slots', '[ ] Define an action or response for the bot to take when the form is completed', \"[ ] Update your domain with new intents and actions you've defined\"]\n\nTo try out your newly defined form, retrain the bot's model by running `rasa train` and start `rasa shell`.\nBecause the DucklingEntityExtractor is being used to extract\nentities, you'll need to start Duckling in the background as well\n(see the [instructions for running Duckling](components.mdx#DucklingEntityExtractor))."
          },
          "metadata": {
            "id": "business-logic",
            "sidebar_label": "Handling Business Logic",
            "title": "Handling Business Logic",
            "abstract": "Conversational assistants often need to ask users for information in order to help them. You can use Forms to collect the required user information and fulfill a request."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 3]"
        },
        {
          "title": "Chitchat and FAQs",
          "description": null,
          "content": {
            "Step-by-step Guide on Using Response Selector for FAQs and Chitchat": {
              "1. Updating the configuration": "For FAQs and chitchat, you always want the assistant to respond the same way every time\nthe same type of question is asked. [Rules](rules.mdx) allow you to do exactly that.\nTo use rules, the you need to add the [RulePolicy](./policies.mdx#rule-policy) to your policies in your configuration file:\n\n```\npolicies:\n# other policies\n- name: RulePolicy\n```\n\nNext, include the ResponseSelector in your NLU pipeline in your configuration file.\nThe ResponseSelector requires a featurizer and intent classifier to work, so\nit should come after these components in your pipeline, for example:\n\n```\npipeline:\n  - name: WhitespaceTokenizer\n  - name: RegexFeaturizer\n  - name: LexicalSyntacticFeaturizer\n  - name: CountVectorsFeaturizer\n  - name: CountVectorsFeaturizer\n    analyzer: char_wb\n    min_ngram: 1\n    max_ngram: 4\n  - name: DIETClassifier\n    epochs: 100\n  - name: EntitySynonymMapper\n  - name: ResponseSelector\n    epochs: 100\n```\n\nBy default, the ResponseSelector will build a single retrieval model for all retrieval intents.\nTo retrieve responses for FAQs and chitchat separately, use multiple ResponseSelector components\nand specify the `retrieval_intent` key:\n\n```\npipeline:\n# Other components\n- name: ResponseSelector\n  epochs: 100\n  retrieval_intent: faq\n- name: ResponseSelector\n  epochs: 100\n  retrieval_intent: chitchat\n```",
              "2. Defining Retrieval Intents and the ResponseSelector": "Consider an example where you have 20 different FAQs. Although each question is represented as an individual intent, all FAQ intents are handled the same way in the dialogue. For each FAQ intent, the assistant **retrieves** the proper response depending on which question has been asked.\n\nInstead of writing 20 rules, you can use a single action, e.g. `utter_faq` to handle all FAQs with a single rule by grouping them together under a single [retrieval intent](glossary.mdx#retrieval-intent)\ncalled e.g. `faq`.\n\nThe single action uses the output of the\n[ResponseSelector](./components.mdx#responseselector) to return\nthe correct response for the specific FAQ that the user asked.",
              "3. Creating rules": "You need to write only one rule for each retrieval intent. All intents\ngrouped under that retrieval intent will then be handled the same way.\nThe action name starts with `utter_` and ends with the retrieval intent's name.\nWrite rules for responding to FAQs and chitchat:\n\n```\nrules:\n  - rule: respond to FAQs\n    steps:\n    - intent: faq\n    - action: utter_faq\n  - rule: respond to chitchat\n    steps:\n    - intent: chitchat\n    - action: utter_chitchat\n```\n\nThe actions `utter_faq` and `utter_chitchat` will use the ResponseSelector's prediction to return the actual response message.",
              "4. Updating the NLU Training Data": "NLU training examples for the ResponseSelector look the same as\nregular training examples, except that their names must refer to the retrieval\nintent they are grouped under:\n\n```\nnlu:\n  - intent: chitchat/ask_name\n    examples: |\n      - What is your name?\n      - May I know your name?\n      - What do people call you?\n      - Do you have a name for yourself?\n  - intent: chitchat/ask_weather\n    examples: |\n      - What's the weather like today?\n      - Does it look sunny outside today?\n      - Oh, do you mind checking the weather for me please?\n      - I like sunny days in Berlin.\n```\n\nBe sure to update your domain file to include the added `chitchat` intent:\n\n```\nintents:\n# other intents\n- chitchat\n```",
              "5. Defining the responses": "Responses for the ResponseSelector follow the same naming convention as\nretrieval intents. Besides this, they can have all the characteristics of\nnormal bot [response](domain.mdx#responses). For the chitchat intents\nlisted above, our responses could look like:\n\n```\nresponses:\n  utter_chitchat/ask_name:\n  - image: \"https://i.imgur.com/zTvA58i.jpeg\"\n    text: Hello, my name is Retrieval Bot.\n  - text: I am called Retrieval Bot!\n  utter_chitchat/ask_weather:\n  - text: Oh, it does look sunny right now in Berlin.\n    image: \"https://i.imgur.com/vwv7aHN.png\"\n  - text: I am not sure of the whole week but I can see the sun is out today.\n```"
            },
            "Summary": "Once you've done the following, you can train your bot and try it out!\n\n['[ ] Add RulePolicy to your policies and ResponseSelector to your pipeline in `config.yml`', '[ ] Add at least one rule for responding to FAQs/chitchat', '[ ] Add examples for your FAQs/chitchat intents', '[ ] Add responses for your FAQs/chitchat intents', '[ ] Update the intents in your domain']\n\nNow, your assistant should be able to respond correctly and consistently to FAQs or chitchat, even if these interjections happen while your assistant is helping the user with another task."
          },
          "metadata": {
            "id": "chitchat-faqs",
            "sidebar_label": "Chitchat and FAQs",
            "title": "Chitchat and FAQs",
            "abstract": "FAQ assistants are the simplest assistants to build and typically the first kind of assistant anyone builds. This page is a guide to the concepts and training data you need to handle non-contextual questions like FAQs and chitchat."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 4]"
        },
        {
          "title": "Command Line Interface",
          "description": "Command line interface for open source chatbot framework Rasa. Learn how to train, test and run your machine learning-based conversational AI assistants",
          "content": {
            "Cheat Sheet": "|        Command           |                                                                  Effect                                                                  |\n|--------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n|`rasa init`               |Creates a new project with example training data, actions, and config files.                                                              |\n|`rasa train`              |Trains a model using your NLU data and stories, saves trained model in `./models`.                                                        |\n|`rasa interactive`        |Starts an interactive learning session to create new training data by chatting to your assistant.                                         |\n|`rasa shell`              |Loads your trained model and lets you talk to your assistant on the command line.                                                         |\n|`rasa run`                |Starts a server with your trained model.                                                                                                  |\n|`rasa run actions`        |Starts an action server using the Rasa SDK.                                                                                               |\n|`rasa visualize`          |Generates a visual representation of your stories.                                                                                        |\n|`rasa test`               |Tests a trained Rasa model on any files starting with `test_`.                                                                            |\n|`rasa test e2e`           |Runs end-to-end testing fully integrated with the action server that serves as acceptance testing.                                        |\n|`rasa data split nlu`     |Performs a 80/20 split of your NLU training data.                                                                                         |\n|`rasa data split stories` |Do the same as `rasa data split nlu`, but for your stories data.                                                                          |\n|`rasa data convert`       |Converts training data between different formats.                                                                                         |\n|`rasa data migrate`       |Migrates 2.0 domain to 3.0 format.                                                                                                        |\n|`rasa data validate`      |Checks the domain, NLU and conversation data for inconsistencies.                                                                         |\n|`rasa export`             |Exports conversations from a tracker store to an event broker.                                                                            |\n|`rasa evaluate markers`   |Extracts markers from an existing tracker store.                                                                                          |\n|`rasa marker upload`      |Upload marker configurations to Analytics Data Pipeline                                                                                   |\n|`rasa license`            |Display licensing information.                                                                                                            |\n|`rasa -h`                 |Shows all available commands.                                                                                                             |\n\n:::note\nIf you run into character encoding issues on Windows like: `UnicodeEncodeError: 'charmap' codec can't encode character ...` or\nthe terminal is not displaying colored messages properly, prepend `winpty` to the command you would like to run.\nFor example `winpty rasa init` instead of `rasa init`\n:::",
            "Log Level": "Rasa produces log messages at several different levels (eg. warning, info, error and so on). You can control which level of logs you would like to see with `--verbose` (same as `-v`) or `--debug` (same as `-vv`) as optional command line arguments. See each command below for more explanation on what these arguments mean.\n\nIn addition to CLI arguments, several environment variables allow you to control log output in a more granular way. With these environment variables, you can configure log levels for messages created by external libraries such as Matplotlib, Pika, and Kafka. These variables follow [standard logging level in Python](https://docs.python.org/3/library/logging.html#logging-levels). Currently, following environment variables are supported:\n\n['LOG_LEVEL_LIBRARIES: This is the general environment variable to configure log level for the main libraries Rasa uses. It covers Tensorflow, `asyncio`, APScheduler, SocketIO, Matplotlib, RabbitMQ, Kafka.', 'LOG_LEVEL_MATPLOTLIB: This is the specialized environment variable to configure log level only for Matplotlib.', 'LOG_LEVEL_RABBITMQ: This is the specialized environment variable to configure log level only for AMQP libraries, at the moment it handles log levels from `aio_pika` and `aiormq`.', 'LOG_LEVEL_KAFKA: This is the specialized environment variable to configure log level only for kafka.', 'LOG_LEVEL_PRESIDIO: This is the specialized environment variable to configure log level only for Presidio, at the moment it handles log levels from `presidio_analyzer` and `presidio_anonymizer`.', 'LOG_LEVEL_FAKER: This is the specialized environment variable to configure log level only for Faker.']\n\nGeneral configuration (`LOG_LEVEL_LIBRARIES`) has less priority than library level specific configuration (`LOG_LEVEL_MATPLOTLIB`, `LOG_LEVEL_RABBITMQ` etc); and CLI parameter sets the lowest level log messages which will be handled. This means variables can be used together with a predictable result. As an example:\n\n```\nLOG_LEVEL_LIBRARIES=ERROR LOG_LEVEL_MATPLOTLIB=WARNING LOG_LEVEL_KAFKA=DEBUG rasa shell --debug\n```\n\nThe above command run will result in showing:\n\n['messages with `DEBUG` level and higher by default (due to `--debug`)', 'messages with `WARNING` level and higher for Matplotlib', 'messages with `DEBUG` level and higher for kafka', 'messages with `ERROR` level and higher for other libraries not configured']\n\nNote that CLI config sets the lowest level log messages to be handled, hence the following command will set the log level to `INFO` (due to `--verbose`) and no debug messages will be seen (library level configuration will not have any effect):\n\n```\nLOG_LEVEL_LIBRARIES=DEBUG LOG_LEVEL_MATPLOTLIB=DEBUG rasa shell --verbose\n```\n\nAs an aside, CLI log level sets the level at the root logger (which has the important handler - `coloredlogs` handler); this means even if an environment variable sets a library logger to a lower level, the root logger will reject messages from that library. If not specified, the CLI log level is set to `INFO`.",
            "Custom logging configuration": ":::info New in 3.4\n\nThe Rasa CLI now includes a new argument `--logging-config-file` which accepts a YAML file as value.\n\n:::\n\nYou can now configure any logging formatters or handlers in a separate YAML file.\nThe logging config YAML file must follow the [Python built-in dictionary schema](https://docs.python.org/3/library/logging.config.html#dictionary-schema-details), otherwise it will fail validation.\nYou can pass this file as argument to the `--logging-config-file` CLI option and use it with any of the rasa commands.",
            "rasa init": "This command sets up a complete assistant for you with some example training data:\n\n```\nrasa init\n```\n\nIt creates the following files:\n\n```\n.\n├── actions\n│   ├── __init__.py\n│   └── actions.py\n├── config.yml\n├── credentials.yml\n├── data\n│   ├── nlu.yml\n│   └── stories.yml\n├── domain.yml\n├── endpoints.yml\n├── models\n│   └── <timestamp>.tar.gz\n└── tests\n   └── test_stories.yml\n```\n\nIt will ask you if you want to train an initial model using this data.\nIf you answer no, the `models` directory will be empty.\n\nAny of the default CLI commands will expect this project setup, so this is the\nbest way to get started. You can run `rasa train`, `rasa shell` and `rasa test`\nwithout any additional configuration.",
            "rasa train": {
              "Incremental training": ":::info New in 2.2\nThis feature is experimental.\nWe introduce experimental features to get feedback from our community, so we encourage you to try it out!\nHowever, the functionality might be changed or removed in the future.\nIf you have feedback (positive or negative) please share it with us on the [Rasa Forum](https://forum.rasa.com).\n\n:::\n\nIn order to improve the performance of an assistant, it's helpful to practice [CDD](./conversation-driven-development.mdx)\nand add new training examples based on how your users have talked to your assistant. You can use `rasa train --finetune`\nto initialize the pipeline with an already trained model and further finetune it on the\nnew training dataset that includes the additional training examples. This will help reduce the\ntraining time of the new model.\n\nBy default, the command picks up the latest model in the `models/` directory. If you have a specific model\nwhich you want to improve, you may specify the path to this by\nrunning `rasa train --finetune <path to model to finetune>`. Finetuning a model usually\nrequires fewer epochs to train machine learning components like `DIETClassifier`, `ResponseSelector` and `TEDPolicy` compared to training from scratch.\nEither use a model configuration for finetuning\nwhich defines fewer epochs than before or use the flag\n`--epoch-fraction`. `--epoch-fraction` will use a fraction of the epochs specified for each machine learning component\nin the model configuration file. For example, if `DIETClassifier` is configured to use 100 epochs,\nspecifying `--epoch-fraction 0.5` will only use 50 epochs for finetuning.\n\nYou can also finetune an NLU-only or dialogue management-only model by using\n`rasa train nlu --finetune` and `rasa train core --finetune` respectively.\n\nTo be able to fine tune a model, the following conditions must be met:\n\n['The configuration supplied should be exactly the same as the\\nconfiguration used to train the model which is being finetuned.\\nThe only parameter that you can change is `epochs` for the individual machine learning components and policies.', 'The set of labels(intents, actions, entities and slots) for which the base model is trained\\nshould be exactly the same as the ones present in the training data used for finetuning. This\\nmeans that you cannot add new intent, action, entity or slot labels to your training data\\nduring incremental training. You can still add new training examples for each of the existing\\nlabels. If you have added/removed labels in the training data, the pipeline needs to be trained\\nfrom scratch.', 'The model to be finetuned is trained with `MINIMUM_COMPATIBLE_VERSION` of the currently installed rasa version.']"
            },
            "rasa interactive": "You can start an interactive learning session by running:\n\n```\nrasa interactive\n```\n\nThis will first train a model and then start an interactive shell session.\nYou can then correct your assistants predictions as you talk to it.\nIf [`UnexpecTEDIntentPolicy`](./policies.mdx#unexpected-intent-policy) is\nincluded in the pipeline, [`action_unlikely_intent`](./default-actions.mdx#action_unlikely_intent)\ncan be triggered at any conversation turn. Subsequently, the following message will be displayed:\n\n```\n The bot wants to run 'action_unlikely_intent' to indicate that the last user message was unexpected\n at this point in the conversation. Check out UnexpecTEDIntentPolicy docs to learn more.\n```\n\nAs the message states, this is an indication that you have explored a conversation path\nwhich is unexpected according to the current set of training stories and hence adding this\npath to training stories is recommended. Like other bot actions, you can choose to confirm\nor deny running this action.\n\nIf you provide a trained model using the `--model` argument, training is skipped\nand that model will be loaded instead.\n\nDuring interactive learning, Rasa will plot the current conversation\nand a few similar conversations from the training data to help you\nkeep track of where you are. You can view the visualization\nat http://localhost:5005/visualization.html\nas soon as the session has started. This diagram can take some time to generate.\nTo skip the visualization, run `rasa interactive --skip-visualization`.\n\n:::info Add the `assistant_id` key introduced in 3.5\n\nRunning interactive learning with a pre-trained model whose metadata does not include the `assistant_id`\nwill exit with an error. If this happens, add the required key with a unique identifier value in `config.yml`\nand re-run training.\n\n:::\n\nThe following arguments can be used to configure the interactive learning session:\n\n```\n```",
            "rasa shell": "You can start a chat session by running:\n\n```\nrasa shell\n```\n\nBy default, this will load up the latest trained model.\nYou can specify a different model to be loaded by using the `--model` flag.\n\nIf you start the shell with an NLU-only model, `rasa shell` will output the\nintents and entities predicted for any message you enter.\n\nIf you have trained a combined Rasa model but only want to see what your model\nextracts as intents and entities from text, you can use the command `rasa shell nlu`.\n\nTo increase the logging level for debugging, run:\n\n```\nrasa shell --debug\n```\n\n:::note\nIn order to see the typical greetings and/or session start behavior you might see\nin an external channel, you will need to explicitly send `/session_start`\nas the first message. Otherwise, the session start behavior will begin as described in\n[Session configuration](./domain.mdx#session-configuration).\n:::\n\nThe following arguments can be used to configure the command.\nMost arguments overlap with `rasa run`; see the [following section](#rasa-run) for more info on those arguments.\n\nNote that the `--connector` argument will always be set to `cmdline` when running `rasa shell`.\nThis means all credentials in your credentials file will be ignored,\nand if you provide your own value for the `--connector` argument it will also be ignored.\n\n```\n```",
            "rasa run": "To start a server running your trained model, run:\n\n```\nrasa run\n```\n\nBy default the Rasa server uses HTTP for its communication. To secure the communication with\nSSL and run the server on HTTPS, you need to provide a valid certificate and the corresponding\nprivate key file. You can specify these files as part of the `rasa run` command.\nIf you encrypted your keyfile with a password during creation,\nyou need to add the `--ssl-password` as well.\n\n```\nrasa run --ssl-certificate myssl.crt --ssl-keyfile myssl.key --ssl-password mypassword\n```\n\nRasa by default listens on each available network interface. You can limit this to a specific\nnetwork interface using the `-i` command line option.\n\n```\nrasa run -i 192.168.69.150\n```\n\nRasa will by default connect to all channels specified in your credentials file.\nTo connect to a single channel and ignore all other channels in your credentials file,\nspecify the name of the channel in the `--connector` argument.\n\n```\nrasa run --connector rest\n```\n\nThe name of the channel should match the name you specify in your credentials file.\nFor supported channels see [the page about messaging and voice channels](./messaging-and-voice-channels.mdx).\n\nThe following arguments can be used to configure your Rasa server:\n\n```\n```\n\nFor more information on the additional parameters, see [Model Storage](./model-storage.mdx).\nSee the Rasa [HTTP API](./http-api.mdx) page for detailed documentation of all the endpoints.",
            "rasa run actions": "To start an action server with the Rasa SDK, run:\n\n```\nrasa run actions\n```\n\nThe following arguments can be used to adapt the server settings:\n\n```\n```",
            "rasa visualize": "To generate a graph of your stories in the browser, run:\n\n```\nrasa visualize\n```\n\nIf your stories are located somewhere other than the default location `data/`,\nyou can specify their location with the `--stories` flag.\n\nThe following arguments can be used to configure this command:\n\n```\n```",
            "rasa test": "To evaluate a model on your test data, run:\n\n```\nrasa test\n```\n\nThis will test your latest trained model on any end-to-end stories you have\ndefined in files with the `test_` prefix.\nIf you want to use a different model, you can specify it using the `--model` flag.\n\nTo evaluate the dialogue and NLU\nmodels separately, use the commands below:\n\n```\nrasa test core\n```\n\nand\n\n```\nrasa test nlu\n```\n\nYou can find more details on specific arguments for each testing type in\n[Evaluating an NLU Model](./testing-your-assistant.mdx#evaluating-an-nlu-model) and\n[Evaluating a Dialogue Management Model](./testing-your-assistant.mdx#evaluating-a-dialogue-model).\n\nThe following arguments are available for `rasa test`:\n\n```\n```",
            "rasa test e2e": "<RasaProLabel />\n\n<RasaProBanner />\n\n:::info New in 3.5\n\nYou can now use end-to-end testing to test your assistant as a whole, including dialogue management and custom actions.\n\n:::\n\nTo run [end-to-end testing](./testing-your-assistant.mdx#end-to-end-testing) on your trained model, run:\n\n```\nrasa test e2e\n```\n\nThis will test your latest trained model on any end-to-end test cases you have.\nIf you want to use a different model, you can specify it using the `--model` flag.\n\nThe following arguments are available for `rasa test e2e`:\n\n```\nusage: rasa test e2e [-h] [-v] [-vv] [--quiet] [--logging-config-file LOGGING_CONFIG_FILE] [--fail-fast] [-o] [--remote-storage REMOTE_STORAGE] [-m MODEL] [--endpoints ENDPOINTS] [path-to-test-cases]\n\nRuns end-to-end testing.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -o, --e2e-results     Results file containing end-to-end testing summary. (default: None)\n  --remote-storage REMOTE_STORAGE\n                        Set the remote location where your Rasa model is stored, e.g. on AWS. (default: None)\n  -m MODEL, --model MODEL\n                        Path to a trained Rasa model. If a directory is specified, it will use the latest model in this directory. (default: models)\n  --endpoints ENDPOINTS\n                        Configuration file for the model server and the connectors as a yml file. (default: endpoints.yml)\n\nPython Logging Options:\n  You can control level of log messages printed. In addition to these arguments, a more fine grained configuration can be achieved with environment variables. See online documentation for more info.\n\n  -v, --verbose         Be verbose. Sets logging level to INFO. (default: None)\n  -vv, --debug          Print lots of debugging statements. Sets logging level to DEBUG. (default: None)\n  --quiet               Be quiet! Sets logging level to WARNING. (default: None)\n  --logging-config-file LOGGING_CONFIG_FILE\n                        If set, the name of the logging configuration file will be set to the given name. (default: None)\n\nTesting Settings:\n  path-to-test-cases    Input file or folder containing end-to-end test cases. (default: tests/e2e_test_cases.yml)\n  --fail-fast           Fail the test suite as soon as a unit test fails. (default: False)\n```",
            "rasa data split": "To create a train-test split of your NLU training data, run:\n\n```\nrasa data split nlu\n```\n\nThis will create a 80/20 split of train/test data by default.\nYou can specify the training data, the fraction, and the output directory using\nthe following arguments:\n\n```\n```\n\nIf you have NLG data for retrieval actions, this will be saved to separate files:\n\n```\nls train_test_split\n\n      nlg_test_data.yml     test_data.yml\n      nlg_training_data.yml training_data.yml\n```\n\nTo split your stories, you can use the following command:\n\n```\nrasa data split stories\n```\n\nIt has the same arguments as `split nlu` command, but loads yaml files with stories and perform random splitting.\nDirectory `train_test_split` will contain all yaml files processed with prefixes `train_` or `test_` containing\ntrain and test parts.",
            "rasa data convert nlu": "You can convert NLU data from\n\n['LUIS data format,', 'WIT data format,', 'Dialogflow data format, or', 'JSON']\n\nto\n\n['YAML or', 'JSON']\n\nYou can start the converter by running:\n\n```\nrasa data convert nlu\n```\n\nYou can specify the input file or directory, output file or directory, and the output format with the following arguments:\n\n```\n```",
            "rasa data migrate": "The domain is the only data file whose format changed between 2.0 and 3.0.\nYou can automatically migrate a 2.0 domain to the 3.0 format.\n\nYou can start the migration by running:\n\n```\nrasa data migrate\n```\n\nYou can specify the input file or directory and the output file or directory with the following arguments:\n\n```\nrasa data migrate -d DOMAIN --out OUT_PATH\n```\n\nIf no arguments are specified, the default domain path (`domain.yml`) will be used for both input and output files.\n\nThis command will also back-up your 2.0 domain file(s) into a different `original_domain.yml` file or\ndirectory labeled `original_domain`.\n\nNote that the slots in the migrated domain will contain [mapping conditions](./domain.mdx#mapping-conditions) if these\nslots are part of a form's `required_slots`.\n\n:::caution\nExceptions will be raised and the migration process terminated if invalid domain files are provided or if they are\nalready in the 3.0 format, if slots or forms are missing from your original files or if the slots or forms sections\nare spread across multiple domain files.\nThis is done to avoid duplication of migrated sections in your domain files.\nPlease make sure all your slots' or forms' definitions are grouped into a single file.\n\n:::\n\nYou can learn more about this command by running:\n\n```\nrasa data migrate --help\n```",
            "rasa data validate": "You can check your domain, NLU data, or story data for mistakes and inconsistencies.\nTo validate your data, run this command:\n\n```\nrasa data validate\n```\n\nThe validator searches for errors in the data, e.g. two intents that have some\nidentical training examples.\nThe validator also checks if you have any stories where different assistant actions follow from the same\ndialogue history. Conflicts between stories will prevent a model from learning the correct\npattern for a dialogue.\n\n:::info Searching for the `assistant_id` key introduced in 3.5\n\nThe validator will check whether the `assistant_id` key is present in the config file and will issue a warning if this\nkey is missing or if the default value has not been changed.\n\n:::\n\nIf you pass a `max_history` value to one or more policies in your `config.yml` file, provide the\nsmallest of those values in the validator command using the `--max-history <max_history>` flag.\n\nYou can also validate only the story structure by running this command:\n\n```\nrasa data validate stories\n```\n\n:::note\nRunning `rasa data validate` does **not** test if your [rules](./rules.mdx) are consistent with your stories.\nHowever, during training, the `RulePolicy` checks for conflicts between rules and stories. Any such conflict will abort training.\n\nAlso, if you use end-to-end stories, then this might not capture all conflicts. Specifically, if two user inputs\nresult in different tokens yet exactly the same featurization, then conflicting actions after these inputs\nmay exist but will not be reported by the tool.\n:::\n\nTo interrupt validation even for minor issues such as unused intents or responses, use the `--fail-on-warnings` flag.\n\n:::caution check your story names\nThe `rasa data validate stories` command assumes that all your story names are unique!\n:::\n\nYou can use `rasa data validate` with additional arguments, e.g. to specify the location of your data and\ndomain files:\n\n```\n```",
            "rasa export": "To export events from a tracker store using an event broker, run:\n\n```\nrasa export\n```\n\nYou can specify the location of the environments file, the minimum and maximum\ntimestamps of events that should be published, as well as the conversation IDs that\nshould be published:\n\n```\n```\n\n:::tip Import conversations into Rasa X/Enterprise\nThis command is most commonly used to import old conversations into Rasa X/Enterprise to annotate\nthem. Read more about [importing conversations into Rasa X/Enterprise](https://rasa.com/docs/rasa-enterprise/installation-and-setup/deploy#1-import-existing-conversations-from-rasa-open-source).\n:::",
            "rasa evaluate markers": ":::caution\n\nThis feature is currently experimental and might change or be removed in the future. Share your feedback in the forum to help us make it production-ready.\n\n:::\n\nThe following command applies the [markers](./markers.mdx) you defined in your marker configuration file,\nto pre-existing dialogues stored in your [tracker store](./tracker-stores.mdx), and produces `.csv` files containing\nthe extracted markers and summary statistics:\n\n```\nrasa evaluate markers all extracted_markers.csv\n```\n\nUse the following arguments to configure the marker extraction process:\n\n```\nusage: rasa evaluate markers [-h] [-v] [-vv] [--quiet] [--config CONFIG] [--no-stats | --stats-file-prefix [STATS_FILE_PREFIX]] [--endpoints ENDPOINTS] [-d DOMAIN] output_filename {first_n,sample,all} ...\n\npositional arguments:\n  output_filename       The filename to write the extracted markers to (CSV format).\n  {first_n,sample,all}\n    first_n             Select trackers sequentially until N are taken.\n    sample              Select trackers by sampling N.\n    all                 Select all trackers.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config CONFIG       The config file(s) containing marker definitions. This can be a single YAML file, or a directory that contains several files with marker definitions in it. The content of these files will be read and\n                        merged together. (default: markers.yml)\n  --no-stats            Do not compute summary statistics. (default: True)\n  --stats-file-prefix [STATS_FILE_PREFIX]\n                        The common file prefix of the files where we write out the compute statistics. More precisely, the file prefix must consist of a common path plus a common file prefix, to which suffixes `-overall.csv` and\n                        `-per-session.csv` will be added automatically. (default: stats)\n  --endpoints ENDPOINTS\n                        Configuration file for the tracker store as a yml file. (default: endpoints.yml)\n  -d DOMAIN, --domain DOMAIN\n                        Domain specification. This can be a single YAML file, or a directory that contains several files with domain specifications in it. The content of these files will be read and merged together. (default:\n                        domain.yml)\n\nPython Logging Options:\n  -v, --verbose         Be verbose. Sets logging level to INFO. (default: None)\n  -vv, --debug          Print lots of debugging statements. Sets logging level to DEBUG. (default: None)\n  --quiet               Be quiet! Sets logging level to WARNING. (default: None)\n```",
            "rasa markers upload": "<RasaProLabel />\n\n<RasaProBanner />\n\n:::info New in 3.6\n\nThis command is available from Rasa Pro 3.6.0 and requires [Rasa Analytics Data Pipeline](./monitoring/analytics/getting-started-with-analytics.mdx)\n:::\n\nThis command applies to [markers](./markers.mdx) and their [real-time processing](./monitoring/analytics/realtime-markers.mdx).\nRunning this command validates the marker configuration file against the domain file and uploads the configuration to Analytics Data Pipeline\n\n```\nusage: rasa markers upload [-h] [-v] [-vv] [--quiet]\n                           [--logging-config-file LOGGING_CONFIG_FILE]\n                           [--config CONFIG]\n                           [--rasa-pro-services-url RASA_PRO_SERVICES_URL]\n                           [-d DOMAIN]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config CONFIG       The marker configuration file(s) containing marker\n                        definitions. This can be a single YAML file, or a\n                        directory that contains several files with marker\n                        definitions in it. The content of these files will be\n                        read and merged together. (default: markers.yml)\n  --rasa-pro-services-url RASA_PRO_SERVICES_URL\n                        The URL of the Rasa Pro Services instance to upload\n                        markers to.Specified URL should not contain a trailing\n                        slash. (default: )\n  -d DOMAIN, --domain DOMAIN\n                        Domain specification. This can be a single YAML file,\n                        or a directory that contains several files with domain\n                        specifications in it. The content of these files will\n                        be read and merged together. (default: domain.yml)\n\nPython Logging Options:\n  You can control level of log messages printed. In addition to these\n  arguments, a more fine grained configuration can be achieved with\n  environment variables. See online documentation for more info.\n\n  -v, --verbose         Be verbose. Sets logging level to INFO. (default:\n                        None)\n  -vv, --debug          Print lots of debugging statements. Sets logging level\n                        to DEBUG. (default: None)\n  --quiet               Be quiet! Sets logging level to WARNING. (default:\n                        None)\n  --logging-config-file LOGGING_CONFIG_FILE\n                        If set, the name of the logging configuration file\n                        will be set to the given name. (default: None)\n\nDescription:\n  The `rasa markers upload` command allows you to upload markers to the Rasa Pro Services. Markers are custom conversational events that provide additional context for analysis and insights generation. By uploading markers, you can enable real-time analysis and enhance the performance of your Rasa Assistant.\n\nExamples:\n  Upload Markers to Rasa Pro Services:\n    rasa markers upload --config markers.yml --rasa-pro-services-url https://example.com/rasa-pro -d domain.yml\n\n```",
            "rasa license": "<RasaProLabel />\n\n<RasaProBanner />\n\n:::info New in 3.3\n\nThis command was introduced.\n:::\n\nUse `rasa license` to display information about licensing in Rasa Pro, especially information about\n3rd party dependencies licenses.\n\nHere is the list of all possible arguments:\n\n```\nusage: rasa license [-h] [-v] [-vv] [--quiet] [--logging-config-file LOGGING_CONFIG_FILE]\n\nDisplay licensing information.\n\noptions:\n  -h, --help            show this help message and exit\n\nPython Logging Options:\n  You can control level of log messages printed. In addition to these arguments, a more fine grained configuration can be achieved with environment variables. See online documentation for more info.\n\n  -v, --verbose         Be verbose. Sets logging level to INFO. (default: None)\n  -vv, --debug          Print lots of debugging statements. Sets logging level to DEBUG. (default: None)\n  --quiet               Be quiet! Sets logging level to WARNING. (default: None)\n  --logging-config-file LOGGING_CONFIG_FILE\n                        If set, the name of the logging configuration file will be set to the given name. (default: None)\n```"
          },
          "metadata": {
            "id": "command-line-interface",
            "sidebar_label": "Command Line Interface",
            "title": "Command Line Interface",
            "description": "Command line interface for open source chatbot framework Rasa. Learn how to train, test and run your machine learning-based conversational AI assistants",
            "abstract": "The command line interface (CLI) gives you easy-to-remember commands for common tasks. This page describes the behavior of the commands and the parameters you can pass to them."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 5]"
        },
        {
          "title": "Compatibility Matrix",
          "description": "Information about compatibility between Rasa Pro Services and Rasa Plus.\n",
          "content": {
            "root": [
              "When choosing a version of Rasa Pro Services to [deploy](./deploy/deploy-rasa-pro-services.mdx), you can refer to the\ntable below to see which one is compatible with your installation of Rasa Plus.",
              "Rasa Pro Services version is independent of Rasa Plus version, except that they share the same major version number.",
              "| Rasa Pro Services |  Rasa Plus          |\n|------------------:|--------------------:|\n| 3.0.x             | 3.3.x, 3.4.x, 3.5.x |\n| 3.1.x             | 3.6.x               |"
            ]
          },
          "metadata": {
            "id": "compatibility-matrix",
            "sidebar_label": "Compatibility Matrix",
            "title": "Compatibility Matrix",
            "description": "Information about compatibility between Rasa Pro Services and Rasa Plus.\n"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 6]"
        },
        {
          "title": "Components",
          "description": null,
          "content": {
            "Language Models": {
              "MitieNLP": [
                "**Short**",
                "MITIE initializer"
              ],
              "SpacyNLP": [
                "**Short**",
                "spaCy language initializer"
              ]
            },
            "Tokenizers": {
              "WhitespaceTokenizer": [
                "**Short**",
                "Tokenizer using whitespaces as a separator"
              ],
              "JiebaTokenizer": [
                "**Short**",
                "Tokenizer using Jieba for Chinese language"
              ],
              "MitieTokenizer": [
                "**Short**",
                "Tokenizer using MITIE"
              ],
              "SpacyTokenizer": [
                "**Short**",
                "Tokenizer using spaCy"
              ]
            },
            "Featurizers": {
              "MitieFeaturizer": [
                "**Short**",
                "Creates a vector representation of user message and response (if specified) using the MITIE featurizer."
              ],
              "SpacyFeaturizer": [
                "**Short**",
                "Creates a vector representation of user message and response (if specified) using the spaCy featurizer."
              ],
              "ConveRTFeaturizer": [
                "**Short**",
                "Creates a vector representation of user message and response (if specified) using\n[ConveRT](https://github.com/PolyAI-LDN/polyai-models) model."
              ],
              "LanguageModelFeaturizer": [
                "**Short**",
                "Creates a vector representation of user message and response (if specified) using a pre-trained language model."
              ],
              "RegexFeaturizer": [
                "**Short**",
                "Creates a vector representation of user message using regular expressions."
              ],
              "CountVectorsFeaturizer": [
                "**Short**",
                "Creates bag-of-words representation of user messages, intents, and responses."
              ],
              "LexicalSyntacticFeaturizer": [
                "**Short**",
                "Creates lexical and syntactic features for a user message to support entity extraction."
              ]
            },
            "Intent Classifiers": {
              "MitieIntentClassifier": [
                "**Short**",
                "MITIE intent classifier (using a\n[text categorizer](https://github.com/mit-nlp/MITIE/blob/master/examples/python/text_categorizer_pure_model.py))"
              ],
              "LogisticRegressionClassifier": [
                "**Short**",
                "Logistic regression intent classifier, using the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
              ],
              "SklearnIntentClassifier": [
                "**Short**",
                "Sklearn intent classifier"
              ],
              "KeywordIntentClassifier": [
                "**Short**",
                "Simple keyword matching intent classifier, intended for small, short-term projects."
              ],
              "DIETClassifier": [
                "**Short**",
                "Dual Intent Entity Transformer (DIET) used for intent classification and entity extraction"
              ],
              "FallbackClassifier": [
                "**Short**",
                "Classifies a message with the intent `nlu_fallback` if the NLU intent classification\nscores are ambiguous. The confidence is set to be the same as the `fallback threshold`.",
                "**Outputs**",
                "`entities`, `intent` and `intent_ranking`",
                "**Requires**",
                "`intent` and `intent_ranking` output from a previous intent classifier",
                "**Output-Example**",
                "```\n\n    {\n        \"intent\": {\"name\": \"nlu_fallback\", \"confidence\": 0.7183846840434321},\n        \"intent_ranking\": [\n            {\n                \"confidence\": 0.7183846840434321,\n                \"name\": \"nlu_fallback\"\n            },\n            {\n                \"confidence\": 0.28161531595656784,\n                \"name\": \"restaurant_search\"\n            }\n        ],\n        \"entities\": [{\n            \"end\": 53,\n            \"entity\": \"time\",\n            \"start\": 48,\n            \"value\": \"2017-04-10T00:00:00.000+02:00\",\n            \"confidence\": 1.0,\n            \"extractor\": \"DIETClassifier\"\n        }]\n    }\n```",
                "**Description**",
                "The `FallbackClassifier` classifies a user message with the intent `nlu_fallback`\nin case the previous intent classifier wasn't\nable to classify an intent with a confidence greater or equal than the `threshold`\nof the `FallbackClassifier`. It can also predict the fallback intent in the\ncase when the confidence scores of the two top ranked intents are closer than the the\n`ambiguity_threshold`.",
                "You can use the `FallbackClassifier` to implement a\n[Fallback Action](./fallback-handoff.mdx#fallbacks) which handles message with uncertain\nNLU predictions.",
                "```\nrules:\n\n- rule: Ask the user to rephrase in case of low NLU confidence\n  steps:\n  - intent: nlu_fallback\n  - action: utter_please_rephrase\n```",
                "**Configuration**",
                "The `FallbackClassifier` will only add its prediction for the `nlu_fallback`\nintent in case no other intent was predicted with a confidence greater or equal\nthan `threshold`.",
                [
                  "`threshold`:\nThis parameter sets the threshold for predicting the `nlu_fallback` intent.\nIf no intent predicted by a previous\nintent classifier has a confidence\nlevel greater or equal than `threshold` the `FallbackClassifier` will add\na prediction of the `nlu_fallback` intent with a confidence `1.0`.",
                  "`ambiguity_threshold`: If you configure an `ambiguity_threshold`, the\n`FallbackClassifier` will also predict the `nlu_fallback` intent in case\nthe difference of the confidence scores for the two highest ranked intents is\nsmaller than the `ambiguity_threshold`."
                ]
              ]
            },
            "Entity Extractors": {
              "MitieEntityExtractor": [
                "**Short**",
                "MITIE entity extraction (using a [MITIE NER trainer](https://github.com/mit-nlp/MITIE/blob/master/mitielib/src/ner_trainer.cpp))"
              ],
              "SpacyEntityExtractor": [
                "**Short**",
                "spaCy entity extraction"
              ],
              "CRFEntityExtractor": [
                "**Short**",
                "Conditional random field (CRF) entity extraction"
              ],
              "DucklingEntityExtractor": [
                "**Short**",
                "Duckling lets you extract common entities like dates,\namounts of money, distances, and others in a number of languages."
              ],
              "DIETClassifier": [
                "**Short**",
                "Dual Intent Entity Transformer (DIET) used for intent classification and entity extraction"
              ],
              "RegexEntityExtractor": [
                "**Short**",
                "Extracts entities using the lookup tables and/or regexes defined in the training data"
              ],
              "EntitySynonymMapper": [
                "**Short**",
                "Maps synonymous entity values to the same value."
              ]
            },
            "Combined Intent Classifiers and Entity Extractors": {
              "DIETClassifier": [
                "**Short**",
                "Dual Intent Entity Transformer (DIET) used for intent classification and entity extraction"
              ]
            },
            "Selectors": {
              "ResponseSelector": [
                "**Short**",
                "Response Selector"
              ]
            },
            "Custom Components": ":::info New in 3.0\nRasa 3.0 unified the implementation of NLU components and policies.\nThis requires changes to custom components written for earlier versions of Rasa Open\nSource. Please see the\n[migration guide](migration-guide.mdx#custom-policies-and-custom-components) for a\nstep-by-step guide for the migration.\n\n:::\n\nYou can create a custom component to perform a specific task which NLU doesn't currently offer (for example, sentiment analysis).\n\nYou can add a custom component to your pipeline by adding the module path.\nSo if you have a module called `sentiment`\ncontaining a `SentimentAnalyzer` class:\n\n```\npipeline:\n- name: \"sentiment.SentimentAnalyzer\"\n```\n\nSee the [guide on custom graph components](custom-graph-components.mdx) for a complete guide on custom components.\nAlso be sure to read the section on the [Component Lifecycle](./tuning-your-model.mdx#component-lifecycle)."
          },
          "metadata": {
            "id": "components",
            "sidebar_label": "Pipeline Components",
            "title": "Components",
            "abstract": "Components make up your NLU pipeline and work sequentially to process user input into structured output. There are components for entity extraction, for intent classification, response selection, pre-processing, and more."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 7]"
        },
        {
          "title": "Contextual Conversations",
          "description": null,
          "content": {
            "Step-by-step Guide on Creating Contextual Conversation Patterns": {
              "1. Defining Slots": "[Slots](domain.mdx#slots) are your assistant's memory. Slots store pieces of information that your\nassistant needs to refer to later and can direct the flow of the conversation\nbased on `slot_was_set` events. There are different [types of slots](domain.mdx#slot-types),\nand each affects the conversation flow in its own way.\n\nIn the concert bot example, the `likes_music` slot is a boolean slot. If it is true, the bot sends an intro message. If it is false, the bot sends a different message.\nYou define a slot and its type in the domain:\n\n```\nslots:\n  likes_music:\n    type: bool\n    mappings:\n    - type: custom\n```",
              "2. Creating Stories": "[Stories](./stories.mdx) are examples of how conversations should go.\nIn the example above, the concert bot responds differently for users who like music\nand users who don't because of these two stories:\n\n```\nstories:\n  - story: User likes music\n    steps:\n    - intent: how_to_get_started\n    - action: utter_get_started\n    - intent: affirm\n    - action: action_set_music_preference\n    - slot_was_set:\n      - likes_music: True\n    - action: utter_awesome\n\n  - story: User doesn't like music\n    steps:\n    - intent: how_to_get_started\n    - action: utter_get_started\n    - intent: deny\n    - action: action_set_music_preference\n    - slot_was_set:\n      - likes_music: False\n    - action: utter_goodbye\n```\n\nThese stories diverge based on the user's intent (`affirm` or `deny`). Based on\nthe user's intent, a custom action sets a slot that further directs\nthe conversation.",
              "3. Configuring the TEDPolicy": {
                "Setting `max_history`": "Usually, only a certain amount of context is relevant to your assistant.\n[`max_history`](policies.mdx#max-history) is a hyperparameter for Rasa dialogue management policies\nthat controls how many steps in a dialogue the model looks at to decide which\naction to take next.\n\nIn the story below, the user asks for help three times in a row. The first two times,\nthe bot sends the same message, but the third time, it hands them off to a human\n\n```\nstories:\n  - story: user persists in asking for help\n    steps:\n    - intent: help\n    - action: utter_help\n    - intent: help\n    - action: utter_help\n    - intent: help\n    - action: action_human_handoff\n```\n\nIn order for the model to learn this pattern, it needs to know at least the previous\nfour steps i.e. `max_history` of four. If `max_history` were 3, the model would not have\nenough context to see that the user had already sent two help requests, and would never\npredict the human handoff action.\n\nYou can set the `max_history` by passing it to your policy's settings\nin your config file, for example:\n\n```\npolicies:\n  - name: \"TEDPolicy\"\n    max_history: 5\n```\n\nYou want to make sure `max_history` is set high enough\nto account for the most context your assistant will need to make an accurate\nprediction about what to do next.\nFor more details see the docs on [featurizers](policies.mdx#featurizers)."
              }
            },
            "Summary": "Here's a summary of the concepts you can apply to enable your assistant to have contextual conversations:\n\n['[ ] Write stories for contextual conversations', '[ ] Use slots to store contextual information for later use', '[ ] Set the `max_history` for your policies appropriately for the amount of context your bot needs', '[ ] Use the TEDPolicy for generalization to unseen conversation paths']"
          },
          "metadata": {
            "id": "contextual-conversations",
            "sidebar_label": "Contextual Conversations",
            "title": "Contextual Conversations",
            "abstract": "Taking context into account is often key to providing a good user experience. This page is a guide to creating contextual conversation patterns."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 8]"
        },
        {
          "title": "Conversation-Driven Development",
          "description": "Find out about best practices for conversational AI using Conversation-Driven Development.",
          "content": {
            "What is CDD?": "Conversation-Driven Development (CDD) is the process of listening to your users and using those insights to improve your AI assistant. It is the overarching best practice approach for chatbot development.\n\nDeveloping great AI assistants is challenging because users will always say something you didn't anticipate. The principle behind CDD is that in every conversation users are telling you—in their own words—exactly what they want. By practicing CDD at every stage of bot development, you orient your assistant towards real user language and behavior.\n\nCDD includes the following actions:\n\n['**Share** your assistant with users as soon as possible', '**Review** conversations on a regular basis', '**Annotate** messages and use them as NLU training data', '**Test** that your assistant always behaves as you expect', '**Track** when your assistant fails and measure its performance over time', '**Fix** how your assistant handles unsuccessful conversations']\n\nCDD is not a linear process; you'll circle back to the same actions over and over as you develop and improve your bot.\n\nRead more about these actions and the concept of CDD on the [Rasa Blog](https://blog.rasa.com/conversation-driven-development-a-better-approach-to-building-ai-assistants/).\n\nYou can also check out [Rasa X/Enterprise](https://rasa.com/docs/rasa-enterprise/), a purpose-built tool for CDD.",
            "CDD in early stages of development": "If you're at the earliest stage of bot development, it might seem like CDD has no role to play - after all, you have no conversations yet! However, there are CDD actions you can take at the very beginning of bot development:\n\n['See the best practices for [NLU data](generating-nlu-data.mdx) and [Stories](writing-stories.mdx) for details on creating training data with CDD in mind.', 'Give your bot to test users early on.', 'CDD is all about listening to your users, so the earlier you find some, the better.', \"Test users can be anyone who doesn't already know how your bot works from the inside. People on the bot development team should not be test users, since they know exactly what the bot can and can't do. Don't overinstruct your test users; they should have only as much knowledge of the bot's domain as your end users will have.\", 'Set up a CI/CD pipeline.', 'CDD leads to frequent, smaller updates to your bot as you gather insights from bot conversations. [Setting up a CI/CD pipeline](setting-up-ci-cd.mdx) early on in development will enable you to act quickly on what you see in conversations.']",
            "CDD with a bot in production": {
              "Review": "Look in conversations for what users are really asking for.\n\nYour test users had at least some instruction about what the bot was intended to do; real users often either have no idea, or ignore instructions given to them. You can't cater to every unexpected user behavior, but you can try to address the main friction points you notice. Here are some things you could consider looking for:\n\n['Look at conversations where an “out_of_scope” intent or fallback behavior occurred. These could indicate a potential new skill, or just a misclassified user utterance.', 'Look for user frustration, such as requests for transfer to a human.', 'If the assistant was trained with [`UnexpecTEDIntentPolicy`](./policies.mdx#unexpected-intent-policy) included in the pipeline,\\nyou can look for conversations where `action_unlikely_intent` is predicted at any conversation turn.\\nAn `action_unlikely_intent` is predicted when the last intent expressed by the user is\\nunexpected in the current conversation context. You can also filter out such conversations by\\nrunning a [standalone script](https://gist.github.com/alwx/b426b7b573ff963c85c65ea6466528d7) which does the following:', ['Fetch real conversations from a tracker store.', 'Run `rasa test` on the fetched conversations and filter conversations containing `action_unlikely_intent`\\nin a separate warnings file. You can read more on [how to interpret these warnings](./testing-your-assistant.mdx#interpreting-the-generated-warnings).'], 'Reviewing this subset of conversations can help you understand if real users have taken a\\nconversation path which is not present in the training data and hence \"surprising\"\\nfor machine learning policies like `TEDPolicy`. Adding these conversation paths (with potential\\ncorrections if `TEDPolicy` subsequently failed) as training stories will result in more robust action prediction\\nby policies such as `TEDPolicy`. Users are encouraged to [adjust the `tolerance` parameter of\\n`UnexpecTEDIntentPolicy`](./policies.mdx#tuning-the-tolerance-parameter) to control how\\n\"surprising\" a conversation should be to be included in the warnings file.']",
              "Annotate": "Continue to follow [best practices for NLU](generating-nlu-data.mdx) as you add new user utterances from real conversations to your training data. Be careful not to overfit your NLU model to utterances like those already in your training data. This can happen when you continuously add user utterances that were already predicted correctly and with high confidence to your training data.  To avoid overfitting and help your model generalize to more diverse user utterances, add only user utterances that the model previously predicted incorrectly or with low confidence.",
              "Test": "Add successful user conversations to your [test conversations](testing-your-assistant.mdx). Doing this consistently will help ensure you don't introduce regressions as you make other fixes to your bot.",
              "Track": "Look for clues to success and failure to help you track your bot's performance.\n\nSome metrics are external to your bot. For example, if you are building a bot to relieve demand on a customer service call center, one metric for success could be the reduction in traffic to the call center. Others you can get directly from conversations, such as whether a user reaches a certain action that represents achieving the user goal.\n\nAutomatically tracked metrics are by nature proxy metrics; the only way to get a true measure of success would be to individually review and rate every single conversation with your bot. While this clearly isn't realistic, just keep in mind that no metric is a perfect representation of your bot's performance, so don't rely only on metrics to see where your bot needs improvement.",
              "Fix": "Continue to follow [best practices for Stories](writing-stories.mdx) as you expand and improve your bot's skills. Let user demand guide which skills you add and which fixes you make. Make smaller changes frequently rather than making big changes only once in a while. This will help you gauge the effectiveness of changes you're making, since you'll get user feedback more frequently. Your [CI/CD pipeline](setting-up-ci-cd.mdx) should allow you to do so with confidence."
            }
          },
          "metadata": {
            "id": "conversation-driven-development",
            "sidebar_label": "Conversation-Driven Development",
            "title": "Conversation-Driven Development",
            "description": "Find out about best practices for conversational AI using Conversation-Driven Development."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 9]"
        },
        {
          "title": "Custom Actions",
          "description": null,
          "content": {
            "root": [
              "For details on how to implement a custom action, see the [SDK documentation](./action-server/running-action-server.mdx).\nAny custom action that you want to use in your stories should be added into the\nactions section of your [domain](./domain.mdx).",
              "When the dialogue engine predicts a custom action to be executed, it will call\nthe action server, with the following information:",
              "```\n{\n  \"next_action\": \"string\",\n  \"sender_id\": \"string\",\n  \"tracker\": {\n    \"conversation_id\": \"default\",\n    \"slots\": {},\n    \"latest_message\": {},\n    \"latest_event_time\": 1537645578.314389,\n    \"followup_action\": \"string\",\n    \"paused\": false,\n    \"events\": [],\n    \"latest_input_channel\": \"rest\",\n    \"active_loop\": {},\n    \"latest_action\": {}\n  },\n  \"domain\": {\n    \"config\": {},\n    \"session_config\": {},\n    \"intents\": [],\n    \"entities\": [],\n    \"slots\": {},\n    \"responses\": {},\n    \"actions\": [],\n    \"forms\": {},\n    \"e2e_actions\": []\n  },\n  \"version\": \"version\"\n}\n```",
              "Your action server should respond with a list of events and responses:",
              "```\n{\n  \"events\": [{}],\n  \"responses\": [{}]\n}\n```"
            ]
          },
          "metadata": {
            "id": "custom-actions",
            "sidebar_label": "Custom Actions",
            "title": "Custom Actions",
            "abstract": "A custom action can run any code you want, including API calls, database queries etc. They can turn on the lights, add an event to a calendar, check a user's bank balance, or anything else you can imagine."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 10]"
        },
        {
          "title": "Custom Graph Components",
          "description": null,
          "content": {
            "Graph Components": "Rasa uses the passed in [model configuration](model-configuration.mdx) to build a\n[directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph).\nThis graph describes the dependencies between the items in your model configuration and\nhow data flows between them. This has two major benefits:\n\n['Rasa can use the computational graph to optimize the execution of your\\nmodel. Examples for this are efficient caching of training steps or executing\\nindependent steps in parallel.', 'Rasa can represent different model architectures flexibly. As long as the\\ngraph remains acyclic Rasa can in theory pass any data to any graph\\ncomponent based on the model configuration without having to tie the underlying\\nsoftware architecture to the used model architecture.']\n\nWhen translating the model configuration to the computational graph\n[policies](policies.mdx) and [NLU components](components.mdx) become nodes within this graph.\nWhile there is a distinction between policies and NLU components in your model\nconfiguration, the distinction is abstracted away when they are placed within the graph.\nAt this point policies and NLU components become abstract *graph components*.\nIn practice this is represented by the\n[`GraphComponent`](custom-graph-components.mdx#the-graphcomponent-interface)\ninterface: Both policies and NLU components have to inherit from this interface to\nbecome compatible and executable for Rasa's graph.\n\n<div align=\"center\">\n    <img alt=\"Visualization of the Rasa Graph Architecture\" src={useBaseUrl(\"/img/graph_architecture.png\")} width=\"100%\" />\n</div>",
            "Getting Started": "Before you get started, you have to decide whether you want to implement a custom\n[NLU component](components.mdx) or a [policy](policies.mdx). If you are implementing\na custom policy, then we recommend extending the existing\n`rasa.core.policies.policy.Policy` class which already implements the `GraphComponent`\ninterface.\n\n```\nfrom rasa.core.policies.policy import Policy\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\n\n# TODO: Correctly register your graph component\n@DefaultV1Recipe.register(\n    [DefaultV1Recipe.ComponentType.POLICY_WITHOUT_END_TO_END_SUPPORT], is_trainable=True\n)\nclass MyPolicy(Policy):\n    ...\n```\n\nIf you want to implement a custom NLU component then start out with the following\nskeleton:\n\n```\n```\n\nRead the following sections to find out how to solve the `TODO`s in the example above\nand what other methods need to be implemented in your custom component.\n\n:::note custom tokenizers\nIf you create a custom tokenizer, you should extend the\n`rasa.nlu.tokenizers.tokenizer.Tokenizer` class. The `train` and `process` methods are\nalready implemented so you only need to overwrite the `tokenize` method.\n\n:::",
            "The `GraphComponent` interface": {
              "`create`": "The `create` method is used to instantiate your graph component during training and has to be\noverridden. Rasa passes the following parameters when calling the method:\n\n[\"`config`: This is your component's default configuration merged with the\\nconfiguration provided to the graph component in the model configuration file.\", '`model_storage`: You can use this to persist and load your graph component. See the\\n[model persistence](#model-persistence) section for further details on its usage.', '`resource`: The unique identifier of your component within the `model_storage`.\\nSee the [model persistence](#model-persistence) section for further\\ndetails on its usage.', '`execution_context`: This provides additional information about the current\\nmode of execution:', ['`model_id`: A unique identifier for the model used during inference. This\\nparameter is `None` during training.', \"`should_add_diagnostic_data`: If `True` then additional diagnostic metadata\\nshould be added to your graph component's predictions on top of the actual prediction.\", '`is_finetuning`: If `True` then the graph component can be trained using\\n[finetuning](command-line-interface.mdx#incremental-training).', '`graph_schema`: The `graph_schema` describes the computational graph which is used\\nto train your assistant or to make predictions with it.', '`node_name`: The `node_name` is a unique identifier for the step in the graph\\nschema which is fulfilled by the called graph component']]",
              "`load`": "The `load` method is used to instantiate your graph component during inference. The default\nimplementation of this method calls your `create` method. It is recommended to override\nthis if your graph component\n[persists data as part of the training](#model-persistence).\nSee [`create`](#create) for a description of the individual parameters.",
              "`get_default_config`": "The method `get_default_config` returns the default configuration for your graph\ncomponent. Its default implementation returns an empty dictionary which implies that\nthe graph component\ndoes not have any configuration. Rasa will update the default configuration with the given\nin the configuration file at runtime.",
              "`supported_languages`": "The method `supported_languages` specifies which\n[languages](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)\na graph component supports.\nRasa will use the `language` key in the model configuration file to\nvalidate that the graph component is valid for usage with the specified language.\nIf a graph component returns `None` (this is the default implementation), it indicates\nthat the graph component supports all languages which are not part of\n`not_supported_languages`.\n\nExamples:\n\n['`[]`: The graph component does not support any language', '`None`: All languages are supported expect the languages defined in\\n`not_supported_languages`', '`[\"en\"]`: The graph component can only be used with English conversations.']",
              "`not_supported_languages`": "The method `not_supported_languages` specifies which [languages](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) your\ngraph component does not support. Rasa will use the `language` key in the\nmodel configuration file to validate that your graph component is valid for usage with\nthe specified language. If your graph component returns `None` (this is the default\nimplementation), you indicate that it supports all languages which are specified in\n`supported_languages`.\n\nExamples:\n\n['`None` or `[]`: All languages specified in `supported_languages` are supported.', '`[\"en\"]`: The graph component can be used with any language except English.']",
              "`required_packages`": "The `required_packages` method indicates which extra Python packages need to\nbe installed to use this graph component. Rasa will raise an error\nduring execution if the required libraries are not found at runtime. By default, this method returns an empty list which implies that your graph\ncomponent does not have any extra dependencies.\n\nExamples:\n\n['`[]`: No extra packages are required to use this graph component', '`[\"spacy\"]`: The Python package `spacy` needs to be installed to use\\nthis graph component.']"
            },
            "Model Persistence": {
              "Writing to the Model Storage": "The snippet below illustrates how to write your graph component's data to the model\nstorage.\nTo persist your graph component after training, the `train` method will need to access\nto the values of `model_storage` and `resource`. Therefore, you should store the values\nof `model_storage` and `resource` at initialization time.\n\nYour graph component's train method must return the value of `resource` so that Rasa can cache\nthe training results between trainings.\nThe `self._model_storage.write_to(self._resource)` context manager provides a path to\na directory where you can persist any data required by your\ngraph component.\n\n```\nfrom __future__ import annotations\nimport json\nfrom typing import Optional, Dict, Any, Text\n\nfrom rasa.engine.graph import GraphComponent, ExecutionContext\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\nclass MyComponent(GraphComponent):\n\n    def __init__(\n        self,\n        model_storage: ModelStorage,\n        resource: Resource,\n        training_artifact: Optional[Dict],\n    ) -> None:\n        # Store both `model_storage` and `resource` as object attributes to be able\n        # to utilize them at the end of the training\n        self._model_storage = model_storage\n        self._resource = resource\n\n    @classmethod\n    def create(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n    ) -> MyComponent:\n        return cls(model_storage, resource, training_artifact=None)\n\n    def train(self, training_data: TrainingData) -> Resource:\n        # Train your graph component\n        ...\n\n        # Persist your graph component\n        with self._model_storage.write_to(self._resource) as directory_path:\n            with open(directory_path / \"artifact.json\", \"w\") as file:\n                json.dump({\"my\": \"training artifact\"}, file)\n\n        # Return resource to make sure the training artifacts\n        # can be cached.\n        return self._resource\n\n```",
              "Reading from the Model Storage": "Rasa will call the `load` method of your graph component to instantiate it for inference. You can use the context manager `self._model_storage.read_from(resource)` to get a path to the directory where your graph component's data was persisted. Using the provided path you can then load the\npersisted data and initialize your graph component with it. Note that the `model_storage`\nwill throw a `ValueError` in case no persisted data was found for the given `resource`.\n\n```\nfrom __future__ import annotations\nimport json\nfrom typing import Optional, Dict, Any, Text\n\nfrom rasa.engine.graph import GraphComponent, ExecutionContext\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\n\nclass MyComponent(GraphComponent):\n\n    def __init__(\n        self,\n        model_storage: ModelStorage,\n        resource: Resource,\n        training_artifact: Optional[Dict],\n    ) -> None:\n        self._model_storage = model_storage\n        self._resource = resource\n\n    @classmethod\n    def load(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        **kwargs: Any,\n    ) -> MyComponent:\n        try:\n            with model_storage.read_from(resource) as directory_path:\n                with open(directory_path / \"artifact.json\", \"r\") as file:\n                    training_artifact = json.load(file)\n                    return cls(\n                        model_storage, resource, training_artifact=training_artifact\n                    )\n        except ValueError:\n            # This allows you to handle the case if there was no\n            # persisted data for your component\n            ...\n```"
            },
            "Registering Graph Components with the Model Configuration": "To make your graph component available to Rasa you may have to register your\ngraph component with a recipe. Rasa uses recipes to translate the content\nof your model configuration to executable\n[graphs](custom-graph-components.mdx#graph-components).\nCurrently, Rasa supports the `default.v1` and the experimental `graph.v1` recipes.\nFor `default.v1` recipe, you need to register your graph component by using the `DefaultV1Recipe.register`\ndecorator:\n\n```\n```\n\nRasa uses the information provided in the `register` decorator and the\nposition of your graph component within the configuration file to schedule the execution\nof your graph component with its required data. The `DefaultV1Recipe.register` decorator allows you\nto specify the following details:\n\n['`component_types`: This specifies what purpose your graph component fulfills within the\\nassistant. It is possible to specify multiple types (e.g. if your graph component is both\\nintent classifier and entity extractor):', [\"`ComponentType.MODEL_LOADER`: Component type for\\n[language models](components.mdx#language-models). Graph components of this type provide\\npretrained models to other graph components' `train`, `process_training_data` and\\n`process` methods if they have specified `model_from=<model loader name>`.\\nThis graph component is run during training and\\ninference. Rasa will use the graph component's `provide` method to\\nretrieve the model which should be provided to dependent graph components.\", \"`ComponentType.MESSAGE_TOKENIZER`: Component type for\\n[tokenizers](components.mdx#tokenizers). This graph component is run during training and\\ninference. Rasa will use the graph component's `train` method if\\n`is_trainable=True` is specified. Rasa will use\\n`process_training_data` for tokenizing training data examples and `process`\\nto tokenize messages during inference.\", \"`ComponentType.MESSAGE_FEATURIZER`: Component type for\\n[featurizers](components.mdx#featurizers). This graph component is run during training and\\ninference. Rasa will use the graph component's `train` method if\\n`is_trainable=True` is specified. Rasa will use\\n`process_training_data` for featurizing training data examples and `process`\\nto featurize messages during inference.\", \"`ComponentType.INTENT_CLASSIFIER`: Component type for\\n[intent classifiers](components.mdx#intent-classifiers). This graph component is run only\\nduring training if `is_trainable=True`. The graph component is always run during\\ninference.\\nRasa will use the graph component's `train` method if\\n`is_trainable=True` is specified. Rasa will use\\nthe graph component's `process` method to classify the intent of messages during\\ninference.\", \"`ComponentType.ENTITY_EXTRACTOR`: Component type for\\n[entity extractors](components.mdx#entity-extractors). This graph component is run only\\nduring training if `is_trainable=True`. The graph component is always run during\\ninference.\\nRasa will use the graph component's `train` method if\\n`is_trainable=True` is specified. Rasa will use\\nthe graph component's `process` method to extract entities during inference.\", \"`ComponentType.POLICY_WITHOUT_END_TO_END_SUPPORT`: Component type for\\n[policies](policies.mdx) which don't require additional end-to-end features\\n(see [end-to-end training](stories.mdx#end-to-end-training) for more information).\\nThis graph component is run only during training if `is_trainable=True`.\\nThe graph component is always run during inference.\\nRasa will use the graph component's `train` method if\\n`is_trainable=True` is specified. Rasa will use\\nthe graph component's `predict_action_probabilities` to make predictions for the\\nnext action which should be run within a conversation.\", \"`ComponentType.POLICY_WITH_END_TO_END_SUPPORT`: Component type for\\n[policies](policies.mdx) which require additional end-to-end features\\n(see [end-to-end training](stories.mdx#end-to-end-training) for more information).\\nThe end-to-end features are passed into the graph component's `train` and\\n`predict_action_probabilities` as `precomputations` parameter.\\nThis graph component is run only during training if `is_trainable=True`.\\nThe graph component is always run during inference.\\nRasa will use the graph component's `train` method if\\n`is_trainable=True` is specified. Rasa will use\\nthe graph component's `predict_action_probabilities` to make predictions for the\\nnext action which should be run within a conversation.\"], '`is_trainable`: Specifies if the graph component is required to train itself before it can\\nprocess training data for other dependent graph components or before it can make\\npredictions', '`model_from`: Specifies if a pretrained\\n[language model](components.mdx#language-models) needs to be provided to the `train`,\\n`process_training_data` and `process` methods of the graph component. These methods\\nhave to support the parameter `model` to receive the language model. Note that you\\nstill need to make sure that the graph component which provides this model is part\\nof your model configuration. A common use case for this is if you want to expose the\\n[SpacyNLP](components.mdx#spacynlp) language model to your other NLU components.']",
            "Using Custom Components in your Model Configuration": "You can use custom graph components like any other NLU component or policy within your\n[model configuration](model-configuration.mdx). The only change is that you have to specify\nthe full module name instead of the class name only. The full module name depends on\nyour module's location in relation to the specified\n[PYTHONPATH](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONPATH).\nBy default, Rasa adds the directory from where you run the CLI to the\n`PYTHONPATH`. If you e.g. run the CLI from `/Users/<user>/my-rasa-project`\nand your module `MyComponent` is in `/Users/<user>/my-rasa-project/custom_components/my_component.py`\nthen the module path is `custom_components.my_component.MyComponent`. Everything except\nthe `name` entry will be passed as `config` to your component.\n\n```\nrecipe: default.v1\nlanguage: en\npipeline:\n# other NLU components\n- name: your.custom.NLUComponent\n  setting_a: 0.01\n  setting_b: string_value\n\npolicies:\n# other dialogue policies\n- name: your.custom.Policy\n```",
            "Implementation Hints": {
              "Message Metadata": "When you [define metadata for your intent examples in your training data](./training-data-format.mdx#training-examples),\nyour NLU component can access both the intent metadata and the intent example metadata during processing:\n\n```\n# in your component class\n\n    def process(self, message: Message, **kwargs: Any) -> None:\n        metadata = message.get(\"metadata\")\n        print(metadata.get(\"intent\"))\n        print(metadata.get(\"example\"))\n```",
              "Sparse and Dense Message Features": "If you create a custom message featurizer, you can return two different kind of\nfeatures: sequence features and sentence\nfeatures. The sequence features are a matrix of size `(number-of-tokens x\nfeature-dimension)`, i.e.\nthe matrix contains a feature vector for every token in the sequence.\nThe sentence features are represented by a matrix of size `(1 x feature-dimension)`."
            },
            "Examples of Custom Components": {
              "Dense Message Featurizer": "The following is the example of a dense [message featurizer](components.mdx#featurizers)\nwhich uses a pretrained model:\n\n```\n```",
              "Sparse Message Featurizer": "The following is the example of a dense [message featurizer](components.mdx#featurizers)\nwhich trains a new model:\n\n```\n```",
              "NLU Meta Learners": ":::info Advanced use case\n\nNLU Meta learners are an advanced use case. The following section is only relevant\nif you have a component that learns parameters based on the output of previous\nclassifiers. For components that have manually set parameters or logic, you can create\na component with `is_trainable=False` and not worry about the preceding classifiers.\n\n:::\n\nNLU Meta learners are intent classifiers or entity extractors that use the predictions\nof other trained intent classifiers or entity extractors and try to improve upon their\nresults. An example for a meta learner would be a component that averages the output\nof two previous intent classifiers or a fallback classifier that sets it's threshold\naccording to the confidence of the intent classifier on training examples.\n\nConceptually, to built a trainable fallback classifier you first need to create that\nfallback classifier as a custom component:\n\n```\n```\n\nNext, you will need to create a custom intent classifier that is also a featurizer,\nas the classifiers' output needs to be consumed by another component downstream.\nFor the custom intent classifier component you also need to define how its predictions\nshould be added to the message data specifying the `process_training_data` method.\nMake sure to not overwrite the true labels for the intents. Here's a template that\nshows how to subclass DIET for this purpose:\n\n```\n```"
            }
          },
          "metadata": {
            "id": "custom-graph-components",
            "sidebar_label": "Custom Graph Components",
            "title": "Custom Graph Components",
            "abstract": "You can extend Rasa with custom NLU components and policies. This page provides a guide on how to develop your own custom graph components."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 11]"
        },
        {
          "title": "Default Actions",
          "description": null,
          "content": {
            "action_listen": "This action is predicted to signal that the assistant should do nothing and wait\nfor the next user input.",
            "action_restart": "This action resets the whole conversation history, including any slots that were\nset during it.\n\nIt can be triggered by the user in a conversation by sending a\n\"/restart\" message, if the [RulePolicy](./rules.mdx) is included in the model configuration.\nIf you define an `utter_restart` response in your domain, this will be sent to the user as well.",
            "action_session_start": {
              "Customization": "The default behavior of the session start action is to take all existing slots and to\ncarry them over into the next session. Let's say you do not want to carry over all\nslots, but only a user's name and their phone number. To do that, you'd override the\n`action_session_start` with a custom action that might look like this:\n\n```\nfrom typing import Any, Text, Dict, List\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.events import SlotSet, SessionStarted, ActionExecuted, EventType\n\n\nclass ActionSessionStart(Action):\n    def name(self) -> Text:\n        return \"action_session_start\"\n\n    @staticmethod\n    def fetch_slots(tracker: Tracker) -> List[EventType]:\n        \"\"\"Collect slots that contain the user's name and phone number.\"\"\"\n\n        slots = []\n        for key in (\"name\", \"phone_number\"):\n            value = tracker.get_slot(key)\n            if value is not None:\n                slots.append(SlotSet(key=key, value=value))\n        return slots\n\n    async def run(\n      self, dispatcher, tracker: Tracker, domain: Dict[Text, Any]\n    ) -> List[Dict[Text, Any]]:\n\n        # the session should begin with a `session_started` event\n        events = [SessionStarted()]\n\n        # any slots that should be carried over should come after the\n        # `session_started` event\n        events.extend(self.fetch_slots(tracker))\n\n        # an `action_listen` should be added at the end as a user message follows\n        events.append(ActionExecuted(\"action_listen\"))\n\n        return events\n```\n\nIf you want to access the metadata which was sent with the user message which triggered\nthe session start, you can access the special slot `session_started_metadata`:\n\n```\nfrom typing import Any, Text, Dict, List\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.events import SessionStarted, ActionExecuted\n\n\nclass ActionSessionStart(Action):\n    def name(self) -> Text:\n        return \"action_session_start\"\n\n    async def run(\n      self, dispatcher, tracker: Tracker, domain: Dict[Text, Any]\n    ) -> List[Dict[Text, Any]]:\n        metadata = tracker.get_slot(\"session_started_metadata\")\n\n        # Do something with the metadata\n        print(metadata)\n\n        # the session should begin with a `session_started` event and an `action_listen`\n        # as a user message follows\n        return [SessionStarted(), ActionExecuted(\"action_listen\")]\n```"
            },
            "action_default_fallback": "This action undoes the last user-bot interaction and sends the `utter_default` response if it is defined.\nIt is triggered by low action prediction confidence, if you have this [fallback mechanism](./fallback-handoff.mdx) enabled.",
            "action_deactivate_loop": "This action deactivates the active loop and resets the requested slot. This is used when\n[handling unhappy paths in forms](./forms.mdx#writing-stories--rules-for-unhappy-form-paths).\n\n:::note\nIf you wish to reset all slots, we recommend using a custom action\nthat returns the [`AllSlotsReset`](https://rasa.com/docs/rasa/reference/rasa/shared/core/events#allslotsreset-objects) event after form deactivation.\n:::",
            "action_two_stage_fallback": "This is a fallback loop that can be used to handle low NLU confidence. Read more about\n[handling low NLU confidence](./fallback-handoff.mdx#nlu-fallback).",
            "action_default_ask_affirmation": "This action is used by the `action_two_stage_fallback` loop. It asks the user to confirm\nthe intent of their message. This action can be customized to be more personalized\nto your specific use case.",
            "action_default_ask_rephrase": "This action is used by the `action_two_stage_fallback` loop if the user denies the\nintent `action_default_ask_affirmation` displays. It asks the user to rephrase\ntheir message.",
            "action_back": "This action undoes the last user-bot interaction. It can be triggered by the user\nby sending a \"/back\" message to the assistant if the [RulePolicy](./policies.mdx#rule-policy) is configured.\n|",
            "Form Action": "By default Rasa uses `FormAction` for processing any\n[form logic](forms.mdx). You can override this default action with a custom action by\nadding a custom action with the form's name to the domain.\nOverriding the default action for forms should **only** be used during the process of\nmigrating from Rasa 1.0 to 2.0.",
            "action_unlikely_intent": {
              "Customization": "You can customize your assistant's behaviour to configure what should happen once `action_unlikely_intent`\nis triggered. For example, as a follow up you can trigger a hand-off to a human agent with a rule:\n\n```\n- rule: trigger human handoff with action_unlikely_intent\n  steps:\n    - action: action_unlikely_intent\n    - action: ask_human_handoff\n    - intent: affirm\n    - action: trigger_human_handoff\n```\n\nAlternatively, you can also override it's behaviour as a [`custom action`](./custom-actions.mdx) by\nadding `action_unlikely_intent` to the list of actions in the domain and implementing the custom behaviour:\n\n```\nclass ActionUnlikelyIntent(Action):\n\n    def name(self) -> Text:\n        return \"action_unlikely_intent\"\n\n    async def run(\n        self, dispatcher, tracker: Tracker, domain: Dict[Text, Any],\n    ) -> List[Dict[Text, Any]]:\n\n        # Implement custom logic here\n        return []\n```\n\n:::note\nSince `action_unlikely_intent` can be triggered at any conversation step during inference,\nall policies which are trained on only story data, for example - `TEDPolicy`, `UnexpecTEDIntentPolicy`,\n`MemoizationPolicy` ignore its presence in the tracker when making a prediction. However, `RulePolicy`\ntakes its presence into account so that [conversation behaviour is customizable](./default-actions.mdx#customization-1).\n\n:::\n\n:::note\n`action_unlikely_intent` cannot be included in the training stories. It can **only** be added to rules.\n\n:::"
            },
            "action_extract_slots": "This action runs after each user turn, before the next assistant action prediction and execution.\n`action_extract_slots` loops through the [slot mappings](./domain.mdx#slot-mappings) of each domain slot in order to set or update\nslots throughout the conversation with information extracted from the latest user message.\n\nIf `action_extract_slots` finds a [custom slot mapping](./domain.mdx#custom-slot-mappings), it will check first if a custom action was defined in the\nmapping via the `action` key and then run it.\n\nAfter applying all the slot mappings, `action_extract_slots` will run the custom validation action\n`action_validate_slot_mappings` if it is present in the domain actions. Otherwise it will immediately return the already\nextracted slots.\n\nNote that custom actions used by slot mappings or slot mapping validation should only return events of type `SlotSet` or\n`BotUttered`. Events of any other type are not permitted and will be ignored when updating the tracker.\n\nThe default action `action_extract_slots` replaces the slot extraction previously executed by `FormAction`.\nIf you wish to set a slot based on information extracted from intents that trigger forms, you must explicitly specify a\nmapping that does not contain the `conditions` key. A slot mapping with `conditions` applies only once the specified form is active.\n`action_extract_slots` runs directly after each user message, and thus before the activation of the form.\nTherefore a mapping that should apply to user messages that trigger a form must not specify `conditions`, or the form\nwill re-ask for the slot once it is activated.\n\n:::note\nIf `action_default_fallback` is the next action predicted and executed by the assistant, this will result in a\n`UserUtteranceReverted` event which will unset the slots previously filled in the last user turn.\n\n:::"
          },
          "metadata": {
            "id": "default-actions",
            "sidebar_label": "Default Actions",
            "title": "Default Actions",
            "abstract": "Default actions are actions that are built into the dialogue manager by default. Most of these are automatically predicted based on certain conversation situations. You may want to customize these to personalize your assistant."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 12]"
        },
        {
          "title": "Domain",
          "description": null,
          "content": {
            "Multiple Domain Files": "The domain can be defined as a single YAML file or split across multiple files in a directory.\nWhen split across multiple files, the domain contents will be read and automatically merged together.\n\nUsing the [command line interface](./command-line-interface.mdx#rasa-train),\nyou can train a model with split domain files by running:\n\n```\nrasa train --domain path_to_domain_directory\n```",
            "Intents": {
              "Ignoring Entities for Certain Intents": "To ignore all entities for certain intents, you can\nadd the `use_entities: []` parameter to the intent in your domain\nfile like this:\n\n```\nintents:\n  - greet:\n      use_entities: []\n```\n\nTo ignore some entities or explicitly take only certain entities\ninto account you can use this syntax:\n\n```\nintents:\n- greet:\n    use_entities:\n      - name\n      - first_name\n- farewell:\n    ignore_entities:\n      - location\n      - age\n      - last_name\n```\n\nYou can only `use_entities` _or_ `ignore_entities` for any single intent.\n\nExcluded entities for those intents will be unfeaturized and therefore\nwill not impact the next action predictions. This is useful when you have\nan intent where you don't care about the entities being picked up.\n\nIf you list your intents without a `use_entities` or `ignore_entities` \nparameter, the entities will be featurized as normal.\n\nIt is also possible to ignore an entity for all intents\nby setting the `influence_conversation` flag to `false` for the entity itself.\nSee [the entities section](#entities) for details.\n\nExcluded entities for intents will be unfeaturized and therefore\nwill not impact the next action predictions. This is useful when you have\nan intent where you don't care about the entities being picked up.\n\nIf you list your intents without this parameter, and without setting \n`influence_conversation` to `false` for any entities, all entities will be\nfeaturized as normal.\n\n:::note\nIf you want these entities not to influence action prediction via slots either,\nset the [`influence_conversation: false`](./domain.mdx#slots-and-conversation-behavior)\nparameter for slots with the same name.\n\n:::"
            },
            "Entities": ":::info New in 3.1\n\nAs of 3.1, you can use the `influence_conversation` flag under entities.\nThe flag can be set to `false` to declare that an entity should not\nbe featurized for any intents. It is a shorthand syntax for adding an entity to\nthe `ignore_entities` list of every intent in the domain. The flag is optional \nand default behaviour remains unchanged.\n\n:::\n\nThe `entities` section lists all entities that can be\nextracted by any [entity extractor](./components.mdx) in your\nNLU pipeline.\n\nFor example:\n\n```\nentities:\n   - PERSON           # entity extracted by SpacyEntityExtractor\n   - time             # entity extracted by DucklingEntityExtractor\n   - membership_type  # custom entity extracted by DIETClassifier\n   - priority         # custom entity extracted by DIETClassifier\n```\n\nWhen using multiple domain files, entities can be specified in any domain file, \nand can be used or ignored by any intent in any domain file.\n\nIf you are using the feature [Entity Roles and Groups](./nlu-training-data.mdx#entities-roles-and-groups) you also\nneed to list the roles and groups of an entity in this section.\n\nFor example:\n\n```\nentities:\n   - city:            # custom entity extracted by DIETClassifier\n       roles:\n       - from\n       - to\n   - topping:         # custom entity extracted by DIETClassifier\n       groups:\n       - 1\n       - 2\n   - size:            # custom entity extracted by DIETClassifier\n       groups:\n       - 1\n       - 2\n```\n\nBy default, entities influence action prediction. To prevent extracted entities from\ninfluencing the conversation for specific intents you can [ignore entities for certain intents](#ignoring-entities-for-certain-intents).\nTo ignore an entity for all intents, without having to list it under the `ignore_entities` flag of each intent, \nyou can set the flag `influence_conversation` to `false` under the entity:\n\n```\nentities:\n- location:\n    influence_conversation: false\n```\n\nThis syntax has the same effect as adding the entity to the `ignore_entities` \nlist for every intent in the domain.\n\nExplicitly setting `influence_conversation: true` does not change any behaviour. This is the default setting.",
            "Slots": {
              "Slots and Conversation Behavior": "You can specify whether or not a slot influences the conversation with the\n`influence_conversation` property. \n\nIf you want to store information in a slot without it influencing the conversation,\nset `influence_conversation: false` when defining your slot. \n\nThe following example defines a slot `age` which will store information about the\nuser's age, but which will *not* influence the flow of the conversation. This means\nthat the assistant will ignore the value of the slot each time it predicts the next action.\n\n```\nslots:\n  age:\n    type: text\n    # this slot will not influence the predictions\n    # of the dialogue policies\n    influence_conversation: false\n```\n\nWhen defining a slot, if you leave out `influence_conversation` or set it to `true`,\nthat slot will influence the next action prediction, unless it has slot type `any`.\nThe way the slot influences the conversation\nwill depend on its [slot type](./domain.mdx#slot-types).\n\nThe following example defines a slot `home_city` that influences the conversation. \nA [`text` slot](domain.mdx#text-slot) will\ninfluence the assistant's behavior depending on whether the slot has a value.\nThe specific value of a `text` slot (e.g. *Bangalore* or *New York* or *Hong Kong*)\ndoesn't make any difference. \n\n```\nslots:\n  # this slot will influence the conversation depending on\n  # whether the slot is set or not\n  home_city:\n    type: text\n    influence_conversation: true\n```\n\nAs an example, consider the two inputs \"What is the weather like?\" and \"What is the\nweather like in Bangalore?\" The conversation should diverge based on whether\nthe `home_city` slot was set automatically by the NLU. If the slot is already set, the bot\ncan predict the `action_forecast` action. If the slot is not set, it needs to get the `home_city`\ninformation before it is able to predict the weather.",
              "Slot Types": {
                "Text Slot": [
                  "**Type**",
                  "`text`",
                  "**Use For**",
                  "Storing text values.",
                  "**Example**",
                  "```\nslots:\n   cuisine:\n      type: text\n      mappings:\n      - type: from_entity\n        entity: cuisine\n```",
                  "**Description**",
                  "If `influence_conversation` is set to `true`, the assistant's behavior will change\ndepending on whether the slot is set or not. Different texts do not influence the\nconversation any further. This means the following two stories are equal:",
                  "```\nstories:\n- story: French cuisine\n  steps:\n  - intent: inform\n  - slot_was_set:\n    - cuisine: french\n\n- story: Vietnamese cuisine\n  steps:\n  - intent: inform\n  - slot_was_set:\n    - cuisine: vietnamese\n```"
                ],
                "Boolean Slot": [
                  "**Type**",
                  "`bool`",
                  "**Use For**",
                  "Storing `true` or `false` values.",
                  "**Example**",
                  "```\nslots:\n   is_authenticated:\n      type: bool\n      mappings:\n      - type: custom\n```",
                  "**Description**",
                  "If `influence_conversation` is set to `true`, the assistant's behavior will change\ndepending on whether the slot is empty, set to `true` or set to `false`. Note that an\nempty `bool` slot influences the conversation differently than if the slot was set to\n`false`."
                ],
                "Categorical Slot": [
                  "**Type**",
                  "`categorical`",
                  "**Use For**",
                  "Storing slots which can take one of N values.",
                  "**Example**",
                  "```\nslots:\n  risk_level:\n    type: categorical\n    values:\n      - low\n      - medium\n      - high\n    mappings:\n    - type: custom\n```",
                  "**Description**",
                  "If `influence_conversation` is set to `true`, the assistant's behavior will change\ndepending on the concrete value of the slot. This means the assistant's behavior is\ndifferent depending on whether the slot in the above example has the value `low`,\n`medium`, or `high`.",
                  "A default value `__other__` is automatically added to the user-defined\nvalues. All values encountered which are not explicitly defined in the slot's `values`\nare mapped to `__other__`.\n`__other__` should not be used as a user-defined value; if it is, it\nwill still behave as the default to which all unseen values are mapped."
                ],
                "Float Slot": [
                  "**Type**",
                  "`float`",
                  "**Use For**",
                  "Storing real numbers.",
                  "**Example**",
                  "```\nslots:\n  temperature:\n    type: float\n    min_value: -100.0\n    max_value:  100.0\n    mappings:\n    - type: custom\n```",
                  "**Defaults**",
                  "`max_value=1.0`, `min_value=0.0`",
                  "**Description**",
                  "If `influence_conversation` is set to `true`, the assistant's behavior will change\ndepending on the value of the slot. If the value is between `min_value` and\n`max_value`, the specific value of the number is used.\nAll values below `min_value` will be treated as `min_value`, and all values above\n`max_value` will be treated as `max_value`. Hence, if `max_value` is set to `1`,\nthere is no difference between the slot values `2` and `3.5`."
                ],
                "List Slot": [
                  "**Type**",
                  "`list`",
                  "**Use For**",
                  "Storing lists of values.",
                  "**Example**",
                  "```\nslots:\n  shopping_items:\n    type: list\n    mappings:\n    - type: from_entity\n      entity: shopping_item\n```",
                  "**Description**",
                  "If `influence_conversation` is set to `true`, the assistant's behavior will change\ndepending on whether the list is empty or not. The length of the list stored in\nthe slot does not influence the dialogue. It only matters whether list length is zero or non-zero."
                ],
                "Any Slot": [
                  "**Type**",
                  "`any`",
                  "**Use For**",
                  "Storing arbitrary values (they can be of any type, such as dictionaries or lists).",
                  "**Example**",
                  "```\nslots:\n  shopping_items:\n    type: any\n    mappings:\n    - type: custom\n```",
                  "**Description**",
                  "Slots of type `any` are always ignored during conversations. The property\n`influence_conversation` cannot be set to `true` for this slot type. If you want to\nstore a custom data structure which should influence the conversation, use a\n[custom slot type](domain.mdx#custom-slot-types)."
                ],
                "Custom Slot Types": "Maybe your restaurant booking system can only handle bookings\nfor up to 6 people. In this case you want the *value* of the\nslot to influence the next selected action (and not just whether\nit's been specified). You can do this by defining a custom slot class.\n\nThe code below defines a custom slot class called `NumberOfPeopleSlot`. \nThe featurization defines how the value of this slot gets converted to a vector\nso Rasa machine learning model can deal with it.\nThe `NumberOfPeopleSlot` has three possible “values”, which can be represented with\na vector of length `2`.\n\n|        |                |\n|--------|----------------|\n|`(0,0)` |not yet set     |\n|`(1,0)` |between 1 and 6 |\n|`(0,1)` |more than 6     |\n\n```\nfrom rasa.shared.core.slots import Slot\n\nclass NumberOfPeopleSlot(Slot):\n\n    def feature_dimensionality(self):\n        return 2\n\n    def as_feature(self):\n        r = [0.0] * self.feature_dimensionality()\n        if self.value:\n            if self.value <= 6:\n                r[0] = 1.0\n            else:\n                r[1] = 1.0\n        return r\n```\n\nYou can implement a custom slot class as an independent python module, \nseparate from custom action code. Save the code for your custom slot in a directory \nalongside an empty file called \"\\_\\_init\\_\\_.py\" so that it will be recognized as a python module.\nYou can then refer to the custom slot class by it's module path.\n\nFor example, say you have saved the code above in \"addons/my_custom_slots.py\", a directory relative to your bot project:\n\n```\n└── rasa_bot\n    ├── addons\n    │   ├── __init__.py\n    │   └── my_custom_slots.py\n    ├── config.yml\n    ├── credentials.yml\n    ├── data\n    ├── domain.yml\n    ├── endpoints.yml\n```\n\nYour custom slot type's module path\nis then `addons.my_custom_slots.NumberOfPeopleSlot`.\nUse the module path to refer to the custom slot type in your domain file:\n\n```\nslots:\n  people:\n    type: addons.my_custom_slots.NumberOfPeopleSlot\n    influence_conversation: true\n    mappings:\n    - type: custom\n```\n\nNow that your custom slot class can be used by Rasa, add training stories that diverge based on the value of the `people` slot.\nYou could write one story for the case where `people` has a value between 1 and 6, and one for a value greater than six. You can choose any value within these ranges to put in your stories, since they are all featurized the same way (see the featurization table above).\n\n```\nstories:\n- story: collecting table info\n  steps:\n  # ... other story steps\n  - intent: inform\n    entities:\n    - people: 3\n  - slot_was_set:\n    - people: 3\n  - action: action_book_table\n\n- story: too many people at the table\n  steps:\n  # ... other story steps\n  - intent: inform\n    entities:\n    - people: 9\n  - slot_was_set:\n    - people: 9\n  - action: action_explain_table_limit\n```"
              },
              "Slot Mappings": {
                "from_entity": {
                  "Unique `from_entity` mapping matching": "There is an intentional limitation on applying `from_entity` slot mappings in the context of a form.\nWhen a form is active, a `from_entity` slot mapping will be applied only if one or more of the following conditions are met:\n\n['The slot with the `from_entity` mapping has just been requested by the form', \"Only one of the active form's `required_slots` has that specific `from_entity` mapping,\\nincluding all the attributes of the extracted entity (i.e, entity name, role, group). This is known as a **unique entity mapping** for the form. \\nThe extracted entity will be ignored if the mapping is not unique within the list of `required_slots`.\"]\n\nThis limitation exists to prevent a form from filling multiple required slots with the same extracted entity value.\n\nFor example, in the example below, an entity `date` uniquely sets the slot `arrival_date`,\nan entity `city` with a role `from` uniquely sets the slot `departure_city` and\nan entity `city` with a role `to` uniquely sets the slot `arrival_city`,\ntherefore they can be used to fit corresponding slots\neven if these slots were not requested.\nHowever, entity `city` without a role can fill both `departure_city` and `arrival_city`\nslots, depending which one is requested, so if an entity `city` is extracted when\nslot `arrival_date` is requested, it'll be ignored by the form.\n\n```\nslots:\n  departure_city:\n    type: text\n    mappings:\n    - type: from_entity\n      entity: city\n      role: from\n    - type: from_entity\n      entity: city\n  arrival_city:\n    type: text\n    mappings:\n    - type: from_entity\n      entity: city\n      role: to\n    - type: from_entity\n      entity: city\n  arrival_date:\n    type: any\n    mappings:\n    - type: from_entity\n      entity: date\nforms:\n  your_form:\n    required_slots:\n    - departure_city\n    - arrival_city\n    - arrival_date\n```\n\nNote that the unique `from_entity` mapping constraint will **not** prevent filling slots which are not in the active form's `required_slots`;\nthose mappings will apply as usual, regardless of the uniqueness of the mapping. To limit applicability of a slot mapping to\na specific form, see [Mapping Conditions](./domain.mdx#mapping-conditions)."
                },
                "from_text": "The `from_text` mapping will use the text of the last user utterance to fill the slot\n`slot_name`. If `intent_name` is `None`, the slot will be filled regardless of intent name.\nOtherwise, the slot will only be filled if the user's intent is `intent_name`.\n\nThe slot mapping will not apply if the intent of the message is `excluded_intent`.\n\n```\nslots:\n  slot_name:\n    type: text\n    mappings:\n    - type: from_text\n      intent: intent_name\n      not_intent: excluded_intent\n```\n\n:::note\nTo maintain the 2.x form behavior when using `from_text` slot mappings, you must use [mapping conditions](./domain.mdx#mapping-conditions),\nwhere both `active_loop` and `requested_slot` keys are defined.\n:::",
                "from_intent": "The `from_intent` mapping will fill slot `slot_name` with value `my_value` if\nuser intent is `intent_name`. If you choose not to specify the parameter `intent`,\nthe slot mapping will apply regardless of the intent of the message as long as\nthe intent is not listed under `not_intent` parameter.\n\nThe following parameter is required:\n\n['`value`: the value that fills the slot `slot_name`']\n\nThe following parameters are optional and can be used to further specify when the mapping applies:\n\n['`intent`: Only applies the mapping when this intent is predicted.', '`not_intent`: Does not apply the mapping when this intent is predicted']\n\nNote that if you choose not to define the parameter `intent`, the slot mapping will apply regardless of the intent\nof the message as long as the intent is not listed under the `not_intent` parameter.\n\n```\nslots:\n  slot_name:\n    type: any\n    mappings:\n    - type: from_intent\n      value: my_value\n      intent: intent_name\n      not_intent: excluded_intent\n```",
                "from_trigger_intent": "The `from_trigger_intent` mapping will fill slot `slot_name` with value `my_value`\nif a form is activated by a user message with intent `intent_name`.\nThe slot mapping will not apply if the intent of the message is\n`excluded_intent`.\n\n```\nslots:\n  slot_name:\n    type: any\n    mappings:\n    - type: from_trigger_intent\n      value: my_value\n      intent: intent_name\n      not_intent: excluded_intent\n```"
              },
              "Mapping Conditions": "To apply a slot mapping only within the context of a form, specify\nthe name of the form in the `conditions` key of a slot mapping. Conditions list the form name(s)\nfor which the mapping is applicable in the `active_loop` key.\n\n:::info New in 3.6\nSlot mappings can now specify `null` as the value of `active_loop` to indicate that the slot should only be filled when\nno form is active. Note that `requested_slot` cannot be used in conjunction with `active_loop: null`.\n:::\n\nConditions can also include the name of the `requested_slot`. If `requested_slot` is not mentioned,\nthen the slot will be set if relevant information is extracted, regardless of which slot is being\nrequested by the form.\n\n```\nslots:\n  slot_name:\n    type: text\n    mappings:\n    - type: from_text\n      intent: intent_name\n      conditions:\n      - active_loop: your_form\n        requested_slot: slot_name\n      - active_loop: another_form\n```\n\n:::note\nIf `conditions` are not included in a slot mapping, the slot mapping will be applicable regardless of whether\nany form is active. As long as a slot is listed in a form's `required_slots`, the form will prompt for the slot\nif it is empty when the form is activated.\n:::",
              "Custom Slot Mappings": "You can define custom slot mappings using [slot validation actions](./slot-validation-actions.mdx) when none of the\npredefined mappings fit your use case. You must define this slot mapping to be of type `custom`, for example:\n\n```\nslots:\n  day_of_week:\n    type: text\n    mappings:\n    - type: custom\n      action: action_calculate_day_of_week\n```\n\nYou can also use the `custom` slot mapping to list slots that will be filled by arbitrary custom actions in the course\nof a conversation, by listing the type and no specific action. For example:\n\n```\nslots:\n  handoff_completed:\n    type: boolean\n    mappings:\n    - type: custom\n```\n\nThis slot will not be updated on every user turn, but only once a custom action that returns a `SlotSet` event for it is predicted.",
              "Initial slot values": "You can provide an initial value for a slot in your domain file:\n\n```\nslots:\n  num_fallbacks:\n    type: float\n    initial_value: 0\n    mappings:\n    - type: custom\n```"
            },
            "Responses": "Responses are actions that send a message to a user without running any custom code or\nreturning events. These responses can be defined directly in the domain file under the `responses` key\nand can include rich content such as buttons and attachments. For more information on responses and how to define them,\nsee [Responses](./responses.mdx).",
            "Forms": "Forms are a special type of action meant to help your assistant collect information from a user.\nDefine forms under the `forms` key in your domain file.\nFor more information on form and how to define them, see [Forms](./forms.mdx).",
            "Actions": {
              "Select which actions should receive domain": ":::info New in 3.4.3\nYou can control if an action should receive a domain or not.\n:::\n\nTo do this you must first enable selective domain in you endpoint configuration for\n`action_endpoint` in `endpoints.yml`.\n\n```\n# endpoints.yml\naction_endpoint:\n url: \"http://localhost:5055/webhook\" # URL to your action server\n enable_selective_domain: true\n```\n\n**After selective domain for custom actions is enabled, domain will be sent only to\nthose custom actions which have specifically stated that they need it.**\nCustom actions inheriting from rasa-sdk [`FormValidationAction`](./action-server/validation-action.mdx#formvalidationaction-class)\nparent class are an exception to this rule as they will always have the domain sent to them.\nTo specify if an action needs the domain add `{send_domain: true}` to custom action in the list\nof actions in `domain.yml`:\n\n```\n# domain.yml\nactions:\n  - action_hello_world: {send_domain: True} # will receive domain\n  - action_calculate_mass_of_sun # will not receive domain\n  - validate_my_form # will receive domain\n```"
            },
            "Session configuration": "A conversation session represents the dialogue between the assistant and the user.\nConversation sessions can begin in three ways:\n\n['the user begins the conversation with the assistant,', 'the user sends their first message after a configurable period of inactivity, or', 'a manual session start is triggered with the `/session_start` intent message.']\n\nYou can define the period of inactivity after which a new conversation\nsession is triggered in the domain under the `session_config` key.\n\nAvailable parameters are:\n\n['`session_expiration_time` defines the time of inactivity in minutes after which a\\nnew session will begin.', '`carry_over_slots_to_new_session` determines whether\\nexisting set slots should be carried over to new sessions.']\n\nThe default session configuration looks as follows:\n\n```\nsession_config:\n  session_expiration_time: 60  # value in minutes, 0 means infinitely long\n  carry_over_slots_to_new_session: true  # set to false to forget slots between sessions\n```\n\nThis means that if a user sends their first message after 60 minutes of inactivity, a\nnew conversation session is triggered, and that any existing slots are carried over\ninto the new session. Setting the value of `session_expiration_time` to `0` means\nthat sessions will not end (note that the `action_session_start` action will still\nbe triggered at the very beginning of conversations).\n\n:::note\nA session start triggers the default action `action_session_start`. Its default\nimplementation moves all existing slots into the new session. Note that all\nconversations begin with an `action_session_start`. Overriding this action could\nfor instance be used to initialize the tracker with slots from an external API\ncall, or to start the conversation with a bot message. The docs on\n[Customizing the session start action](./default-actions.mdx#customization) shows you how to do that.\n\n:::",
            "Config": "The `config` key in the domain file maintains the `store_entities_as_slots` parameter.\nThis parameter is used only in the context of reading stories and turning them into trackers. If the parameter is set\nto `True`, this will result in slots being implicitly set from entities if applicable entities are present in the story.\nWhen an entity matches the `from_entity` slot mapping, `store_entities_as_slots` defines whether the entity value should\nbe placed in that slot. Therefore, this parameter skips adding an explicit `slot_was_set` step manually in the story.\nBy default, this behaviour is switched on.\n\nYou can turn off this functionality by setting the `store_entities_as_slots` parameter to `false`:\n\n```\nconfig:\n  store_entities_as_slots: false\n```\n\n:::note looking for config.yml?\nIf you're looking for information on the `config.yml` file, check out the docs on\n[Model Configuration](./model-configuration.mdx).\n:::"
          },
          "metadata": {
            "id": "domain",
            "sidebar_label": "Domain",
            "title": "Domain",
            "abstract": "The domain defines the universe in which your assistant operates. It specifies the intents, entities, slots, responses, forms, and actions your bot should know about. It also defines a configuration for conversation sessions."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 13]"
        },
        {
          "title": "Event Brokers",
          "description": "Find out how open source chatbot framework Rasa allows you to stream events to a message broker.",
          "content": {
            "Format": "All events are streamed to the broker as serialized dictionaries every time\nthe tracker updates its state. An example event emitted from the `default`\ntracker looks like this:\n\n```\n{\n    \"sender_id\": \"default\",\n    \"timestamp\": 1528402837.617099,\n    \"event\": \"bot\",\n    \"text\": \"what your bot said\",\n    \"data\": \"some data about e.g. attachments\"\n    \"metadata\" {\n          \"a key\": \"a value\",\n     }\n}\n```\n\nThe `event` field takes the event's `type_name` (for more on event\ntypes, check out the [events](./action-server/events.mdx) docs).",
            "Pika Event Broker": {
              "Adding a Pika Event Broker Using the Endpoint Configuration": "You can instruct Rasa to stream all events to your Pika event broker by adding an `event_broker` section to your\n`endpoints.yml`:\n\n```\n```\n\nA comprehensive list of all arguments that can be customized in the `endpoints.yml` file can be found in the [reference documentation](https://rasa.com/docs/rasa/reference/rasa/core/brokers/pika/#__init__).\nRasa will automatically start streaming events when you restart the Rasa server.",
              "Adding SSL options to the Pika Event Broker": "You can create RabbitMQ SSL options by setting the following required environment variables:\n\n['`RABBITMQ_SSL_CLIENT_CERTIFICATE`: path to the SSL client certificate', '`RABBITMQ_SSL_CLIENT_KEY`: path to the SSL client key']\n\nPlease note that specifying 'RABBITMQ_SSL_CA_FILE' via environment variables is no longer supported, as well as\nspecifying `RABBITMQ_SSL_KEY_PASSWORD` environment variable - please use a key file that is not encrypted instead.",
              "Adding a Pika Event Broker in Python": "Here is how you add it using Python code:\n\n```\nimport asyncio\n\nfrom rasa.core.brokers.pika import PikaEventBroker\nfrom rasa.core.tracker_store import InMemoryTrackerStore\n\npika_broker = PikaEventBroker('localhost',\n                              'username',\n                              'password',\n                              queues=['rasa_events'],\n                              event_loop=event_loop\n                              )\nasyncio.run(pika_broker.connect())\n\ntracker_store = InMemoryTrackerStore(domain=domain, event_broker=pika_broker)\n```",
              "Implementing a Pika Event Consumer": "You need to have a RabbitMQ server running, as well as another application\nthat consumes the events. This consumer to needs to implement Pika's\n`start_consuming()` method with a `callback` action. Here's a simple\nexample:\n\n```\nimport json\nimport pika\n\n\ndef _callback(ch, method, properties, body):\n        # Do something useful with your incoming message body here, e.g.\n        # saving it to a database\n        print(\"Received event {}\".format(json.loads(body)))\n\nif __name__ == \"__main__\":\n\n    # RabbitMQ credentials with username and password\n    credentials = pika.PlainCredentials(\"username\", \"password\")\n\n    # Pika connection to the RabbitMQ host - typically 'rabbit' in a\n    # docker environment, or 'localhost' in a local environment\n    connection = pika.BlockingConnection(\n        pika.ConnectionParameters(\"rabbit\", credentials=credentials)\n    )\n\n    # start consumption of channel\n    channel = connection.channel()\n    channel.basic_consume(queue=\"rasa_events\", on_message_callback=_callback, auto_ack=True)\n    channel.start_consuming()\n```"
            },
            "Kafka Event Broker": {
              "Partition Key": "Rasa's Kafka producer can optionally be configured to partition messages by conversation ID.\nThis can be configured by setting `partition_by_sender` in the `endpoints.yml` file to True.\nBy default, this parameter is set to `False` and the producer will randomly assign a partition to each message.  \n\n```\nevent_broker:\n  type: kafka\n  partition_by_sender: True\n  security_protocol: PLAINTEXT\n  topic: topic\n  url: localhost\n  client_id: kafka-python-rasa\n```",
              "Authentication and Authorization": "Rasa's Kafka producer accepts the following types of security protocols: `SASL_PLAINTEXT`, `SSL`, `PLAINTEXT`\nand `SASL_SSL`.\n\nFor development environments, or if the brokers servers and clients are located\ninto the same machine, you can use simple authentication with `SASL_PLAINTEXT` or `PLAINTEXT`.\nBy using this protocol, the credentials and messages exchanged between the clients and servers\nwill be sent in plaintext. Thus, this is not the most secure approach, but since it's simple\nto configure, it is useful for simple cluster configurations.\n`SASL_PLAINTEXT` protocol requires the setup of the `username` and `password`\npreviously configured in the broker server.\n\nIf the clients or the brokers in the kafka cluster are located in different\nmachines, it's important to use the `SSL` or `SASL_SSL` protocol to ensure encryption of data\nand client authentication. After generating valid certificates for the brokers and the\nclients, the path to the certificate and key generated for the producer must\nbe provided as arguments, as well as the CA's root certificate.\n\nWhen using the `SASL_PLAINTEXT` and `SASL_SSL` protocols, the `sasl_mechanism` can be\noptionally configured and is set to `PLAIN` by default. Valid values for `sasl_mechanism`\nare: `PLAIN`, `GSSAPI`, `OAUTHBEARER`, `SCRAM-SHA-256`, and `SCRAM-SHA-512`.\n\nIf `GSSAPI` is used for the `sasl_mechanism`, you will need to additionally install\n[python-gssapi](https://pypi.org/project/python-gssapi/) and the necessary C library\nKerberos dependencies.\n\nIf the `ssl_check_hostname` parameter is enabled, the clients will verify\nif the broker's hostname matches the certificate. It's used on client's connections\nand inter-broker connections to prevent man-in-the-middle attacks.",
              "Adding a Kafka Event Broker Using the Endpoint Configuration": "You can instruct Rasa to stream all events to your Kafka event broker by adding an `event_broker` section to your\n`endpoints.yml`.\n\nUsing the `SASL_PLAINTEXT` protocol the endpoints file must have the following entries:\n\n```\n```\n\nUsing the `PLAINTEXT` protocol the endpoints file must have the following entries:\n\n```\n```\n\nIf using the `SSL` protocol, the endpoints file should look like:\n\n```\n```\n\nIf using the `SASL_SSL` protocol, the endpoints file should look like:\n\n```\n```"
            },
            "SQL Event Broker": {
              "Adding a SQL Event Broker Using the Endpoint Configuration": "To instruct Rasa to save all events to your SQL event broker, add an `event_broker` section to your\n`endpoints.yml`. For example, a valid SQLite configuration\ncould look like this:\n\n```\nevent_broker:\n  type: SQL\n  dialect: sqlite\n  db: events.db\n```\n\nPostgreSQL databases can be used as well:\n\n```\nevent_broker:\n  type: SQL\n  url: 127.0.0.1\n  port: 5432\n  dialect: postgresql\n  username: myuser\n  password: mypassword\n  db: mydatabase\n```\n\nWith this configuration applied, Rasa will create a table called `events` on the database,\nwhere all events will be added."
            },
            "FileEventBroker": "It is possible to use the `FileEventBroker` as an event broker. This implementation will log events to a file in json format.\nYou can provide a path key in the `endpoints.yml` file if you wish to override the default file name: `rasa_event.log`.",
            "Custom Event Broker": {
              "Configuration": "Put the module path to your custom event broker and the parameters you require in your `endpoints.yml`:\n\n```\nevent_broker:\n  type: path.to.your.module.Class\n  url: localhost\n  a_parameter: a value\n  another_parameter: another value\n```"
            }
          },
          "metadata": {
            "id": "event-brokers",
            "sidebar_label": "Event Brokers",
            "title": "Event Brokers",
            "description": "Find out how open source chatbot framework Rasa allows you to stream events to a message broker."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 14]"
        },
        {
          "title": "Fallback and Human Handoff",
          "description": null,
          "content": {
            "Handling Out-of-scope Messages": {
              "1. Creating an Out-of-scope Intent": "You will need to define an `out_of_scope` intent in your NLU training data and add any known\nout-of-scope requests as training examples, for example:\n\n```\nnlu:\n- intent: out_of_scope\n  examples: |\n    - I want to order food\n    - What is 2 + 2?\n    - Who's the US President?\n```\n\nAs with every intent, you should source the majority of your examples\n[from real conversations](conversation-driven-development.mdx \"Learning from real conversations via conversation-driven development\").",
              "2. Defining the response message": "You'll need to define an out-of-scope response in the domain file.\nUsing the utterance `utter_out_of_scope` as the default response, that would look like:\n\n```\nresponses:\n  utter_out_of_scope:\n  - text: Sorry, I can't handle that request.\n```",
              "3. Creating an Out-of-Scope Rule": "Finally, you will need to write a rule for what should happen for in out-of-scope request:\n\n```\nrules:\n- rule: out-of-scope\n  steps:\n  - intent: out_of_scope\n  - action: utter_out_of_scope\n```",
              "Handling Specific Out-of-scope Messages": "If you observe your users asking for certain things that you'll\nwant to turn into a user goal in future, you can handle these as separate intents, to let\nthe user know you've understood their message, but don't have a solution quite yet. For example,\nif the user asks “I want to apply for a job at Rasa”, we can then reply with\n“I understand you're looking for a job, but I'm afraid I can't handle that skill yet.”\n\nSimilar to the `out_of_scope` intent example, you'll need to create a new intent with\ntraining examples, define the response message, and create a rule."
            },
            "Fallbacks": {
              "NLU Fallback": {
                "1. Updating the configuration": "To use the FallbackClassifier, add it to your NLU pipeline:\n\n```\npipeline:\n# other components\n- name: FallbackClassifier\n  threshold: 0.7\n```",
                "2. Defining the response message": "Define the message the bot should send when a message is classified with low confidence\nby adding a response:\n\n```\nresponses:\n  utter_please_rephrase:\n  - text: I'm sorry, I didn't quite understand that. Could you rephrase?\n```",
                "3. Creating an NLU fallback rule": "The following\n[Rule](./rules.mdx) will ask the user to rephrase when they send a message that is\nclassified with low confidence:\n\n```\nrules:\n- rule: Ask the user to rephrase whenever they send a message with low NLU confidence\n  steps:\n  - intent: nlu_fallback\n  - action: utter_please_rephrase\n```"
              },
              "Handling Low Action Confidence": {
                "1. Updating the configuration": "You will need to add the RulePolicy to your policies in config.yml.\nBy default, the rule policy comes with the settings below:\n\n```\npolicies:\n- name: RulePolicy\n  # Confidence threshold for the `core_fallback_action_name` to apply.\n  # The action will apply if no other action was predicted with\n  # a confidence >= core_fallback_threshold\n  core_fallback_threshold: 0.4\n  core_fallback_action_name: \"action_default_fallback\"\n  enable_fallback_prediction: True\n```",
                "2. Defining the default response message": "To define what your bot will say when action confidence is below the threshold,\ndefine a response `utter_default`:\n\n```\nresponses:\n  utter_default:\n  - text: Sorry I didn't get that. Can you rephrase?\n```\n\nWhen an action confidence is below the threshold, Rasa will run the action\n`action_default_fallback`. This will send the response `utter_default` and revert back to the\nstate of the conversation before the user message that caused the\nfallback, so it will not influence the prediction of future actions.",
                "3. Customizing the default action (optional)": "`action_default_fallback` is a default action in Rasa that sends the\n`utter_default` response to the user. You can create your own custom action to use as a\nfallback (see [Custom Actions](./actions.mdx#custom-actions) for more info on custom actions).\nThe following snippet is an implementation of a custom action which does the same as\n`action_default_fallback` but dispatches a different template\n`utter_fallback_template`:\n\n```\nfrom typing import Any, Text, Dict, List\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.events import UserUtteranceReverted\nfrom rasa_sdk.executor import CollectingDispatcher\n\nclass ActionDefaultFallback(Action):\n    \"\"\"Executes the fallback action and goes back to the previous state\n    of the dialogue\"\"\"\n\n    def name(self) -> Text:\n        return ACTION_DEFAULT_FALLBACK_NAME\n\n    async def run(\n        self,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: Dict[Text, Any],\n    ) -> List[Dict[Text, Any]]:\n        dispatcher.utter_message(template=\"my_custom_fallback_template\")\n\n        # Revert user message which led to fallback.\n        return [UserUtteranceReverted()]\n```"
              },
              "Two-Stage Fallback": {
                "1. Updating the configuration": "Add FallbackClassifier to your pipeline and the [RulePolicy](./policies.mdx#rule-policy)\nto your policy configuration:\n\n```\nrecipe: default.v1\npipeline:\n# other components\n- name: FallbackClassifier\n  threshold: 0.7\n\npolicies:\n# other policies\n- RulePolicy\n```",
                "2. Defining the fallback responses": "To define how your bot asks the user to rephrase their message,\ndefine  the response `utter_ask_rephrase`:\n\n```\nresponses:\n  utter_ask_rephrase:\n  - text: I'm sorry, I didn't quite understand that. Could you rephrase?\n```\n\nRasa provides default implementations for asking which intent the user\nmeant and for asking the user to rephrase. To customize the behavior of these actions,\nsee the documentation on [default actions](default-actions.mdx).",
                "3. Defining a Two-Stage Fallback rule": "Add the following [Rule](./rules.mdx) to your training data. This rule will make sure\nthat the Two-Stage-Fallback will be activated whenever a message is received with\nlow classification confidence:\n\n```\nrules:\n- rule: Implementation of the Two-Stage-Fallback\n  steps:\n  - intent: nlu_fallback\n  - action: action_two_stage_fallback\n  - active_loop: action_two_stage_fallback\n```"
              },
              "4. Defining an ultimate fallback action": "To define the bot's response when the user denies the rephrased intent, define the response `utter_default`:\n\n```\nresponses:\n  utter_default:\n  - text: I'm sorry, I can't help you.\n```\n\nOr, you can customize `action_default_fallback` for more complex behavior by writing a [Custom Action](./actions.mdx#custom-actions). \nFor example, if you want the bot to call a human and stop interacting with the user:\n\n```\nfrom typing import Any, Dict, List, Text\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.events import UserUtteranceReverted\nfrom rasa_sdk.executor import CollectingDispatcher\n\nclass ActionDefaultFallback(Action):\n    def name(self) -> Text:\n        return \"action_default_fallback\"\n\n    def run(\n        self,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: Dict[Text, Any],\n    ) -> List[Dict[Text, Any]]:\n\n        # tell the user they are being passed to a customer service agent\n        dispatcher.utter_message(text=\"I am passing you to a human...\")\n        \n        # assume there's a function to call customer service\n        # pass the tracker so that the agent has a record of the conversation between the user\n        # and the bot for context\n        call_customer_service(tracker)\n     \n        # pause the tracker so that the bot stops responding to user input\n        return [ConversationPaused(), UserUtteranceReverted()]\n```\n\n:::caution Events Returned By A Custom Ultimate Fallback Action\nYou should include `UserUtteranceReverted()` as one of the events returned by your custom\n`action_default_fallback`. Not including this event will cause the tracker to include all events that happened\nduring the Two-Stage Fallback process which could interfere with subsequent action predictions from the bot's policy\npipeline. It is better to treat events that occurred during the Two-Stage Fallback process as if they did not happen\nso that your bot can apply its rules or memorized stories to correctly predict the next action.\n:::"
            },
            "Human Handoff": "As part of your fallback action, you may want the bot to hand over to a human agent\ne.g. as the final action in Two-Stage-Fallback, or when the user explicitly asks\nfor a human. A straightforward way to achieve human handoff is to configure your\n[messaging or voice channel](messaging-and-voice-channels.mdx) to switch\nwhich host it listens to based on a specific bot or user message.\n\nFor example, as the final action of Two-Stage-Fallback, the bot could ask the user,\n\"Would you like to be transferred to a human assistant?\" and if they say yes, the\nbot sends a message with a specific payload like\ne.g. \"handoff_to_human\" to the channel. When the channel sees this message, it stops listening\nto the Rasa server, and sends a message to the human channel with the transcript\nof the chat conversation up to that point.\n\nThe implementation for handing off to a human from the front end will depend on which\nchannel you're using. You can\nsee an example implementation using an adaption of the [chatroom](https://github.com/scalableminds/chatroom) channel\nin the [Financial Demo](https://github.com/RasaHQ/financial-demo) and\n[Helpdesk-Assistant](https://github.com/RasaHQ/helpdesk-assistant)\nstarterpacks.",
            "Summary": "To let your assistant gracefully handle failures, you should handle known\nout-of-scope messages and add a form of fallback behavior. If you want to add human\nhandoff, you can add it in addition or as a final step in your fallback\nset up.\nHere's a summary of changes you need to make for each method:\n\nFor out-of-scope intents:\n\n['[ ] Add training examples for each out-of-scope intent to your NLU data', '[ ] Define the out-of-scope response or action', '[ ] Define rules for each out-of-scope intent', '[ ] Add the RulePolicy to config.yml']\n\nFor single stage NLU fallback:\n\n['[ ] Add FallbackClassifier to your pipeline in config.yml', '[ ] Define the fallback response or action', '[ ] Define a rule for the `nlu_fallback` intent', '[ ] Add the RulePolicy to config.yml']\n\nFor handling low core confidence:\n\n['[ ] Configure the RulePolicy for core fallback in config.yml', '[ ] Optionally customize the fallback action you configure', '[ ] Define an `utter_default` response']\n\nFor Two-Stage Fallback:\n\n['[ ] Add FallbackClassifier to your pipeline in `config.yml`', '[ ] Define a rule for the `nlu_fallback` intent that triggers the `action_two_stage_fallback` action', '[ ] Define an out-of-scope intent in your domain', '[ ] Add RulePolicy to config.yml']\n\nFor handing off to a human:\n\n['[ ] Configure your front end to switch hosts', '[ ] Write a custom action (which could be your fallback action) to send the handoff payload', '[ ] Add a rule for triggering handoff (if not part of fallback)', '[ ] Add RulePolicy to config.yml']"
          },
          "metadata": {
            "id": "fallback-handoff",
            "sidebar_label": "Fallback and Human Handoff",
            "title": "Fallback and Human Handoff",
            "abstract": "This is a guide on how to handle various failures of your assistant."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 15]"
        },
        {
          "title": "Forms",
          "description": "Follow a rule-based process of information gathering using forms in open source bot framework Rasa.",
          "content": {
            "Usage": {
              "Defining a Form": "Define a form by adding it to the `forms` section in your [domain](./domain.mdx).\nThe name of the form is also the name of the action which you can use in\n[stories](./stories.mdx) or [rules](./rules.mdx) to handle form executions.\nYou will need to specify a list of slot names to the mandatory `required_slots` key.\n\nThe following example form `restaurant_form` will fill the slot\n`cuisine` and slot `num_people`.\n\n```\nentities:\n- cuisine\n- number\nslots:\n  cuisine:\n    type: text\n    mappings:\n    - type: from_entity\n      entity: cuisine\n  num_people:\n    type: any\n    mappings:\n    - type: from_entity\n      entity: number\nforms:\n  restaurant_form:\n    required_slots:\n        - cuisine\n        - num_people\n```\n\nYou can define a list of intents to ignore for the whole form under the \n`ignored_intents` key. Intents listed under `ignored_intents` will be added to the\n`not_intent` key of each slot mapping.\n\nFor example, if you do not want any of the required slots of a form to be filled when\nthe intent is `chitchat`, then you would need to define the following (after the form\nname and under the `ignored_intents` keyword):\n\n```\nentities:\n- cuisine\n- number\nslots:\n  cuisine:\n    type: text\n    mappings:\n    - type: from_entity\n      entity: cuisine\n  num_people:\n    type: any\n    mappings:\n    - type: from_entity\n      entity: number\nforms:\n  restaurant_form:\n    ignored_intents: \n    - chitchat\n    required_slots:\n        - cuisine\n        - num_people\n```\n\nOnce the form action gets called for the first time, the form gets activated and will\nprompt the user for the next required slot value. It does this by\nlooking for a [response](./responses.mdx) called\n`utter_ask_<form_name>_<slot_name>` or `utter_ask_<slot_name>` if the former isn't\nfound. Make sure to define these responses in your domain file for\neach required slot.",
              "Activating a Form": "To activate a form you need to add a [story](./stories.mdx) or [rule](./rules.mdx),\nwhich describes when the assistant should run the form. In the case a specific intent\ntriggering a form, you can for example use the following rule:\n\n```\nrules:\n- rule: Activate form\n  steps:\n  - intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant_form\n```\n\n:::note\nThe `active_loop: restaurant_form` step indicates that the form should be activated after\n`restaurant_form` was run.\n:::",
              "Deactivating a Form": "A form will automatically deactivate itself once all required slots are filled.\nYou can describe your assistant's behavior for the end of a form with a rule or a story.\nIf you don't add an applicable story or rule, the assistant will automatically listen\nfor the next user message after the form is finished.\nThe following example runs the utterances `utter_submit` and `utter_slots_values` as soon as the form\n`your_form` filled all required slots.\n\n```\nrules:\n- rule: Submit form\n  condition:\n  # Condition that form is active.\n  - active_loop: restaurant_form\n  steps:\n  # Form is deactivated\n  - action: restaurant_form\n  - active_loop: null\n  - slot_was_set:\n    - requested_slot: null\n  # The actions we want to run when the form is submitted.\n  - action: utter_submit\n  - action: utter_slots_values\n```\n\nUsers might want to break out of a form early. Please see\n[Writing Stories / Rules for Unhappy Form Paths](./forms.mdx#writing-stories--rules-for-unhappy-form-paths) on how to\nwrite stories or rules for this case.",
              "Slot Mappings": ":::caution Changed in 3.0\nAs of 3.0, [slot mappings](./domain.mdx#slot-mappings) are defined in the `slots` section of the domain.\nThis change allows the same slot mapping to be reused across multiple forms, removing any unnecessary duplication.\nPlease follow the [migration guide](./migration-guide.mdx#slot-mappings) to update your assistant.\n\nNote specifically the role of [Mapping Conditions](./domain.mdx#mapping-conditions) and the\n[unique entity mapping](./domain.mdx#unique-from_entity-mapping-matching) constraint.\n:::",
              "Writing Stories / Rules for Unhappy Form Paths": "Your users will not always respond with the information you ask of them.\nTypically, users will ask questions, make chitchat, change their mind, or otherwise\nstray from the happy path. \n\nWhile a form is active, if a user's input does not fill the requested slot, the execution of \nthe form action will be rejected i.e. the form will automatically raise an `ActionExecutionRejection`. \nThese are the specific scenarios in which a form will raise an `ActionExecutionRejection`:\n\n[\"a slot was requested, but the user didn't fill the slot with their last message and\\nyou didn't define a custom action for\\n[validating slots](forms.mdx#validating-form-input) or\\n[extracting slots](forms.mdx#custom-slot-mappings).\", \"a slot was requested, but your custom action for\\n[validating slots](forms.mdx#validating-form-input) or\\n[extracting slots](forms.mdx#custom-slot-mappings) didn't return any `SlotSet` events.\"]\n\nTo intentionally reject the form execution, you can also return an `ActionExecutionRejected` event as part of your\ncustom validations or slot mappings.\n\nTo handle situations that might cause a form's execution to be rejected, you can write rules\nor stories that include the expected interruptions. For example, if you expect your users to chitchat with your bot,\nyou could add a rule to handle this:\n\n```\nrules:\n- rule: Example of an unhappy path\n  condition:\n  # Condition that form is active.\n  - active_loop: restaurant_form\n  steps:\n  # This unhappy path handles the case of an intent `chitchat`.\n  - intent: chitchat\n  - action: utter_chitchat\n  # Return to form after handling the `chitchat` intent\n  - action: restaurant_form\n  - active_loop: restaurant_form\n```\n\nIn some situations, users may change their mind in the middle of the form action\nand decide not to go forward with their initial request. In cases like this, the\nassistant should stop asking for the requested slots.\n\nYou can handle such situations\ngracefully using a default action `action_deactivate_loop` which will deactivate\nthe form and reset the requested slot. An example story of such conversation could\nlook as follows:\n\n```\nstories:\n- story: User interrupts the form and doesn't want to continue\n  steps:\n  - intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant_form\n  - intent: stop\n  - action: utter_ask_continue\n  - intent: stop\n  - action: action_deactivate_loop\n  - active_loop: null\n```\n\nIt is **strongly** recommended that you build these rules or stories using\n[interactive learning](./writing-stories.mdx#using-interactive-learning).\nIf you write these rules / stories by hand you will likely miss important\nthings."
            },
            "Advanced Usage": {
              "Validating Form Input": "After extracting a slot value from user input, you can validate the extracted slots.\nBy default Rasa only validates if any slot was filled after requesting\na slot.\n\nYou can implement a [Custom Action](./actions.mdx#custom-actions) `validate_<form_name>`\nto validate any extracted slots. Make sure to add this action to the `actions`\nsection of your domain:\n\n```\nactions:\n- validate_restaurant_form\n```\n\nWhen the form is executed it will run your custom action after every user turn to validate the latest filled slots.\n\nThis custom action can extend `FormValidationAction` class to simplify\nthe process of validating extracted slots. In this case, you need to write functions\nnamed `validate_<slot_name>` for every extracted slot.\n\nThe following example shows the implementation of a custom action\nwhich validates that the slot named `cuisine` is valid.\n\n```\nfrom typing import Text, List, Any, Dict\n\nfrom rasa_sdk import Tracker, FormValidationAction\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom rasa_sdk.types import DomainDict\n\n\nclass ValidateRestaurantForm(FormValidationAction):\n    def name(self) -> Text:\n        return \"validate_restaurant_form\"\n\n    @staticmethod\n    def cuisine_db() -> List[Text]:\n        \"\"\"Database of supported cuisines\"\"\"\n\n        return [\"caribbean\", \"chinese\", \"french\"]\n\n    def validate_cuisine(\n        self,\n        slot_value: Any,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: DomainDict,\n    ) -> Dict[Text, Any]:\n        \"\"\"Validate cuisine value.\"\"\"\n\n        if slot_value.lower() in self.cuisine_db():\n            # validation succeeded, set the value of the \"cuisine\" slot to value\n            return {\"cuisine\": slot_value}\n        else:\n            # validation failed, set this slot to None so that the\n            # user will be asked for the slot again\n            return {\"cuisine\": None}\n```\n\nYou can also extend the `Action` class and retrieve extracted slots with `tracker.slots_to_validate`\nto fully customize the validation process.",
              "Custom Slot Mappings": ":::caution Changed in 3.0\nThe `slots_mapped_in_domain` argument provided to the `required_slots` method of `FormValidationAction`\nhas been replaced by the `domain_slots` argument, please update your custom actions to the new argument name.\n:::\n\nIf none of the predefined [Slot Mappings](./domain.mdx#slot-mappings) fit your use\ncase, you can use the\n[Custom Action](./actions.mdx#custom-actions) `validate_<form_name>` to write your own\nextraction code. Rasa will trigger this action when the form is run.\n\nIf you're using the Rasa SDK we recommend you to extend the provided\n`FormValidationAction`. When using the `FormValidationAction`, three steps are required\nto extract customs slots:\n\n['Define a method `extract_<slot_name>` for every slot that should be mapped in a custom way.\\nEach slot which has been defined in the `domain.yml` file with a custom mapping **must** have its own independent\\nimplementation of an `extract_<slot_name>` method.', \"In your domain file, for your form's `required_slots`, list all required slots, with both predefined and custom mappings.\"]\n\nIn addition, you can override the `required_slots` method to add dynamically requested slots: you can read more in the\n[Dynamic Form Behavior](./forms.mdx#dynamic-form-behavior) section.\n\n:::note\nIf you have added a slot with a custom mapping in the `slots` section of the domain file which you only\nwant to be validated within the context of a form by a custom action extending `FormValidationAction`,\nplease make sure that this slot has a mapping of type `custom` and that the slot name is included in the\nform's `required_slots`.\n:::\n\nThe following example shows the implementation of a form which extracts a slot\n`outdoor_seating` in a custom way, in addition to the slots which use predefined mappings.\nThe method `extract_outdoor_seating` sets the slot `outdoor_seating` based on whether\nthe keyword `outdoor` was present in the last user message.\n\n```\nfrom typing import Dict, Text, List, Optional, Any\n\nfrom rasa_sdk import Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom rasa_sdk.forms import FormValidationAction\n\n\nclass ValidateRestaurantForm(FormValidationAction):\n    def name(self) -> Text:\n        return \"validate_restaurant_form\"\n\n    async def extract_outdoor_seating(\n        self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict\n    ) -> Dict[Text, Any]:\n        text_of_last_user_message = tracker.latest_message.get(\"text\")\n        sit_outside = \"outdoor\" in text_of_last_user_message\n\n        return {\"outdoor_seating\": sit_outside}\n```\n\nBy default the `FormValidationAction`\nwill automatically set the [`requested_slot`](forms.mdx#the-requested_slot-slot) to the\nfirst slot specified in `required_slots` which is not filled.",
              "Dynamic Form Behavior": "By default, Rasa will ask for the next empty slot from the slots\nlisted for your form in the domain file. If you use\n[custom slot mappings](forms.mdx#custom-slot-mappings) and the `FormValidationAction`,\nit will ask for the first empty slot returned by the `required_slots` method. If all\nslots in `required_slots` are filled the form will be deactivated.\n\nYou can update the required slots of your form dynamically.\nThis is, for example, useful when you need to fill additional slots based on how\na previous slot was filled or when you want to change the order in which slots are requested.\n\nIf you are using the Rasa SDK, we strongly recommend that you use the `FormValidationAction` and\noverride `required_slots` to fit your dynamic behavior. You must implement\na method `extract_<slot name>` for every slot which doesn't use a predefined mapping,\nas described in [Custom Slot Mappings](forms.mdx#custom-slot-mappings).\nThe example below will ask the user if they want to sit in\nthe shade or in the sun in case they said they want to sit outside.\n\n```\nfrom typing import Text, List, Optional\n\nfrom rasa_sdk.forms import FormValidationAction\n\nclass ValidateRestaurantForm(FormValidationAction):\n    def name(self) -> Text:\n        return \"validate_restaurant_form\"\n\n    async def required_slots(\n        self,\n        domain_slots: List[Text],\n        dispatcher: \"CollectingDispatcher\",\n        tracker: \"Tracker\",\n        domain: \"DomainDict\",\n    ) -> List[Text]:\n        additional_slots = [\"outdoor_seating\"]\n        if tracker.slots.get(\"outdoor_seating\") is True:\n            # If the user wants to sit outside, ask\n            # if they want to sit in the shade or in the sun.\n            additional_slots.append(\"shade_or_sun\")\n\n        return additional_slots + domain_slots\n```\n\nIf conversely, you want to remove a slot from the form's `required_slots` defined in the domain file under certain conditions,\nyou should copy the `domain_slots` over to a new variable and apply changes to that new variable instead of directly modifying\n`domain_slots`. Directly modifying `domain_slots` can cause unexpected behaviour. For example:\n\n```\nfrom typing import Text, List, Optional\n\nfrom rasa_sdk.forms import FormValidationAction\n\nclass ValidateBookingForm(FormValidationAction):\n    def name(self) -> Text:\n        return \"validate_booking_form\"\n\n    async def required_slots(\n        self,\n        domain_slots: List[Text],\n        dispatcher: \"CollectingDispatcher\",\n        tracker: \"Tracker\",\n        domain: \"DomainDict\",\n    ) -> List[Text]:\n        updated_slots = domain_slots.copy()\n        if tracker.slots.get(\"existing_customer\") is True:\n            # If the user is an existing customer,\n            # do not request the `email_address` slot\n            updated_slots.remove(\"email_address\")\n\n        return updated_slots\n```",
              "The requested_slot slot": "The slot `requested_slot` is automatically added to the domain as a\nslot of type [`text`](domain.mdx#text-slot). The value of the `requested_slot` will be\nignored during conversations. If you want to change this behavior, you need to add\nthe `requested_slot`  to your domain file as a categorical slot with\n`influence_conversation` set to `true`.\nYou might want to do this if you\nwant to handle your unhappy paths differently, depending on what slot is\ncurrently being asked from the user. For example, if your users respond\nto one of the bot's questions with another question, like *why do you need to know that?*\nThe response to this `explain` intent depends on where we are in the story.\nIn the restaurant case, your stories would look something like this:\n\n```\nstories:\n- story: explain cuisine slot\n  steps:\n  - intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant\n  - slot_was_set:\n    - requested_slot: cuisine\n  - intent: explain\n  - action: utter_explain_cuisine\n  - action: restaurant_form\n  - active_loop: null\n\n- story: explain num_people slot\n  steps:\n  - intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant\n  - slot_was_set:\n    - requested_slot: cuisine\n  - slot_was_set:\n    - requested_slot: num_people\n  - intent: explain\n  - action: utter_explain_num_people\n  - action: restaurant_form\n  - active_loop: null\n```\n\nAgain, it is **strongly** recommended that you use\n[interactive learning](./writing-stories.mdx#using-interactive-learning) to build these stories.",
              "Using a Custom Action to Ask For the Next Slot": "As soon as the form determines which slot has to be filled next by the user, it will\nexecute the action `utter_ask_<form_name>_<slot_name>` or `utter_ask_<slot_name>`\nto ask the user to provide the necessary information. If a regular utterance is not\nenough, you can also use a custom action `action_ask_<form_name>_<slot_name>` or\n`action_ask_<slot_name>` to ask for the next slot.\n\n```\nfrom typing import Dict, Text, List\n\nfrom rasa_sdk import Tracker\nfrom rasa_sdk.events import EventType\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom rasa_sdk import Action\n\n\nclass AskForSlotAction(Action):\n    def name(self) -> Text:\n        return \"action_ask_cuisine\"\n\n    def run(\n        self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict\n    ) -> List[EventType]:\n        dispatcher.utter_message(text=\"What cuisine?\")\n        return []\n```\n\nIf there is more than one asking option for the slot, Rasa prioritizes in the following order:\n\n['`action_ask_<form_name>_<slot_name>`', '`utter_ask_<form_name>_<slot_name>`', '`action_ask_<slot_name>`', '`utter_ask_<slot_name>`']"
            }
          },
          "metadata": {
            "id": "forms",
            "sidebar_label": "Forms",
            "title": "Forms",
            "description": "Follow a rule-based process of information gathering using forms in open source bot framework Rasa.",
            "abstract": "One of the most common conversation patterns is to collect a few pieces of information from a user in order to do something (book a restaurant, call an API, search a database, etc.). This is also called **slot filling**."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 16]"
        },
        {
          "title": "Generating NLU Data",
          "description": null,
          "content": {
            "Conversation-Driven Development for NLU": {
              "Gather Real Data": "When it comes to building out NLU training data, developers are sometimes tempted\nto use text generation tools or templates to quickly increase the number of training examples.\nThis is a bad idea for two reasons:\n\n[\"First, your synthetic data won't look like the messages\\nthat users actually send to your assistant, so your model will underperform.\", \"Second, by training and testing on synthetic data, you trick yourself into thinking that your\\nmodel *is* actually performing well, and you won't notice major issues.\"]\n\nRemember that if you use a script to generate training data, the only thing your model can\nlearn is how to reverse-engineer the script. \n\nTo avoid these problems, it is always a good idea to collect as much real user data\nas possible to use as training data. Real user messages can be messy, contain typos,\nand be far from 'ideal' examples of your intents. But keep in mind that those are the\nmessages you're asking your model to make predictions about!\nYour assistant will always make mistakes initially, but\nthe process of training & evaluating on user data will set your model up to generalize\nmuch more effectively in real-world scenarios.",
              "Share with Test Users Early": "In order to gather real data, you’re going to need real user messages. A bot developer\ncan only come up with a limited range of examples, and users will always surprise you\nwith what they say. This means you should share your bot with test users outside the\ndevelopment team as early as possible.\nSee the full [CDD guidelines](./conversation-driven-development.mdx) for more details."
            },
            "Avoiding Intent Confusion": {
              "Splitting on Entities vs Intents": "Intent confusion often occurs when you want your assistant's response to be conditioned on\ninformation provided by the user. For example,\n\"How do I migrate to Rasa from IBM Watson?\" versus \"I want to migrate from Dialogflow.\"\n\nSince each of these messages will lead to a different response, your initial approach might be to create\nseparate intents for each migration type, e.g. `watson_migration` and `dialogflow_migration`.\nHowever, these intents are trying to achieve the same goal (migrating to Rasa) and will\nlikely be phrased similarly, which may cause the model to confuse these intents.\n\nTo avoid intent confusion, group these training examples into single `migration` intent and make\nthe response depend on the value of a categorical `product` slot that comes from an entity.\nThis also makes it easy to handle the case when no entity is provided,\ne.g. \"How do I migrate to Rasa?\" For example:\n\n```\nstories:\n- story: migrate from IBM Watson\n  steps:\n    - intent: migration\n      entities:\n      - product\n    - slot_was_set:\n      - product: Watson\n    - action: utter_watson_migration\n\n- story: migrate from Dialogflow\n  steps:\n    - intent: migration\n      entities:\n      - product\n    - slot_was_set:\n      - product: Dialogflow\n    - action: utter_dialogflow_migration\n\n- story: migrate from unspecified\n  steps:\n    - intent: migration\n    - action: utter_ask_migration_product\n```"
            },
            "Improving Entity Recognition": {
              "Pre-trained Entity Extractors": "Common entities such as names, addresses, and cities require a large amount of training\ndata for an NLU model to generalize effectively.\n\nRasa provides two great options for\npre-trained extraction: [SpacyEntityExtractor](./components.mdx#SpacyEntityExtractor)\nand [DucklingEntityExtractor](./components.mdx#DucklingEntityExtractor).\nBecause these extractors have been pre-trained on a large corpus of data, you can use them\nto extract the entities they support without annotating them in your training data.",
              "Regexes": "Regexes are useful for performing entity extraction on structured patterns such as 5-digit\nU.S. zip codes. Regex patterns can be used to generate features for the NLU model to learn,\nor as a method of direct entity matching.\nSee [Regular Expression Features](./training-data-format.mdx#regular-expressions)\nfor more information.",
              "Lookup Tables": "Lookup tables are processed as a regex pattern that checks if any of the lookup table\nentries exist in the training example. Similar to regexes, lookup tables can be used\nto provide features to the model to improve entity recognition, or used to perform\nmatch-based entity recognition. Examples of useful applications of lookup tables are\nflavors of ice cream, brands of bottled water, and even sock length styles\n(see [Lookup Tables](./training-data-format.mdx#lookup-tables)).",
              "Synonyms": "Adding synonyms to your training data is useful for mapping certain entity values to a\nsingle normalized entity. Synonyms, however, are not meant for improving your model's\nentity recognition and have no effect on NLU performance.\n\nA good use case for synonyms is when normalizing entities belonging to distinct groups.\nFor example, in an assistant that asks users what insurance policies they're interested\nin, they might respond with \"my truck,\" \"a car,\" or \"I drive a batmobile.\"\nIt would be a good idea to map `truck`, `car`, and `batmobile` to the normalized value\n`auto` so that the processing logic will only need to account for a narrow set of\npossibilities (see [synonyms](./training-data-format.mdx#synonyms)).\n\nSynonyms can also be used to standardize the extracted entities. A synonym for `iPhone` can \nmap `iphone` or `IPHONE` to the synonym without adding these options in the synonym examples."
            },
            "Handling Edge Cases": {
              "Misspellings": "Coming across misspellings is inevitable, so your bot needs an effective way to\nhandle this. Keep in mind that the goal is not to correct misspellings, but to\ncorrectly identify intents and entities. For this reason, while a spellchecker may\nseem like an obvious solution, adjusting your featurizers and training data is often\nsufficient to account for misspellings.\n\nAdding a character-level featurizer provides\nan effective defense against spelling errors by accounting for parts of words, instead\nof only whole words. You can add character level featurization to your pipeline by\nusing the `char_wb` analyzer for the `CountVectorsFeaturizer`, for example:\n\n```\npipeline:\n# <other components>\n- name: CountVectorsFeaturizer\n  analyze: char_wb\n  min_ngram: 1\n  max_ngram: 4\n# <other components>\n```\n\nIn addition to character-level featurization, you can add common misspellings to\nyour training data.",
              "Defining an Out-of-scope Intent": "It is always a good idea to define an `out_of_scope` intent in your bot to capture\nany user messages outside of your bot's domain. When an `out_of_scope` intent is\nidentified, you can respond with messages such as \"I'm not sure how to handle that,\nhere are some things you can ask me...\" to gracefully guide the user towards a\nsupported skill."
            },
            "Shipping Updates": "Treat your data like code. In the same way that you would never ship code updates\nwithout reviews, updates to your training data should be carefully reviewed because\nof the significant influence it can have on your model's performance.\n\nUse a version control system such as Github or Bitbucket to track changes to your\ndata and rollback updates when necessary.\n\nBe sure to build tests for your NLU models to [evaluate performance](./testing-your-assistant.mdx) as training data\nand hyper-parameters change. Automate these tests in a [CI pipeline](./setting-up-ci-cd.mdx) such as Jenkins\nor Git Workflow to streamline your development process and ensure that only\nhigh-quality updates are shipped."
          },
          "metadata": {
            "id": "generating-nlu-data",
            "sidebar_label": "Generating NLU Data",
            "title": "Generating NLU Data"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 17]"
        },
        {
          "title": "Rasa Glossary",
          "description": "Glossary for all Rasa-related terms",
          "content": {
            "[Action](./actions.mdx)": "A single step that a bot takes in a conversation (e.g. calling an API or sending a response back to the user).",
            "[Action Server](./action-server/index.mdx)": "The server that runs custom action code, separate from Rasa. Rasa maintains the Rasa SDK in Python for implementing custom actions, although it's also possible to write custom actions in other languages.",
            "Annotation": "Adding labels to messages and conversations so that they can be used to train a model.",
            "[Anonymization](./pii-management.mdx)": "The process of replacing personally identifiable information (PII) with masked, artificial or constant text values.\nThis is done to protect the privacy of users.",
            "[Business Logic](./business-logic.mdx)": "Conditions that need to be fulfilled due to business requirements. For example: requiring a first and last name, an address, and a password before an account can be created. In a Rasa assistant, business logic is implemented using rule-based actions like [forms](./forms.mdx).",
            "[Chitchat](./chitchat-faqs.mdx)": "A conversation pattern where the user says something that isn't directly related to their goal.\nThis can include things like greetings, asking how you are etc.\nRead about handling [Chitchat and FAQs](./chitchat-faqs.mdx) to learn how to implement this with Rasa.",
            "CMS": "A way to store bot responses externally instead of including them directly in the domain. Content Management Systems decouple response text from training data. For more information, see [NLG Servers](./nlg.mdx).",
            "[Conversation-Driven Development (CDD)](./conversation-driven-development.mdx)": "The process of using user messages and conversation data to influence the design of an assistant and train the model, combined with engineering best practices. There are 6 steps that make up CDD: Share, Review, Annotate, Fix, Track, and Test.",
            "[Conversation Tests](./testing-your-assistant.mdx)": "Modified story format that includes the full text of the user message in addition to the intent label. Test conversations are saved to a test set file (conversation_tests.md), which is used to evaluate the model’s predictions across an entire conversation.",
            "[Component](./components.mdx)": "An element in the an assistant's [NLU pipeline](./tuning-your-model.mdx#how-to-choose-a-pipeline) in the [Model Configuration](./model-configuration.mdx).\n\nIncoming messages are processed by a sequence of components called a pipeline. A component can perform tasks ranging from entity extraction to intent classification to pre-processing.",
            "[Conditional Response Variation](./responses.mdx#conditional-response-variations)": "Response variation that can only be used when the current dialogue state satisfies some constraints as defined in the domain or responses files. If there's a match between the constraints and the dialogue state, Rasa can use this variation.",
            "[Custom Action](./actions.mdx#custom-actions)": "An action written by a bot developer that can run arbitrary code, mainly to interact with external systems and APIs.",
            "[Default Action](./actions.mdx#default-actions)": "A built-in action that comes with predefined functionality.",
            "[DIET](./components.mdx#dietclassifier)": "Dual Intent and Entity Transformer. The default NLU architecture used by Rasa, which performs both intent classification and entity extraction.",
            "[Domain](./domain.mdx)": "Defines the inputs and outputs of an assistant.\n\nIt includes a list of all the intents, entities, slots, actions, and forms that the assistant knows about.",
            "[Entity](./training-data-format.mdx#entities)": "Keywords that can be extracted from a user message. For example: a telephone number, a person's name, a location, the name of a product",
            "[Event](./action-server/events.mdx)": "Something that happens in a conversation. For instance, a `UserUttered` event represents a user entering a message, and an `ActionExecuted` event represents the assistant executing an action. All conversations in Rasa are represented as a sequence of events.",
            "FAQs": "Frequently asked questions (FAQs) are common questions that your users ask. In the context of building an assistant,\nthis typically means the user sends a message and the assistant send a response without needing to consider the context of the conversation.\nRead about handling [Chitchat and FAQs](./chitchat-faqs.mdx) to learn how to implement this with Rasa.",
            "[Form](./forms.mdx)": "A type of custom action that asks the user for multiple pieces of information.\n\nFor example, if you need a city, a cuisine, and a price range to recommend a restaurant, you can create a restaurant form to collect the information. You can describe [business logic](#business-logic) inside a form, like offering the customer a different set of menu options if they mention a food allergy.",
            "Happy / Unhappy Paths": "Terms used to describe whether the user’s input is expected or unexpected. If your assistant asks a user for some information and the user provides it, we call that a happy path. Unhappy paths are all possible edge cases. For example, the user refusing to give the requested input, changing the topic of conversation, or correcting something they said earlier.",
            "[Intent](./nlu-training-data.mdx)": "In a given user message, the thing that a user is trying to convey or accomplish (e,g., greeting, specifying a location).",
            "[Interactive Learning](./writing-stories.mdx#using-interactive-learning)": "In the Rasa CLI, a training mode where the developer corrects and validates the assistant’s predictions at every step of the conversation. The conversation can be saved to the story format and added to the assistant’s training data.",
            "[Knowledge Base / Knowledge Graph](./action-server/knowledge-base-actions.mdx)": "A queryable database that represents complex relationships and hierarchies between objects. Knowledge Base Actions allow Rasa to fetch information from a knowledge base and use it in responses.",
            "[Level 3 Assistant](https://blog.rasa.com/5-levels-of-conversational-ai-2020-update/)": "An assistant that can handle conversations more complex than simple back-and-forth exchanges. Level 3 assistants are capable of using the context of previous conversation turns to choose the appropriate next action.",
            "[Messaging Channels](./messaging-and-voice-channels.mdx)": "Connectors that integrate Rasa with external messaging platforms, where end-users can send and receive messages. Rasa includes built-in messaging channels like Slack, Facebook Messenger, and web chat, as well as the ability to create custom connectors.",
            "[Minimum Viable Assistant](./conversation-driven-development.mdx#cdd-in-early-stages-of-development)": "A basic assistant that can handle the most important happy path stories.",
            "[NLG](./nlg.mdx)": "Natural Language Generation (NLG) is the process of generating natural language messages to send to a user.\n\nRasa uses a simple template-based approach for NLG. Data-driven approaches (such as neural NLG) can be implemented by creating a custom NLG component.",
            "[NLU](./nlu-training-data.mdx)": "Natural Language Understanding (NLU) deals with parsing and understanding human language into a structured format.",
            "[Pipeline](./tuning-your-model.mdx)": "The list of NLU components (see [NLU Component](#nlu-component)) that defines a Rasa assistant’s NLU system. A user message is processed by each component one by one, before returning the final structured output.",
            "[Policy](./policies.mdx)": "Rasa components that predict the dialogue system’s next actionPolicies make decisions about how the conversation flow should proceed. A typical configuration includes multiple policies, and the policy with the highest confidence decides the next action to be taken in the conversation.",
            "Rasa Core": "(Outdated - Rasa Core and Rasa NLU were merged into one package in 1.x. The functionality of Core is now referred to as dialogue management)\n\nThe dialogue engine that decides what to do next in a conversation based on the context. Part of the Rasa library.",
            "Rasa NLU": "(Outdated - Rasa Core and Rasa NLU were merged into one package in 1.x. The functionality of Rasa NLU is now referred to as NLU)\n\nRasa NLU is the part of Rasa that performs Natural Language Understanding ([NLU](#nlu)), including intent classification and entity extraction.",
            "[NLU Component](./components.mdx)": "An element in the Rasa NLU pipeline (see [Pipeline](#pipeline)) that processes incoming messages. Components perform tasks ranging from entity extraction to intent classification to pre-processing.",
            "[Rasa X/Enterprise](https://rasa.com/docs/rasa-enterprise/)": "A tool for [conversation-driven development](./conversation-driven-development.mdx). Rasa X/Enterprise helps teams share and test an assistant built with Rasa, annotate user messages, and view conversations.",
            "[Retrieval Intent](./chitchat-faqs.mdx)": "A special type of intent that can be divided into smaller sub-intents. For example, an FAQ retrieval intent has sub-intents that represent each individual question the assistant knows how to answer.",
            "[REST Channel](./connectors/your-own-website.mdx)": "A messaging channel used to build custom connectors. Includes an input channel, where user messages can be posted to Rasa, and the ability to specify a callback URL, where the bot’s response actions will be sent.",
            "[Response / Template / Utterance](./responses.mdx)": "A message that an assistant sends to a user. This can include text, buttons, images, and other content.",
            "[Rules](./rules.mdx)": "Special training data to specify rule-like behavior, where a specific condition always predicts a specific next action. Examples include\nanswering FAQs, filling [Forms](./forms.mdx), or handling\n[Fallbacks](./fallback-handoff.mdx#fallbacks).",
            "[Slot](./domain.mdx#slots)": "A key-value store that Rasa uses to track information over the course of a conversation.",
            "[Story](./stories.mdx)": "Training data format for the dialogue model, consisting of a conversation between a user and a bot. The user's messages are represented as annotated intents and entities, and the bot’s responses are represented as a sequence of actions.",
            "[TED Policy](./policies.mdx#ted-policy)": "Transformer Embedding Dialogue Policy. TED is the default machine learning-based dialogue policy used by Rasa. TED complements rule-based policies by handling previously unseen situations, where no rule exists to determine the next action.",
            "[Template / Response / Utterance](./responses.mdx)": "A message template used to respond to a user. Can include text, buttons, images, and other attachments.",
            "[Tracker](./tracker-stores.mdx)": "Rasa component that maintains the state of the dialogue, which is represented as a JSON object listing the events from the current session.",
            "User Goal": "The overall goal that a user wants to achieve, e.g. looking up the answer to a question, booking an appointment, or purchasing an insurance policy.\n\nSome tools refer to the user goal as the “intent,” but in Rasa terminology, an intent is associated with each individual user message.",
            "Word embedding / Word vector": "A vector of floating point numbers that represent the meaning of a word. Words that have similar meanings tend to have similar vectors. Word embeddings are often used as an input to machine learning algorithms.",
            "Rasa Primitive": "A foundational component used for structuring conversations within Rasa, such as an intent, entity, slot, form, response, action, rule, or story."
          },
          "metadata": {
            "id": "glossary",
            "sidebar_label": "Rasa Glossary",
            "title": "Rasa Glossary",
            "description": "Glossary for all Rasa-related terms"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 18]"
        },
        {
          "title": "Graph Recipe",
          "description": "Learn about Graph Recipe for Rasa.",
          "content": {
            "Differences with Default Recipe": "There are some differences between the default recipe and the new graph recipe. Main differences are:\n\n['Default recipe is named `default.v1` in the config file whereas graph recipes are named `graph.v1`.', 'Default recipes provide an easy to use recipe structure whereas graph recipes are more advanced and powerful.', 'Default recipes are very opinionated and provide various defaults whereas graph recipes are more explicit.', 'Default recipes can auto-configure themselves and dump the defaults used to the file if some sections in `config.yml` are missing, whereas graph recipes do none of this and assume what you see is what you get. There are no surprises with graph recipes.', 'Default recipe divides graph configuration into mainly two parts: `pipeline` and `policies`. These can also be described as NLU and core (dialogue management) parts. For graph recipe on the other hand, the separation is between training (ie. `train_schema`) and prediction (ie. `predict_schema`).']\n\n:::tip Starting from scratch?\n\nIf you don't know which recipe to choose, use the default recipe to bootstrap your project fast. If later you find that you need more fine-grained control, you can always change your recipe to be a graph recipe.\n\n:::",
            "Graph Configuration File Structure": "Graph recipes share `recipe` and `language` keys with the same meaning. Similarities end there as graph recipes do not have `pipeline` or `policies` keys but they do have `train_schema` and `predict_schema` keys for determining the graph nodes during train and predict runs respectively. In addition to this, target nodes for NLU and core can be specified explicitly with graph recipes, these can be declared with `nlu_target` and `core_target`. If targets are omitted, node names used by default recipe will take over, and these are `run_RegexMessageHandler` and `select_prediction` for nlu and core respectively.\n\nHere's an example graph recipe:\n\n```\n```\n\n:::note graph targets\nFor NLU, default target name of `run_RegexMessageHandler` will be used, while for core (dialogue management) the target will be called `select_prediction` if omitted. Make sure you have graph nodes with relevant names in your schema definitions.\n\nIn a similar fashion, note that the default resource needed by the first graph node is fixed to be `__importer__` (representing configuration, training data etc.) for training task and it is `__message__` (representing the message received) for prediction task. Make sure your first nodes make use of these dependencies.\n\n:::",
            "Graph Node Configuration": "As you can see in the example above, graph recipes are very much explicit and you can configure each graph node as you would like. Here is an explanation of what some of the keys mean:\n\n['`needs`: You can define here what data your graph node requires and from which parent node. Key is the data name, whereas the value would refer to the node name.']\n\n```\nneeds:\n    messages: nlu_message_converter\n```\n\nCurrent graph node needs `messages` which is provided by `nlu_message_converter` node.\n\n['`uses`: You can provide the class used to instantiate this node with this key. Please provide the full path in Python path syntax, eg.']\n\n```\nuses: rasa.graph_components.converters.nlu_message_converter.NLUMessageConverter\n```\n\nYou are not required to use Rasa internal graph component classes and you\ncan use your own components here. Refer to [custom graph\ncomponents](custom-graph-components.mdx) pages to find out how to write your\nown graph components.\n\n['`constructor_name`: This is the constructor used to instantiate your component. Example:']\n\n```\nconstructor_name: load\n```\n\n['`fn`: This is the function used in executing the graph component. Example:']\n\n```\nfn: combine_predictions_from_kwargs\n```\n\n['`config`: You can provide any configuration parameters for your components using this key.']\n\n```\nconfig:\n    language: en\n    persist: false\n```\n\n['`eager`: This determines if your component should be eagerly loaded\\nwhen the graph is constructed or if it should wait until the\\nruntime (this is called lazy instantiation). Usually we always\\ninstantiate lazily during training and eagerly during inference (to\\navoid slow first prediction).']\n\n```\neager: true\n```\n\n['`resource`: If given, graph node is loaded from this resource instead of instantiated from scratch. This is e.g. used to load a trained component for predictions.']\n\n```\nresource:\n    name: train_RulePolicy1\n```\n\n[\"`is_target`: Boolean value, if `True` then this node can't be pruned\\nduring fingerprinting (it might be replaced with a cached value\\nthough). This\\nis e.g. used for all components which train as their result always needs\\nto be added to the model archive so that the data is available during\\ninference.\"]\n\n```\nis_target: false\n```\n\n['`is_input`: Boolean value; nodes with `is_input` are _always_ run (also during the\\nfingerprint run). This makes sure that we e.g. detect changes in file\\ncontents.']\n\n```\n is_input: false\n```"
          },
          "metadata": {
            "id": "graph-recipe",
            "sidebar_label": "Graph Recipe",
            "title": "Graph Recipe",
            "description": "Learn about Graph Recipe for Rasa.",
            "abstract": "Graph recipes provide a more fine tuned configuration for your executable graphs."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 19]"
        },
        {
          "title": "Rasa HTTP API",
          "description": "Read about Rasa's HTTP API that has endpoints for conversations, training models, and configuring your bot.",
          "content": {
            "Enabling the HTTP API": "By default, running a Rasa server does not enable the API endpoints. Interactions\nwith the bot can happen over the exposed `webhooks/<channel>/webhook` endpoints.\n\nTo enable the API for direct interaction with conversation trackers and other\nbot endpoints, add the `--enable-api` parameter to your run command:\n\n```\nrasa run --enable-api\n```\n\nNote that you start the server with an NLU-only model, not all the available endpoints\ncan be called. Some endpoints will return a 409 status code, as a trained\ndialogue model is needed to process the request.\n\n:::caution\nMake sure to secure your server, either by restricting access to the server (e.g. using firewalls), or\nby enabling an authentication method. See [Security Considerations](./http-api.mdx#security-considerations).\n\n:::\n\nBy default, the HTTP server runs as a single process. You can change the number\nof worker processes using the `SANIC_WORKERS` environment variable. It is\nrecommended that you set the number of workers to the number of available CPU cores\n(check out the\n[Sanic docs](https://sanicframework.org/en/guide/deployment/running.html#workers)\nfor more details). This will only work in combination with the\n`RedisLockStore` (see [Lock Stores](./lock-stores.mdx).\n\n:::caution\nThe [SocketIO channel](./connectors/your-own-website.mdx#websocket-channel) does not support multiple worker processes. \n\n:::",
            "Security Considerations": {
              "Token Based Auth": "To use a plaintext token to secure your server, specify the token in the argument `--auth-token thisismysecret` when starting\nthe server:\n\n```\nrasa run \\\n    --enable-api \\\n    --auth-token thisismysecret\n```\n\nAny clients sending requests to the server must pass the token\nas a query parameter, or the request will be rejected. For example, to fetch a tracker from the server:\n\n```\ncurl -XGET localhost:5005/conversations/default/tracker?token=thisismysecret\n```",
              "JWT Based Auth": "To use JWT based authentication, specify the JWT secret in the argument `--jwt-secret thisismysecret`\non startup of the server:\n\n```\nrasa run \\\n    --enable-api \\\n    --jwt-secret thisismysecret\n```\n\nIf you want to sign a JWT token with asymmetric algorithms, you can specify the JWT private key to the `--jwt-private-key`\nCLI argument. You must pass the public key to the `--jwt-secret` argument, and also specify the algorithm to the\n`--jwt-method` argument:\n\n```\nrasa run \\\n    --enable-api \\\n    --jwt-secret <public_key> \\\n    --jwt-private-key <private_key> \\\n    --jwt-method RS512\n```\n\nClient requests to the server will need to contain a valid JWT token in\nthe `Authorization` header that is signed using this secret\nand the `HS256` algorithm e.g.\n\n```\n\"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ\"\n                 \"zdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIi\"\n                 \"wiaWF0IjoxNTE2MjM5MDIyfQ.qdrr2_a7Sd80gmCWjnDomO\"\n                 \"Gl8eZFVfKXA6jhncgRn-I\"\n```\n\nThe token's payload must contain an object under the `user` key,\nwhich in turn must contain the `username` and `role` attributes.\nThe following is an example payload for a JWT token:\n\n```\n{\n    \"user\": {\n        \"username\": \"<sender_id>\",\n        \"role\": \"user\"\n    }\n}\n```\n\nIf the `role` is `admin`, all endpoints are accessible.\nIf the `role` is `user`, endpoints with a `sender_id` parameter are only accessible\nif the `sender_id` matches the payload's `username` property.\n\n```\nrasa run \\\n    -m models \\\n    --enable-api \\\n    --jwt-secret thisismysecret\n```\n\nTo create and encode the token, you can use tools such as the [JWT Debugger](https://jwt.io/), or a Python module such as [PyJWT](https://pyjwt.readthedocs.io/en/latest/)."
            }
          },
          "metadata": {
            "id": "http-api",
            "sidebar_label": "HTTP API",
            "title": "Rasa HTTP API",
            "description": "Read about Rasa's HTTP API that has endpoints for conversations, training models, and configuring your bot.",
            "abstract": "You can use the HTTP API to interact with a running Rasa server. With the API, you can train models, send messages, run tests, and more."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 20]"
        },
        {
          "title": "Introduction to Rasa Open Source & Rasa Pro",
          "description": "Learn more about open-source natural language processing library Rasa for conversation handling, intent classification and entity extraction in on premise chatbots.",
          "content": {
            "Rasa Open Source": "Rasa Open Source is an open source conversational AI platform that allows you to understand and hold conversations, and connect to messaging channels and third party systems through a set of APIs. It supplies the building blocks for creating virtual (digital) assistants or chatbots.",
            "Rasa Pro": "<RasaProBanner/>\n\nRasa Pro is the commercial, pro-code offering of Rasa that’s built to address enterprise needs\naround security, observability and scale. [Read more here](./rasa-pro.mdx).\n\nWhen the content is only applicable to Rasa Pro, you will see the below label:\n\n<RasaProLabel />\n\nThis means you need to have a Rasa Pro License in order to access the capabilities detailed in that section."
          },
          "metadata": {
            "slug": "/",
            "sidebar_label": "Introduction",
            "title": "Introduction to Rasa Open Source & Rasa Pro",
            "hide_table_of_contents": true,
            "description": "Learn more about open-source natural language processing library Rasa for conversation handling, intent classification and entity extraction in on premise chatbots."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 21]"
        },
        {
          "title": "Jupyter Notebooks",
          "description": "Learn how to integrate open source chatbot platform Rasa into Jupyter notebooks, alongside all your machine learning code.",
          "content": {
            "Train a Model": "Now we can train a model by passing in the paths to the `rasa.train` function.\nNote that the training files are passed as a list.\nWhen training has finished, `rasa.train` returns the path where the trained model has been saved.\n\n```\nimport rasa\n\nmodel_path = rasa.train(domain, config, [training_files], output)\nprint(model_path)\n```",
            "Chat with your assistant": "To start chatting to an assistant, call the `chat` function, passing\nin the path to your saved model. If you do not have custom actions you can set `endpoints = None` or omit it:\n\n```\nfrom rasa.jupyter import chat\n\nendpoints = \"endpoints.yml\"\nchat(model_path, endpoints)\n```",
            "Evaluate your model against test data": "Rasa has a convenience function for getting your training data.\nRasa's `get_core_directory` and `get_nlu_directory` are functions which\nrecursively find all the stories or NLU data files\nand copies them into temporary directories.\nThe return values are the paths to these newly created directories.\n\n```\nimport rasa.shared.data as data\nnlu_data_directory = data.get_nlu_directory(training_files)\nstories_directory = data.get_core_directory(training_files)\nprint(stories_directory, nlu_data_directory)\n```\n\nTo test your model, call the `test` function, passing in the path\nto your saved model and directories containing the stories and nlu data\nto evaluate on.\n\n```\nrasa.test(model_path, stories_directory, nlu_data_directory)\nprint(\"Done testing.\")\n```\n\nThe results of the core evaluation will be written to a file called `results`.\nNLU errors will be reported to `errors.json`.\nTogether, they contain information about the accuracy of your model's\npredictions and other metrics.\n\n```\nif os.path.isfile(\"errors.json\"):\n    print(\"NLU Errors:\")\n    print(open(\"errors.json\").read())\nelse:\n    print(\"No NLU errors.\")\n\nif os.path.isdir(\"results\"):\n      print(\"\\n\")\n      print(\"Core Errors:\")\n      print(open(\"results/failed_test_stories.yml\").read())\n```"
          },
          "metadata": {
            "id": "jupyter-notebooks",
            "sidebar_label": "Jupyter Notebooks",
            "title": "Jupyter Notebooks",
            "description": "Learn how to integrate open source chatbot platform Rasa into Jupyter notebooks, alongside all your machine learning code."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 22]"
        },
        {
          "title": "Language Support",
          "description": null,
          "content": {
            "Training a Model in Any Languages": "The following pipeline can be used to train models in whitespace tokenizable languages:\n\n```\n```\n\nTo train a Rasa model in your preferred language, define the pipeline in your ``config.yml``.\nAfter you define the pipeline and generate some [NLU training data](./training-data-format.mdx)\nin your chosen language, train the model by running the command:\n\n```\nrasa train nlu\n```\n\nOnce the training is finished, you can test your model's language skills.\nSee how your model interprets different input messages by running:\n\n```\nrasa shell nlu\n```\n\n:::note\nEven more so when training word embeddings from scratch, more training data will lead to a\nbetter model! If you find your model is having trouble discerning your inputs, try training\nwith more example sentences.\n:::",
            "Using Pre-trained Language Models": {
              "spaCy": "With the [Pre-trained Spacy Embeddings](./components.mdx#spacynlp), you can use spaCy's\n[pre-trained language models](https://spacy.io/usage/models#languages) or load fastText vectors, which are available\nfor [hundreds of languages](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md). If you want\nto incorporate a custom model you've found into spaCy, check out their page on\n[adding languages](https://spacy.io/docs/usage/adding-languages). As described in the documentation, you need to\nregister your language model and link it to the language identifier, which will allow Rasa to load and use your new language\nby passing in your language identifier as the ``language`` option.",
              "MITIE": "You can also pre-train your own word vectors from a language corpus using [MITIE](./components.mdx#mitienlp). To do so:\n\n['Get a clean language corpus (a Wikipedia dump works) as a set of text files.', \"Build and run `MITIE Wordrep Tool`_ on your corpus.\\nThis can take several hours/days depending on your dataset and your workstation.\\nYou'll need something like 128GB of RAM for wordrep to run -- yes, that's a lot: try to extend your swap.\", 'Set the path of your new ``total_word_feature_extractor.dat`` as the ``model`` parameter in your\\n[configuration](./components.mdx#mitienlp).']\n\nFor a full example of how to train MITIE word vectors, check out\n[this blogpost](http://www.crownpku.com/2017/07/27/%E7%94%A8Rasa_NLU%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%AD%E6%96%87NLU%E7%B3%BB%E7%BB%9F.html)\nof creating a MITIE model from a Chinese Wikipedia dump."
            }
          },
          "metadata": {
            "id": "language-support",
            "sidebar_label": "Language Support",
            "title": "Language Support",
            "abstract": "You can use Rasa to build assistants in any language you want."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 23]"
        },
        {
          "title": "Lock Stores",
          "description": "Messages that are being processed lock Rasa for a given conversation ID to ensure that multiple incoming messages for that conversation do not interfere with each other. Rasa provides multiple implementations to maintain conversation locks.",
          "content": {
            "InMemoryLockStore (default)": [
              "**Description**",
              "`InMemoryLockStore` is the default lock store. It maintains conversation locks\nwithin a single process.",
              ":::note\nThis lock store should not be used when multiple Rasa servers are run\nparallel.",
              ":::",
              "**Configuration**",
              "To use the `InMemoryTrackerStore` no configuration is needed."
            ],
            "ConcurrentRedisLockStore": {
              "Description": "The `ConcurrentRedisLockStore` uses Redis as a persistence layer for instances of issued [tickets](https://rasa.com/docs/rasa/reference/rasa/core/lock/#ticket-objects) and the last issued ticket number.\n\nThe ticket number initialization begins at 1, in contrast to that of the`RedisLockStore` which begins at 0.\nIf the ticket expires, the ticket number will not be reassigned to future tickets; as a result ticket numbers are unique to ticket instances.\nTicket numbers are incremented using the Redis atomic transaction INCR on the persisted last issued ticket number.\n\nThe `ConcurrentRedisLockStore` ensures that only one Rasa instance can handle a conversation at any point in time.\nTherefore, this Redis implementation of the `LockStore` can handle messages received in parallel for the same conversation by different Rasa servers\nThis is the recommended lock store for running a replicated set of Rasa servers.",
              "Configuration": "To set up Rasa with Redis the following steps are required:\n\n['Start your Redis instance', 'Add required configuration to your `endpoints.yml`']\n\n```\nlock_store:\n    type: rasa_plus.components.concurrent_lock_store.ConcurrentRedisLockStore\n    host: <host of the redis instance, e.g. localhost>\n    port: <port of your redis instance, usually 6379>\n    password: <password used for authentication>\n    db: <number of your database within redis, e.g. 0>\n    key_prefix: <alphanumeric value to prepend to lock store keys>\n```\n\n['To start the Rasa Core server using your Redis backend, add the `--endpoints`\\nflag, e.g.:', '```\\nrasa run -m models --endpoints endpoints.yml\\n```']",
              "Parameters": [
                "`url` (default: `localhost`): The url of your redis instance",
                "`port` (default: `6379`): The port which redis is running on",
                "`db` (default: `1`): The number of your redis database",
                "`key_prefix` (default: `None`): The prefix to prepend to lock store keys. Must\nbe alphanumeric",
                "`password` (default: `None`): Password used for authentication\n(`None` equals no authentication)",
                "`use_ssl` (default: `False`): Whether or not the communication is encrypted -`socket_timeout` (default: `10`): Time in seconds after which an\nerror is raised if Redis doesn't answer"
              ],
              "Migration Guide": "To switch from the `RedisLockStore` to the `ConcurrentRedisLockStore`, specify the complete module path to the `ConcurrentRedisLockStore` class as `type` in `endpoints.yml`:\n\n```\nlock_store:\n    type: rasa_plus.components.concurrent_lock_store.ConcurrentRedisLockStore\n    host: <host of the redis instance, e.g. localhost>\n    port: <port of your redis instance, usually 6379>\n    password: <password used for authentication>\n    db: <number of your database within redis, e.g. 0>\n    key_prefix: <alphanumeric value to prepend to lock store keys>\n```\n\nYou must replace the `url` field in the redis lock store configuration with a field `host` containing the hostname of the redis instance.\n\nNo database migration is required when switching to the `ConcurrentRedisLockStore`. You can use the same Redis instance and database number as you did previously when using the `RedisLockStore`.\nYou may want to delete all the preexisting keys if using the same Redis database number.\nThese former key-value items are no longer required by the `ConcurrentRedisLockStore` and the database can be cleared.\n\nThere is no overlap in key-value items stored when using the `RedisLockStore` and the `ConcurrentRedisLockStore`, because the `RedisLockStore` persists serialized [TicketLock](https://rasa.com/docs/rasa/reference/rasa/core/lock/#ticketlock-objects) instances while the `ConcurrentRedisLockStore` instead stores individual [`Ticket`](https://rasa.com/docs/rasa/reference/rasa/core/lock/#ticket-objects) instances, as well as the last issued ticket number.\nThe `ConcurrentRedisLockStore` recreates the `TicketLock` from the persisted `Ticket` instances, which allows it to handle concurrent messages for the same conversation ID."
            },
            "RedisLockStore": [
              "**Description**",
              "`RedisLockStore` maintains conversation locks using Redis as a persistence layer.\nThis is the recommended lock store for running a replicated set of Rasa servers.",
              "**Configuration**",
              "To set up Rasa with Redis the following steps are required:",
              [
                "Start your Redis instance",
                "Add required configuration to your `endpoints.yml`",
                "```\nlock_store:\n    type: \"redis\"\n    url: <url of the redis instance, e.g. localhost>\n    port: <port of your redis instance, usually 6379>\n    password: <password used for authentication>\n    db: <number of your database within redis, e.g. 0>\n    key_prefix: <alphanumeric value to prepend to lock store keys>\n```",
                "To start the Rasa Core server using your Redis backend, add the `--endpoints`\nflag, e.g.:"
              ],
              "```\nrasa run -m models --endpoints endpoints.yml\n```",
              "**Parameters**",
              [
                "`url` (default: `localhost`): The url of your redis instance",
                "`port` (default: `6379`): The port which redis is running on",
                "`db` (default: `1`): The number of your redis database",
                "`key_prefix` (default: `None`): The prefix to prepend to lock store keys. Must\nbe alphanumeric",
                "`username` (default: `None`): Username used for authentication",
                "`password` (default: `None`): Password used for authentication\n(`None` equals no authentication)",
                "`use_ssl` (default: `False`): Whether or not the communication is encrypted",
                "`ssl_keyfile` (default: `None`): Path to an ssl private key",
                "`ssl_certfile` (default: `None`): Path to an ssl certificate",
                "`ssl_ca_certs` (default: `None`): The path to a file of concatenated CA certificates in PEM format",
                "`socket_timeout` (default: `10`): Time in seconds after which an\nerror is raised if Redis doesn't answer"
              ]
            ],
            "Custom Lock Store": {
              "Configuration": "Put the module path to your custom event broker and the parameters you require in your `endpoints.yml`:\n\n```\nlock_store:\n  type: path.to.your.module.Class\n  url: localhost\n  a_parameter: a value\n  another_parameter: another value\n```"
            }
          },
          "metadata": {
            "id": "lock-stores",
            "sidebar_label": "Lock Stores",
            "title": "Lock Stores",
            "description": "Messages that are being processed lock Rasa for a given conversation ID to ensure that multiple incoming messages for that conversation do not interfere with each other. Rasa provides multiple implementations to maintain conversation locks."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 24]"
        },
        {
          "title": "Markers",
          "description": "Find out how to mark points of interest in dialogues using Marker conditions.",
          "content": {
            "Overview": "Markers are conditions that allow you to describe and mark points of interest in dialogues for evaluating your bot.\n\nIn Rasa, a dialogue is represented as a sequence of events,\nwhich include bot actions that were executed, intents that were detected, and slots that were set.\nMarkers allow you to describe conditions over such events. When the conditions are met, the relevant events are marked for further analysis or inspection.\n\nThere are several downstream applications for Markers. For example, they can be used to define and measure your bot's **Key Performance Indicators (KPIs)**,\nsuch as **dialogue completion** or **task success**. Take [Carbon Bot](https://rasa.com/blog/using-conversation-tags-to-measure-carbon-bots-success-rate/)\nfor example, which helps users offset their carbon emissions from flying. For Carbon Bot, you can define dialogue completion as \"all mandatory slots have been filled\",\nand task success as \"all mandatory slots have been filled and a carbon estimate has been successfully computed\".\nMarking when these important events occur allows you to measure Carbon Bot's success rate.\n\nMarkers also allow you to **diagnose your dialogues** by surfacing important events for further inspection.\nFor example, you might observe that Carbon Bot tends to successfully set the `travel_departure` and `travel_destination` slots,\nbut fails to set the `travel_flight_class` slot. You can define a marker to quantify how often this behavior occurs\nand surface relevant dialogues for review as part of\n[Conversation Driven Development (CDD)](./conversation-driven-development.mdx).\n\nMarker definitions are written in `YAML` in a marker configuration file. For example, here are the markers that define dialogue completion and task success for Carbon Bot:\n\n```\nmarker_dialogue_completion:\n  and:\n    - slot_was_set: travel_departure\n    - slot_was_set: travel_destination\n    - slot_was_set: travel_flight_class\n\nmarker_task_success:\n  description: \"Measure task success where all required slots are set and the custom action was triggered\"\n  and:\n    - slot_was_set: travel_departure\n    - slot_was_set: travel_destination\n    - slot_was_set: travel_flight_class\n    - action: provide_carbon_estimate\n```\n\nAnd here is the marker for surfacing dialogues where all mandatory slots are set except `travel_flight_class`:\n\n```\nmarker_dialogue_mandatory_slot_failure:\n  and:\n    - slot_was_set: travel_departure\n    - slot_was_set: travel_destination\n    - not:\n      - slot_was_set: travel_flight_class\n```\n\nThe next sections explain how to write marker definitions, how to apply them to your existing dialogues, and what the output format looks like.",
            "Defining Markers": {
              "Event Conditions": "The following event condition labels are supported:\n\n['`action`: the specified bot action was executed.', '`intent`: the specified user intent was detected.', '`slot_was_set`: the specified slot was set.']\n\nThe negated forms of the labels are also supported:\n\n['`not_action`: the event is not the specified bot action.', '`not_intent`: the event is not the specified user intent.', '`slot_was_not_set`: the specified slot has not been set.']",
              "Operators": "The following operators are supported:\n\n['`and`: all listed conditions applied.', '`or`: any of the listed conditions applied.', '`not`: the condition did not apply. This operator only accepts 1 condition.', '`seq`: the list of conditions applied in the specified order, with any number of events occurring in-between.', '`at_least_once`: the listed marker definitions occurred at least once. Only the first occurrence will be marked.', '`never`: the listed marker definitions never occurred.']",
              "Marker Configuration": "Here is an example of a marker configuration file containing several marker definitions. The example is created for mood bot,\nwith a new slot `name` to illustrate the use of the label `slot_was_set`:\n\n```\nmarker_name_provided:\n  description: \"slot `name` was provided\"\n  slot_was_set: name\n\nmarker_mood_expressed:\n  or:\n    - intent: mood_unhappy\n    - intent: mood_great\n\nmarker_cheer_up_failed:\n  seq:\n    - intent: mood_unhappy\n    - action: utter_cheer_up\n    - action: utter_did_that_help\n    - intent: deny\n\nmarker_bot_not_challenged:\n  description: \"Example of a negated marker, it can be used to surface conversations without bot_challenge intent\"\n  never:\n    - intent: bot_challenge\n\nmarker_cheer_up_attempted:\n  at_least_once:\n    - action: utter_cheer_up\n\nmarker_mood_expressed_and_name_not_provided:\n  and:\n    - or:\n      - intent: mood_unhappy\n      - intent: mood_great\n    - not:\n      - slot_was_set: name\n```\n\nNote the following:\n\n['Each marker has a unique identifier (or name) such as `marker_name_provided`.', 'Each marker can have an optional `description` key that can be used for documentation.', 'A marker definition can contain a single condition, as shown in `marker_name_provided`.', 'A marker definition can contain a single operator with a list of conditions, as shown in `marker_mood_expressed`,\\n`marker_cheer_up_failed`, `marker_bot_not_challenged`, and `marker_cheer_up_attempted`.', 'A marker definition can contain nested operators, as shown in `marker_mood_expressed_and_name_not_provided`.', \"The values assigned to event conditions must be valid according to your bot's `domain.yml` file. For example, in\\n`marker_mood_expressed`, the intents `mood_unhappy` and `mood_unhappy` are both intents listed in the mood bot's `domain.yml` file.\"]\n\n:::note\nYou cannot reuse an existing marker name in the definition of another marker.\n:::"
            },
            "Extracting Markers": {
              "Extracted Markers": "For each marker defined in your marker configuration file, the following information is extracted:\n\n['**The index of the event** at which the marker applied.', '**The number of user turns preceding the event** at which the marker applied. Each `UserUttered` event is treated as a user turn.']\n\nThe **index of the event** and the **number of preceding user turns** both give an indication of how\nlong it took to reach an important event, such as task success.\nThe index of the event will count all events, including ones that are not part of the dialogue,\nsuch as starting a new session or executing a custom action.\nThe number of preceding user turns, on the other hand, gives you a more intuitive indication of the dialogue length,\nand in particular from the perspective of your end user.\n\nThe number of preceding user turns can be used to evaluate and improve your bot.\nFor example, suppose a user had to rephrase their utterances multiple times, which caused their dialogue to become longer.\nThe dialogue may eventually reach task success, however, surfacing it would allow you identify utterances that your bot failed to understand.\nYou can then use these challenging utterances as additional training data to further improve your bot as part of\n[Conversation Driven Development (CDD)](./conversation-driven-development.mdx).\n\n:::note\nFor markers defined using the `at_least_once` operator, the information above will only be extracted for the first occurrence.\n:::\n\nThe extracted markers are stored in a tabular format in the `.csv` file you specify in the script, for example, `extracted_markers.csv`.\nThe extracted markers output file contains the following columns:\n\n['`sender_id`: taken from the trackers.', '`session_idx`: an integer indexing sessions, starting with `0`.', '`marker`: the unique marker identifier.', '`event_idx`: an integer indexing events, starting with `0`.', '`num_preceding_user_turns`: an integer indicating the number of user turns preceding the event at which the marker applied.']\n\nHere is an example of the extracted markers output file (for a marker configuration file containing two markers: `marker_mood_expressed` and `marker_cheer_up_failed`):\n\n```\nsender_id,session_idx,marker,event_idx,num_preceding_user_turns\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_mood_expressed,2,0\n4d55093e9696452c8d1157fa33fd54b2,0,marker_mood_expressed,7,1\n4d55093e9696452c8d1157fa33fd54b2,0,marker_cheer_up_failed,14,2\nc00b3de97713427d85524c4374125db1,0,marker_mood_expressed,2,0\n```\n\nEach row represents an occurrence of the marker specified under the `marker` column, for each `sender_id` and `session_idx`.",
              "Computed Statistics": "By default, the command computes summary statistics about the information gathered. To disable the statistics computation, use the optional flag `--no-stats`.\n\nThe script computes the following statistics:\n\n['**For each session and each marker**: \"per-session statistics\" which include the arithmetic mean, median, minimum, and maximum number of user turns preceding the event at which the marker applied.', '**For all sessions and for each marker**:', ['Overall statistics including the arithmetic mean, median, minimum, and maximum number of user turns preceding the event where the marker applied in any session.', 'The number of sessions and the percentage of sessions where each marker applied at least once.']]\n\nThe results are stored in a tabular format in `stats-overall.csv` and `stats-per-session.csv`. You can change prefix `stats` in the file names using the optional argument `--stats-file-prefix `.\nFor example, the following script will produce the files: `my-statistics-overall.csv` and `my-statistics-per-session.csv`:\n\n```\nrasa evaluate markers all --stats-file-prefix \"my-statistics\" extracted_markers.csv\n```\n\nThe two statistics files contain the following columns:\n\n['`sender_id`: taken from the trackers. If the statistic is computed over all sessions this will be equal to `all`.', '`session_idx`: an integer indexing sessions, starting with `0`. If the statistic is computed over all sessions, this will be equal to `nan` (not a number).', '`marker`: the unique marker identifier.', '`statistic`: a description of the statistic computed.', '`value`: an integer or float value of the computed statistic. If the statistic is not available then `value` will be equal to `nan` (not a number).']\n\nHere is a sample `stats-per-session.csv` output:\n\n```\nsender_id,session_idx,marker,statistic,value\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_cheer_up_failed,count(number of preceding user turns),0\n4d55093e9696452c8d1157fa33fd54b2,0,marker_cheer_up_failed,count(number of preceding user turns),1\nc00b3de97713427d85524c4374125db1,0,marker_cheer_up_failed,count(number of preceding user turns),0\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_cheer_up_failed,max(number of preceding user turns),nan\n4d55093e9696452c8d1157fa33fd54b2,0,marker_cheer_up_failed,max(number of preceding user turns),2\nc00b3de97713427d85524c4374125db1,0,marker_cheer_up_failed,max(number of preceding user turns),nan\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_cheer_up_failed,mean(number of preceding user turns),nan\n4d55093e9696452c8d1157fa33fd54b2,0,marker_cheer_up_failed,mean(number of preceding user turns),2.0\nc00b3de97713427d85524c4374125db1,0,marker_cheer_up_failed,mean(number of preceding user turns),nan\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_cheer_up_failed,median(number of preceding user turns),nan\n4d55093e9696452c8d1157fa33fd54b2,0,marker_cheer_up_failed,median(number of preceding user turns),2.0\nc00b3de97713427d85524c4374125db1,0,marker_cheer_up_failed,median(number of preceding user turns),nan\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_cheer_up_failed,min(number of preceding user turns),nan\n4d55093e9696452c8d1157fa33fd54b2,0,marker_cheer_up_failed,min(number of preceding user turns),2\nc00b3de97713427d85524c4374125db1,0,marker_cheer_up_failed,min(number of preceding user turns),nan\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_mood_expressed,count(number of preceding user turns),1\n4d55093e9696452c8d1157fa33fd54b2,0,marker_mood_expressed,count(number of preceding user turns),1\nc00b3de97713427d85524c4374125db1,0,marker_mood_expressed,count(number of preceding user turns),1\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_mood_expressed,max(number of preceding user turns),0\n4d55093e9696452c8d1157fa33fd54b2,0,marker_mood_expressed,max(number of preceding user turns),1\nc00b3de97713427d85524c4374125db1,0,marker_mood_expressed,max(number of preceding user turns),0\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_mood_expressed,mean(number of preceding user turns),0.0\n4d55093e9696452c8d1157fa33fd54b2,0,marker_mood_expressed,mean(number of preceding user turns),1.0\nc00b3de97713427d85524c4374125db1,0,marker_mood_expressed,mean(number of preceding user turns),0.0\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_mood_expressed,median(number of preceding user turns),0.0\n4d55093e9696452c8d1157fa33fd54b2,0,marker_mood_expressed,median(number of preceding user turns),1.0\nc00b3de97713427d85524c4374125db1,0,marker_mood_expressed,median(number of preceding user turns),0.0\n3c1afa1ed72c4116ba6670a1668f1b4a,0,marker_mood_expressed,min(number of preceding user turns),0\n4d55093e9696452c8d1157fa33fd54b2,0,marker_mood_expressed,min(number of preceding user turns),1\nc00b3de97713427d85524c4374125db1,0,marker_mood_expressed,min(number of preceding user turns),0\n```\n\nNote that the value for unavailable statistics is `nan`. For example, because `marker_cheer_up_failed` never occurred in\ntracker `3c1afa1ed72c4116ba6670a1668f1b4a` session `0`, then the `min`, `max`, `median`, and `mean` number of preceding user turns\nare equal to `nan`.\n\nHere is a sample `stats-overall.csv` output:\n\n```\nsender_id,session_idx,marker,statistic,value\nall,nan,-,total_number_of_sessions,3\nall,nan,marker_cheer_up_failed,number_of_sessions_where_marker_applied_at_least_once,1\nall,nan,marker_cheer_up_failed,percentage_of_sessions_where_marker_applied_at_least_once,33.333\nall,nan,marker_mood_expressed,number_of_sessions_where_marker_applied_at_least_once,3\nall,nan,marker_mood_expressed,percentage_of_sessions_where_marker_applied_at_least_once,100.0\nall,nan,marker_cheer_up_failed,count(number of preceding user turns),1\nall,nan,marker_cheer_up_failed,mean(number of preceding user turns),2.0\nall,nan,marker_cheer_up_failed,median(number of preceding user turns),2.0\nall,nan,marker_cheer_up_failed,min(number of preceding user turns),2\nall,nan,marker_cheer_up_failed,max(number of preceding user turns),2\nall,nan,marker_mood_expressed,count(number of preceding user turns),3\nall,nan,marker_mood_expressed,mean(number of preceding user turns),0.333\nall,nan,marker_mood_expressed,median(number of preceding user turns),0.0\nall,nan,marker_mood_expressed,min(number of preceding user turns),0\nall,nan,marker_mood_expressed,max(number of preceding user turns),1\n```\n\nNote that because each row computes a statistic over all sessions, the `sender_id` is equal to `all`,\nand the `session_idx` is equal to `nan`."
            },
            "Configuring the CLI command": "Visit our [CLI page](./command-line-interface.mdx#rasa-evaluate-markers) for more information on configuring the marker extraction and statistics computation process."
          },
          "metadata": {
            "id": "markers",
            "sidebar_label": "Markers",
            "title": "Markers",
            "description": "Find out how to mark points of interest in dialogues using Marker conditions.",
            "abstract": "Markers are conditions used to describe and mark points of interest in dialogues."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 25]"
        },
        {
          "title": "Connecting to Messaging and Voice Channels",
          "description": "Check out how to make your Rasa assistant available on platforms like Facebook Messenger, Slack, Telegram or even your very own website.",
          "content": {
            "Connecting to A Channel": "Learn how to make your assistant available on:\n\n['[Your Own Website](./connectors/your-own-website.mdx)', '[Facebook Messenger](./connectors/facebook-messenger.mdx)', '[Slack](./connectors/slack.mdx)', '[Telegram](./connectors/telegram.mdx)', '[Twilio](./connectors/twilio.mdx)', '[Microsoft Bot Framework](./connectors/microsoft-bot-framework.mdx)', '[Cisco Webex Teams](./connectors/cisco-webex-teams.mdx)', '[RocketChat](./connectors/rocketchat.mdx)', '[Mattermost](./connectors/mattermost.mdx)', '[Google Hangouts Chat](./connectors/hangouts.mdx)', '[Custom Connectors](./connectors/custom-connectors.mdx)']",
            "Testing Channels on Your Local Machine": "If you're running a Rasa server on `localhost`, \nmost external channels won't be able to find your server URL, since `localhost` is not open to the internet. \n\nTo make a port on your local machine publicly available on the internet, \nyou can use [ngrok](https://ngrok.com/). Alternatively, see this [list](https://github.com/anderspitman/awesome-tunneling)\ntracking and comparing other tunneling solutions.\n\nAfter installing ngrok, run:\n\n```\nngrok http 5005; rasa run\n```\n\nWhen you follow the instructions to make your assistant available on a channel, use the ngrok URL.\nSpecifically, wherever the instructions say to use `https://<host>:<port>/webhooks/<CHANNEL>/webhook`,\nuse `<ngrok_url>/webhooks/<CHANNEL>/webhook`, replacing `<ngrok_url>` with the randomly generated\nURL displayed in your ngrok terminal window. For example, if connecting your bot to Slack, \nyour URL should resemble `https://26e7e7744191.ngrok.io/webhooks/slack/webhook`.\n\n:::caution\nWith the free-tier of ngrok, you can run into limits on how many connections you can make per minute.\nAs of writing this, it is set to 40 connections / minute.\n\n:::\n\nAlternatively you can make your assistant listen on a specific address using the `-i` command line\noption:\n\n```\nrasa run -p 5005 -i 192.168.69.150\n```\n\nThis is particularly useful when your internet facing machines connect to backend servers using a VPN \ninterface."
          },
          "metadata": {
            "id": "messaging-and-voice-channels",
            "sidebar_label": "Connecting to a Channel",
            "title": "Connecting to Messaging and Voice Channels",
            "description": "Check out how to make your Rasa assistant available on platforms like Facebook Messenger, Slack, Telegram or even your very own website.",
            "abstract": "Rasa provides many built-in connectors to connect to common messaging and voice channels. You can also connect to your website or app with pre-configured REST channels or build your own custom connector."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 26]"
        },
        {
          "title": "Migrate From Other Tools (beta)",
          "description": null,
          "content": {
            "root": [
              "Here are a few reasons why we see developers switching from other tools to Rasa:",
              [
                "**Faster**: Runs locally - no HTTP requests or server round trips required",
                "**Customizable**: Tune models and get higher accuracy with your data set",
                "**Open source**: No risk of vendor lock-in - Rasa is under the Apache 2.0 license and you can \nuse it in commercial projects"
              ],
              "In addition, our open source tools allow developers to build contextual AI assistants and manage dialogues \nwith machine learning instead of rules - check it out in <a className=\"reference external\" href=\"http://blog.rasa.com/a-new-approach-to-conversational-software/\" target=\"_blank\">this blog post</a>.",
              "Learn how to migrate from:",
              [
                "[Google Dialogflow](./migrate-from/google-dialogflow-to-rasa.mdx)",
                "[Wit.ai](./migrate-from/facebook-wit-ai-to-rasa.mdx)",
                "[Microsoft LUIS](./migrate-from/microsoft-luis-to-rasa.mdx)",
                "[IBM Watson](./migrate-from/ibm-watson-to-rasa.mdx)"
              ]
            ]
          },
          "metadata": {
            "id": "migrate-from",
            "sidebar_label": "Migrate From (beta)",
            "title": "Migrate From Other Tools (beta)"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 27]"
        },
        {
          "title": "Version Migration Guide",
          "description": "Information about changes between major versions of chatbot framework Rasa Core\nand how you can migrate from one version to another.\n",
          "content": {
            "Rasa 3.0 to 3.1": {
              "Machine Learning Components": {
                "TensorFlow Upgrade": "Due to the TensorFlow upgrade, we can't guarantee the exact same output and hence\nmodel performance if your configuration uses `LanguageModelFeaturizer`.\nThis applies to the case where the model is re-trained with the new Rasa\nversion without changing the configuration, random seeds, and data as well as to the\ncase where a model trained with a previous version of Rasa is loaded with\nthis new version for inference.\n\nPlease check whether your trained model still performs as expected and retrain if needed."
              },
              "NLU JSON Format": "[NLU training data](nlu-training-data.mdx) in JSON format is deprecated and will be\nremoved in Rasa 4.0.\nPlease use `rasa data convert nlu -f yaml --data <path to NLU data>` to convert your\nNLU JSON data to YAML format before support for NLU JSON data is removed."
            },
            "Rasa 2.x to 3.0": {
              "Markdown Data": "Markdown is no longer supported — all the supporting code that was previously deprecated is\nnow removed, and the convertors are removed as well.\n\nThe related CLI commands `rasa data convert responses` and `rasa data convert config`\nwere removed.\n\nIf you still have training data in Markdown format then the recommended approach is to use Rasa 2.x\nto convert your data from Markdown to YAML. Please use the commands described\n[here](./migration-guide.mdx#training-data-files).",
              "Model Configuration": "It is required to specify the used `recipe` within the\n[model configuration](model-configuration.mdx). As of now Rasa only supports\nthe `default.v1` recipe and will continue using it even if you don't specify a recipe\nin the model configuration. To avoid breaking changes in the future you should\nto specify `recipe: \"default.v1\"` at the top of your model configuration:\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-config\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nlanguage: en\n\npipeline:\n  ...\npolicies:\n  ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nrecipe: default.v1\n\nlanguage: en\n\npipeline:\n  ...\npolicies:\n  ...\n```\n\n</TabItem>\n</Tabs>",
              "Custom Policies and Custom Components": {
                "Changes to Custom NLU Components": "**Inheriting from `GraphComponent`**\n\nNLU components which previously inherited from one of the following classes additionally\nneed to inherit from the\n[`GraphComponent` interface](custom-graph-components.mdx#the-graphcomponent-interface):\n\n['`SparseFeaturizer`', '`DenseFeaturizer`', '`IntentClassifier`', '`EntityExtractor`', '`Component`']\n\nThis snippet shows the required changes:\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-parent-class\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\n\nclass MyNLUComponent(SparseFeaturizer):\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom rasa.engine.graph import GraphComponent\nfrom rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\n\nclass MyNLUComponent(GraphComponent, SparseFeaturizer):\n    ...\n```\n\n</TabItem>\n</Tabs>\n\n** Inheriting from `EntityExtractorMixin` instead of `EntityExtractor`**\n\nThe `EntityExtractor` class was renamed to `EntityExtractorMixin`:\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-entity-extractor\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom rasa.nlu.extractors.extractor import EntityExtractor\n\nclass MyNLUComponent(EntityExtractor):\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom rasa.engine.graph import GraphComponent\nfrom rasa.nlu.extractors.extractor import EntityExtractorMixin\n\nclass MyNLUComponent(GraphComponent, EntityExtractorMixin):\n    ...\n```\n\n</TabItem>\n</Tabs>\n\n**Instantiating a NLU Component for Training**\n\nNLU components are no longer instantiated via their constructor. Instead, all NLU\ncomponents have to override the `create` method of the\n[`GraphComponent` interface](custom-graph-components.mdx#the-graphcomponent-interface). The\npassed in configuration is your NLU component's default configuration including any updates\nfrom your model configuration file.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-init\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom typing import Optional, Dict, Text, Any\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\n\nclass MyNLUComponent(IntentClassifier):\n\n  def __init__(self, component_config: Optional[Dict[Text, Any]] = None) -> None:\n      super().__init__(component_config)\n      ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import Dict, Text, Any\n\nfrom rasa.engine.graph import GraphComponent, ExecutionContext\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\n\nclass MyNLUComponent(GraphComponent, IntentClassifier):\n\n  def __init__(self, component_config: Dict[Text, Any]) -> None:\n      self.component_config = component_config\n    ...\n\n  @classmethod\n  def create(\n      cls,\n      config: Dict[Text, Any],\n      model_storage: ModelStorage,\n      resource: Resource,\n      execution_context: ExecutionContext,\n  ) -> GraphComponent:\n      return cls(config)\n```\n\n</TabItem>\n</Tabs>\n\n**Persisting a Trained NLU Component**\n\nNLU components used to be persisted by a call to the NLU component's `persist` method\nfrom outside the NLU component itself.\nWith Rasa 3.0 NLU components are responsible for persisting themselves.\nUse the provided `model_storage` and `resource` parameters\nto persist your NLU component at the end of the training and then return the `resource`\nas result of your NLU component's `train` method.\nSee [component persistence](custom-graph-components.mdx#model-persistence) for more details.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-persistence\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom pathlib import Path\nfrom typing import Optional, Any, Text, Dict\n\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.nlu.config import RasaNLUModelConfig\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\nclass MyNLUComponent(IntentClassifier):\n    def train(\n        self,\n        training_data: TrainingData,\n        config: Optional[RasaNLUModelConfig] = None,\n        **kwargs: Any,\n    ) -> None:\n        ...\n\n    def persist(self, file_name: Text, model_dir: Text) -> Dict[Text, Any]:\n        file_path = Path(model_dir) / \"{file_name}.model_data.json\"\n        rasa.shared.utils.io.create_directory_for_file(file_path)\n        rasa.shared.utils.io.dump_obj_as_json_to_file(file_path,\n            self.get_model_data())\n\n        return {\"file\": file_name}\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom rasa.engine.graph import GraphComponent\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\nclass MyNLUComponent(GraphComponent, IntentClassifier):\n    def train(self, training_data: TrainingData) -> Resource:\n        ...\n        self.persist()\n        return self._resource\n\n    def persist(self) -> None:\n        with self._model_storage.write_to(self._resource) as directory:\n            model_data_file = directory / \"model_data.json\"\n            rasa.shared.utils.io.dump_obj_as_json_to_file(model_data_file,\n                self.get_model_data())\n    ...\n```\n\n</TabItem>\n</Tabs>\n\n**Instantiating a Trained NLU Component**\n\nPreviously NLU components had to persist their own configuration. Now the config passed\ninto `load` will automatically contain the configuration which your model was trained with.\nTo instantiate a persisted NLU component, you need to use `model_storage` and `resource` in your NLU component's\n`load` method.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-loading\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nimport json\nfrom pathlib import Path\nfrom typing import Text, Dict, Any, Optional\n\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.nlu.model import Metadata\n\nclass MyNLUComponent(IntentClassifier):\n    @classmethod\n    def load(\n        cls,\n        meta: Dict[Text, Any],\n        model_dir: Text,\n        model_metadata: Metadata = None,\n        cached_component: Optional[\"DIETClassifier\"] = None,\n        should_finetune: bool = False,\n        **kwargs: Any,\n    ) -> \"MyNLUComponent\":\n        file_name = meta.get(\"file\")\n        file_path = Path(model_dir) / \"{file_name}.model_data.json\"\n        model_data = json.loads(rasa.shared.utils.io.read_file(file_path))\n\n        return cls(model_data)\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom __future__ import annotations\nimport json\nfrom typing import Any, Text, Dict\n\nfrom rasa.engine.graph import GraphComponent, ExecutionContext\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.shared.exceptions import FileIOException\n\nclass MyNLUComponent(GraphComponent, IntentClassifier):\n    @classmethod\n    def load(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        **kwargs: Any,\n    ) -> MyNLUComponent:\n        model_data = {}\n\n        try:\n            with model_storage.read_from(resource) as path:\n\n                model_data_file = path / \"model_data.json\"\n                model_data = json.loads(rasa.shared.utils.io.read_file(model_data_file))\n\n        except (ValueError, FileNotFoundError, FileIOException):\n            logger.debug(\n                f\"Couldn't load metadata for component '{cls.__name__}' as the persisted \"\n                f\"model data couldn't be loaded.\"\n            )\n\n        return cls(\n            config, model_data=model_data\n        )\n```\n\n</TabItem>\n</Tabs>\n\n**Providing a Default Configuration for an NLU Component**\n\nThe default configuration is no longer a static class property but instead returned\nby the static method `get_default_config`:\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-default-config\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\n\nclass MyNLUComponent(IntentClassifier):\n    ...\n    defaults = {\"key1\": \"value1\"}\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import Text, Any, Dict\n\nfrom rasa.engine.graph import GraphComponent\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\n\nclass MyNLUComponent(GraphComponent, IntentClassifier):\n    ...\n    @staticmethod\n    def get_default_config() -> Dict[Text, Any]:\n        return {\"key1\": \"value1\"}\n\n```\n\n</TabItem>\n</Tabs>\n\n**Augmenting Training Data in an NLU Component**\n\nNLU Components like [tokenizers](components.mdx#tokenizers) or\n[featurizers](components.mdx#featurizers) augment the training data with their\noutput during the model training. Their output is required by NLU components later in the\npipeline. Typically, featurizers require *tokenized* messages and intent\nclassifiers require *featurized* training data to train themselves. Rasa\n3.0 makes these different purposes explicit. Previously both NLU component training and\ntraining data augmentation were done as part of the `train` method. In Rasa\n3.0 they are split into `train` and `process_training_data`:\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-component-train-process-split\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom typing import Optional, Any\n\nfrom rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\nfrom rasa.nlu.config import RasaNLUModelConfig\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\nclass MyNLUComponent(SparseFeaturizer):\n    def train(\n        self,\n        training_data: TrainingData,\n        config: Optional[RasaNLUModelConfig] = None,\n        **kwargs: Any,\n    ) -> None:\n        self.train_featurizer(training_data)\n\n        for message in training_data.training_examples:\n            self.add_features(message)\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom rasa.engine.graph import GraphComponent\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\nclass MyNLUComponent(GraphComponent, SparseFeaturizer):\n    def train(self, training_data: TrainingData) -> Resource:\n        self.train_featurizer(training_data)\n\n        self.persist()\n        return self._resource\n\n    def process_training_data(self, training_data: TrainingData) -> TrainingData:\n        for message in training_data.training_examples:\n            self.add_features(message)\n\n        return training_data\n```\n\n</TabItem>\n</Tabs>\n\n**Handling Lists of Messages During Inference in an NLU Component**\n\nNLU components used to receive a single `Message` object during inference.\nStarting with Rasa 3.0 all NLU components have to support a list of\nmessages during inference. Unless your component supports batch predictions the easiest\nway to handle this is to loop over the messages. It is also required to return the\nmessage objects at the end of the `process` method.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-component-process-messages-split\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom typing import Any\n\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.shared.nlu.training_data.message import Message\n\nclass MyNLUComponent(IntentClassifier):\n    def process(self, message: Message, **kwargs: Any) -> None:\n      self.predict(message)\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import List\n\nfrom rasa.engine.graph import GraphComponent\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.shared.nlu.training_data.message import Message\n\nclass MyNLUComponent(GraphComponent, IntentClassifier):\n    def process(self, messages: List[Message]) -> List[Message]:\n        for message in messages:\n            self.predict(message)\n\n        return messages\n```\n\n</TabItem>\n</Tabs>\n\n**Registering your NLU Component**\n\nBefore you can use your custom NLU component you have to register your NLU component using the\n`DefaultV1Recipe.register` decorator. The NLU component types correspond to the existing\nparent classes:\n\n['`Tokenizer`: `ComponentType.MESSAGE_TOKENIZER`', '`SparseFeaturizer` / `DenseFeaturizer`: `ComponentType.MESSAGE_FEATURIZER`', '`IntentClassifier`: `ComponentType.INTENT_CLASSIFIER`', '`EntityExtractor`: `ComponentType.ENTITY_EXTRACTOR`', 'If your NLU component provides a pretrained model which should be used by other\\nNLU components during training and inference use `ComponentType.MODEL_LOADER`']\n\nSpecify `is_trainable=True` if the `train` method of your component should be called\nduring training.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-register\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\n\nclass MyNLUComponent(IntentClassifier):\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.engine.graph import GraphComponent\n\n@DefaultV1Recipe.register(\n    DefaultV1Recipe.ComponentType.INTENT_CLASSIFIER, is_trainable=True\n)\nclass MyNLUComponent(GraphComponent, IntentClassifier):\n    ...\n```\n\n</TabItem>\n</Tabs>\n\n**Using a Model Provider with your NLU Component**\n\nIf your NLU component requires a pretrained model such as a [Spacy](components.mdx#spacynlp) or\n[Mitie](components.mdx#mitienlp) language model you have to specify the NLU component which\nprovides this model in your model's pipeline before the NLU component which requires\nthe model. In addition to this you now also need to specify the model loading component in the `model_from`\nparameter in the `register` decorator. The model will then be passed to your model's\n`train`, `process_training_data` and `process` methods:\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-nlu-model-provider\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom typing import Optional, Any\n\nfrom rasa.nlu.config import RasaNLUModelConfig\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.shared.nlu.training_data.message import Message\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\nclass MyNLUComponent(IntentClassifier):\n    def train(\n        self,\n        training_data: TrainingData,\n        cfg: Optional[RasaNLUModelConfig] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Train the featurizer.\"\"\"\n        spacy_nlp = kwargs.get(\"spacy_nlp\")\n        ...\n\n    def process(self, message: Message, **kwargs: Any) -> None:\n        spacy_nlp = kwargs.get(\"spacy_nlp\", None)\n        ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import List\n\nfrom rasa.engine.graph import GraphComponent\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nfrom rasa.nlu.utils.spacy_utils import SpacyModel\nfrom rasa.shared.nlu.training_data.message import Message\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\n@DefaultV1Recipe.register(\n    DefaultV1Recipe.ComponentType.INTENT_CLASSIFIER, is_trainable=True, model_from=\"SpacyNLP\"\n)\nclass MyNLUComponent(GraphComponent, IntentClassifier):\n    def train(\n        self, training_data: TrainingData, model: SpacyModel) -> Resource:\n        spacy_nlp = model.model\n        ...\n\n    def process(self, messages: List[Message], model: SpacyModel) -> List[Message]:\n        spacy_nlp = model.model\n        ...\n```\n\n</TabItem>\n</Tabs>",
                "Changes to Custom Policies": "This guide leads you through the migration of a custom policy step by step.\n\n**Instantiating a Policy for Training**\n\nPolicies are no longer instantiated via their constructor. Instead, all policies have\nto implement a `create` method. During the policy instantiation the configuration from\nthe [model configuration](model-configuration.mdx) is passed in as a dictionary instead\nof as separate parameters. Similarly, the`featurizers` are no longer instantiated\noutside of policies.\nInstead, the super class `rasa.core.policies.policy.Policy` instantiates the\nfeaturizers itself.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-policy-init\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom typing import Optional, Any\n\nfrom rasa.core.constants import DEFAULT_POLICY_PRIORITY\nfrom rasa.core.featurizers.tracker_featurizers import TrackerFeaturizer\nfrom rasa.core.policies.policy import Policy\n\nclass MyPolicy(Policy):\n    def __init__(\n        self,\n        featurizer: Optional[TrackerFeaturizer] = None,\n        priority: int = DEFAULT_POLICY_PRIORITY,\n        max_history: Optional[int] = None,\n        **kwargs: Any\n    ) -> None:\n        super().__init__(featurizer, priority, **kwargs)\n        ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import Optional, Dict, Text, Any\n\nfrom rasa.core.featurizers.tracker_featurizers import TrackerFeaturizer\nfrom rasa.core.policies.policy import Policy\nfrom rasa.engine.graph import ExecutionContext\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\n\nclass MyPolicy(Policy):\n    def __init__(\n        self,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        featurizer: Optional[TrackerFeaturizer] = None,\n    ) -> None:\n        super().__init__(\n          config, model_storage, resource, execution_context, featurizer\n        )\n        ...\n    ...\n\n  @classmethod\n  def create(\n      cls,\n      config: Dict[Text, Any],\n      model_storage: ModelStorage,\n      resource: Resource,\n      execution_context: ExecutionContext,\n  ) -> MyPolicy:\n      return cls(config, model_storage, resource, execution_context)\n\n```\n\n</TabItem>\n</Tabs>\n\n**Persisting a Trained Policy**\n\nPolicies used to be persisted by a call to the policy's `persist` method from outside the policy itself.\nWith Rasa 3.0 policies are responsible for persisting themselves.\nUse the provided `model_storage` and `resource` parameters\nto persist your graph component at the end of the training and then return the `resource`\nas result of your policy's `train` method. See [graph component persistence](custom-graph-components.mdx#model-persistence) for more details.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-policy-persistence\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom pathlib import Path\nfrom typing import List, Any, Union, Text\n\nfrom rasa.core.policies.policy import Policy\nfrom rasa.shared.core.domain import Domain\nfrom rasa.shared.core.generator import TrackerWithCachedStates\nfrom rasa.shared.nlu.interpreter import NaturalLanguageInterpreter\n\nclass MyPolicy(Policy):\n    def train(\n        self,\n        training_trackers: List[TrackerWithCachedStates],\n        domain: Domain,\n        interpreter: NaturalLanguageInterpreter,\n        **kwargs: Any,\n    ) -> None:\n        ...\n\n    def persist(self, path: Union[Text, Path]) -> None:\n        if self.featurizer is not None:\n            self.featurizer.persist(path)\n\n        file_path = Path(path) / \"model_data.json\"\n        rasa.shared.utils.io.create_directory_for_file(file_path)\n        rasa.shared.utils.io.dump_obj_as_json_to_file(file_path,\n            self.get_model_data())\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import List, Any\n\nfrom rasa.core.policies.policy import Policy\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.shared.core.domain import Domain\nfrom rasa.shared.core.generator import TrackerWithCachedStates\n\nclass MyPolicy(Policy):\n    def train(\n        self,\n        training_trackers: List[TrackerWithCachedStates],\n        domain: Domain,\n        **kwargs: Any,\n    ) -> Resource:\n        ...\n        self.persist()\n        return self._resource\n\n    def persist(self) -> None:\n        with self._model_storage.write_to(self._resource) as directory:\n            if self.featurizer is not None:\n                self.featurizer.persist(directory)\n\n            file_path = directory / \"model_data.json\"\n            rasa.shared.utils.io.dump_obj_as_json_to_file(file_path,\n                self.get_model_data())\n    ...\n```\n\n</TabItem>\n</Tabs>\n\n**Instantiating a Trained Policy**\n\nPreviously policies had to persist their own configuration. Now the config passed\ninto `load` will automatically contain the configuration which your model was trained with.\n\nTo instantiate a persisted policy, you need to use `model_storage` and `resource` in your policy's\n`load` method.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-policy-loading\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nimport json\nfrom pathlib import Path\nfrom types import Union\nfrom typing import Text, Any\n\nfrom rasa.core.policies.policy import Policy\n\nclass MyPolicy(Policy):\n    @classmethod\n    def load(cls, path: Union[Text, Path], **kwargs: Any) -> \"Policy\":\n        featurizer = None\n        if (Path(path) / FEATURIZER_FILE).is_file():\n            featurizer = TrackerFeaturizer.load(path)\n\n        model_data = {}\n        model_data_file = Path(path) / \"model_data.json\"\n        if metadata_file.is_file():\n            model_data = json.loads(rasa.shared.utils.io.read_file(model_data_file))\n\n        return cls(model_data, featurizer)\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nimport json\nfrom typing import Dict, Text, Any\n\nfrom rasa.core.featurizers.tracker_featurizers import TrackerFeaturizer\nfrom rasa.core.policies.policy import Policy\nfrom rasa.engine.graph import ExecutionContext\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\nfrom rasa.shared.exceptions import FileIOException\n\nclass MyPolicy(Policy):\n    @classmethod\n    def load(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        **kwargs: Any,\n    ) -> MyPolicy:\n        featurizer = None\n        model_data = {}\n\n        try:\n            with model_storage.read_from(resource) as path:\n                if (Path(path) / FEATURIZER_FILE).is_file():\n                    featurizer = TrackerFeaturizer.load(path)\n\n                model_data_file = path / \"model_data.json\"\n                model_data = json.loads(rasa.shared.utils.io.read_file(model_data_file))\n\n        except (ValueError, FileNotFoundError, FileIOException):\n            logger.debug(\n                f\"Couldn't load metadata for policy '{cls.__name__}' as the persisted \"\n                f\"metadata couldn't be loaded.\"\n            )\n\n        return cls(\n            config, model_storage, resource, execution_context,\n            featurizer=featurizer, model_data=model_data\n        )\n```\n\n</TabItem>\n</Tabs>\n\n**Providing a Default Configuration for a Policy**\n\nThe default configuration is no longer provided via default values in your policy's\nconstructor but instead returned by the static method `get_default_config`:\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-policy-default-config\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom typing import Text\nfrom rasa.core.policies.policy import Policy\n\nclass MyPolicy(Policy):\n\n    def __init__(key1: Text = \"value1\") -> None:\n        ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import Dict, Text, Any\nfrom rasa.core.policies.policy import Policy\n\nclass MyPolicy(Policy):\n\n    def __init__(self, config: Dict[Text, Any]) -> None:\n        ...\n\n    @staticmethod\n    def get_default_config() -> Dict[Text, Any]:\n        return {\"key1\": \"value1\"}\n```\n\n</TabItem>\n</Tabs>\n\n**Using End-To-End Features in a Policy**\n\nTo use a custom [end-to-end policy](stories.mdx#end-to-end-training) in Rasa\nOpen Source 2, you had to use the `interpreter` parameter to featurize the tracker\nevents manually. In Rasa 3.0,\nyou need to [register](custom-graph-components.mdx#registering-graph-components-with-the-model-configuration) a policy that requires end-to-end features with type `ComponentType.POLICY_WITH_END_TO_END_SUPPORT`. The features\nwill be precomputed and passed into your policy during training and inference.\n\n:::caution\nEnd-To-End features will only be computed and provided to your policy if your training\ndata actually contains [end-to-end training data](stories.mdx#end-to-end-training).\n:::\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-policy-end-to-end-features\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom typing import List, Any\n\nfrom rasa.core.policies.policy import Policy, PolicyPrediction\nfrom rasa.shared.core.domain import Domain\nfrom rasa.shared.core.generator import TrackerWithCachedStates\nfrom rasa.shared.core.trackers import DialogueStateTracker\nfrom rasa.shared.nlu.interpreter import NaturalLanguageInterpreter\n\nclass MyPolicy(Policy):\n    def train(\n        self,\n        training_trackers: List[TrackerWithCachedStates],\n        domain: Domain,\n        interpreter: NaturalLanguageInterpreter,\n        **kwargs: Any,\n    ) -> None:\n        ...\n        model_data, label_ids = self._prepare_for_training(\n            training_trackers, domain, interpreter, **kwargs\n        )\n        ...\n\n    def predict_action_probabilities(\n        self,\n        tracker: DialogueStateTracker,\n        domain: Domain,\n        interpreter: NaturalLanguageInterpreter,\n        **kwargs: Any,\n    ) -> PolicyPrediction:\n        ...\n        tracker_state_features = self._featurize_tracker_for_e2e(\n            tracker, domain, interpreter\n        )\n        ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import List, Optional, Dict, Text, Any\n\nfrom rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization\nfrom rasa.core.policies.policy import PolicyPrediction, Policy\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.shared.core.domain import Domain\nfrom rasa.shared.core.generator import TrackerWithCachedStates\nfrom rasa.shared.core.trackers import DialogueStateTracker\n\n@DefaultV1Recipe.register(\n    DefaultV1Recipe.ComponentType.POLICY_WITH_END_TO_END_SUPPORT, is_trainable=True\n)\nclass MyPolicy(Policy):\n    def train(\n        self,\n        training_trackers: List[TrackerWithCachedStates],\n        domain: Domain,\n        precomputations: Optional[MessageContainerForCoreFeaturization] = None,\n    ) -> Resource:\n        ...\n        model_data, label_ids = self._prepare_for_training(\n          training_trackers, domain, precomputations,\n        )\n        ...\n\n    def predict_action_probabilities(\n        self,\n        tracker: DialogueStateTracker,\n        domain: Domain,\n        precomputations: Optional[MessageContainerForCoreFeaturization] = None,\n        rule_only_data: Optional[Dict[Text, Any]] = None,\n        **kwargs: Any,\n    ) -> PolicyPrediction:\n        ...\n        tracker_state_features = self._featurize_tracker(\n            tracker, domain, precomputations, rule_only_data=rule_only_data\n        )\n        ...\n```\n\n</TabItem>\n</Tabs>\n\n**Registering a Policy**\n\nBefore you can use your custom policy you have to register your policy using the\n`DefaultV1Recipe.register` decorator. If your policy requires end-to-end features\nspecify the graph component type `POLICY_WITH_END_TO_END_SUPPORT`. Otherwise, use\n`POLICY_WITHOUT_END_TO_END_SUPPORT`. Specify `is_trainable=True` if the `train`\nmethod of your policy should be called during the training. If your policy is only\nused during inference use `is_trainable=False`.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-policy-register\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom rasa.core.policies.policy import Policy\n\nclass MyPolicy(Policy):\n    ...\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom rasa.core.policies.policy import Policy\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\n\n@DefaultV1Recipe.register(\n    DefaultV1Recipe.ComponentType.POLICY_WITH_END_TO_END_SUPPORT,\n    is_trainable=True\n)\nclass MyPolicy(Policy):\n    ...\n```\n\n</TabItem>\n</Tabs>\n\n**Providing Rule-only Data to a Policy**\n\nRasa allows excluding [forms](forms.mdx) or [slots](domain.mdx#slots) which\nare completely handled by\n[rules](rules.mdx) from becoming features in other policies.\nIn Rasa 2 this information was passed onto the\npolicies using the `set_shared_policy_states` method which set the policy attribute\n`_rule_only_data`. Rasa passes the names of rule-only slots and forms via the\n`predict_action_probabilities` method. The passed `rule_only_data` can be `None`\nin case the [`RulePolicy`](policies.mdx#rule-policy) is not part of your model\nconfiguration.\n\n<Tabs values={[{\"label\": \"Rasa 2.0 (old)\", \"value\": \"old\"}, {\"label\": \"Rasa 3.0 (new)\", \"value\": \"new\"}]}  groupId=\"3-0-policy-rule-only-data\" defaultValue=\"new\">\n<TabItem value=\"old\">\n\n```\nfrom rasa.core.policies.policy import Policy\nfrom typing import Any\n\nclass MyPolicy(Policy):\n\n    def set_shared_policy_states(self, **kwargs: Any) -> None:\n        \"\"\"Sets policy's shared states for correct featurization.\"\"\"\n        self._rule_only_data = kwargs.get(\"rule_only_data\", {})\n```\n\n</TabItem>\n<TabItem value=\"new\">\n\n```\nfrom typing import Optional, Dict, Text, Any\n\nfrom rasa.core.policies.policy import Policy, PolicyPrediction\nfrom rasa.shared.core.domain import Domain\nfrom rasa.shared.core.trackers import DialogueStateTracker\n\nclass MyPolicy(Policy):\n    def predict_action_probabilities(\n        self,\n        tracker: DialogueStateTracker,\n        domain: Domain,\n        rule_only_data: Optional[Dict[Text, Any]] = None,\n  ) -> PolicyPrediction:\n      ...\n```\n\n</TabItem>\n</Tabs>"
              },
              "Training data": {
                "Upgrading `version` from `2.0` to `3.0`": "At the top of your training data files, you need to change `version: \"2.0\"` to `version: \"3.1\"`.\n\nWe follow semantic versioning for training data versions. This means breaking changes result in a new\nmajor version, while new features result in a new minor version. The latest training data version is 3.1.\n\nThe improvements to `slot mappings` in Rasa 3.0 were breaking changes, so we needed to upgrade\nfrom major version `2.0` to major version `3.0`.",
                "`TrainingDataImporter`": "`TrainingDataImporter` and all its implementations are updated to contain only synchronous methods.\nIf you have a custom data importer or rely on some functions provided by `TrainingDataImporter`, you need\nto update your implementation and function calls.\n\nFor example, this is how data loading should look like in Rasa 3.0:\n\n```\nfrom typing import Text\nfrom rasa.shared.importers.importer import TrainingDataImporter\n\ndef load_data(domain_path: Text, config_path: Text):\n    file_importer = TrainingDataImporter.load_from_config(\n        config_path, domain_path\n    )\n    # note that all the functions below were async before:\n    config = file_importer.get_config()\n    domain = file_importer.get_domain()\n    stories = file_importer.get_stories()\n    nlu_data = file_importer.get_nlu_data()\n```\n\nSince any custom importer implements `TrainingDataImporter`, you should update your custom\nimporter to contain only sync methods as well:\n\n```\nfrom typing import Dict\n\nfrom rasa.shared.core.domain import Domain\nfrom rasa.shared.importers.importer import TrainingDataImporter\n\n\nclass MyImporter(TrainingDataImporter):\n    \"\"\"Example partial implementation of a custom importer component.\"\"\"\n\n    # this function was async before\n    def get_domain(self) -> Domain:\n        pass\n\n    # this function was also async before\n    def get_config(self) -> Dict:\n        pass\n\n    # ...\n```\n\n`template_variables` and `e2e` arguments also got removed from `get_stories` method of `TrainingDataImporter`.\nIts new signature looks this way:\n\n```\nfrom typing import Optional\n\nfrom rasa.shared.nlu.interpreter import RegexInterpreter\nfrom rasa.shared.core.training_data.structures import StoryGraph\n\nclass TrainingDataImporter:\n    # ...\n\n    def get_stories(\n        self,\n        interpreter: \"NaturalLanguageInterpreter\" = RegexInterpreter(),\n        exclusion_percentage: Optional[int] = None,\n    ) -> StoryGraph:\n        pass\n\n    # ...\n```"
              },
              "Training": {
                "`rasa train --dry-run`": "Due to changes in the model architecture the behavior of `rasa train --dry-run` changed.\nThe exit codes now have the following meaning:\n\n['`0` means that the model does not require an expensive retraining. However, the\\nresponses might still require updating by running `rasa train`', '`1` means that one or multiple components require to be retrained.', '`8` means that the `--force` flag was used and hence any cached results are ignored\\nand the entire model is retrained.']"
              },
              "Machine Learning Components": {
                "Normalization of Confidences in `DIETClassifier` and `ResponseSelector`": "`DIETClassifier` and `ResponseSelector` will no longer automatically report\nre-normalized confidences when `ranking_length` is set to a value greater than `0`.\nThis change affects the reported confidences but does not influence the final\npredicted intent, which might be used by policies.\nHowever, since the reported confidences are affected you might have to tune the\nthresholds for fallback mechanisms again.\nThe previous behavior can still be enforced by setting `renormalize_confidences=True`\nwhen using `model_confidence=softmax`.",
                "Normalization of confidences in `TEDPolicy`": "Predictions of `TEDPolicy` will no longer be modified by masking and renormalizing\nconfidences. This change can affect the maximum confidence predicted by the\n`TEDPolicy` and thereby affect the final result of the policy ensemble.\nHowever, the previous behavior can still be enforced by setting\n`ranking_length=10` and `renormalize_confidences=True`.",
                "Removed Policies": "Several dialogue policies that were deprecated in Rasa 2.x have been removed in Rasa 3.0.\nIf you are migrating a config file with a removed policy,\nconsult the following migration guides for the individual policies:\n\n['`FallbackPolicy` [migration guide](#manually-migrating-from-the-fallback-policy)', '`TwoStageFallbackPolicy` [migration guide](#manually-migrating-from-the-two-stage-fallback-policy)', '`MappingPolicy` [migration guide](#manually-migrating-from-the-mapping-policy)', '`FormPolicy` [migration guide](#forms)', '`SklearnPolicy` should be replaced with [TEDPolicy](/policies#ted-policy).\\nIt is recommended to use the [default TEDPolicy config](/model-configuration#suggested-config) as a starting point.']",
                "Removed Tokenizers and Featurizers": "The `ConveRTTokenizer`, `LanguageModelTokenizer`, and `HFTransformersNLP` featurizer\ncomponents were deprecated in Rasa 2.x and have been removed in Rasa 3.0. See the\n[migration guide for Rasa 2.x](#deprecations-2) for replacing these components in your pipeline."
              },
              "Slot Mappings": {
                "Automatic migration from 2.0 domain format to the 3.0 format": "The only data file that has changed in format is the domain file.\nTo migrate automatically to the 3.0 domain format, you can run the following command:\n\n```\nrasa data migrate -d DOMAIN --out OUT_PATH\n```\n\nIn addition to creating a valid 3.0 domain in the indicated out path, this command will automatically backup your\noriginal domain file(s) in a file labeled `original_domain.yml` or `original_domain` directory if a directory was\nprovided instead.\n\nTo maintain the behavior of forms in the 2.0 format, all migrated slot mappings will include mapping conditions for\neach form. This can be changed manually according to your use case.\nSee the docs on [mapping conditions](./domain.mdx#mapping-conditions) for more information.",
                "Manually migrating from 2.0 domain format to the 3.0 format": "Each slot in the `slots` section of the domain will need a new key `mappings`.\nThis key is a list of mappings moved from forms, while the `required_slots` field collapses to a list of slot names.\n\nLet's consider the following 2.0 domain file:\n\n```\nentities:\n - cuisine\n - number\nslots:\n   cuisine:\n     type: text\n   num_people:\n     type: float\n   outdoor_seating:\n     type: bool\nforms:\n   restaurant_form:\n     required_slots:\n         cuisine:\n           - type: from_entity\n             entity: cuisine\n         num_people:\n           - type: from_entity\n             entity: number\n         outdoor_seating:\n           - type: from_intent\n             intent: affirm\n             value: true\n           - type: from_intent\n             intent: deny\n             value: false\n```\n\nThe initial result of migrating this domain to 3.0 format would look like this:\n\n```\nentities:\n - cuisine\n - number\nslots:\n   cuisine:\n     type: text\n     mappings:\n     - type: from_entity\n       entity: cuisine\n   num_people:\n     type: float\n     mappings:\n     - type: from_entity\n       entity: number\n   outdoor_seating:\n     type: bool\n     mappings:\n     - type: from_intent\n       intent: affirm\n       value: true\n     - type: from_intent\n       intent: deny\n       value: false\nforms:\n   restaurant_form:\n     required_slots:\n     - cuisine\n     - num_people\n     - outdoor_seating\n```\n\nFor slots that should be filled only in the context of a form, add [mapping conditions](./domain.mdx#mapping-conditions)\nto specify which form(s) should be active, as well as indicate if the `requested_slot` should be the same slot.\nAdding `conditions` is required to preserve the behavior of slot mappings from 2.0, since without them\nthe mappings will be applied on each user turn regardless of whether a form is active or not.\n\n```\n slots:\n   outdoor_seating:\n     type: bool\n     mappings:\n     - type: from_intent\n       intent: affirm\n       value: true\n       conditions:\n       - active_loop: restaurant_form\n         requested_slot: outdoor_seating\n     - type: from_intent\n       intent: deny\n       value: false\n       conditions:\n       - active_loop: restaurant_form\n         requested_slot: outdoor_seating\n```",
                "Rasa-SDK Modifications": "If you have used `FormValidationAction` to define custom extraction and validation code in which you override the\n`required_slots` method, note that `slots_mapped_in_domain` argument has been replaced by the `domain_slots` argument.\nYou must make this replacement to continue using your custom code.\n\nIf you have been dynamically filling slots not present in the form's `required_slots` defined in the `domain.yml`\nfile, note that this behaviour is no longer supported in 3.x. Any dynamic slots with custom mappings, which are set in\nthe last user turn, will be filled **only if** they are returned by the `required_slots` method of the custom action\ninheriting from `FormValidationAction`. To maintain the 2.x behaviour, you must now override the `required_slots` method\nof this custom action as per the strong recommendation listed in the [dynamic form documentation](./forms.mdx#dynamic-form-behavior).\n\nTo extract custom slots that are not defined in any form's `required_slots`, you should now use a global [custom slot mapping](./domain.mdx#custom-slot-mappings)\nand extend the [ValidationAction class](./action-server/validation-action.mdx#validationaction-class).\n\n:::note\nIf you have custom validation actions extending `FormValidationAction` which override `required_slots` method, you should\ndouble-check the dynamic form behavior of your migrated assistant. Slots set by the default action\n[`action_extract_slots`](./default-actions.mdx#action_extract_slots) may need to be reset within the context of your\nform by the custom validation actions for the form's required slots. For example, if your form dynamically adds a required\nslot after the first slot is filled, you may want to reset the potential required slot as part of the first required slot's\nvalidation method to ensure it will be empty when added.\n\n:::"
              }
            },
            "Rasa 2.7 to 2.8": {
              "Deprecations": {
                "Tracker Featurizers": "`training_states_actions_and_entities` method of `TrackerFeaturizer`, `FullDialogueTrackerFeaturizer` and\n`MaxHistoryTrackerFeaturizer` classes is deprecated and will be removed in Rasa 3.0 .\nIf you had a custom tracker featurizer which relied on this method from any of the above classes, please use\n`training_states_labels_and_entities` instead.\n\n`training_states_and_actions` method of `TrackerFeaturizer`, `FullDialogueTrackerFeaturizer` and\n`MaxHistoryTrackerFeaturizer` classes is deprecated and will be removed in Rasa 3.0 .\nIf you had a custom tracker featurizer which relied on this method from any of the above classes, please use\n`training_states_and_labels` instead.",
                "State Featurizer": "`encode_all_actions` method of `SingleStateFeaturizer` class is deprecated and will be removed in Rasa 3.0 .\nIt is recommended to use the method `encode_all_labels` instead."
              },
              "Incremental Training": "Users don't need to specify an additional buffer size for sparse featurizers anymore during incremental training.\n\nSpace for new sparse features are created dynamically inside the downstream machine learning\nmodels - `DIETClassifier`, `ResponseSelector`. In other words, no extra buffer is created in\nadvance for additional vocabulary items and space will be dynamically allocated for them inside the model.\n\nThis means there's no need to specify `additional_vocabulary_size` for\n[`CountVectorsFeaturizer`](./components.mdx#countvectorsfeaturizer) or\n`number_additional_patterns` for [`RegexFeaturizer`](./components.mdx#regexfeaturizer).\nThese parameters are now deprecated.\n\n**Before**\n\n```\npipeline:\n  - name: \"WhitespaceTokenizer\"\n  - name: \"RegexFeaturizer\"\n    number_additional_patterns: 100\n  - name: \"CountVectorsFeaturizer\"\n    additional_vocabulary_size: {text: 100, response: 20}\n```\n\n**Now**\n\n```\npipeline:\n  - name: \"WhitespaceTokenizer\"\n  - name: \"RegexFeaturizer\"\n  - name: \"CountVectorsFeaturizer\"\n```",
              "Machine Learning Components": "The option `model_confidence=linear_norm` is deprecated and will be removed in Rasa `3.0.0`.\n\nRasa `2.3.0` introduced `linear_norm` as a possible value for `model_confidence`\nparameter in machine learning components such as `DIETClassifier`, `ResponseSelector` and `TEDPolicy`.\nBased on user feedback, we have identified multiple problems with this option.\nTherefore, `model_confidence=linear_norm` is now deprecated and\nwill be removed in Rasa `3.0.0`. If you were using `model_confidence=linear_norm` for any of the mentioned components,\nwe recommend to revert it back to `model_confidence=softmax` and re-train the assistant. After re-training,\nwe also recommend to [re-tune the thresholds for fallback components](./fallback-handoff.mdx#fallbacks)."
            },
            "Rasa 2.5 to 2.6": {
              "Forms": {
                "New `ignored_intents` parameter in Forms": "There is a new parameter under Forms called `ignored_intents`. This parameter\ncan be used to prevent any required slots in a form from being filled with the specified\nintent or intents. Please see the [Forms documentation](forms.mdx) for examples and more\ninformation on how to use it in your `domain.yml` file.\n\nBefore, if a user did not want to fill any slots of a form with a specified intent\nthey would have to define it under the `not_intent` parameter for every slot mapping\nas shown in the following example :\n\n```\nforms:\n  restaurant_form:\n      cuisine:\n      - type: from_entity\n        entity: cuisine\n        not_intent: chitchat\n      num_people:\n      - type: from_entity\n        entity: number\n        intent: [inform, request_restaurant]\n        not_intent: chitchat\n      feedback:\n      - type: from_entity\n        entity: feedback\n        not_intent: chitchat\n```\n\nBy introducing the `ignored_intents` parameter, we now only need to define it\nin one place and it will affect all the slots of the form :\n\n```\nforms:\n  restaurant_form:\n    ignored_intents: chitchat\n    required_slots:\n      cuisine:\n      - type: from_entity\n        entity: cuisine\n      num_people:\n      - type: from_entity\n        entity: number\n        intent: [inform, request_restaurant]\n      feedback:\n      - type: from_entity\n        entity: feedback\n      - type: from_text\n```"
              }
            },
            "Rasa 2.4 to 2.5": {
              "Machine Learning Components": {
                "`DIET`, `TED`, and `ResponseSelector`": "The former `weight_sparsity` parameter of the `DIETClassifier`, `TEDPolicy`, and the `ResponseSelector`,\nis now deprecated and superseded by the new `connection_density` parameter.\nThe old `weight_sparsity` is roughly equivalent to `1 - connection_density`, except at very low densities\n(high sparsities).\n\nTo avoid deprecation issues, you should set `connection_density` to\n`1 - your former weight_sparsity setting` throughout the config file. (If you left\n`weight_sparsity` at its default setting, you don't need to do anything.)",
                "SpaCy 3.0": "Rasa now supports spaCy 3.0. This means that we can support more features for more\nlanguages but this also introduced a breaking change. SpaCy 3.0 deprecated the\n`spacy link <language model>` command. So from now on you need to use the\n[the full model name](https://spacy.io/models) in the `config.yml` file.\n\n**Before**\n\nBefore you could run `spacy link en en_core_web_md` and then we would be able\nto pick up the correct model from the `language` parameter.\n\n```\nlanguage: en\n\npipeline:\n   - name: SpacyNLP\n```\n\n**Now**\n\nThis behavior will be deprecated and instead you'll want to be explicit in `config.yml`.\n\n```\nlanguage: en\n\npipeline:\n   - name: SpacyNLP\n     model: en_core_web_md\n```\n\n**Fallback**\n\nTo make the transition easier, Rasa will try to fall back to a medium spaCy model whenever\na compatible language is configured for the entire pipeline in `config.yml`, even if you don't\nspecify a `model`. This fallback behavior is temporary and will be deprecated in Rasa 3.0.0.\n\nWe've updated our docs to reflect these changes. All examples now show a direct link to the\ncorrect spaCy model. We've also added a warning to the [SpaCyNLP](components.mdx#spacynlp)\ndocs that explains the fallback behavior."
              }
            },
            "Rasa 2.3 to Rasa 2.4": {
              "Deprecating `template` for `response`": "NLG Server\n\n['Changed request format to send `response` as well as `template` as a field. The `template` field will be removed in Rasa 3.0.0.']\n\n`rasa.core.agent`\n\n['The terminology `template` is deprecated and replaced by `response`. Support for `template` from the NLG response will be removed in Rasa 3.0.0. Please see [here](./nlg.mdx) for more details.']\n\n`rasa.core.nlg.generator`\n\n['`generate()` now takes in  `utter_action` as a parameter.', 'The terminology `template` is deprecated and replaced by `response`. Support for `template` in the `NaturalLanguageGenerator` will be removed in Rasa 3.0.0.']\n\n`rasa.shared.core.domain`\n\n['The property `templates` is deprecated. Use `responses` instead. It will be removed in Rasa 3.0.0.', '`retrieval_intent_templates` will be removed in Rasa 3.0.0. Please use `retrieval_intent_responses` instead.', '`is_retrieval_intent_template` will be removed in Rasa 3.0.0. Please use `is_retrieval_intent_response` instead.', '`check_missing_templates` will be removed in Rasa 3.0.0. Please use `check_missing_responses` instead.']\n\nResponse Selector\n\n['The field `template_name` will be deprecated in Rasa 3.0.0. Please use `utter_action` instead. Please see [here](./components.mdx#selectors) for more details.', 'The field `response_templates` will be deprecated in Rasa 3.0.0. Please use `responses` instead. Please see [here](./components.mdx#selectors) for more details.']"
            },
            "Rasa 2.3.3 to Rasa 2.3.4": {
              "Machine Learning Components": "Rasa `2.3.0` introduced the option of using cosine similarities for model confidences by setting `model_confidence=cosine`. Some post-release experiments revealed that using `model_confidence=cosine` is wrong as it can change the order of predicted labels. That's why this option was removed in Rasa version `2.3.4`.\n\n`model_confidence=inner` is deprecated as it produces an unbounded range of confidences which can break\nthe logic of assistants in various other places.\n\nWe encourage you to try `model_confidence=linear_norm` which will produce a linearly normalized version of dot product similarities with each value in the range `[0,1]`. This can be done with the following config:\n\n```\n- name: DIETClassifier\n  model_confidence: linear_norm\n  constrain_similarities: True\n```\n\nIf you trained a model with `model_confidence=cosine` or `model_confidence=inner` setting using previous versions of Rasa, please re-train by either removing the `model_confidence` option from the configuration or setting it to `linear_norm`."
            },
            "Rasa 2.2 to Rasa 2.3": {
              "General": "If you want to use Tensorboard for `DIETClassifier`, `ResponseSelector`, or `TEDPolicy` and log metrics after\nevery (mini)batch, please use 'batch' instead of 'minibatch' as 'tensorboard_log_level'.",
              "Machine Learning Components": "A few changes have been made to the loss function inside machine learning (ML)\ncomponents `DIETClassifier`, `ResponseSelector` and `TEDPolicy`. These include:\n\n['Configuration option `loss_type=softmax` is now deprecated and will be removed in Rasa 3.0.0. Use `loss_type=cross_entropy` instead.', 'The default loss function (`loss_type=cross_entropy`) can add an optional sigmoid cross-entropy loss of all similarity values to constrain\\nthem to an approximate range. You can turn on this option by setting `constrain_similarities=True`. This should help the models to perform better on real world test sets.']\n\nA new option `model_confidence` has been added to each ML component. It affects how the model's confidence for each label is computed during inference. It can take one of three values:\n\n['`softmax` - Dot product similarities between input and label embeddings are post-processed with a softmax function, as a result of which confidence for all labels sum up to 1.', '`cosine` - Cosine similarity between input and label embeddings. Confidence for each label will be in the range `[-1,1]`.', '`linear_norm` - Dot product similarities between input and label embeddings are post-processed with a linear normalization function. Confidence for each label will be in the range `[0,1]`.']\n\nThe default value is `softmax`, but we recommend trying `linear_norm`. This should make it easier to [tune thresholds for triggering fallback](./fallback-handoff.mdx#fallbacks).\nThe value of this option does not affect how confidences are computed for entity predictions in `DIETClassifier`.\n\nWe encourage you to try both the above recommendations. This can be done with the following config:\n\n```\n- name: DIETClassifier\n  model_confidence: linear_norm\n  constrain_similarities: True\n  ...\n```\n\nOnce the assistant is re-trained with the above configuration, users should also [tune fallback confidence thresholds](./fallback-handoff.mdx#fallbacks).\n\n**EDIT**: Some post-release experiments revealed that using `model_confidence=cosine` is wrong as it can change the order of predicted labels. That's why this option was removed in Rasa version `2.3.4`."
            },
            "Rasa 2.1 to Rasa 2.2": {
              "General": "`TEDPolicy`'s  `transformer_size`, `number_of_transformer_layers`,\nand `dense_dimensions` parameters have been renamed.\nPlease update your configuration files using the following mapping:\n\n|      Old Model Parameter    |                 New Model Parameter                    |\n|-----------------------------|--------------------------------------------------------|\n|`transformer_size`           |dictionary `transformer_size` with keys                 |\n|                             |`text`, `action_text`, `label_action_text`, `dialogue`  |\n|`number_of_transformer_layers`|dictionary `number_of_transformer_layers` with keys    |\n|                             |`text`, `action_text`, `label_action_text`, `dialogue`  |\n|`dense_dimension`            |dictionary `dense_dimension` with keys                  |\n|                             |`text`, `action_text`, `label_action_text`, `intent`,   |\n|                             |`action_name`, `label_action_name`, `entities`, `slots`,|\n|                             |`active_loop`                                           |\n\nFor example:\n\n```\npolicies:\n  - name: TEDPolicy\n    transformer_size:\n      text: 128\n      action_text: 128\n      label_action_text: 128\n      dialogue: 128\n    number_of_transformer_layers:\n      text: 1\n      action_text: 1\n      label_action_text: 1\n      dialogue: 1\n    dense_dimension:\n      text: 128\n      action_text: 128\n      label_action_text: 128\n      intent: 20\n      action_name: 20\n      label_action_name: 20\n      entities: 20\n      slots: 20\n      active_loop: 20\n```",
              "Deprecations": {
                "Markdown Data": "Training and test data in Markdown format is now deprecated. This includes:\n\n['reading and writing of story files in Markdown format', 'reading and writing of NLU data in Markdown format', 'reading and writing of retrieval intent data in Markdown format']\n\nSupport for Markdown data will be removed entirely in Rasa 3.0.0.\n\nPlease convert your existing Markdown data by using the commands\ndescribed [here](./migration-guide.mdx#training-data-files)."
              },
              "Policies": {
                "Other": [
                  "`Domain.random_template_for` is deprecated and will be removed in Rasa\n3.0.0. You can alternatively use the `TemplatedNaturalLanguageGenerator`.",
                  "`Domain.action_names` is deprecated and will be removed in Rasa\n3.0.0. Please use `Domain.action_names_or_texts` instead."
                ]
              }
            },
            "Rasa 2.0 to Rasa 2.1": {
              "Deprecations": "`ConveRTTokenizer` is now deprecated. [ConveRTFeaturizer](./components.mdx#convertfeaturizer) now implements\nits behaviour. To migrate, replace `ConveRTTokenizer` with any other tokenizer, for e.g.:\n\n```\npipeline:\n    - name: WhitespaceTokenizer\n    - name: ConveRTFeaturizer\n      model_url: <Remote/Local path to model files>\n    ...\n```\n\n`HFTransformersNLP` and `LanguageModelTokenizer` components are now deprecated.\n[LanguageModelFeaturizer](./components.mdx#languagemodelfeaturizer) now implements their behaviour.\nTo migrate, replace both the above components with any tokenizer and specify the model architecture and model weights\nas part of `LanguageModelFeaturizer`, for e.g.:\n\n```\npipeline:\n    - name: WhitespaceTokenizer\n    - name: LanguageModelFeaturizer\n      model_name: \"bert\"\n      model_weights: \"rasa/LaBSE\"\n    ...\n```"
            },
            "Rasa 1.10 to Rasa 2.0": {
              "General": "A lot has changed in version 2.0. Make sure you read\nthrough this guide thoroughly, to make sure all parts of your bot are updated.\nA lot of updates can be done automatically with inbuilt commands, others will need\nsome manual conversion. If you have any feedback about these updates or the migration process, please post it\nin the [forum](https://forum.rasa.com/t/rasa-open-source-2-0-is-out-now-internal-draft/35577).",
              "Training data files": "As of version 2.0, the new default training data format is yaml. Markdown is still supported,\nbut this will be deprecated in Rasa 3.0.0.\n\nYou can convert existing NLU, Stories, and NLG (i.e. `responses.md`) training data\nfiles in the Markdown format to the new YAML format using following commands:\n\n```\nrasa data convert nlu -f yaml --data={SOURCE_DIR} --out={TARGET_DIR}\nrasa data convert nlg -f yaml --data={SOURCE_DIR} --out={TARGET_DIR}\nrasa data convert core -f yaml --data={SOURCE_DIR} --out={TARGET_DIR}\n```\n\nConverted files will have the same names as the original ones but with a\n`_converted.yml` suffix.\n\nIf you are using [forms](./migration-guide.mdx#forms) or [response selectors](./migration-guide.mdx#response-selectors),\nsome additional changes will need to be made as described in their respective sections.",
              "Policies": {
                "Manually migrating from the Mapping Policy": "If you previously used the [Mapping Policy](https://rasa.com/docs/rasa/2.x/policies#mapping-policy), you\ncan follow the documentation on [FAQs](./chitchat-faqs.mdx) to convert your mapped\nintents to rules. Suppose you previously mapped an intent `ask_is_bot` as follows:\n\n```\nintents:\n - ask_is_bot:\n     triggers: action_is_bot\n```\n\nThis becomes the following rule:\n\n```\nrules:\n- rule: Rule to map `ask_is_bot` intent\n  steps:\n  - intent: ask_is_bot\n  - action: action_is_bot\n```\n\nAnd you can safely remove any `triggers:` from your domain:\n\n```\nintents:\n - ask_is_bot\n```\n\nFinally, you can replace the Mapping Policy with the\n[Rule Policy](./policies.mdx#rule-policy) in your model configuration:\n\n```\npolicies:\n  # Other policies\n  - name: RulePolicy\n```",
                "Manually migrating from the Fallback Policy": "If you previously used the [Fallback Policy](https://rasa.com/docs/rasa/2.x/policies#fallback-policy), the following model\nconfiguration would translate as follows given a previous configuration like this:\n\n```\npolicies:\n  - name: \"FallbackPolicy\"\n    nlu_threshold: 0.4\n    core_threshold: 0.3\n    fallback_action_name: \"action_default_fallback\"\n    ambiguity_threshold: 0.1\n```\n\nThe new configuration would then look like:\n\n```\nrecipe: default.v1\npipeline:\n  # Other components\n  - name: FallbackClassifier\n    threshold: 0.4\n    ambiguity_threshold: 0.1\n\npolicies:\n  # Other policies\n  - name: RulePolicy\n    core_fallback_threshold: 0.3\n    core_fallback_action_name: \"action_default_fallback\"\n```\n\nIn addition, you need to add a [rule](./rules.mdx) to specify which action to run\nin case of low NLU confidence:\n\n```\nrules:\n  - rule: Ask the user to rephrase whenever they send a message with low NLU confidence\n    steps:\n    - intent: nlu_fallback\n    - action: utter_please_rephrase\n```\n\nSee the documentation on [fallback](./fallback-handoff.mdx#fallbacks) for more\ninformation.",
                "Manually migrating from the Two-Stage-Fallback Policy": "If you previously used the\n[Two-Stage Fallback Policy](https://rasa.com/docs/rasa/2.x/policies#two-stage-fallback-policy), with a configuration\nlike this for example:\n\n```\npolicies:\n  - name: TwoStageFallbackPolicy\n    nlu_threshold: 0.4\n    ambiguity_threshold: 0.1\n    core_threshold: 0.3\n    fallback_core_action_name: \"action_default_fallback\"\n    fallback_nlu_action_name: \"action_default_fallback\"\n    deny_suggestion_intent_name: \"out_of_scope\"\n```\n\nThe new configuration would look like this:\n\n```\nrecipe: default.v1\npipeline:\n  # Other components\n  - name: FallbackClassifier\n    threshold: 0.4\n    ambiguity_threshold: 0.1\n\npolicies:\n  # Other policies\n  - name: RulePolicy\n    core_fallback_threshold: 0.3\n    core_fallback_action_name: \"action_default_fallback\"\n```\n\nIn addition you need to add a [rule](./rules.mdx) to activate the Two-Stage Fallback for\nmessages with low NLU confidence.\n\n```\nrules:\n  - rule: Implementation of the TwoStageFallbackPolicy\n    steps:\n    # This intent is automatically triggered by the `FallbackClassifier` in the NLU\n    # pipeline in case the intent confidence was below the specified threshold.\n    - intent: nlu_fallback\n    # The Fallback is now implemented as a form.\n    - action: action_two_stage_fallback\n    - active_loop: action_two_stage_fallback\n```\n\nNote that the previous parameters `fallback_nlu_action_name` and\n`deny_suggestion_intent_name` are no longer configurable and have the fixed values\n`action_default_fallback` and `out_of_scope`.\n\nSee the [fallback](./fallback-handoff.mdx#fallbacks) documentation for more\ninformation."
              },
              "Forms": "As of version 2.0 the logic for [forms](./forms.mdx) has been moved from the\nRasa SDK to Rasa to simplify implementation and make it easier to write\naction servers in other languages.\n\nThis means that forms are no longer implemented using a `FormAction`, but instead\ndefined in the domain. Any customizations around requesting slots or\n[slot validation](./forms.mdx#validating-form-input) can be handled with a `FormValidationAction`.\n\nConsider a custom form action from 1.x like this:\n\n```\nfrom typing import Text, List, Any, Dict, Union\nfrom rasa_sdk import Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom rasa_sdk.forms  import FormAction\n\nclass RestaurantForm(FormAction):\n    def name(self) -> Text:\n        return \"restaurant_form\"\n\n    @staticmethod\n    def required_slots(tracker: Tracker) -> List[Text]:\n        return [\"cuisine\"]\n\n    def slot_mappings(self) -> Dict[Text, Union[Dict, List[Dict]]]:\n        return {\n            \"cuisine\": self.from_entity(entity=\"cuisine\", not_intent=\"chitchat\"),\n        }\n\n    @staticmethod\n    def cuisine_db() -> List[Text]:\n        \"\"\"Database of supported cuisines\"\"\"\n\n        return [\"caribbean\", \"chinese\", \"french\"]\n\n    def validate_cuisine(\n        self,\n        value: Text,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: Dict[Text, Any],\n    ) -> Dict[Text, Any]:\n        \"\"\"Validate cuisine value.\"\"\"\n\n        if value.lower() in self.cuisine_db():\n            # validation succeeded, set the value of the \"cuisine\" slot to value\n            return {\"cuisine\": value}\n        else:\n            dispatcher.utter_message(template=\"utter_wrong_cuisine\")\n            # validation failed, set this slot to None, meaning the\n            # user will be asked for the slot again\n            return {\"cuisine\": None}\n\n    def submit(\n        self,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: Dict[Text, Any],\n    ) -> List[Dict]:\n        \"\"\"Define what the form has to do\n            after all required slots are filled\"\"\"\n\n        # utter submit template\n        dispatcher.utter_message(template=\"utter_submit\")\n        return []\n```\n\nStart the migration by removing the FormPolicy and adding the [RulePolicy](./policies.mdx#rule-policy)\n(if not there already) to your model configuration:\n\n```\npolicies:\n  # Other policies\n  # ...\n  - name: RulePolicy\n```\n\nThen you need to define the form, required slots and their slot mappings\nin the domain as described in the documentation on [forms](./forms.mdx#defining-a-form):\n\n```\nforms:\n  restaurant_form:\n    cuisine:\n    - type: from_entity\n      entity: cuisine\n      not_intent: chitchat\n```\n\nIf you ran the command to [convert your stories](./migration-guide.mdx#training-data-Files),\nyou will have a story that handles form activation and deactivation like this:\n\n```\nstories:\n  - story: cuisine form\n    steps:\n    - intent: request_restaurant\n    - action: restaurant_form\n    - active_loop: restaurant_form\n    - active_loop: null\n    - action: utter_submit\n```\n\nThis will work fine, but the best way to handle form behavior is to remove this story and instead\ndefine two separate rules for form activation and submission:\n\n```\nrules:\n  - rule: Activate form\n    steps:\n    - intent: request_restaurant\n    - action: restaurant_form\n    - active_loop: restaurant_form\n\n  - rule: Submit form\n    condition:\n    # Condition that form is active.\n    - active_loop: restaurant_form\n    steps:\n    - action: restaurant_form\n    - active_loop: null\n    # The action we want to run when the form is submitted.\n    - action: utter_submit\n```\n\nThe last step is to implement a custom action to validate the form slots. Start by\nadding the custom action to your domain:\n\n```\nactions:\n  # Other actions\n  # ...\n  - validate_restaurant_form\n```\n\nThen add a custom action which validates the `cuisine` slot:\n\n```\nfrom typing import Text, List, Any, Dict, Union\nfrom rasa_sdk import Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom rasa_sdk import FormValidationAction\nfrom rasa_sdk.types import DomainDict\n\nclass RestaurantFormValidator(FormValidationAction):\n    def name(self) -> Text:\n        return \"validate_restaurant_form\"\n\n    @staticmethod\n    def cuisine_db() -> List[Text]:\n        \"\"\"Database of supported cuisines\"\"\"\n\n        return [\"caribbean\", \"chinese\", \"french\"]\n\n    def validate_cuisine(\n        self,\n        slot_value: Any,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: DomainDict,\n    ) -> Dict[Text, Any]:\n        \"\"\"Validate cuisine value.\"\"\"\n\n        if slot_value.lower() in self.cuisine_db():\n            # validation succeeded, set the value of the \"cuisine\" slot to value\n            return {\"cuisine\": slot_value}\n        else:\n            # validation failed, set this slot to None, meaning the\n            # user will be asked for the slot again\n            return {\"cuisine\": None}\n```\n\nYou can also migrate forms from Rasa SDK to Rasa 2 iteratively. You can for\nexample migrate one form to the Rasa 2 implementation while continue using\nthe deprecated Rasa SDK implementation for another form. To continue to use\nthe deprecated Rasa SDK `FormAction`s, add a custom action with the name of your form to your domain. Note that you should complete the migration as soon as possible as the deprecated `FormAction`\nwill be removed from the Rasa SDK in Rasa 3.\n\n```\nactions:\n# Adding a custom action for a form will\n# instruct Rasa to use the\n# deprecated Rasa SDK implementation of forms.\n- my_form\n\nforms:\n my_form:\n```\n\nSee the [forms](./forms.mdx) documentation for more details.",
              "Response Selectors": "Response Selectors are a stable feature as of version 2.0.\n\nThe [conversion command](./migration-guide.mdx#training-data-files) will automatically\nconvert your `responses.md` file, stories and nlu training data to the new yaml format.\nIt will also take care of adding the `utter_` prefix to your responses.\nAdditionally you will need to rename the `respond_` actions in your stories files to use the\n`utter_` prefix instead. Run the following command to apply these changes:\n\n```\nrasa data convert responses --data {SOURCE_DIR} --out={TARGET_DIR}\n```\n\nYou can also apply these changes manually. For example:\n\n```\nstories:\n  - story: chitchat\n    steps:\n    - intent: chitchat\n    - action: respond_chitchat\n```\n\nbecomes\n\n```\nstories:\n  - story: chitchat\n    steps:\n    - intent: chitchat\n    - action: utter_chitchat\n```\n\nand you will need to add the `utter_` prefix to the response names in your `responses.md`\nas well. For example:\n\n```\nresponses:\n  chitchat/ask_name:\n    - text: Oh yeah, I am called the retrieval bot.\n\n  chitchat/ask_weather:\n    - text: Oh, it does look sunny right now in Berlin.\n```\n\nbecomes\n\n```\nresponses:\n  utter_chitchat/ask_name:\n    - text: Oh yeah, I am called the retrieval bot.\n\n  utter_chitchat/ask_weather:\n    - text: Oh, it does look sunny right now in Berlin.\n```\n\nFinally, you should remove any actions with the `respond_` prefix from the actions\nlist in your domain.\n\nThis behavior will work fine when defined as a story, but even better when defined\nas a rule. You should consider transferring your retrieval stories to rules. More information\non what that looks like in the [chitchat and FAQs documentation](./chitchat-faqs.mdx).\n\nResponse Selectors are now trained on retrieval intent labels by default instead\nof the actual response text. For most models, this should improve training time\nand accuracy of the `ResponseSelector`.\n\nIf you want to revert to the pre-2.0 default behavior, add the `use_text_as_label: true`\nparameter to your `ResponseSelector` component:\n\n```\npipeline:\n  # other components\n  - name: ResponseSelector\n    use_text_as_label: true\n```\n\nThe output schema of `ResponseSelector` has changed. An example output looks like this:\n\n```\n{\n  \"response_selector\": {\n    \"all_retrieval_intents\": [\n      \"faq\"\n    ],\n    \"default\": {\n      \"response\": {\n        \"id\": 1388783286124362000,\n        \"confidence\": 1,\n        \"intent_response_key\": \"faq/is_legit\",\n        \"response_templates\": [\n          {\n            \"text\": \"absolutely\",\n            \"image\": \"https://i.imgur.com/nGF1K8f.jpg\"\n          },\n          {\n            \"text\": \"I think so.\"\n          }\n        ]\n        \"template_name\": \"utter_faq/is_legit\"\n      },\n      \"ranking\": [\n        {\n          \"id\": 1388783286124362000,\n          \"confidence\": 1,\n          \"intent_response_key\": \"faq/is_legit\"\n        }\n      ]\n    }\n  }\n}\n```\n\nAs a result of this, if you were previously querying for the key `full_retrieval_intent` as:\n\n```\nresponse_selector_output.get(\"default\")\n                        .get(\"full_retrieval_intent\")\n```\n\nyou should instead now do this:\n\n```\nresponse_selector_output.get(\"default\")\n                        .get(\"response\")\n                        .get(\"intent_response_key\")\n```",
              "Unfeaturized Slots": "[Slots](domain.mdx#slots) of type `unfeaturized` are\ndeprecated and will be removed in version 3.0. To ignore slot values during\na conversation, set the `influence_conversation` property of the slot to `false`.\n\nThe following snippet is an example of the deprecated unfeaturized slot usage:\n\n```\nslots:\n  username:\n    type: unfeaturized\n```\n\nTo update this to the new format, you can specify the expected data type `text` and\ndefine that the slot should be ignored during the conversation.\n\n```\nslots:\n  username:\n    type: text\n    # Set `influence_conversation` to `false`\n    # to ignore the slot value during the conversation.\n    influence_conversation: false\n```\n\nIf you don't require the slot to have a specific data type, you can use the new slot\ntype [any](domain.mdx#any-slot). This slot type is always ignored during a conversation\nand does not make any assumptions regarding the data type of the slot value.\n\n```\nslots:\n  username:\n    type: any\n```\n\nPlease see the updated [slots documentation](domain.mdx#slots) for more information.",
              "Conversation sessions": "[Conversation sessions](domain.mdx#session-configuration) are now enabled by default\nif your [Domain](domain.mdx) does not contain a session configuration. Previously a\nmissing session configuration was treated as if conversation sessions were disabled.\nYou can explicitly disable conversation sessions using the following snippet:\n\n```\nsession_config:\n  # A session expiration time of `0`\n  # disables conversation sessions\n  session_expiration_time: 0\n```",
              "Dialogue Featurization": "This section is only relevant if you explicitly defined [featurizers](./policies.mdx#featurizers)\nin your policy configuration.\n\nLabelTokenizerSingleStateFeaturizer is deprecated and will be removed in the future.\nIt should be replaced with SingleStateFeaturizer and some changes should be made to the NLU pipeline.\nAdd a `Tokenizer` with the option `intent_tokenization_flag: True` and `CountVectorsFeaturizer`\nto the NLU pipeline.\n\nFor example:\n\n```\nlanguage: en\npipeline:\n  - name: WhitespaceTokenizer\n    intent_tokenization_flag: True\n  - name: CountVectorsFeaturizer\n  # other components\npolicies:\n  # other policies\n  - name: TEDPolicy\n    featurizer:\n    - name: SingleStateFeaturizer\n\n```\n\nBinarySingleStateFeaturizer is deprecated and will be removed in the future.\nYou should replace it with `SingleStateFeaturizer` and a NLU pipeline\nwhere `intent_tokenization_flag` of a Tokenizer is set to `False`.\n\nFor example:\n\n```\nlanguage: en\npipeline:\n  - name: WhitespaceTokenizer\n    intent_tokenization_flag: False\n  # other components\npolicies:\n  # other policies\n  - name: TEDPolicy\n    featurizer:\n    - name: SingleStateFeaturizer\n\n```",
              "Deprecations": "The deprecated [event brokers](./event-brokers.mdx) FileProducer, KafkaProducer, PikaProducer\nand SQLProducer have been removed. If you used these brokers in your\n`endpoints.yml` make sure to use the renamed variants instead:\n\n['FileProducer became FileEventBroker', 'KafkaProducer became KafkaEventBroker', 'PikaProducer became PikaEventBroker', 'SQLProducer became  SQLEventBroker']\n\nThe deprecated EmbeddingIntentClassifier has been removed. If you used this\ncomponent in your pipeline configuration (`config.yml`) you can replace it\nwith [DIETClassifier](./components.mdx#dietclassifier).\nIt accepts the same configuration parameters.\n\nThe deprecated KerasPolicy has been removed. If you used this\ncomponent in your policies configuration (`config.yml`) you can replace it\nwith [TEDPolicy](./policies.mdx#ted-policy). It accepts the same configuration parameters."
            },
            "Rasa 1.7 to Rasa 1.8": {
              "General": [
                "The [TED Policy](./policies.mdx#ted-policy) replaced the `keras_policy` as recommended machine\nlearning policy. New projects generated with `rasa init` will automatically use\nthis policy. In case you want to change your existing model configuration to use the\n[TED Policy](./policies.mdx#ted-policy) add this to the `policies` section in your `config.yml`\nand remove potentially existing `KerasPolicy` entries:",
                "```\npolicies:\n# - ... other policies\n- name: TEDPolicy\n  max_history: 5\n  epochs: 100\n```",
                "The given snippet specifies default values for the parameters `max_history` and\n`epochs`. `max_history` is particularly important and strongly depends on your stories.\nPlease see the docs of the [TED Policy](./policies.mdx#ted-policy) if you want to customize them.",
                "All pre-defined pipeline templates are deprecated. **Any templates you use will be\nmapped to the new configuration, but the underlying architecture is the same**.\nTake a look at [Tuning Your Model](./tuning-your-model.mdx) to decide on what components you should use\nin your configuration file.",
                "The Embedding Policy was renamed to [TED Policy](./policies.mdx#ted-policy). The functionality of the policy stayed the same.\nPlease update your configuration files to use `TEDPolicy` instead of `EmbeddingPolicy`.",
                "Most of the model options for `EmbeddingPolicy`, `EmbeddingIntentClassifier`, and `ResponseSelector` got\nrenamed. Please update your configuration files using the following mapping:",
                "|      Old model option       |                  New model option                   |\n|-----------------------------|-----------------------------------------------------|\n|hidden_layers_sizes_a        |dictionary “hidden_layers_sizes” with key “text”     |\n|hidden_layers_sizes_b        |dictionary “hidden_layers_sizes” with key “label”    |\n|hidden_layers_sizes_pre_dial |dictionary “hidden_layers_sizes” with key “dialogue” |\n|hidden_layers_sizes_bot      |dictionary “hidden_layers_sizes” with key “label”    |\n|num_transformer_layers       |number_of_transformer_layers                         |\n|num_heads                    |number_of_attention_heads                            |\n|max_seq_length               |maximum_sequence_length                              |\n|dense_dim                    |dense_dimension                                      |\n|embed_dim                    |embedding_dimension                                  |\n|num_neg                      |number_of_negative_examples                          |\n|mu_pos                       |maximum_positive_similarity                          |\n|mu_neg                       |maximum_negative_similarity                          |\n|use_max_sim_neg              |use_maximum_negative_similarity                      |\n|C2                           |regularization_constant                              |\n|C_emb                        |negative_margin_scale                                |\n|droprate_a                   |droprate_dialogue                                    |\n|droprate_b                   |droprate_label                                       |\n|evaluate_every_num_epochs    |evaluate_every_number_of_epochs                      |\n|evaluate_on_num_examples     |evaluate_on_number_of_examples                       |",
                "Old configuration options will be mapped to the new names, and a warning will be thrown.\nHowever, these will be deprecated in a future release.",
                "The Embedding Intent Classifier is now deprecated and will be replaced by [DIETClassifier](./components.mdx#dietclassifier)\nin the future.\n`DIETClassfier` performs intent classification as well as entity recognition.\nIf you want to get the same model behavior as the current `EmbeddingIntentClassifier`, you can use\nthe following configuration of `DIETClassifier`:",
                "```\npipeline:\n# - ... other components\n- name: DIETClassifier\n  hidden_layers_sizes:\n    text: [256, 128]\n  number_of_transformer_layers: 0\n  weight_sparsity: 0\n  intent_classification: True\n  entity_recognition: False\n  use_masked_language_model: False\n  BILOU_flag: False\n  scale_loss: True\n  use_sparse_input_dropout: False\n  use_dense_input_dropout: False\n  # ... any other parameters\n```",
                "See [DIETClassifier](./components.mdx#dietclassifier) for more information about the new component.\nSpecifying `EmbeddingIntentClassifier` in the configuration maps to the above component definition, and results in\nthe same behaviour within the same Rasa version.",
                "`CRFEntityExtractor` is now deprecated and will be replaced by `DIETClassifier` in the future. If you want to\nget the same model behavior as the current `CRFEntityExtractor`, you can use the following configuration:",
                "```\npipeline:\n# - ... other components\n- name: LexicalSyntacticFeaturizer\n  features: [\n    [\"low\", \"title\", \"upper\"],\n    [\n      \"BOS\",\n      \"EOS\",\n      \"low\",\n      \"prefix5\",\n      \"prefix2\",\n      \"suffix5\",\n      \"suffix3\",\n      \"suffix2\",\n      \"upper\",\n      \"title\",\n      \"digit\",\n    ],\n    [\"low\", \"title\", \"upper\"],\n  ]\n- name: DIETClassifier\n  intent_classification: False\n  entity_recognition: True\n  use_masked_language_model: False\n  number_of_transformer_layers: 0\n  # ... any other parameters\n```",
                "`CRFEntityExtractor` featurizes user messages on its own, it does not depend on any featurizer.\nWe extracted the featurization from the component into the new featurizer [LexicalSyntacticFeaturizer](./components.mdx#lexicalsyntacticfeaturizer). Thus,\nin order to obtain the same results as before, you need to add this featurizer to your pipeline before the\n[DIETClassifier](./components.mdx#dietclassifier).\nSpecifying `CRFEntityExtractor` in the configuration maps to the above component definition, the behavior\nis unchanged from previous versions.",
                "If your pipeline contains `CRFEntityExtractor` and `EmbeddingIntentClassifier` you can substitute both\ncomponents with [DIETClassifier](./components.mdx#dietclassifier). You can use the following pipeline for that:",
                "```\npipeline:\n# - ... other components\n- name: LexicalSyntacticFeaturizer\n  features: [\n    [\"low\", \"title\", \"upper\"],\n    [\n      \"BOS\",\n      \"EOS\",\n      \"low\",\n      \"prefix5\",\n      \"prefix2\",\n      \"suffix5\",\n      \"suffix3\",\n      \"suffix2\",\n      \"upper\",\n      \"title\",\n      \"digit\",\n    ],\n    [\"low\", \"title\", \"upper\"],\n  ]\n- name: DIETClassifier\n  number_of_transformer_layers: 0\n  # ... any other parameters\n```"
              ]
            },
            "Rasa 1.6 to Rasa 1.7": {
              "General": [
                "By default, the `EmbeddingIntentClassifier`, `EmbeddingPolicy`, and `ResponseSelector` will\nnow normalize the top 10 confidence results if the `loss_type` is `\"softmax\"` (which has been\ndefault since 1.3, see [Rasa 1.2 to Rasa 1.3](./migration-guide.mdx#rasa-12-to-rasa-13)). This is configurable via the `ranking_length`\nconfiguration parameter; to turn off normalization to match the previous behavior, set `ranking_length: 0`."
              ]
            },
            "Rasa 1.2 to Rasa 1.3": {
              "General": [
                "Default parameters of `EmbeddingIntentClassifier` are changed. See\nthe Components page for details.\nArchitecture implementation is changed as well, so **old trained models cannot be loaded**.\nDefault parameters and architecture for `EmbeddingPolicy` are changed. See [Policies](./policies.mdx) for details.\nIt uses transformer instead of lstm. **Old trained models cannot be loaded**.\nThey use `inner` similarity and `softmax` loss by default instead of\n`cosine` similarity and `margin` loss (can be set in config file).\nThey use `balanced` batching strategy by default to counteract class imbalance problem.\nThe meaning of `evaluate_on_num_examples` is changed. If it is non zero, random examples will be\npicked by stratified split and used as **hold out** validation set, so they will be excluded from training data.\nWe suggest to set it to zero (default) if data set contains a lot of unique examples of dialogue turns.\nRemoved `label_tokenization_flag` and `label_split_symbol` from component. Instead moved intent splitting to `Tokenizer` components via `intent_tokenization_flag` and `intent_split_symbol` flag.",
                "Default `max_history` for `EmbeddingPolicy` is `None` which means it'll use\nthe `FullDialogueTrackerFeaturizer`. We recommend to set `max_history` to\nsome finite value in order to use `MaxHistoryTrackerFeaturizer`\nfor **faster training**. See [Featurizers](./policies.mdx#featurizers) for details.\nWe recommend to increase `batch_size` for `MaxHistoryTrackerFeaturizer`\n(e.g. `\"batch_size\": [32, 64]`)",
                "**Compare** mode of `rasa train core` allows the whole core config comparison.\nTherefore, we changed the naming of trained models. They are named by config file\nname instead of policy name. Old naming style will not be read correctly when\ncreating **compare** plots (`rasa test core`). Please remove old trained models\nin comparison folder and retrain. Normal core training is unaffected.",
                "We updated the **evaluation metric** for our **NER**. We report the weighted precision and f1-score.\nSo far we included `no-entity` in this report. However, as most of the tokens actually don't have\nan entity set, this will influence the weighted precision and f1-score quite a bit. From now on we\nexclude `no-entity` from the evaluation. The overall metrics now only include proper entities. You\nmight see a drop in the performance scores when running the evaluation again.",
                "`/` is reserved as a delimiter token to distinguish between retrieval intent and the corresponding response text\nidentifier. Make sure you don't include `/` symbol in the name of your intents."
              ]
            },
            "Rasa NLU 0.14.x and Rasa Core 0.13.x to Rasa 1.0": {
              "General": [
                "The scripts in `rasa.core` and `rasa.nlu` can no longer be executed. To train, test, run, … an NLU or Core\nmodel, you should now use the command line interface `rasa`. The functionality is, for the most part, the same as before.\nSome changes in commands reflect the combined training and running of NLU and Core models, but NLU and Core can still\nbe trained and used individually. If you attempt to run one of the old scripts in `rasa.core` or `rasa.nlu`,\nan error is thrown that points you to the command you\nshould use instead. See all the new commands at [Command Line Interface](./command-line-interface.mdx).",
                "If you have written a custom output channel, all `send_` methods subclassed\nfrom the `OutputChannel` class need to take an additional `\\*\\*kwargs`\nargument. You can use these keyword args from your custom action code or the\ntemplates in your domain file to send any extra parameters used in your\nchannel's send methods.",
                "If you were previously importing the `Button` or `Element` classes from\n`rasa_core.dispatcher`, these are now to be imported from `rasa_sdk.utils`.",
                "Rasa NLU and Core previously used <a href=\"https://legacy-docs.rasa.com/docs/nlu/0.15.1/migrations/?&_ga=2.218966814.608734414.1560704810-314462423.1543594887#id1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">separate configuration files</a>.\nThese two files should be merged into a single file either named `config.yml`, or passed via the `--config` parameter."
              ],
              "Script parameters": [
                "All script parameter names have been unified to follow the same schema.\nAny underscores (`_`) in arguments have been replaced with dashes (`-`).\nFor example: `--max_history` has been changed to `--max-history`. You can\nsee all of the script parameters in the `--help` output of the commands\nin the [Command Line Interface](./command-line-interface.mdx).",
                "The `--num_threads` parameter was removed from the `run` command. The\nserver will always run single-threaded, but will now run asynchronously. If you want to\nmake use of multiple processes, feel free to check out the [Sanic server\ndocumentation](https://sanicframework.org/en/guide/deployment/running.html#gunicorn).",
                "To avoid conflicts in script parameter names, connectors in the `run` command now need to be specified with\n`--connector`, as `-c` is no longer supported. The maximum history in the `rasa visualize` command needs to be\ndefined with `--max-history`. Output paths and log files cannot be specified with `-o` anymore; `--out` and\n`--log-file` should be used. NLU data has been standarized to be `--nlu` and the name of\nany kind of data files or directory to be `--data`."
              ],
              "HTTP API": [
                "There are numerous HTTP API endpoint changes which can be found [here](./http-api.mdx)."
              ]
            }
          },
          "metadata": {
            "id": "migration-guide",
            "sidebar_label": "Version Migration Guide",
            "title": "Version Migration Guide",
            "description": "Information about changes between major versions of chatbot framework Rasa Core\nand how you can migrate from one version to another.\n"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 28]"
        },
        {
          "title": "Model Configuration",
          "description": "Learn about model configuration for Rasa.",
          "content": {
            "Suggested Config": "You can leave the pipeline and/or policies key out of your configuration file.\nWhen you run `rasa train`, the Suggested Config feature will select a default configuration\nfor the missing key(s) to train the model.\n\nMake sure to specify the language key in your `config.yml` file with the\n2-letter ISO language code.\n\nExample `config.yml` file:\n\n```\n```\n\nThe selected configuration will also be written as comments into the `config.yml` file,\nso you can see which configuration was used. For the example above, the resulting file\nmight look e.g. like this:\n\n```\n```\n\nIf you like, you can then un-comment the suggested configuration for one or both of the\nkeys and make modifications. Note that this will disable automatic suggestions for this\nkey when training again.\nAs long as you leave the configuration commented out and don't specify any configuration\nfor a key yourself, a default configuration will be suggested whenever you train a new\nmodel.\n\n:::note nlu- or dialogue- only models\n\nOnly the default configuration for `pipeline` will be automatically selected\nif you run `rasa train nlu`, and only the default configuration for `policies`\nwill be selected if you run `rasa train core`.\n:::"
          },
          "metadata": {
            "id": "model-configuration",
            "sidebar_label": "Overview",
            "title": "Model Configuration",
            "description": "Learn about model configuration for Rasa.",
            "abstract": "The configuration file defines the components and policies that your model will use to make predictions based on user input."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 29]"
        },
        {
          "title": "Model Storage",
          "description": null,
          "content": {
            "Load Model from Disk": "By default models will be loaded from your local disk. You can specify the path\nto your model with the `--model` parameter:\n\n```\nrasa run --model models/20190506-100418.tar.gz\n```\n\nIf you want to load the latest model in a directory, you can specify\na directory instead of a file:\n\n```\nrasa run --model models/\n```\n\nRasa will check all the models in that directory and load the one that was trained\nmost recently.\n\nIf you don't specify a `--model` argument, Rasa will look for models in the  `models/` directory. The two following calls\nwill load the same model:\n\n```\n# this command will load the same model\nrasa run --model models/\n# ... as this command (using defaults)\nrasa run\n```",
            "Load Model from Server": {
              "How to Configure Rasa": "You can configure the HTTP server to fetch models from another URL\nby adding it to your `endpoints.yml`:\n\n```\nmodels:\n  url: http://my-server.com/models/default\n  wait_time_between_pulls: 10   # In seconds, optional, default: 100\n```\n\nThe server will query the `url` for a zipped model every `wait_time_between_pulls`\nseconds.\n\nIf you want to pull the model only when starting up the server, you can set the time\nbetween pulls to `null`:\n\n```\nmodels:\n  url: http://my-server.com/models/default\n  wait_time_between_pulls: null  # fetches model only once\n```",
              "How to Configure Your Server": "Rasa will send a `GET` request to the URL you specified in the\n`endpoints.yml`, e.g. `http://my-server.com/models/default` in the above examples.\nYou can use any URL.\nThe `GET` request will contain an `If-None-Match` header that contains the\nmodel hash of the last model it downloaded. An example request from Rasa Open\nSource to your server would look like this:\n\n```\ncurl --header \"If-None-Match: d41d8cd98f00b204e9800998ecf8427e\" http://my-server.com/models/default\n```\n\nThe response of your server to this `GET` request should be one of these:\n\n['a status code of `200`, a zipped Rasa Model and set the `ETag` header in\\nthe response to the hash of the model.', 'a status code of `304` and an empty response if the `If-None-Match`\\nheader of the request matches the model you want your server to return.']\n\nRasa uses the `If-None-Match` and `ETag` headers for caching. Setting\nthe headers will avoid re-downloading the same model over and over, saving\nbandwidth and compute resources."
            },
            "Load Model from Cloud": {
              "Amazon S3 Storage": "Amazon S3 is supported using the `boto3` package which you need to install\nas an additional dependency using `pip3`:\n\n```\npip3 install boto3\n```\n\nFor Rasa to be able to authenticate and download the model, you need to set the\nfollowing environment variables before running any command requiring the storage:\n\n['`AWS_SECRET_ACCESS_KEY`: environment variable containing your AWS S3 secret access key', '`AWS_ACCESS_KEY_ID`: environment variable containing your AWS S3 access key ID', '`AWS_DEFAULT_REGION`: environment variable specifying the region of your AWS S3 bucket', '`BUCKET_NAME`: environment variable specifying the S3 bucket', '`AWS_ENDPOINT_URL`: The complete URL to use for the AWS S3 requests. You need to\\nspecify a complete URL (including the \"http/https\" scheme), for example: `https://s3.amazonaws.com`.\\nNote that by setting the bucket name to `BUCKET_NAME` environment variable, you should not provide the bucket or\\nobject URL to `AWS_ENDPOINT_URL`.']\n\nOnce all environment variables are set, you can start the Rasa server with\n`remote-storage` option set to `aws`:\n\n```\nrasa run --model 20190506-100418.tar.gz --remote-storage aws\n```",
              "Google Cloud Storage": "Google Cloud Storage (GCS) is supported using the `google-cloud-storage` package\nwhich you need to install as an additional dependency using `pip3`:\n\n```\npip3 install google-cloud-storage\n```\n\nIf you are running Rasa on Google App Engine or Compute Engine, the auth\ncredentials are already set up (for the GCS in the same project). In this case,\nyou can skip setting any additional environment variables.\n\nIf you are running locally or on a machine outside of GAE or GCE you need to\nprovide the authentication details to Rasa manually:\n\n['Check out the [GCS documentation](https://cloud.google.com/docs/authentication/getting-started#auth-cloud-implicit-python)\\nand follow the descriptions on \"Creating a service account\" and\\n\"Setting the environment variable.\"', 'After you have completed the GCS instructions, you should have set an environment\\nvariable called `GOOGLE_APPLICATION_CREDENTIALS` to the path of a service account\\nkey file with access to your GCS.']\n\nOnce all environment variable is set, you can start the Rasa server with\n`remote-storage` option set to `gcs`:\n\n```\nrasa run --model 20190506-100418.tar.gz --remote-storage gcs\n```",
              "Azure Storage": "Azure Storage is supported using the `azure-storage-blob` package which you need\nto install as an additional dependency using `pip3`:\n\n```\npip3 install azure-storage-blob\n```\n\nFor Rasa to be able to authenticate and download the model, you need to set the\nfollowing environment variables before running any command requiring the storage:\n\n['`AZURE_CONTAINER`: environment variable containing your azure container name', '`AZURE_ACCOUNT_NAME`: environment variable containing your azure account name', '`AZURE_ACCOUNT_KEY`: environment variable containing your account key']\n\nOnce all environment variables are set, you can start the Rasa server with\n`remote-storage` option set to `azure`:\n\n```\nrasa run --model 20190506-100418.tar.gz --remote-storage azure\n```",
              "Other Remote Storages": "If you want to use any other Cloud Storage, you can provide your own python\nimplementation of the [`rasa.nlu.persistor.Persistor`](/reference/rasa/nlu/persistor) class.\n\nYou can start the Rasa server with `remote-storage` option set to\nthe module path of your persistor implementation:\n\n```\nrasa run --remote-storage <your module>.<class name>\n```"
            }
          },
          "metadata": {
            "id": "model-storage",
            "sidebar_label": "Model Storage",
            "title": "Model Storage",
            "abstract": "Models can be stored in different places after you trained your assistant. This\npage explains how to configure Rasa to load your models.\n"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 30]"
        },
        {
          "title": "NLG Servers",
          "description": null,
          "content": {
            "Responding to Requests": {
              "Request Format": "When your model predicts that your bot should send a response to the user,\nit will send a request to your server, giving you the information\nrequired to select or generate a response.\n\nThe body of the `POST` request sent to your NLG endpoint will be structured\nlike this:\n\n:::info New in 3.6\nWe have added an `id` field to the request body.\nThis field contains the ID of the response variation.\nYou can use this information to compose/select a proper response variation on your NLG server.\n\n:::\n\n```\n{\n  \"response\":\"utter_what_can_do\",\n  \"arguments\":{\n    \n  },\n  \"id\": \"<response_variation_id>\",\n  \"tracker\":{\n    \"sender_id\":\"user_0\",\n    \"slots\":{\n      \n    },\n    \"latest_message\":{\n      \"intent\":{\n        \"id\":3014457480322877053,\n        \"name\":\"greet\",\n        \"confidence\":0.9999994039535522\n      },\n      \"entities\":[\n        \n      ],\n      \"text\":\"Hello\",\n      \"message_id\":\"94838d6f49ff4366b254b6f6d23a90cf\",\n      \"metadata\":{\n        \n      },\n      \"intent_ranking\":[\n        {\n          \"id\":3014457480322877053,\n          \"name\":\"greet\",\n          \"confidence\":0.9999994039535522\n        },\n        {\n          \"id\":8842445304628198686,\n          \"name\":\"ask_forget_reminders\",\n          \"confidence\":5.675940428773174e-07\n        },\n        {\n          \"id\":-2566831912141022859,\n          \"name\":\"bye\",\n          \"confidence\":3.418941929567154e-08\n        },\n        {\n          \"id\":8340513453672591403,\n          \"name\":\"ask_id\",\n          \"confidence\":2.5274500714544956e-08\n        },\n        {\n          \"id\":5822154213939471096,\n          \"name\":\"ask_remind_call\",\n          \"confidence\":2.4177523982871207e-08\n        }\n      ]\n    },\n    \"latest_event_time\":1599476297.694504,\n    \"followup_action\":null,\n    \"paused\":false,\n    \"events\":[\n      {\n        \"event\":\"action\",\n        \"timestamp\":1599476297.68784,\n        \"name\":\"action_session_start\",\n        \"policy\":null,\n        \"confidence\":null\n      },\n      {\n        \"event\":\"session_started\",\n        \"timestamp\":1599476297.6878452\n      },\n      {\n        \"event\":\"action\",\n        \"timestamp\":1599476297.6878562,\n        \"name\":\"action_listen\",\n        \"policy\":null,\n        \"confidence\":null\n      },\n      {\n        \"event\":\"user\",\n        \"timestamp\":1599476297.694504,\n        \"text\":\"Hello\",\n        \"parse_data\":{\n          \"intent\":{\n            \"id\":3014457480322877053,\n            \"name\":\"greet\",\n            \"confidence\":0.9999994039535522\n          },\n          \"entities\":[\n            \n          ],\n          \"text\":\"Hello\",\n          \"message_id\":\"94838d6f49ff4366b254b6f6d23a90cf\",\n          \"metadata\":{\n            \n          },\n          \"intent_ranking\":[\n            {\n              \"id\":3014457480322877053,\n              \"name\":\"greet\",\n              \"confidence\":0.9999994039535522\n            },\n            {\n              \"id\":8842445304628198686,\n              \"name\":\"ask_forget_reminders\",\n              \"confidence\":5.675940428773174e-07\n            },\n            {\n              \"id\":-2566831912141022859,\n              \"name\":\"bye\",\n              \"confidence\":3.418941929567154e-08\n            },\n            {\n              \"id\":8340513453672591403,\n              \"name\":\"ask_id\",\n              \"confidence\":2.5274500714544956e-08\n            },\n            {\n              \"id\":5822154213939471096,\n              \"name\":\"ask_remind_call\",\n              \"confidence\":2.4177523982871207e-08\n            }\n          ]\n        },\n        \"input_channel\":\"rest\",\n        \"message_id\":\"94838d6f49ff4366b254b6f6d23a90cf\",\n        \"metadata\":{\n          \n        }\n      }\n    ],\n    \"latest_input_channel\":\"rest\",\n    \"active_loop\":{\n      \n    },\n    \"latest_action_name\":\"action_listen\"\n  },\n  \"channel\":{\n    \"name\":\"collector\"\n  }\n}\n```\n\nHere is an overview of the high-level keys in the post request: \n\nKey | Description\n---|---\n`response` | The name of the response predicted by Rasa.\n`id` | An optional string representing the response variation ID, can be null.\n`arguments` | Optional keyword arguments that can be provided by custom actions.\n`tracker` | A dictionary containing the entire conversation history.\n`channel` | The output channel this message will be sent to.\n\nYou can use any or all of this information to decide\nhow to generate your response.",
              "Response Format": "The endpoint needs to respond with the generated response. \nRasa will then send this response back to the user.\n\nBelow are the possible keys of a response and their (empty) types: \n\n```\n{\n    \"text\": \"Some text\",\n    \"buttons\": [],\n    \"image\": null,  # string of image URL\n    \"elements\": [],\n    \"attachments\": [], \n    \"custom\": {}\n}\n```\n\nYou can choose to provide just text, or a combination of different types of rich responses. \nJust like [the responses defined in the domain file](./responses.mdx), a response needs to contain at the very least\neither `text` or `custom` to be a valid response. \n\n:::caution Calling responses from stories\nIf you use an external NLG service, you don't need to specify the\nresponses under `responses` in the domain. However, you still need to add the response names\nto the `actions` list of the domain if you want to call them directly from\nyour stories.\n\n:::"
            },
            "Configuring the Server URL": "To tell Rasa where to find your NLG server, add the URL to your `endpoints.yml`: \n\n```\nnlg:\n  url: http://localhost:5055/nlg\n```\n\nIf your NLG server is protected and Rasa will need authentication to\naccess it, you can configure authentication in the endpoints:\n\n```\nnlg:\n  url: http://localhost:5055/nlg\n  # \n  # You can also specify additional parameters, if you need them:\n  # headers:\n  #   my-custom-header: value\n  # token: \"my_authentication_token\"  # will be passed as a GET parameter\n  # basic_auth:\n  #   username: user\n  #   password: pass\n```"
          },
          "metadata": {
            "id": "nlg",
            "sidebar_label": "NLG",
            "title": "NLG Servers"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 31]"
        },
        {
          "title": "NLU-Only Server",
          "description": "Read about connecting to a Rasa NLU-only server using the HTTP API.",
          "content": {
            "Connecting to an NLU server": "You can connect a [Rasa NLU-only server](./nlu-only.mdx#running-an-nlu-server) to a separately running Rasa dialogue management only server\nby adding the connection details to the dialogue management server's endpoint configuration file:\n\n```\nnlu:\n    url: \"http://<your nlu host>:<your nlu port>\"\n    token: <token>  # [optional]\n    token_name: <name of the token> # [optional] (default: token)\n```\n\nThe `token` and `token_name` refer to optional [authentication parameters](./http-api.mdx#token-based-auth).\n\nThe dialogue management server should serve a model that does not include an NLU model.\nTo obtain a dialogue management only model, train a model with `rasa train core` or use \n`rasa train` but exclude all NLU data.\n\nWhen the dialogue management server receives a message, it will [send a request](https://rasa.com/docs/rasa/pages/http-api#operation/parseModelMessage) to \n`http://<your nlu host>:<your nlu port>/model/parse` and use the parsing information returned.\n\n:::note endpoint configuration\nThe endpoint configuration for the dialogue management server will include an `nlu` endpoint that refers to your NLU only server. Therefore you should **use a separate endpoint configuration file** for the NLU server, excluding the `nlu` endpoint.\n:::\n\nIf you are implementing a custom NLU server (i.e. not Rasa NLU), your server should provide a `/model/parse` endpoint that responds to requests in the same \nformat as a Rasa NLU server does."
          },
          "metadata": {
            "id": "nlu-only-server",
            "sidebar_label": "NLU-Only Server",
            "title": "NLU-Only Server",
            "description": "Read about connecting to a Rasa NLU-only server using the HTTP API.",
            "abstract": "You can run an NLU-only server and use the HTTP API to connect to it."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 32]"
        },
        {
          "title": "Using NLU Only",
          "description": null,
          "content": {
            "Training NLU-only models": "To train an NLU model only, run:\n\n```\nrasa train nlu\n```\n\nThis will look for NLU training data files in the ``data/`` directory\nand saves a trained model in the ``models/`` directory.\nThe name of the model will start with ``nlu-``.",
            "Testing your NLU model on the command line": "To try out your NLU model on the command line, run the following command:\n\n```\nrasa shell nlu\n```\n\nThis will start the rasa shell and ask you to type in a message to test.\nYou can keep typing in as many messages as you like.\n\nAlternatively, you can leave out the ``nlu`` argument and pass in a nlu-only model directly:\n\n```\nrasa shell -m models/nlu-20190515-144445.tar.gz\n```",
            "Running an NLU server": "To start a server with your NLU model, pass in the model name at runtime:\n\n```\nrasa run --enable-api -m models/nlu-20190515-144445.tar.gz\n```\n\nYou can then request predictions from your model using the ``/model/parse`` endpoint.\nTo do this, run:\n\n```\ncurl localhost:5005/model/parse -d '{\"text\":\"hello\"}'\n```"
          },
          "metadata": {
            "id": "nlu-only",
            "sidebar_label": "Using NLU Only",
            "title": "Using NLU Only",
            "abstract": "Find out how to use only Rasa NLU as a standalone NLU service for your chatbot or virtual assistant."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 33]"
        },
        {
          "title": "NLU Training Data",
          "description": "Read more about how to format training data with Rasa NLU for open source natural language processing.",
          "content": {
            "Training Examples": "NLU training data consists of example user utterances categorized by\nintent. \nTo make it easier to use your intents, give them names that relate to what the user wants to accomplish with that intent, keep them in lowercase, and avoid spaces and special characters. \n\n:::note\nThe `/` symbol is reserved as a delimiter to separate [retrieval intents](glossary.mdx#retrieval-intent) from response text identifiers. Make sure not\nto use it in the name of your intents.\n\n:::",
            "Entities": "[Entities](glossary.mdx#entity) are structured pieces of information inside a user message. \nFor entity extraction to work, you need to either specify training data to train an ML model or you need to define [regular expressions](#regular-expressions-for-entity-extraction) to extract entities using the [`RegexEntityExtractor`](components.mdx#regexentityextractor) based on a character pattern.\n\nWhen deciding which entities you need to extract, think about what information your assistant needs for its user goals. The user might provide additional pieces of information that you don't need for any user goal; you don't need to extract these as entities.\n\nSee the [training data format](./training-data-format.mdx) for details on how to annotate entities in your training data.",
            "Synonyms": "Synonyms map extracted entities to a value other than the literal text extracted in a case-insensitive manner. \nYou can use synonyms when there are multiple ways users refer to the same\nthing. Think of the end goal of extracting an entity, and figure out from there which values should be considered equivalent. \n\nLet's say you had an entity `account` that you use to look up the user's balance. One of the possible account types is \"credit\". Your users also refer to their \"credit\" account as \"credit\naccount\" and \"credit card account\".\n\nIn this case, you could define \"credit card account\" and \"credit account\" as\nsynonyms to \"credit\":\n\n```\nnlu:\n- synonym: credit\n  examples: |\n    - credit card account\n    - credit account\n```\n\nThen, if either of these phrases is extracted as an entity, it will be\nmapped to the value `credit`. Any alternate casing of these phrases (e.g. `CREDIT`, `credit ACCOUNT`) will also be mapped to the synonym.\n\n:::note Provide Training Examples\nSynonym mapping only happens **after** entities have been extracted.\nThat means that your training examples should include the synonym examples\n(`credit card account` and `credit account`) so that the model will learn to\nrecognize these as entities and replace them with `credit`.\n:::\n\nSee the [training data format](./training-data-format.mdx) for details on how to include synonyms in your training data.",
            "Regular Expressions": {
              "Regular Expressions for Intent Classification": "You can use regular expressions to improve intent classification by including the `RegexFeaturizer` component in your pipeline. When using the `RegexFeaturizer`, a regex does not act as a rule for classifying an intent. It only provides a feature that the intent classifier will use\nto learn patterns for intent classification.\nCurrently, all intent classifiers make use of available regex features.\n\nThe name of a regex in this case is a human readable description. It can help you remember what a regex is used for, and it is the title of the corresponding pattern feature. It does not have to match any intent or entity name. A regex for a \"help\" request might look like this:\n\n```\nnlu:\n- regex: help\n  examples: |\n    - \\bhelp\\b\n```\n\nThe intent being matched could be `greet`,`help_me`, `assistance` or anything else.\n\nTry to create your regular expressions in a way that they match as few\nwords as possible. E.g. using `\\bhelp\\b` instead of `help.*`, as the\nlater one might match the whole message whereas the first one only\nmatches a single word.\n\n:::note Provide Training Examples\nThe `RegexFeaturizer` provides features to the intent classifier, but it doesn't predict the intent directly. Include enough examples containing the regular expression so that the intent classifier can learn to use the regular expression feature.\n:::",
              "Regular Expressions for Entity Extraction": {
                "Regular Expressions as Features": "You can use regular expressions to create features for the [`RegexFeaturizer`](components.mdx#regexfeaturizer) component in your NLU pipeline.\n\nWhen using a regular expression with the  `RegexFeaturizer`, the\nname of the regular expression does not matter.\nWhen using the `RegexFeaturizer`, a regular expression provides a feature\nthat helps the model learn an association between intents/entities and inputs\nthat fit the regular expression. \n\n:::note Provide Training Examples\nThe `RegexFeaturizer` provides features to the entity extractor, but it doesn't predict the entity directly. Include enough examples containing the regular expression so that the entity extractor can learn to use the regular expression feature.\n\n:::\n\nRegex features for entity extraction\nare currently only supported by the `CRFEntityExtractor` and `DIETClassifier` components. Other entity extractors, like\n`MitieEntityExtractor` or `SpacyEntityExtractor`, won't use the generated\nfeatures and their presence will not improve entity recognition for\nthese extractors.",
                "Regular Expressions for Rule-based Entity Extraction": "You can use regular expressions for rule-based entity extraction using the [`RegexEntityExtractor`](components.mdx#regexentityextractor) component in your NLU pipeline.\n\nWhen using the `RegexEntityExtractor`, the name of the regular expression should\nmatch the name of the entity you want to extract.\nFor example, you could extract account numbers of 10-12 digits by including this regular expression and at least two annotated examples in your training data:\n\n```\nnlu:\n- regex: account_number\n  examples: |\n    - \\d{10,12}\n- intent: inform\n  examples: |\n    - my account number is [1234567891](account_number)\n    - This is my account number [1234567891](account_number)\n```\n\nWhenever a user message contains a sequence of 10-12 digits, it will be extracted as an `account_number` entity. `RegexEntityExtractor` doesn't require training examples to learn to extract the entity, but you do need at least two annotated examples of the entity so that the NLU model can register it as an entity at training time."
              }
            },
            "Lookup Tables": "Lookup tables are lists of words used to generate\ncase-insensitive regular expression patterns. They can be used in the same ways as [regular expressions](#regular-expressions) are used, in combination with the [`RegexFeaturizer`](components.mdx#regexfeaturizer) and [`RegexEntityExtractor`](components.mdx#regexentityextractor) components in the pipeline.\n\nYou can use lookup tables to help extract entities which have a known set of possible values. Keep your lookup tables as specific as possible. For example, to extract country names, you could add a lookup table of all countries in the world:\n\n```\nnlu:\n- lookup: country\n  examples: |\n    - Afghanistan\n    - Albania\n    - ...\n    - Zambia\n    - Zimbabwe\n```\n\nWhen using lookup tables with `RegexFeaturizer`, provide enough examples for the intent or entity you want to match so that the model can learn to use the generated regular expression as a feature. When using lookup tables with `RegexEntityExtractor`, provide at least two annotated examples of the entity so that the NLU model can register it as an entity at training time.",
            "Entities Roles and Groups": {
              "Entity Roles and Groups influencing dialogue predictions": "If you want to influence the dialogue predictions by roles or groups, you need to modify your stories to contain\nthe desired role or group label. You also need to list the corresponding roles and groups of an entity in your\n[domain file](./domain.mdx#entities).\n\nLet's assume you want to output a different sentence depending on what the user's location is. E.g.\nif the user just arrived from London, you might want to ask how the trip to London was. But if the user is on the way\nto Madrid, you might want to wish the user a good stay. You can achieve this with the\nfollowing two stories:\n\n```\nstories:\n- story: The user just arrived from another city.\n  steps:\n    - intent: greet\n    - action: utter_greet\n    - intent: inform_location\n      entities:\n        - city: London\n          role: from\n    - action: utter_ask_about_trip\n\n- story: The user is going to another city.\n  steps:\n    - intent: greet\n    - action: utter_greet\n    - intent: inform_location\n      entities:\n        - city: Madrid\n          role: to\n    - action: utter_wish_pleasant_stay\n```"
            },
            "BILOU Entity Tagging": "The [DIETClassifier](./components.mdx#dietclassifier) and [CRFEntityExtractor](./components.mdx#crfentityextractor)\nhave the option `BILOU_flag`, which refers to a tagging schema that can be\nused by the machine learning model when processing entities.\n`BILOU` is short for Beginning, Inside, Last, Outside, and Unit-length.\n\nFor example, the training example\n\n```\n[Alex]{\"entity\": \"person\"} is going with [Marty A. Rick]{\"entity\": \"person\"} to [Los Angeles]{\"entity\": \"location\"}.\n```\n\nis first split into a list of tokens. Then the machine learning model applies the tagging schema\nas shown below depending on the value of the option `BILOU_flag`:\n\n| token   | `BILOU_flag = true`  | `BILOU_flag = false`  |\n|---------|----------------------|-----------------------|\n| alex    | U-person             | person                |\n| is      | O                    | O                     |\n| going   | O                    | O                     |\n| with    | O                    | O                     |\n| marty   | B-person             | person                |\n| a       | I-person             | person                |\n| rick    | L-person             | person                |\n| to      | O                    | O                     |\n| los     | B-location           | location              |\n| angeles | L-location           | location              |\n\nThe BILOU tagging schema is richer compared to the normal tagging schema. It may help to improve the\nperformance of the machine learning model when predicting entities.\n\n:::note inconsistent BILOU tags\nWhen the option `BILOU_flag` is set to `True`, the model may predict inconsistent BILOU tags, e.g.\n`B-person I-location L-person`. Rasa uses some heuristics to clean up the inconsistent BILOU tags.\nFor example, `B-person I-location L-person` would be changed into `B-person I-person L-person`.\n:::"
          },
          "metadata": {
            "id": "nlu-training-data",
            "sidebar_label": "NLU Training Data",
            "title": "NLU Training Data",
            "description": "Read more about how to format training data with Rasa NLU for open source natural language processing.",
            "abstract": "NLU training data stores structured information about user messages."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 34]"
        },
        {
          "title": "PII Management",
          "description": null,
          "content": {
            "Architecture Overview": {
              "Supported Rasa Events": "The Rasa events that are anonymized include the following:\n\n['`user`', '`bot`', '`slot`', '`entities`']",
              "Supported PII entity types": "The anonymization pipeline uses [Microsoft Presidio](https://microsoft.github.io/presidio/text_anonymization/) as both entity recognizer and\nanonymizer. Presidio is an open-source library that supports a wide range of entity types and anonymization methods.\n\nYou can specify any of the out-of-the-box [supported Presidio entity types](https://microsoft.github.io/presidio/supported_entities/) in the anonymization rules.\nNote that it is currently not possible to add custom entity types for the Rasa Pro anonymization pipeline."
            },
            "How to write anonymization rules": {
              "How to populate the metadata section": {
                "How to install the language  model": "You must install the model you declare to the `model_name` field in your Rasa Pro environment.\nFor example, if you declare `model_name: en_core_web_lg`, you must install the spaCy `en_core_web_lg` model in your Rasa Pro environment.\nYou can follow model installation instructions for spaCy and stanza models in the [Presidio official documentation](https://microsoft.github.io/presidio/analyzer/nlp_engines/spacy_stanza/).\n\nIn the case of the `transformers` model provider, you must install _both_  models that you declare in the `model_name` field\nin your Rasa Pro environment. For example, if you declare `transformers: dslim/bert-base-NER` in `endpoints.yml`, you must\ninstall the `dslim/bert-base-NER` model in your Rasa Pro environment. You can find model download instructions for\n`HuggingFace` in the [Presidio official documentation](https://microsoft.github.io/presidio/analyzer/nlp_engines/transformers/).\n\n:::note\nNot all languages have a pre-trained language model available. If you want to use a language that does not have\na pre-trained language model available, you must train your own [spaCy](https://spacy.io/usage/training), [stanza](https://stanfordnlp.github.io/stanza/training.html)\nor [huggingface](https://huggingface.co/docs/transformers/training) model and install it in your Rasa Pro environment.\n:::"
              },
              "How to populate the rule_lists section": "The `rule_lists` section contains a list of anonymization rule lists. Each rule list must have a unique `id` of type\nstring and a list of `rules`. Each rule must have an `entity` field and a `substitution` field. The `entity` field\nspecifies the Presidio entity type to be anonymized and must be in uppercase. Note that regular expressions are currently\nnot supported for identifying entities.\n\nThe `substitution` field specifies the anonymization method to be used. Currently, the\nfollowing anonymization methods are supported: `text`, `mask`, and `faker`:\n\n['The `text` anonymization method replaces the original entity value with the value specified in the `value` field.\\nIn the following example, the `PERSON` entity value will be replaced with `John Doe`.']\n\n```\nanonymization:\n  metadata:\n    language: en\n    model_name: en_core_web_lg\n    model_provider: spacy\n  rule_lists:\n    - id: rules_1\n      rules:\n        - entity: PERSON\n          substitution: text\n          value: John Doe\n```\n\n[\"The `mask` anonymization method replaces the original entity value with a mask of the same length using the character '*'.\\nFor example, if the original entity value is `John Doe`, the anonymized value will be `********`.\"]\n\n```\nanonymization:\n  metadata:\n    language: en\n    model_name: en_core_web_lg\n    model_provider: spacy\n  rule_lists:\n    - id: rules_1\n      rules:\n        - entity: PERSON\n          substitution: mask\n```\n\n['The `faker` anonymization method replaces the original entity value with a fake value generated by the [Faker](https://faker.readthedocs.io/en/stable/) library.\\nFor example, if the original entity value is `John Doe`, the anonymized value will be replaced with a fake name generated by the Faker library.']\n\n```\nanonymization:\n  metadata:\n    language: en\n    model_name: en_core_web_lg\n    model_provider: spacy\n  rule_lists:\n    - id: rules_1\n      rules:\n        - entity: PERSON\n          substitution: faker\n```\n\nIf no substitution method is specified, the default substitution method is `mask`.\n\nThe `value` field is only required for the `text` anonymization method. It specifies the text to be used as the anonymized value.\nIf the `value` field is not specified, the original entity value to be anonymized will be replaced with the entity type\nname between brackets. For example, if the `value` field is not specified for the `PERSON` entity type, the anonymized\nvalue will be `<PERSON>`.\n\nThe `faker` anonymization method uses the [Faker](https://faker.readthedocs.io/en/stable/) library to generate fake data.\nBy default, the `faker` anonymization method will generate fake data in English unless a localized Presidio\nentity type is used. For example, if you use the `faker` substitution method for the `ES_NIF` entity type, the generated\nfake data will match the format of a Spanish NIF.\n\nThe `faker` substitution method does not support the following Presidio entity types:\n\n['`CRYPTO`, `NRP`, `MEDICAL_LICENSE`', '`US_BANK_NUMBER`, `US_DRIVER_LICENSE`', '`UK_NHS`', '`IT_FISCAL_CODE`, `IT_DRIVER_LICENSE`, `IT_PASSPORT`, `IT_IDENTITY_CARD`', '`SG_NRIC_FIN`', '`AU_ABN`, `AU_ACN`, `AU_TFN`, `AU_MEDICARE`']\n\nIf any of the above entities is used together with the `faker` substitution method, the anonymization pipeline will default\nto the `mask` substitution method."
            },
            "How to update the Kafka event broker configuration": {
              "Streaming anonymized events to Rasa X/Enterprise with Kafka": "Streaming anonymized events to Rasa X/Enterprise is only supported for Rasa X/Enterprise versions `1.3.0` and above.\nIn addition, you must use the Kafka event broker, other event broker types are not supported.\n\nYou can stream anonymized events to Rasa X/Enterprise via Kafka by adding the `rasa_x_consumer: true` key-value pair to\nthe `anonymization_topics` section:\n\n```\nevent_broker:\n  type: kafka\n  partition_by_sender: True\n  url: localhost\n  anonymization_topics:\n    - name: topic_1\n      anonymization_rules: rules_1\n      rasa_x_consumer: true\n    - name: topic_2\n      anonymization_rules: rules_2\n```\n\nIf multiple Kafka anonymization topics contain the `rasa_x_consumer` key-value pair, the anonymized events will be streamed\nto the Kafka topic that is mapped to the first topic in the `anonymization_topics` list that contains the `rasa_x_consumer`\nkey-value pair.\n\nNote that the `rasa_x_consumer` key-value pair is optional. If it is not specified, the anonymized events will be published\nto the Kafka topic, but they will not be streamed to Rasa X/Enterprise."
            },
            "How to enable anonymization of PII in logs": "You can enable anonymization of PII in logs by filling the `logger` section in the `endpoints.yml` file.\nThe `logger` section must have the following structure:\n\n```\nlogger:\n  formatter:\n    anonymization_rules: rules_1\n```\n\nThe `anonymization_rules` field specifies the `id` of the anonymization rule list to be used for the logs.\n\n:::caution\nWe strongly recommend to run with log level INFO in production.\nRunning with log level DEBUG will increase the assistant's response latency because of processing delays.\n:::\n\nNote that running `rasa shell` in debug mode with a Kafka event broker might result in logs related to the event publishing\nto be printed to console **after** the bot message. This behaviour is expected because the event anonymization and publishing\nis done asynchronously as a background task, so it will complete after the assistant has already predicted and executed the\nbot response."
          },
          "metadata": {
            "id": "pii-management",
            "sidebar_label": "PII Management",
            "title": "PII Management",
            "hide_table_of_contents": false
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 35]"
        },
        {
          "title": "Policies",
          "description": null,
          "content": {
            "Action Selection": {
              "Policy Priority": "In the case that two policies predict with equal confidence (for example, the Memoization\nand Rule Policies might both predict with confidence 1), the priority of the\npolicies is considered. Rasa policies have default priorities that are set to ensure the\nexpected outcome in the case of a tie. They look like this, where higher numbers have higher priority:\n\n<!-- We want to have high priority policies first; it's not possible to use a Markdown ordered list for that. -->\n\n['6 - `RulePolicy`', '3 - `MemoizationPolicy` or `AugmentedMemoizationPolicy`', '2 - `UnexpecTEDIntentPolicy`', '1 - `TEDPolicy`']\n\nIn general, it is not recommended to have more\nthan one policy per priority level in your configuration. If you have 2 policies with the same priority and they predict\nwith the same confidence, the resulting action will be chosen randomly.\n\nIf you create your own policy, use these priorities as a guide for figuring out the priority of your policy.\nIf your policy is a machine learning policy, it should most likely have priority 1, the same as the `TEDPolicy`.\n\n:::warning overriding policy priorities\nAll policy priorities are configurable via the `priority` parameter in the policy's configuration,\nbut we **do not recommend** changing them outside of specific cases such as custom policies.\nDoing so can lead to unexpected and undesired bot behavior.\n\n:::"
            },
            "Machine Learning Policies": {
              "TED Policy": "The Transformer Embedding Dialogue (TED) Policy is\na multi-task architecture for next action prediction and entity\nrecognition. The architecture consists of several transformer encoders which are shared for both tasks.\nA sequence of entity labels is predicted through a Conditional Random Field (CRF) tagging layer on top of the\nuser sequence transformer encoder output corresponding to the input sequence of tokens.\nFor the next action prediction, the dialogue transformer encoder output and the system action labels are embedded into a\nsingle semantic vector space. We use the dot-product loss to maximize the similarity with the target label and\nminimize similarities with negative samples.\n\nIf you want to learn more about the model, check out\n[our paper](https://arxiv.org/abs/1910.00486) and on our\n[youtube channel](https://www.youtube.com/watch?v=j90NvurJI4I&list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb&index=14&ab_channel=Rasa).\nwhere we explain the model architecture in detail.\n\nTED Policy architecture comprises the following steps:\n\n['Concatenate features for', ['user input (user intent and entities) or user text processed through a user sequence transformer encoder,', 'previous system actions or bot utterances processed through a bot sequence transformer encoder,', 'slots and active forms'], 'for each time step into an input vector to the embedding layer that precedes the\\ndialogue transformer.', 'Feed the embedding of the input vector into the dialogue transformer encoder.', 'Apply a dense layer to the output of the dialogue transformer to get embeddings of the dialogue for each time step.', 'Apply a dense layer to create embeddings for system actions for each time step.', 'Calculate the similarity between the dialogue embedding and embedded system actions.\\nThis step is based on the [StarSpace](https://arxiv.org/abs/1709.03856) idea.', 'Concatenate the token-level output of the user sequence transformer encoder\\nwith the output of the dialogue transformer encoder for each time step.', 'Apply CRF algorithm to predict contextual entities for each user text input.']\n\n**Configuration:**\n\nYou can pass configuration parameters to the `TEDPolicy` using the `config.yml` file.\nIf you want to fine-tune your model, start by modifying the following parameters:\n\n[\"`epochs`:\\nThis parameter sets the number of times the algorithm will see the training data (default: `1`).\\nOne `epoch` is equals to one forward pass and one backward pass of all the training examples.\\nSometimes the model needs more epochs to properly learn.\\nSometimes more epochs don't influence the performance.\\nThe lower the number of epochs the faster the model is trained.\\nHere is how the config would look like:\", '```\\npolicies:\\n- name: TEDPolicy\\n  epochs: 200\\n```', '`max_history`:\\nThis parameter controls how much dialogue history the model looks at to decide which\\naction to take next. Default `max_history` for this policy is `None`,\\nwhich means that the complete dialogue history since session restart is taken into\\naccount. If you want to limit the model to only see a certain number of previous\\ndialogue turns, you can set `max_history` to a finite value.\\nPlease note that you should pick `max_history` carefully, so that the model has enough\\nprevious dialogue turns to create a correct prediction.\\nSee [Featurizers](#featurizers) for more details.\\nHere is how the config would look like:', '```\\npolicies:\\n- name: TEDPolicy\\n  max_history: 8\\n```', '`number_of_transformer_layers`:\\nThis parameter sets the number of sequence transformer encoder layers to use for\\nsequential transformer encoders for user, action and action label texts and for\\ndialogue transformer encoder.\\n(defaults: `text: 1, action_text: 1, label_action_text: 1, dialogue: 1`).\\nThe number of sequence transformer encoder layers corresponds\\nto the transformer blocks to use for the model.', '`transformer_size`:\\nThis parameter sets the number of units in the sequence transformer encoder layers to use for\\nsequential transformer encoders for user, action and action label texts and for\\ndialogue transformer encoder.\\n(defaults: `text: 128, action_text: 128, label_action_text: 128, dialogue: 128`).\\nThe vectors coming out of the transformer encoders will have the given `transformer_size`.', '`connection_density`:\\nThis parameter defines the fraction of kernel weights that are set to non zero values for all feed forward\\nlayers in the model (default: `0.2`). The value should be between 0 and 1. If you set `connection_density`\\nto 1, no kernel weights will be set to 0, the layer acts as a standard feed forward layer. You should not\\nset `connection_density` to 0 as this would result in all kernel weights being 0, i.e. the model is not able\\nto learn.', '`split_entities_by_comma`:\\nThis parameter defines whether adjacent entities separated by a comma should be treated as one, or split. For example,\\nentities with the type `ingredients`, like \"apple, banana\" can be split into \"apple\" and \"banana\". An entity with type\\n`address`, like \"Schönhauser Allee 175, 10119 Berlin\" should be treated as one.', 'Can either be\\n`True`/`False` globally:', '```\\npolicies:\\n  - name: TEDPolicy\\n    split_entities_by_comma: True\\n```', 'or set per entity type, such as:', '```\\npolicies:\\n  - name: TEDPolicy\\n    split_entities_by_comma:\\n      address: False\\n      ingredients: True\\n```', '`constrain_similarities`:\\nThis parameter when set to `True` applies a sigmoid cross entropy loss over all similarity terms.\\nThis helps in keeping similarities between input and negative labels to smaller values.\\nThis should help in better generalization of the model to real world test sets.', '`model_confidence`:\\nThis parameter allows the user to configure how confidences are computed during inference. Currently, only one value is supported:', ['`softmax`: Confidences are in the range `[0, 1]` (old behavior and current default). Computed similarities are normalized with the `softmax` activation function.'], '`use_gpu`:\\nThis parameter defines whether a GPU (if available) will be used training. By default, `TEDPolicy` will be trained on GPU \\nif a GPU is available (i.e. `use_gpu` is `True`). To enforce that `TEDPolicy` uses only the CPU for training, set `use_gpu` to `False`.']\n\nThe above configuration parameters are the ones you should configure to fit your model to your data.\nHowever, additional parameters exist that can be adapted.\n\n<details><summary>More configurable parameters</summary>\n\n```\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| Parameter                             | Default Value          | Description                                                  |\n+=======================================+========================+==============================================================+\n| hidden_layers_sizes                   | text: []               | Hidden layer sizes for layers before the embedding layers    |\n|                                       | action_text: []        | for user messages and bot messages in previous actions       |\n|                                       | label_action_text: []  | and labels. The number of hidden layers is                   |\n|                                       |                        | equal to the length of the corresponding list.               |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| dense_dimension                       | text: 128              | Dense dimension for sparse features to use after they are    |\n|                                       | action_text: 128       | converted into dense features.                               |\n|                                       | label_action_text: 128 |                                                              |\n|                                       | intent: 20             |                                                              |\n|                                       | action_name: 20        |                                                              |\n|                                       | label_action_name: 20  |                                                              |\n|                                       | entities: 20           |                                                              |\n|                                       | slots: 20              |                                                              |\n|                                       | active_loop: 20        |                                                              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| concat_dimension                      | text: 128              | Common dimension to which sequence and sentence features of  |\n|                                       | action_text: 128       | different dimensions get converted before concatenation.     |\n|                                       | label_action_text: 128 |                                                              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| encoding_dimension                    | 50                     | Dimension size of embedding vectors                          |\n|                                       |                        | before the dialogue transformer encoder.                     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| transformer_size                      | text: 128              | Number of units in user text sequence transformer encoder.   |\n|                                       | action_text: 128       | Number of units in bot text sequence transformer encoder.    |\n|                                       | label_action_text: 128 | Number of units in bot text sequence transformer encoder.    |\n|                                       | dialogue: 128          | Number of units in dialogue transformer encoder.             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| number_of_transformer_layers          | text: 1                | Number of layers in user text sequence transformer encoder.  |\n|                                       | action_text: 1         | Number of layers in bot text sequence transformer encoder.   |\n|                                       | label_action_text: 1   | Number of layers in bot text sequence transformer encoder.   |\n|                                       | dialogue: 1            | Number of layers in dialogue transformer encoder.            |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| number_of_attention_heads             | 4                      | Number of self-attention heads in transformers.              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| unidirectional_encoder                | True                   | Use a unidirectional or bidirectional encoder                |\n|                                       |                        | for `text`, `action_text`, and `label_action_text`.          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_key_relative_attention            | False                  | If 'True' use key relative embeddings in attention.          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_value_relative_attention          | False                  | If 'True' use value relative embeddings in attention.        |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| max_relative_position                 | None                   | Maximum position for relative embeddings.                    |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| batch_size                            | [64, 256]              | Initial and final value for batch sizes.                     |\n|                                       |                        | Batch size will be linearly increased for each epoch.        |\n|                                       |                        | If constant `batch_size` is required, pass an int, e.g. `8`. |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| batch_strategy                        | \"balanced\"             | Strategy used when creating batches.                         |\n|                                       |                        | Can be either 'sequence' or 'balanced'.                      |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| epochs                                | 1                      | Number of epochs to train.                                   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| random_seed                           | None                   | Set random seed to any 'int' to get reproducible results.    |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| learning_rate                         | 0.001                  | Initial learning rate for the optimizer.                     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| embedding_dimension                   | 20                     | Dimension size of dialogue & system action embedding vectors.|\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| number_of_negative_examples           | 20                     | The number of incorrect labels. The algorithm will minimize  |\n|                                       |                        | their similarity to the user input during training.          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| similarity_type                       | \"auto\"                 | Type of similarity measure to use, either 'auto' or 'cosine' |\n|                                       |                        | or 'inner'.                                                  |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| loss_type                             | \"cross_entropy\"        | The type of the loss function, either 'cross_entropy'        |\n|                                       |                        | or 'margin'.                                                 |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| ranking_length                        | 0                      | Number of top actions to include in prediction. Confidences  |\n|                                       |                        | of all other actions will be set to 0. Set to 0 to let the   |\n|                                       |                        | prediction include confidences for all actions.              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| renormalize_confidences               | False                  | Normalize the top predictions. Applicable only with loss     |\n|                                       |                        | type 'cross_entropy' and 'softmax' confidences.              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| maximum_positive_similarity           | 0.8                    | Indicates how similar the algorithm should try to make       |\n|                                       |                        | embedding vectors for correct labels.                        |\n|                                       |                        | Should be 0.0 < ... < 1.0 for 'cosine' similarity type.      |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| maximum_negative_similarity           | -0.2                   | Maximum negative similarity for incorrect labels.            |\n|                                       |                        | Should be -1.0 < ... < 1.0 for 'cosine' similarity type.     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_maximum_negative_similarity       | True                   | If 'True' the algorithm only minimizes maximum similarity    |\n|                                       |                        | over incorrect intent labels, used only if 'loss_type' is    |\n|                                       |                        | set to 'margin'.                                             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| scale_loss                            | True                   | Scale loss inverse proportionally to confidence of correct   |\n|                                       |                        | prediction.                                                  |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| regularization_constant               | 0.001                  | The scale of regularization.                                 |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| negative_margin_scale                 | 0.8                    | The scale of how important it is to minimize the maximum     |\n|                                       |                        | similarity between embeddings of different labels.           |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| drop_rate_dialogue                    | 0.1                    | Dropout rate for embedding layers of dialogue features.      |\n|                                       |                        | Value should be between 0 and 1.                             |\n|                                       |                        | The higher the value the higher the regularization effect.   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| drop_rate_label                       | 0.0                    | Dropout rate for embedding layers of label features.         |\n|                                       |                        | Value should be between 0 and 1.                             |\n|                                       |                        | The higher the value the higher the regularization effect.   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| drop_rate_attention                   | 0.0                    | Dropout rate for attention. Value should be between 0 and 1. |\n|                                       |                        | The higher the value the higher the regularization effect.   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| connection_density                    | 0.2                    | Connection density of the weights in dense layers.           |\n|                                       |                        | Value should be between 0 and 1.                             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_sparse_input_dropout              | True                   | If 'True' apply dropout to sparse input tensors.             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_dense_input_dropout               | True                   | If 'True' apply dropout to sparse features after they are    |\n|                                       |                        | converted into dense features.                               |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| evaluate_every_number_of_epochs       | 20                     | How often to calculate validation accuracy.                  |\n|                                       |                        | Set to '-1' to evaluate just once at the end of training.    |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| evaluate_on_number_of_examples        | 0                      | How many examples to use for hold out validation set.        |\n|                                       |                        | Large values may hurt performance, e.g. model accuracy.      |\n|                                       |                        | Keep at 0 if your data set contains a lot of unique examples |\n|                                       |                        | of dialogue turns.                                           |\n|                                       |                        | Set to 0 for no validation.                                  |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| tensorboard_log_directory             | None                   | If you want to use tensorboard to visualize training         |\n|                                       |                        | metrics, set this option to a valid output directory. You    |\n|                                       |                        | can view the training metrics after training in tensorboard  |\n|                                       |                        | via 'tensorboard --logdir <path-to-given-directory>'.        |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| tensorboard_log_level                 | \"epoch\"                | Define when training metrics for tensorboard should be       |\n|                                       |                        | logged. Either after every epoch ('epoch') or for every      |\n|                                       |                        | training step ('batch').                                     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| checkpoint_model                      | False                  | Save the best performing model during training. Models are   |\n|                                       |                        | stored to the location specified by `--out`. Only the one    |\n|                                       |                        | best model will be saved.                                    |\n|                                       |                        | Requires `evaluate_on_number_of_examples > 0` and            |\n|                                       |                        | `evaluate_every_number_of_epochs > 0`                        |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| e2e_confidence_threshold              | 0.5                    | The threshold that ensures that end-to-end is picked only if |\n|                                       |                        | the policy is confident enough.                              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| featurizers                           | []                     | List of featurizer names (alias names). Only features        |\n|                                       |                        | coming from the listed names are used. If list is empty      |\n|                                       |                        | all available features are used.                             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| entity_recognition                    | True                   | If 'True' entity recognition is trained and entities are     |\n|                                       |                        | extracted.                                                   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| constrain_similarities                | False                  | If `True`, applies sigmoid on all similarity terms and adds  |\n|                                       |                        | it to the loss function to ensure that similarity values are |\n|                                       |                        | approximately bounded.                                       |\n|                                       |                        | Used only when `loss_type=cross_entropy`.                    |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| model_confidence                      | \"softmax\"              | Affects how model's confidence for each action               |\n|                                       |                        | is computed. Currently, only one value is supported:         |\n|                                       |                        | 1. `softmax` - Similarities between input and action         |\n|                                       |                        | embeddings are post-processed with a softmax function,       |\n|                                       |                        | as a result of which confidence for all labels sum up to 1.  |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| BILOU_flag                            | True                   | If 'True', additional BILOU tags are added to entity labels. |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| split_entities_by_comma               | True                   | Splits a list of extracted entities by comma to treat each   |\n|                                       |                        | one of them as a single entity. Can either be `True`/`False` |\n|                                       |                        | globally, or set per entity type, such as:                   |\n|                                       |                        | ```                                                          |\n|                                       |                        | - name: TEDPolicy                                            |\n|                                       |                        |   split_entities_by_comma:                                   |\n|                                       |                        |     address: True                                            |\n|                                       |                        | ```                                                          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n```\n\n:::note\nThe parameter `maximum_negative_similarity` is set to a negative value to mimic the original\nstarspace algorithm in the case `maximum_negative_similarity = maximum_positive_similarity` and\n`use_maximum_negative_similarity = False`. See [starspace paper](https://arxiv.org/abs/1709.03856)\nfor details.\n\n:::\n\n</details>\n\n:::note\nIn addition to the config parameters above, `TEDPolicy` prediction performance and\ntraining time are affected by the `--augmentation` argument of the `rasa train`\ncommand. For more information see\n[Data Augmentation](./policies.mdx#data-augmentation).\n\n:::",
              "UnexpecTED Intent Policy": {
                "Prediction of `action_unlikely_intent`": "`UnexpecTEDIntentPolicy` is invoked immediately after a user utterance and can either\ntrigger `action_unlikely_intent` or abstain (in which case other policies will predict actions).\nTo determine if `action_unlikely_intent` should be triggered, `UnexpecTEDIntentPolicy` computes a score\nfor the user's intent in the current dialogue context and checks if this score is below a\ncertain threshold score.\n\nThis threshold score is computed by collecting the ML model's output on many \"negative examples\".\nThese negative examples are combinations of dialogue contexts and user\nintents that are _incorrect_. `UnexpecTEDIntentPolicy` generates these negative examples from your\ntraining data by picking a random story part and pairing it with a random intent that doesn't\noccur at this point. For example, if you had just one training story:\n\n```\nversion: 2.0\nstories:\n- story: happy path 1\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - intent: mood_great\n  - action: utter_goodbye\n```\n\nand an intent `affirm`, then a valid negative example will be:\n\n```\nversion: 2.0\nstories:\n- story: negative example with affirm unexpected\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - intent: affirm\n```\n\nHere, `affirm` intent is unexpected as it doesn't occur in this particular conversation context across all training stories.\nFor each intent, `UnexpecTEDIntentPolicy` uses these negative examples to figure out the range of scores the model\npredicts. The threshold score is picked from this range of scores in such a way that the predicted score for a\ncertain percentage of negative examples is higher than the threshold score and hence `action_unlikely_intent`\nis not triggered for them. This percentage of negative examples can be controlled by the `tolerance` parameter.\nThe higher the `tolerance`, the lower the intent's score (the more unlikely the intent) needs to be\nbefore `UnexpecTEDIntentPolicy` triggers the `action_unlikely_intent` action.\n\n**Configuration:**\n\nYou can pass configuration parameters to the `UnexpecTEDIntentPolicy` using the `config.yml` file.\nIf you want to fine-tune model's performance, start by modifying the following parameters:\n\n[\"`epochs`:\\nThis parameter sets the number of times the algorithm will see the training data (default: `1`).\\nOne `epoch` is equals to one forward pass and one backward pass of all the training examples.\\nSometimes the model needs more epochs to learn properly.\\nSometimes more epochs don't influence the performance.\\nThe lower the number of epochs the faster the model is trained.\\nHere is how the config would look like:\", '```\\npolicies:\\n- name: UnexpecTEDIntentPolicy\\n  epochs: 200\\n```', '`max_history`:\\nThis parameter controls how much dialogue history the model looks at before making an inference.\\nDefault `max_history` for this policy is `None`, which means that the complete dialogue history\\nsince session (re)start is taken into account. If you want to limit the model\\nto only see a certain number of previous\\ndialogue turns, you can set `max_history` to a finite value.\\nPlease note that you should pick `max_history` carefully, so that the model has enough\\nprevious dialogue turns to create a correct prediction.\\nDepending on your dataset, higher values of `max_history` can result in more frequent prediction of `action_unlikely_intent`\\nas the number of unique possible conversation paths increases as more dialogue context is taken\\ninto account. Similarly, lowering the value of `max_history` can result in `action_unlikely_intent` being\\ntriggered less often but can also be a stronger indicator that the corresponding conversation path\\nis highly unique and hence unexpected.\\nWe recommend you to set the `max_history` of `UnexpecTEDIntentPolicy` equal to that of `TEDPolicy`.\\nHere is how the config would look like:', '```\\npolicies:\\n- name: UnexpecTEDIntentPolicy\\n  max_history: 8\\n```', '`ignore_intents_list`:\\nThis parameter lets you configure `UnexpecTEDIntentPolicy` to not predict `action_unlikely_intent` for\\na subset of intents. You might want to do this if you come across a certain list of intents for which there\\nare too many false warnings generated.', '`tolerance`:\\nThe `tolerance` parameter is a number that ranges from `0.0` to `1.0` (inclusive).\\nIt helps to adjust the threshold score used during\\n[prediction of `action_unlikely_intent`](./policies.mdx#prediction-of-action_unlikely_intent)\\nat inference time.', 'Here, `0.0` means that the threshold score will be adjusted in such a way that `0%` of negative\\nexamples encountered during training are predicted with a score lower than the threshold score.\\nHence, conversation contexts from all negative examples will trigger an `action_unlikely_intent` action.', 'A tolerance of `0.1` means that the threshold score will be adjusted in a way such that 10% of negative\\nexamples encountered during training are predicted with a score lower than the threshold score.', 'A tolerance of `1.0` means that the threshold score is so low that `UnexpecTEDIntentPolicy` would not\\ntrigger `action_unlikely_intent` for any of the negative examples that it has encountered\\nduring training.', '`use_gpu`:\\nThis parameter defines whether a GPU (if available) will be used training. By default, `UnexpecTEDIntentPolicy` will be trained on GPU \\nif a GPU is available (i.e. `use_gpu` is `True`). To enforce that `UnexpecTEDIntentPolicy` uses only the CPU for training, set `use_gpu` to `False`.']\n\nThe above configuration parameters are the ones you should try tweaking according to your use case and training data.\nHowever, additional parameters exist that you could adapt.\n\n<details><summary>More configurable parameters</summary>\n\n```\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| Parameter                             | Default Value          | Description                                                  |\n+=======================================+========================+==============================================================+\n| hidden_layers_sizes                   | text: []               | Hidden layer sizes for layers before the embedding layers    |\n|                                       |                        | for user messages and bot messages in previous actions       |\n|                                       |                        | and labels. The number of hidden layers is                   |\n|                                       |                        | equal to the length of the corresponding list.               |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| dense_dimension                       | text: 128              | Dense dimension for sparse features to use after they are    |\n|                                       | intent: 20             | converted into dense features.                               |\n|                                       | action_name: 20        |                                                              |\n|                                       | label_intent: 20       |                                                              |\n|                                       | entities: 20           |                                                              |\n|                                       | slots: 20              |                                                              |\n|                                       | active_loop: 20        |                                                              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| concat_dimension                      | text: 128              | Common dimension to which sequence and sentence features of  |\n|                                       |                        | different dimensions get converted before concatenation.     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| encoding_dimension                    | 50                     | Dimension size of embedding vectors                          |\n|                                       |                        | before the dialogue transformer encoder.                     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| transformer_size                      | text: 128              | Number of units in user text sequence transformer encoder.   |\n|                                       | dialogue: 128          | Number of units in dialogue transformer encoder.             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| number_of_transformer_layers          | text: 1                | Number of layers in user text sequence transformer encoder.  |\n|                                       | dialogue: 1            | Number of layers in dialogue transformer encoder.            |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| number_of_attention_heads             | 4                      | Number of self-attention heads in transformers.              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| unidirectional_encoder                | True                   | Use a unidirectional or bidirectional encoder                |\n|                                       |                        | for `text`, `action_text`, and `label_action_text`.          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_key_relative_attention            | False                  | If 'True' use key relative embeddings in attention.          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_value_relative_attention          | False                  | If 'True' use value relative embeddings in attention.        |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| max_relative_position                 | None                   | Maximum position for relative embeddings.                    |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| batch_size                            | [64, 256]              | Initial and final value for batch sizes.                     |\n|                                       |                        | Batch size will be linearly increased for each epoch.        |\n|                                       |                        | If constant `batch_size` is required, pass an int, e.g. `8`. |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| batch_strategy                        | \"balanced\"             | Strategy used when creating batches.                         |\n|                                       |                        | Can be either 'sequence' or 'balanced'.                      |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| epochs                                | 1                      | Number of epochs to train.                                   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| random_seed                           | None                   | Set random seed to any 'int' to get reproducible results.    |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| learning_rate                         | 0.001                  | Initial learning rate for the optimizer.                     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| embedding_dimension                   | 20                     | Dimension size of dialogue & system action embedding vectors.|\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| number_of_negative_examples           | 20                     | The number of incorrect labels. The algorithm will minimize  |\n|                                       |                        | their similarity to the user input during training.          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| ranking_length                        | 10                     | Number of top actions to normalize scores for. Applicable    |\n|                                       |                        | only with loss type 'cross_entropy' and 'softmax'            |\n|                                       |                        | confidences. Set to 0 to disable normalization.              |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| scale_loss                            | True                   | Scale loss inverse proportionally to confidence of correct   |\n|                                       |                        | prediction.                                                  |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| regularization_constant               | 0.001                  | The scale of regularization.                                 |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| drop_rate_dialogue                    | 0.1                    | Dropout rate for embedding layers of dialogue features.      |\n|                                       |                        | Value should be between 0 and 1.                             |\n|                                       |                        | The higher the value the higher the regularization effect.   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| drop_rate_label                       | 0.0                    | Dropout rate for embedding layers of label features.         |\n|                                       |                        | Value should be between 0 and 1.                             |\n|                                       |                        | The higher the value the higher the regularization effect.   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| drop_rate_attention                   | 0.0                    | Dropout rate for attention. Value should be between 0 and 1. |\n|                                       |                        | The higher the value the higher the regularization effect.   |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_sparse_input_dropout              | True                   | If 'True' apply dropout to sparse input tensors.             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| use_dense_input_dropout               | True                   | If 'True' apply dropout to sparse features after they are    |\n|                                       |                        | converted into dense features.                               |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| evaluate_every_number_of_epochs       | 20                     | How often to calculate validation accuracy.                  |\n|                                       |                        | Set to '-1' to evaluate just once at the end of training.    |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| evaluate_on_number_of_examples        | 0                      | How many examples to use for hold out validation set.        |\n|                                       |                        | Large values may hurt performance, e.g. model accuracy.      |\n|                                       |                        | Keep at 0 if your data set contains a lot of unique examples |\n|                                       |                        | of dialogue turns.                                           |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| tensorboard_log_directory             | None                   | If you want to use tensorboard to visualize training         |\n|                                       |                        | metrics, set this option to a valid output directory. You    |\n|                                       |                        | can view the training metrics after training in tensorboard  |\n|                                       |                        | via 'tensorboard --logdir <path-to-given-directory>'.        |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| tensorboard_log_level                 | \"epoch\"                | Define when training metrics for tensorboard should be       |\n|                                       |                        | logged. Either after every epoch ('epoch') or for every      |\n|                                       |                        | training step ('batch').                                     |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| checkpoint_model                      | False                  | Save the best performing model during training. Models are   |\n|                                       |                        | stored to the location specified by `--out`. Only the one    |\n|                                       |                        | best model will be saved.                                    |\n|                                       |                        | Requires `evaluate_on_number_of_examples > 0` and            |\n|                                       |                        | `evaluate_every_number_of_epochs > 0`                        |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| featurizers                           | []                     | List of featurizer names (alias names). Only features        |\n|                                       |                        | coming from the listed names are used. If list is empty      |\n|                                       |                        | all available features are used.                             |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| ignore_intents_list                   | []                     | This parameter lets you configure `UnexpecTEDIntentPolicy` to ignore|\n|                                       |                        | the prediction of `action_unlikely_intent` for a subset of   |\n|                                       |                        | intents. You might want to do this if you come across a      |\n|                                       |                        | certain list of intents for which there are too many false   |\n|                                       |                        | warnings generated.                                          |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n| tolerance                             | 0.0                    | The `tolerance` parameter is a number that ranges from `0.0` |\n|                                       |                        | to `1.0` (inclusive). It helps to adjust the threshold score |\n|                                       |                        | used during prediction of `action_unlikely_intent` at        |\n|                                       |                        | inference time. Here, `0.0` means that the score threshold   |\n|                                       |                        | is the one that `UnexpecTEDIntentPolicy` had determined at training |\n|                                       |                        | time. A tolerance of `1.0` means that the threshold score    |\n|                                       |                        | is so low that `IntentTED` would not trigger                 |\n|                                       |                        | `action_unlikely_intent` for any of the \"negative examples\"  |\n|                                       |                        | that it has encountered during training. These negative      |\n|                                       |                        | examples are combinations of dialogue contexts and user      |\n|                                       |                        | intents that are _incorrect_. `UnexpecTEDIntentPolicy` generates    |\n|                                       |                        | these negative examples from your training data by picking a |\n|                                       |                        | random story part and pairing it with a random intent that   |\n|                                       |                        | doesn't occur at this point.                                 |\n+---------------------------------------+------------------------+--------------------------------------------------------------+\n```\n\n</details>",
                "Tuning the tolerance parameter": "When [reviewing real conversations](./conversation-driven-development.mdx#review), we encourage you\nto tune the `tolerance` parameter in `UnexpecTEDIntentPolicy`'s configuration to reduce the number\nof false warnings (intents that actually are likely given the conversation context).\nAs you increase the value of `tolerance` from `0` to `1` in steps of `0.05`,\nthe number of false warnings should decrease. However, increasing the `tolerance` will\nalso result in fewer triggers of `action_unlikely_intent` and hence more conversation\npaths not present in training stories will be missing in the set of flagged conversations.\nIf you change the `max_history` value and retrain a model, you might have to re-adjust the `tolerance` value as well.\n\n:::note\n`UnexpecTEDIntentPolicy` is only trained on [stories](./stories.mdx) and not [rules](./rules.mdx) from the training data.\n\n:::"
              },
              "Memoization Policy": "The `MemoizationPolicy` remembers the stories from your\ntraining data. It checks if the current conversation matches the stories in your\n`stories.yml` file. If so, it will predict the next action from the matching\nstories of your training data with a confidence of `1.0`. If no matching conversation\nis found, the policy predicts `None` with confidence `0.0`.\n\nWhen looking for a match in your training data, the policy will take the last\n`max_history` number of turns of the conversation into account.\nOne “turn” includes the message sent by the user and any actions the\nassistant performed before waiting for the next message.\n\nYou can configure the number of turns the `MemoizationPolicy` should use in your\nconfiguration:\n\n```\npolicies:\n  - name: \"MemoizationPolicy\"\n    max_history: 3\n```",
              "Augmented Memoization Policy": "The `AugmentedMemoizationPolicy` remembers examples from training\nstories for up to `max_history` turns, just like the `MemoizationPolicy`.\nAdditionally, it has a forgetting mechanism that will forget a certain amount\nof steps in the conversation history and try to find a match in your stories\nwith the reduced history. It predicts the next action with confidence `1.0`\nif a match is found, otherwise it predicts `None` with confidence `0.0`.\n\n:::note Slots and predictions\nIf you have dialogues where some slots that are set during\nprediction time might not be set in training stories (e.g. in training\nstories starting with a [reminder](./reaching-out-to-user.mdx#reminders), not all previous slots are set),\nmake sure to add the relevant stories without slots to your training\ndata as well.\n\n:::"
            },
            "Rule-based Policies": {
              "Rule Policy": "The `RulePolicy` is a policy that handles conversation parts that follow\na fixed behavior (e.g. business logic). It makes predictions based on\nany `rules` you have in your training data. See the\n[Rules documentation](./rules.mdx) for further information on how to define rules.\n\nThe `RulePolicy` has the following configuration options:\n\n```\npolicies:\n  - name: \"RulePolicy\"\n    core_fallback_threshold: 0.3\n    core_fallback_action_name: action_default_fallback\n    enable_fallback_prediction: true\n    restrict_rules: true\n    check_for_contradictions: true\n```\n\n['`core_fallback_threshold` (default: `0.3`): Please see the\\n[fallback documentation](fallback-handoff.mdx#handling-low-action-confidence) for\\nfurther information.', '`core_fallback_action_name` (default: `action_default_fallback`): Please see the\\n[fallback documentation](fallback-handoff.mdx#handling-low-action-confidence) for\\nfurther information.', '`enable_fallback_prediction` (default: `true`): Please see the\\n[fallback documentation](fallback-handoff.mdx#handling-low-action-confidence) for\\nfurther information.', '`check_for_contradictions` (default: `true`):\\nBefore training, the RulePolicy will perform a check to make sure that\\nslots and active loops set by actions are defined consistently for all rules.\\nThe following snippet contains an example of an incomplete rule:', '```\\nrules:\\n- rule: complete rule\\n  steps:\\n  - intent: search_venues\\n  - action: action_search_venues\\n  - slot_was_set:\\n    - venues: [{\"name\": \"Big Arena\", \"reviews\": 4.5}]\\n\\n- rule: incomplete rule\\n  steps:\\n  - intent: search_venues\\n  - action: action_search_venues\\n```', 'In the second `incomplete rule`, `action_search_venues` should set\\nthe `venues` slot because it is set in `complete rule`, but this event is missing.\\nThere are several possible ways to fix this rule.', \"In the case when `action_search_venues` can't find\\na venue and the `venues` slot should not be set,\\nyou should explicitly set the value of the slot to `null`.\\nIn the following story `RulePolicy` will predict `utter_venues_not_found`\\nonly if the slot `venues` is not set:\", '```\\nrules:\\n- rule: fixes incomplete rule\\n  steps:\\n  - intent: search_venues\\n  - action: action_search_venues\\n  - slot_was_set:\\n    - venues: null\\n  - action: utter_venues_not_found\\n```', 'If you want the slot setting to be handled by a different rule or story,\\nyou should add `wait_for_user_input: false` to the end of the rule snippet:', '```\\nrules:\\n- rule: incomplete rule\\n  steps:\\n  - intent: search_venues\\n  - action: action_search_venues\\n  wait_for_user_input: false\\n```', 'After training, the RulePolicy will check that none of the rules or stories contradict\\neach other. The following snippet is an example of two contradicting rules:', '```\\nrules:\\n- rule: Chitchat\\n  steps:\\n  - intent: chitchat\\n  - action: utter_chitchat\\n\\n- rule: Greet instead of chitchat\\n  steps:\\n  - intent: chitchat\\n  - action: utter_greet  # `utter_greet` contradicts `utter_chitchat` from the rule above\\n```', '`restrict_rules` (default: `true`): Rules are restricted to one user turn, but\\nthere can be multiple bot events, including e.g. a form being filled and its subsequent submission.\\nChanging this parameter to `false` may result in unexpected behavior.']\n\n:::caution Overusing rules\nOverusing rules for purposes outside of the [recommended use cases](rules.mdx)\nwill make it very hard to maintain your assistant as the complexity grows.\n\n:::"
            },
            "Configuring Policies": {
              "Max History": "One important hyperparameter for Rasa policies is the `max_history`.\nThis controls how much dialogue history the model looks at to decide which\naction to take next.\n\nYou can set the `max_history` by passing it to your policy\nin the policy configuration in your `config.yml`.\nThe default value is `None`, which means that the complete dialogue history since session\nrestart is taken in the account.\n\n```\npolicies:\n  - name: TEDPolicy\n    max_history: 5\n    epochs: 200\n    batch_size: 50\n    max_training_samples: 300\n```\n\n:::note\n`RulePolicy` doesn't have max history parameter, it always consider the full length\nof provided rules. Please see [Rules](./rules.mdx) for further information.\n:::\n\nAs an example, let's say you have an `out_of_scope` intent which\ndescribes off-topic user messages. If your bot sees this intent multiple\ntimes in a row, you might want to tell the user what you can help them\nwith. So your story might look like this:\n\n```\nstories:\n  - story: utter help after 2 fallbacks\n    steps:\n    - intent: out_of_scope\n    - action: utter_default\n    - intent: out_of_scope\n    - action: utter_default\n    - intent: out_of_scope\n    - action: utter_help_message\n```\n\nFor your model to learn this pattern, the `max_history`\nhas to be at least 4.\n\nIf you increase your `max_history`, your model will become bigger and\ntraining will take longer. If you have some information that should\naffect the dialogue very far into the future, you should store it as a\nslot. Slot information is always available for every featurizer.",
              "Data Augmentation": "When you train a model, Rasa will create\nlonger stories by randomly combining\nthe ones in your stories files.\nTake the stories below as an example:\n\n```\nstories:\n  - story: thank\n    steps:\n    - intent: thankyou\n    - action: utter_youarewelcome\n  - story: say goodbye\n    steps:\n    - intent: goodbye\n    - action: utter_goodbye\n```\n\nYou actually want to teach your policy to **ignore** the dialogue history\nwhen it isn't relevant and to respond with the same action no matter\nwhat happened before. To achieve this, individual stories are\nconcatenated into longer stories. From the example above, data augmentation\nmight produce a story by combining `thank` with `say goodbye` and then `thank` again,\nequivalent to:\n\n```\nstories:\n  - story: thank -> say goodbye -> thank\n    steps:\n    - intent: thankyou\n    - action: utter_youarewelcome\n    - intent: goodbye\n    - action: utter_goodbye\n    - intent: thankyou\n    - action: utter_youarewelcome\n```\n\nYou can alter this behavior with the `--augmentation` flag,\nwhich allows you to set the `augmentation_factor`.\nThe `augmentation_factor` determines how many augmented stories are\nsubsampled during training. The augmented stories are subsampled before training\nsince their number can quickly become very large, and you want to limit it.\nThe number of sampled stories is `augmentation_factor` x10.\nBy default `augmentation_factor` is set to 50, resulting in a maximum of 500 augmented stories.\n\n`--augmentation 0` disables all augmentation behavior. `TEDPolicy` is the **only** policy\naffected by augmentation. Other policies like `MemoizationPolicy` or `RulePolicy`\nautomatically ignore all augmented stories (regardless of the `augmentation_factor`).\n\n`--augmentation` is an important parameter when trying to reduce `TEDPolicy` training\ntime. Reducing the `augmentation_factor` decreases the size of the training data\nand subsequently the time to train the policy. However, reducing the amount of data\naugmentation can also reduce the performance of `TEDPolicy`. We recommend using\na memoization based policy along with `TEDPolicy` when reducing the amount of data\naugmentation to compensate.",
              "Featurizers": {
                "State Featurizers": "Every event in a trackers history creates a new state (e.g. running a bot\naction, receiving a user message, setting slots). Featurizing a single state\nof the tracker has two steps:\n\n['**Tracker provides a bag of active features**:', [\"features indicating intents and entities, if this is the first\\nstate in a turn, e.g. it's the first action we will take after\\nparsing the user's message. (e.g.\\n`[intent_restaurant_search, entity_cuisine]` )\", \"features indicating which slots are currently defined, e.g.\\n`slot_location` if the user previously mentioned the area\\nthey're searching for restaurants.\", 'features indicating the results of any API calls stored in\\nslots, e.g. `slot_matches`', 'features indicating what the last bot action or bot utterance was (e.g.\\n`prev_action_listen`)', 'features indicating if any loop is active and which one'], '**Convert all the features into numeric vectors**:', '`SingleStateFeaturizer` uses the Rasa NLU pipeline to convert the intent and\\nbot action names or bot utterances into numeric vectors.\\nSee the [NLU Model Configuration](./model-configuration.mdx) documentation\\nfor the details on how to configure Rasa NLU pipeline.', 'Entities, slots and active loops are featurized as one-hot encodings\\nto indicate their presence.']\n\n:::note\nIf the domain defines the possible `actions`,\n`[ActionGreet, ActionGoodbye]`,\n4 additional default actions are added:\n`[ActionListen(), ActionRestart(),\nActionDefaultFallback(), ActionDeactivateForm()]`.\nTherefore, label `0` indicates default action listen, label `1`\ndefault restart, label `2` a greeting and `3` indicates goodbye.\n\n:::",
                "Tracker Featurizers": {
                  "1. Full Dialogue": "`FullDialogueTrackerFeaturizer` creates a numerical representation of\nstories to feed to a recurrent neural network where the whole dialogue\nis fed to a network and the gradient is backpropagated from all time steps.\nThe target label is the most appropriate bot action or bot utterance which should be triggered in the\ncontext of the conversation.\nThe `TrackerFeaturizer` iterates over tracker\nstates and calls a `SingleStateFeaturizer` for each state to create numeric input features for a policy.",
                  "2. Max History": "`MaxHistoryTrackerFeaturizer` operates very similarly to `FullDialogueTrackerFeaturizer` as\nit creates an array of previous tracker states for each bot action or bot utterance but with the parameter\n`max_history` defining how many states go into each row of input features.\nIf `max_history` is not specified, the algorithm takes\nthe whole length of a dialogue into account.\nDeduplication is performed to filter out duplicated turns (bot actions\nor bot utterances) in terms of their previous states.\n\nFor some algorithms a flat feature vector is needed, so input features\nshould be reshaped to `(num_unique_turns, max_history * num_input_features)`.",
                  "3. Intent Max History": "`IntentMaxHistoryTrackerFeaturizer` inherits from `MaxHistoryTrackerFeaturizer`. Since, it is used by\n[`UnexpecTEDIntentPolicy`](#unexpected-intent-policy), the target labels that it creates are the intents that can be\nexpressed by a user in the context of a conversation tracker. Unlike\nother tracker featurizers, there can be multiple target labels. Hence, it pads the\nlist of target labels with a constant value (`-1`) on the right to return an equally sized list of target labels\nfor each input conversation tracker.\n\nJust like `MaxHistoryTrackerFeaturizer`, it also performs deduplication to\nfilter out duplicated turns. However, it yields one featurized tracker per correct intent\nfor the corresponding tracker. For example, if the correct labels for an input conversation tracker have the following\nindices - `[0, 2, 4]`, then the featurizer will yield three pairs of featurized trackers and target labels.\nThe featurized trackers will be identical to each other but the target labels in each pair will be\n`[0, 2, 4]`, `[4, 0, 2]`, `[2, 4, 0]`."
                }
              }
            },
            "Custom Policies": ":::info New in 3.0\nRasa 3.0 unified the implementation of NLU components and policies.\nThis requires changes to custom policies written for earlier versions of Rasa Open\nSource. Please see the\n[migration guide](migration-guide.mdx#custom-policies-and-custom-components) for a\nstep-by-step guide for the migration.\n\n:::\n\nYou can also write custom policies and reference them in your configuration. In the example below, the\nlast two lines show how to use a custom policy class and pass arguments to it.\nSee the [guide on custom graph components](custom-graph-components.mdx) for a complete guide on custom policies.\n\n```\npolicies:\n  - name: \"TEDPolicy\"\n    max_history: 5\n    epochs: 200\n  - name: \"RulePolicy\"\n  - name: \"path.to.your.policy.class\"\n    arg1: \"...\"\n```"
          },
          "metadata": {
            "id": "policies",
            "sidebar_label": "Policies",
            "title": "Policies",
            "abstract": "Your assistant uses policies to decide which action to take at each step in a conversation. There are machine-learning and rule-based policies that your assistant can use in tandem."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 36]"
        },
        {
          "title": "Rasa Pro Change Log",
          "description": null,
          "content": {
            "Rasa Pro 3.6": {
              "Rasa Plus 3.6": {
                "Features": [
                  "Implemented PII (Personally Identifiable Information) management using Microsoft Presidio as the entity analyzer and\nanonymization engine.\nThe feature covers the following:",
                  [
                    "anonymization of Rasa events (`UserUttered`, `BotUttered`, `SlotSet`, `EntitiesAdded`) before they are streamed to\nKafka event broker anonymization topics specified in `endpoints.yml`.",
                    "anonymization of Rasa logs that expose PII data"
                  ],
                  "The main components of the feature are:",
                  [
                    "anonymization rules that define in `endpoints.yml` the PII entities to be anonymized and the anonymization method to be used",
                    "anonymization executor that executes the anonymization rules on a given text",
                    "anonymization orchestrator that orchestrates the execution of the anonymization rules and publishes\nthe anonymized event to the matched Kafka topic.",
                    "anonymization pipeline that contains a list of orchestrators and is registered to a singleton provider component,\nwhich gets invoked in hook calls in Rasa Open Source when the pipeline must be retrieved for anonymizing events and logs."
                  ],
                  "Please read through the [PII Management](./pii-management.mdx) section in the official documentation to learn how to get started.",
                  "Implemented support for real time evaluation of [Markers](./monitoring/analytics/realtime-markers.mdx) with the Analytics\nData Pipeline. For this feature, we've added support for `rasa markers upload` command. Running this command validates the marker configuration file against the domain file and uploads the configuration to Analytics Data Pipeline."
                ]
              },
              "Improvements": [
                "Add `rasa marker upload` command to upload markers to the Rasa Pro Services.",
                "Enhance the validation of the `anonymization` key in `endpoints.yaml` by introducing checks for required fields and duplicate IDs."
              ],
              "Bugfixes": {
                "Deprecations and Removal": [
                  "Removed Python 3.7 support as [it reaches its end of life in June 2023](https://devguide.python.org/versions/)"
                ]
              },
              "Rasa Pro Services 3.1": {
                "Features": [
                  "You can now process [Markers](./monitoring/analytics/realtime-markers.mdx) with the Analytics Data Pipeline in real-time, enabling you to gain valuable insights and improve the performance of your Rasa Assistant."
                ]
              }
            },
            "Rasa Pro 3.5": {
              "Rasa Plus 3.5": {
                "Features": [
                  "[End-to-end testing](./testing-your-assistant.mdx#end-to-end-testing) is an enhanced and comprehensive CLI-based testing tool that allows you to test conversation scenarios with different pre-configured contexts, execute custom actions, verify response texts or names, and assert when slots are filled. It is available using the new `rasa test e2e` command.",
                  "You can now store your assistant's secrets in an [external credentials manager](./secrets-managers.mdx). In this release, Rasa Pro currently supports credentials manager for the Tracker Store with HashiCorp Vault."
                ]
              },
              "Rasa Pro Services 3.0": "*No significant change from last minor version.*"
            },
            "Rasa Pro 3.4": {
              "Rasa Plus 3.4": {
                "Features": [
                  "Added a new [IVR channel connector](./connectors/audioodes-voiceai-connect.mdx) to connect your assistant to AudioCodes VoiceAI Connect."
                ],
                "Improvements": [
                  "Rasa Pro now supports Python 3.10."
                ]
              },
              "Rasa Pro Services 3.0": "*No significant change from last minor version.*"
            },
            "Rasa Pro 3.3": {
              "Rasa Plus 3.3": {
                "Features": [
                  "[Tracing capabilities](./monitoring/tracing.mdx) for your Rasa Pro assistant. Distributed tracing tracks requests as they flow through a distributed system (in this case: a Rasa assistant), sending data about the requests to a tracing backend which collects all trace data and enables inspecting it. With this version of the Tracing feature, Rasa Pro supports OpenTelemetry.",
                  "[Concurrent Lock Store](./lock-stores.mdx#concurrentredislockstore) is a new lock store that uses Redis as a persistence layer and is safe for use with multiple Rasa server replicas."
                ]
              },
              "Rasa Pro Services 3.0": {
                "Features": [
                  "[Analytics Data Pipeline](./monitoring/analytics/getting-started-with-analytics.mdx) helps visualize and process Rasa assistant metrics in the tooling (BI tools, data warehouses) of your choice. Visualizations and analysis of the production assistant and its conversations allow you to assess ROI and improve the performance of the assistant over time."
                ]
              }
            }
          },
          "metadata": {
            "id": "rasa-pro-changelog",
            "sidebar_label": "Rasa Pro Change Log",
            "title": "Rasa Pro Change Log"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 37]"
        },
        {
          "title": "Rasa Pro",
          "description": null,
          "content": {
            "Rasa Pro Features": {
              "Analytics with Conversational Data Pipeline": "Visualise Rasa metrics in a third-party tool to measure the performance of your assistant.\n\n[Read more here](./monitoring/analytics/getting-started-with-analytics.mdx).",
              "Concurrent Lock Store": "Scale deployment and reliably handle high volumes of traffic across multiple Rasa instances with the confidence that no messages will be dropped.\n\n[Read more here](./lock-stores.mdx#concurrentredislockstore).",
              "End-to-End Testing": "Test your assistant with our end-to-end testing solution designed to meet enterprise-grade integration and acceptance testing criteria.\n\n[Read more here](./testing-your-assistant.mdx#end-to-end-testing).",
              "IVR Voice Connector": "Integrate with best-in-class IVR systems through our OOTB voice connectors.\n\n[Read more here](./connectors/audioodes-voiceai-connect.mdx).",
              "Observability (Tracing)": "Resolve performance issues faster and identify bottlenecks in message handling and model training.\n\n[Read more here](./monitoring/tracing.mdx).",
              "PII Handling": "Anonymize PII (Personal Identifiable Information) in logs and events streamed via the Kafka event broker. \n\n[Read more here](./pii-management.mdx)",
              "Real-Time Markers": "Mark points of interest in conversations to support the targeted analysis of user journeys real time.\n\n[Read more here](./monitoring/analytics/realtime-markers.mdx)",
              "Secrets Management": "Enhance security with our seamless Vault integration, enabling dynamic credential rotation for Rasa databases without system disruptions.\n\n[Read more here](./secrets-managers.mdx).",
              "Security Scanning for Vulnerability Protection": "Nightly and proactive security patches on your docker image to make sure dependencies are always up to date.",
              "Spaces (Alpha Release)": "Modularize your assistant for better scaling and team collaboration.\n\n[Read more here](./spaces.mdx)"
            }
          },
          "metadata": {
            "id": "rasa-pro",
            "sidebar_label": "Rasa Pro",
            "title": "Rasa Pro",
            "hide_table_of_contents": true
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 38]"
        },
        {
          "title": "Reaching Out to the User",
          "description": null,
          "content": {
            "Reaching out first": {
              "1. Update the configuration": "Since you are using a rule for this behavior, you need to add the [RulePolicy](./policies.mdx#rule-policy)\nto your configuration file:\n\n```\npolicies:\n  # other policies\n  - name: RulePolicy\n```",
              "2. Add a rule": "To have the assistant respond to the intent `greet` with a welcome message\nonly at the beginning of a conversation, add the following rule:\n\n```\nrules:\n  - rule: welcome user\n    conversation_start: true  # this rule only applies at the beginning of a conversation\n    steps:\n      - intent: greet\n      - action: utter_welcome\n```",
              "3. Add a response": "Finally, add a response for the `utter_welcome` utter action to your domain:\n\n```\nresponses:\n  utter_welcome:\n  - text: Hi there! What can I help you with today?\n```"
            },
            "External Events": {
              "1. Trigger an Intent": "To have an event from an external device change the course of an ongoing conversation, you can\nhave the device post to the\n[`trigger_intent` endpoint](/pages/http-api#operation/triggerConversationIntent) of your conversation.\nThe `trigger_intent` endpoint injects a user intent (possibly with entities) into your conversation.\nFor Rasa, it is as if you entered a message that got classified with that specific intent and entities.\nThe assistant will then predict and execute the next action as usual.\n\nFor example, the following post request would inject the intent `EXTERNAL_dry_plant` and the `plant` entity\ninto the conversation with id `user123`:\n\n```\ncurl -H \"Content-Type: application/json\" -X POST \\\n  -d '{\"name\": \"EXTERNAL_dry_plant\", \"entities\": {\"plant\": \"Orchid\"}}' \\\n  \"http://localhost:5005/conversations/user123/trigger_intent?output_channel=latest\"\n```",
              "2. Get the Conversation ID": "In a real-life scenario, your external device would get the conversation ID from an API or a database.\nIn the dry plant example, you might have a database of plants, the users that water them, and the users'\nconversation IDs. Your Raspberry Pi would get the conversation ID directly from the database.\nTo try out the reminderbot example locally, you'll need to get the conversation ID manually. See\nthe reminderbot [README](https://github.com/RasaHQ/rasa/blob/main/examples/reminderbot) for more information.",
              "3. Add NLU Training Data": "In the dry plant example, your Raspberry Pi needs to send a message with the intent\n`EXTERNAL_dry_plant` to the `trigger_intent` endpoint. This intent will be reserved for use by the Raspberry Pi, so\nthere won't be any NLU training examples for it.\n\n```\nintents:\n  - EXTERNAL_dry_plant\n```\n\n:::note\nYou should name intents that come from other devices with the `EXTERNAL_` prefix because it makes it\neasier to see which intents are expected to come from external devices when working with your training data.\n:::",
              "4. Update the Domain": {
                "5. Add a Rule": "You'll need a rule that tells your assistant how to respond when it receives a message from the Raspberry Pi.\n\n```\nrules:\n  - rule: warn about dry plant\n    steps:\n    - intent: EXTERNAL_dry_plant\n    - action: utter_warn_dry\n```",
                "6. Add a Response": "You'll need to define the response text for `utter_warn_dry`:\n\n```\nresponses:\n  utter_warn_dry:\n  - text: \"Your {plant} needs some water!\"\n```\n\nThe response will use the value from the slot `plant` to warn about the specific plant that needs watering."
              },
              "Try it out": "To try out the dry plant notification example, you'll need to start a [CallbackChannel](./connectors/your-own-website.mdx#callbackinput).\n\n:::caution\nExternal Events and Reminders don't work in request-response channels like the `rest` channel or `rasa shell`.\nCustom connectors for assistants implementing reminders or external events should be built\noff of the [CallbackInput channel](./connectors/your-own-website.mdx#callbackinput) instead of the RestInput channel.\n\nSee the [reminderbot README](https://github.com/RasaHQ/rasa/blob/main/examples/reminderbot/README.md)\nfor instructions on how to test your reminders locally.\n:::\n\nRun this POST request to simulate the external event, using your conversation ID:\n\n```\ncurl -H \"Content-Type: application/json\" -X POST -d \\\n'{\"name\": \"EXTERNAL_dry_plant\", \"entities\": {\"plant\": \"Orchid\"}}' \\\n\"http://localhost:5005/conversations/user1234/trigger_intent?output_channel=latest\"\n```\n\nYou should see the bot respond in your channel:\n\n<Chat caption=\"A reminder\">\n<ChatBotText>Your Orchid needs some water!</ChatBotText>\n</Chat>"
            },
            "Reminders": {
              "Scheduling Reminders": {
                "1. Define a Reminder": "To schedule a reminder, you need to define a custom action that returns\nthe `ReminderScheduled` event. For example, the following custom action\nschedules a reminder for five minutes from now:\n\n```\nimport datetime\nfrom rasa_sdk.events import ReminderScheduled\nfrom rasa_sdk import Action\n\nclass ActionSetReminder(Action):\n    \"\"\"Schedules a reminder, supplied with the last message's entities.\"\"\"\n\n    def name(self) -> Text:\n        return \"action_set_reminder\"\n\n    async def run(\n        self,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: Dict[Text, Any],\n    ) -> List[Dict[Text, Any]]:\n\n        dispatcher.utter_message(\"I will remind you in 5 minutes.\")\n\n        date = datetime.datetime.now() + datetime.timedelta(minutes=5)\n        entities = tracker.latest_message.get(\"entities\")\n\n        reminder = ReminderScheduled(\n            \"EXTERNAL_reminder\",\n            trigger_date_time=date,\n            entities=entities,\n            name=\"my_reminder\",\n            kill_on_user_message=False,\n        )\n\n        return [reminder]\n```\n\nThe first argument for the `ReminderScheduled` event is the reminder's name, in this case, `EXTERNAL_reminder`.\nThe reminder name will be used later as an intent to trigger a reaction to the reminder.\nName the reminder name with the\n`EXTERNAL_` prefix to make it easier to see what's going on in your training data.\n\nYou can see that the last messages' `entities` are also passed to the reminder.\nThis allows the action that reacts to the reminder to make use of the entities\nfrom the user's scheduling message.\n\nFor example, if you want your assistant to remind you to call a friend, you could\nsend it a message like \"Remind me to call Paul\". If \"Paul\" is extracted as a `PERSON`\nentity, the action reacting to the reminder can use it to say \"Remember to call Paul!\"",
                "2. Add a Rule": "To schedule a reminder, add a rule:\n\n```\nrules:\n- rule: Schedule a reminder\n  steps:\n  - intent: ask_remind_call\n    entities:\n    - PERSON\n  - action: action_set_reminder\n```",
                "3. Add Training Data": "You should add NLU training examples for scheduling the reminder:\n\n```\nnlu:\n- intent: ask_remind_call\n  examples: |\n    - remind me to call John\n    - later I have to call Alan\n    - Please, remind me to call Vova\n    - please remind me to call Tanja\n    - I must not forget to call Juste\n```\n\nYou should also add it to your domain:\n\n```\nintents:\n  - ask_remind_call\n```",
                "4. Update your Pipeline": "By adding SpacyNLP and SpacyEntityExtractor to your pipeline in config.yml, you won't need to annotate any of the\nnames in your training data, since Spacy has a `PERSON` dimension:\n\n```\npipeline:\n# other components\n- name: SpacyNLP\n  model: \"en_core_web_md\"\n- name: SpacyEntityExtractor\n  dimensions: [\"PERSON\"]\n```"
              },
              "Reacting to Reminders": {
                "1. Define a Reaction": "The bot reaches out to the user after receiving a\nPOST request to the `trigger_intent` endpoint. Reminders, however, send\nthe request to the right conversation ID automatically after a certain amount of time using\nthe name that you define in the `ReminderScheduled` event.\n\nTo define a reaction to the reminder, you only need to write a [rule](./rules.mdx) that\ntells the bot what action to take when it receives the reminder intent.\n\nIn the call reminder example, you want to use the entities that come with the\nreminder to be reminded to call specific people, so you need to write a custom\naction that does that:\n\n```\nclass ActionReactToReminder(Action):\n    \"\"\"Reminds the user to call someone.\"\"\"\n\n    def name(self) -> Text:\n        return \"action_react_to_reminder\"\n\n    async def run(\n        self,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: Dict[Text, Any],\n    ) -> List[Dict[Text, Any]]:\n\n        name = next(tracker.get_slot(\"PERSON\"), \"someone\")\n        dispatcher.utter_message(f\"Remember to call {name}!\")\n\n        return []\n```",
                "2. Add a Rule": "To tell your bot what action to run when a reminder is triggered, add a rule.\n\n```\nrules:\n- rule: Trigger `action_react_to_reminder` for `EXTERNAL_reminder`\n  steps:\n  - intent: EXTERNAL_reminder\n  - action: action_react_to_reminder\n```",
                "3. Add Training Data": "You'll need to define the intent that triggers reacting to the reminder. You don't need to add any training examples,\nsince the intent is reserved for the reminder.\n\n```\nintents:\n- intent: EXTERNAL_reminder\n```"
              },
              "Cancelling Reminders": {
                "1. Define an Action that Cancels a Reminder": "To cancel a reminder that you've already scheduled, you need a custom action\nthat returns the `ReminderCancelled()` event.\n\nReturning `ReminderCancelled()` cancels all the reminders that are currently scheduled.\nIf you only want to cancel certain reminders, you can specify some parameters by which to narrow down the scheduled reminders:\n\n['`ReminderCancelled(intent=\"EXTERNAL_greet\")` cancels all reminders with intent `EXTERNAL_greet`', '`ReminderCancelled(entities={})` cancels all reminders with the given entities', '`ReminderCancelled(\"...\")` cancels the one unique reminder with the given name “`...`” that you supplied\\nduring its creation']\n\nFor the call reminder example, you can define a custom action `action_forget_reminders` that cancels\nall reminders:\n\n```\nclass ForgetReminders(Action):\n    \"\"\"Cancels all reminders.\"\"\"\n\n    def name(self) -> Text:\n        return \"action_forget_reminders\"\n\n    async def run(\n        self, dispatcher, tracker: Tracker, domain: Dict[Text, Any]\n    ) -> List[Dict[Text, Any]]:\n\n        dispatcher.utter_message(f\"Okay, I'll cancel all your reminders.\")\n\n        # Cancel all reminders\n        return [ReminderCancelled()]\n```\n\n:::caution\nAll reminders are cancelled whenever you shutdown your Rasa server.\n:::",
                "2. Add a Rule": "You'll need to add a rule for cancelling a reminder.\n\n```\nrules:\n- rule: Cancel a reminder\n  steps:\n  - intent: ask_forget_reminders\n  - action: action_forget_reminders\n```",
                "3. Add Training Data": "You'll need to define an intent that triggers cancelling the reminder.\n\n```\nnlu:\n- intent: ask_forget_reminders\n  examples: |\n    - Forget about the reminder\n    - do not remind me\n    - cancel the reminder\n    - cancel all reminders please\n```\n\nYou should also add it to domain.yml:\n\n```\nintents:\n- intent: ask_forget_reminders\n```"
              },
              "Try it Out": "To try out reminders you'll need to start a [CallbackChannel](./connectors/your-own-website.mdx#callbackinput).\nYou'll also need to start the action server to schedule, react to, and cancel your reminders.\nSee the [reminderbot README](https://github.com/RasaHQ/rasa/blob/main/examples/reminderbot) for details.\n\nThen, if you send the bot a message like `Remind me to call Paul Pots`, you should get a reminder\nback five minutes later that says `Remember to call Paul Pots!`."
            }
          },
          "metadata": {
            "id": "reaching-out-to-user",
            "sidebar_label": "Reaching Out to the User",
            "title": "Reaching Out to the User",
            "abstract": "Sometimes you want your assistant to reach out to the user without the user's prompting. For example, you might want the assistant to send a message when the user opens the chat window, or you might want to prompt the user if they haven't sent a message for a while. This page is a guide to enabling your assistant to reach out to the user proactively."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 39]"
        },
        {
          "title": "Responses",
          "description": null,
          "content": {
            "Defining Responses": {
              "Using Variables in Responses": "You can use variables to insert information into responses.\nWithin a response, a variable is enclosed in curly brackets. For example, see the variable `name` below:\n\n```\nresponses:\n  utter_greet:\n  - text: \"Hey, {name}. How are you?\"\n```\n\nWhen the `utter_greet` response is used, Rasa automatically fills in the variable with the \nvalue found in the slot called `name`. If such a slot doesn't exist or is empty, the variable gets\nfilled with `None`.\n\nAnother way to fill in a variable is within a [custom action](./custom-actions.mdx).\nIn your custom action code, you can supply values to a response to fill in specific variables. \nIf you're using the Rasa SDK for your action server,\nyou can pass a value for the variable as a keyword argument to [`dispatcher.utter_message`](./action-server/sdk-dispatcher.mdx):\n\n```\ndispatcher.utter_message(\n    template=\"utter_greet\",\n    name=\"Sara\"\n)\n```\n\nIf you use a [different custom action server](./action-server/index.mdx#other-action-servers),\nsupply the values by adding extra parameters to the responses your server returns:\n\n```\n{\n  \"events\":[\n    ...\n  ],\n  \"responses\":[\n    {\n      \"template\":\"utter_greet\",\n      \"name\":\"Sara\"\n    }\n  ]\n}\n```",
              "Response Variations": {
                "IDs for Responses": ":::note New in Rasa 3.6\nYou can now set an ID for any response.\nThis is useful when you want to use the [NLG server](./nlg.mdx) to generate the response.\n\nType for ID is string.\n\n:::\n\nExample of response variations with ID:\n\n```\nresponses:\n  utter_greet:\n  - id: \"greet_1\"\n    text: \"Hey, {name}. How are you?\"\n  - id: \"greet_2\"\n    text: \"Hey, {name}. How is your day going?\"\n```"
              },
              "Channel-Specific Response Variations": "To specify different response variations depending on which channel\nthe user is connected to, use channel-specific response variations.\n\nIn the following example, the `channel` key makes the first response variation channel-specific for \nthe `slack` channel while the second variation is not channel-specific:\n\n```\nresponses:\n  utter_ask_game:\n  - text: \"Which game would you like to play on Slack?\"\n    channel: \"slack\"\n  - text: \"Which game would you like to play?\"\n```\n\n:::note\nMake sure the value of the `channel` key matches the value returned by the `name()` method of your \ninput channel. If you are using a built-in channel, this value will also match the channel name used\nin your `credentials.yml` file.\n:::\n\nWhen your assistant looks for suitable response variations under a given response name, it will first \ntry to choose from channel-specific variations for the current channel.\nIf there are no such variations, the assistant will choose from any response variations which are not\nchannel-specific.\n\nIn the above example, the second response variation has no `channel` specified and can be used by\nyour assistant for all channels other than `slack`.\n\n:::caution\nFor each response, try to have at least one response variation without the `channel` key.\nThis allows your assistant to properly respond in all environments, such as in new channels, \nin the shell and in interactive learning.\n:::",
              "Conditional Response Variations": "Specific response variations can also be selected based on one or more slot values using a conditional\nresponse variation. A conditional response variation is defined in the domain or responses YAML files\nsimilarly to a standard response variation but with an additional `condition` key. This key specifies\na list of slot `name` and `value` constraints.\n\nWhen a response is triggered during a dialogue, the constraints of each conditional response variation\nare checked against the current dialogue state. If all constraint slot values are equal to the corresponding\nslot values of the current dialogue state, the response variation is eligible to be used by your conversational\nassistant.\n\n:::note\nThe comparison of dialogue state slot values and constraint slot values is performed by the\nequality \"==\" operator which requires the type of slot values to match too.\nFor example, if the constraint is specified as `value: true`, then the slot needs to be filled\nwith a boolean `true`, not the string `\"true\"`.\n:::\n\nIn the following example, we will define one conditional response variation with one constraint,\nthat the `logged_in` slot is set to `true`:\n\n```\nslots:\n  logged_in:\n    type: bool\n    influence_conversation: False\n    mappings:\n    - type: custom\n  name:\n    type: text\n    influence_conversation: False\n    mappings:\n    - type: custom\n\nresponses:\n  utter_greet:\n    - condition:\n        - type: slot\n          name: logged_in\n          value: true\n      text: \"Hey, {name}. Nice to see you again! How are you?\"\n\n    - text: \"Welcome. How is your day going?\"\n```\n\n```\nstories:\n- story: greet\n  steps:\n  - action: action_log_in\n  - slot_was_set:\n    - logged_in: true\n  - intent: greet\n  - action: utter_greet\n```\n\nIn the example above, the first response variation (``\"Hey, {name}. Nice to see you again! How are you?\"``)\nwill be used whenever the `utter_greet` action is executed and the `logged_in` slot is set to `true`.\nThe second variation, which has no condition, will be treated as the default and used whenever\n`logged_in` is not equal to `true`.\n\n:::caution\nIt is highly recommended to always provide a default response variation without a condition\nto guard against those cases when no conditional response matches filled slots.\n:::\n\nDuring a dialogue, Rasa will choose from all conditional response variations whose constraints are satisfied.\nIf there are multiple eligible conditional response variations, Rasa will pick one at random.\nFor example, consider the following response:\n\n```\nresponses:\n  utter_greet:\n    - condition:\n        - type: slot\n          name: logged_in\n          value: true\n      text: \"Hey, {name}. Nice to see you again! How are you?\"\n\n    - condition:\n        - type: slot\n          name: eligible_for_upgrade\n          value: true\n      text: \"Welcome, {name}. Did you know you are eligible for a free upgrade?\"\n\n    - text: \"Welcome. How is your day going?\"\n```\n\nIf `logged_in` and `eligible_for_upgrade` are both set to `true` then both the first and second response\nvariations are eligible to be used, and will be chosen by the conversational assistant with equal probability.\n\nYou can continue using channel-specific response variations alongside conditional response variations\nas shown in the example below.\n\n```\nslots:\n  logged_in:\n    type: bool\n    influence_conversation: False\n    mappings:\n    - type: custom\n  name:\n    type: text\n    influence_conversation: False\n    mappings:\n    - type: custom\n\nresponses:\n  utter_greet:\n    - condition:\n        - type: slot\n          name: logged_in\n          value: true\n      text: \"Hey, {name}. Nice to see you again on Slack! How are you?\"\n      channel: slack\n\n    - text: \"Welcome. How is your day going?\"\n```\n\nRasa will prioritize the selection of responses in the following order:\n\n['conditional response variations with matching channel', 'default responses with matching channel', 'conditional response variations with no matching channel', 'default responses with no matching channel']"
            },
            "Rich Responses": {
              "Buttons": "Here is an example of a response that uses buttons:\n\n```\nresponses:\n  utter_greet:\n  - text: \"Hey! How are you?\"\n    buttons:\n    - title: \"great\"\n      payload: \"/mood_great\"\n    - title: \"super sad\"\n      payload: \"/mood_sad\"\n```\n\nEach button in the list of `buttons` should have two keys:\n\n['`title`: The text displayed on the buttons that the user sees.', '`payload`: The message sent from the user to the assistant when the button is clicked.']\n\nIf you would like the buttons to also pass entities to the assistant:\n\n```\nresponses:\n  utter_greet:\n  - text: \"Hey! Would you like to purchase motor or home insurance?\"\n    buttons:\n    - title: \"Motor insurance\"\n      payload: '/inform{{\"insurance\":\"motor\"}}'\n    - title: \"Home insurance\"\n      payload: '/inform{{\"insurance\":\"home\"}}'\n```\n\nPassing multiple entities is also possible with:\n\n```\n'/intent_name{{\"entity_type_1\":\"entity_value_1\", \"entity_type_2\": \"entity_value_2\"}}'\n```\n\n:::note overwrite nlu with buttons\nYou can use buttons to overwrite the NLU prediction and trigger a specific intent and entities.\n\nMessages starting with `/` are sent handled by the\n`RegexInterpreter`, which expects NLU input in a shortened `/intent{entities}` format.\nIn the example above, if the user clicks a button, the user input\nwill be classified as either the `mood_great` or `mood_sad` intent.\n\nYou can include entities with the intent to be passed to the `RegexInterpreter` using the following format:\n\n`/inform{\"ORG\":\"Rasa\", \"GPE\":\"Germany\"}`\n\nThe `RegexInterpreter` will classify the message above with the intent `inform` and extract the entities\n`Rasa` and `Germany` which are of type `ORG` and `GPE` respectively.\n\n:::\n\n:::note escaping curly braces in domain.yml\nYou need to write the `/intent{entities}` shorthand response with double curly braces in domain.yml so that the assistant does not\ntreat it as a [variable in a response](#using-variables-in-responses) and interpolate the content within the curly braces. \n:::\n\n:::caution Check your channel\nKeep in mind that it is up to the implementation of the output\nchannel how to display the defined buttons. For example, some\nchannels have a limit on the number of\nbuttons you can provide. Check your channel's documentation under\n**Concepts > Channel Connectors** for any channel-specific restrictions.\n:::",
              "Images": "You can add images to a response by providing a URL to the image under the `image` key:\n\n```\n  utter_cheer_up:\n  - text: \"Here is something to cheer you up:\"\n    image: \"https://i.imgur.com/nGF1K8f.jpg\"\n```",
              "Custom Output Payloads": "You can send any arbitrary output to the output channel using the\n`custom` key. The output channel receives the object stored under the `custom` key\nas a JSON payload.\n\nHere's an example of how to send a\n[date picker](https://api.slack.com/reference/block-kit/block-elements#datepicker) to the\n[Slack Output Channel](connectors/slack.mdx):\n\n```\nresponses:\n  utter_take_bet:\n  - custom:\n      blocks:\n      - type: section\n        text:\n          text: \"Make a bet on when the world will end:\"\n          type: mrkdwn\n        accessory:\n          type: datepicker\n          initial_date: '2019-05-21'\n          placeholder:\n            type: plain_text\n            text: Select a date\n```"
            },
            "Using Responses in Conversations": {
              "Calling Responses as Actions": "If the name of the response starts with `utter_`, the response can\ndirectly be used as an action, without being listed in the `actions` section of your domain. You would add the response\nto the domain:\n\n```\nresponses:\n  utter_greet:\n  - text: \"Hey! How are you?\"\n```\n\nYou can use that same response as an action in your stories:\n\n```\nstories:\n- story: greet user\n  steps:\n  - intent: greet\n  - action: utter_greet\n```\n\nWhen the `utter_greet` action runs, it will send the message from\nthe response back to the user.\n\n:::note Changing responses\nIf you want to change the text, or any other part of the response,\nyou need to retrain the assistant before these changes will be picked up.\n:::",
              "Calling Responses from Custom Actions": "You can use the responses to generate response messages from your\ncustom actions. If you're using Rasa SDK as your action server, you can use the dispatcher to generate the response message, for example: \n\n```\nfrom rasa_sdk.interfaces import Action\n\nclass ActionGreet(Action):\n    def name(self):\n        return 'action_greet'\n\n    def run(self, dispatcher, tracker, domain):\n        dispatcher.utter_message(template=\"utter_greet\")\n        return []\n```\n\nIf you use a\n[different custom action server](./action-server/index.mdx#other-action-servers),\nyour server should return the following JSON to call the `utter_greet` response:\n\n```\n{\n  \"events\":[],\n  \"responses\":[\n    {\n      \"template\":\"utter_greet\"\n    }\n  ]\n}\n```"
            }
          },
          "metadata": {
            "id": "responses",
            "sidebar_label": "Responses",
            "title": "Responses",
            "abstract": "Responses are messages that your assistant sends to the user. A response is usually only text, but can also include content like images and buttons."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 40]"
        },
        {
          "title": "Rules",
          "description": "Use Rasa rules to respond to FAQs, fill forms, or handle fallbacks gracefully.",
          "content": {
            "Writing a Rule": {
              "Rules for the Conversation Start": "To write a rule which only applies at the beginning of a conversation, add a\n`conversation_start: true` to your rule:\n\n```\nrules:\n\n- rule: Say `hello` when the user starts a conversation with intent `greet`\n  conversation_start: true\n  steps:\n  - intent: greet\n  - action: utter_greet\n```\n\nIf a user sends a message with the intent `greet` later in the conversation, the rule will not match.",
              "Rules with Conditions": "Conditions describe requirements which have to be fulfilled for a rule to be\napplicable. To do so, add any information about the prior conversation under the\n`condition` key:\n\n```\nrules:\n\n- rule: Only say `hello` if the user provided a name\n  condition:\n  - slot_was_set:\n    - user_provided_name: true\n  steps:\n  - intent: greet\n  - action: utter_greet\n```\n\nPossible information that you can include under `condition` includes `slot_was_set` events\nand `active_loop` events.",
              "Skip Waiting for User Input at the End of a Rule": "By default, rules will wait for the next user message when finished with the last step:\n\n```\nrules:\n\n- rule: Rule which will wait for user message when it was applied\n  steps:\n  - intent: greet\n  - action: utter_greet\n  # - action: action_listen\n  # Every rule implicitly includes a prediction for `action_listen` as last step.\n  # This means that Rasa will wait for the next user message.\n```\n\nIf you want to hand over the next action prediction to another story or rule, add\n`wait_for_user_input: false` to your rule:\n\n```\nrules:\n\n- rule: Rule which will not wait for user message once it was applied\n  steps:\n  - intent: greet\n  - action: utter_greet\n  wait_for_user_input: false\n```\n\nThis indicates that the assistant should execute another action\nbefore waiting for more user input.",
              "Abort a Rule": "Rules are designed to handle multiple output steps of a chatbot.\nThey are terminated as soon as user interaction is required.\nThis happens automatically via [launching a form](#rules-and-forms), since it starts with the user input of the first slot.\nTherefore all steps after launch are ignored.\n\nTermination, however, can also be achieved manually.\nThis can be useful to implement conditional termination criteria.\nHere is an example:\n\n```\nrules:\n\n- rule: Rule which will be conditionaly terminated\n  steps:\n  - intent: greet\n  - action: action_check_termination\n  - action: utter_greet\n  wait_for_user_input: true\n```\n\n```\nfrom rasa_sdk import Action\nfrom rasa_sdk.events import FollowupAction\n\nclass ActionCheckTermination(Action):\n\n    def name(self):\n        return \"action_check_termination\"\n\n    def run(self, dispatcher, tracker, domain):\n\n        # your business logic here\n        should_terminate = check_for_termination(<params>)\n\n        if should_terminate:\n            return [FollowupAction(\"action_listen\")]\n\n        return []\n```\n\nutter_greet is never executed when termination is done, even after user input, because it causes a new intent prediction.",
              "Rules and Forms": "When a [Form](./forms.mdx) is active, the bot will make predictions based on\nhow the form is defined, ignoring rules. Rules become applicable again if:\n\n['the form fills all required slots', 'the form rejects its execution (see\\n[Handling Unhappy Paths](./forms.mdx#writing-stories--rules-for-unhappy-form-paths) for\\nmore details)']"
            }
          },
          "metadata": {
            "id": "rules",
            "sidebar_label": "Rules",
            "title": "Rules",
            "description": "Use Rasa rules to respond to FAQs, fill forms, or handle fallbacks gracefully.",
            "abstract": "Rules are a type of training data used to train your assistant's dialogue management model. Rules describe short pieces of conversations that should always follow the same path."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 41]"
        },
        {
          "title": "Rasa SDK Change Log",
          "description": null,
          "content": {
            "root": [
              "The Rasa SDK changelog can be found in the [Rasa SDK repository](https://github.com/RasaHQ/rasa-sdk/blob/main/CHANGELOG.mdx)"
            ]
          },
          "metadata": {
            "id": "sdk_changelog",
            "sidebar_label": "Rasa SDK Change Log",
            "title": "Rasa SDK Change Log"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 42]"
        },
        {
          "title": "Secrets Managers",
          "description": "Safeguard credentials your service uses to authenticate to external resources.",
          "content": {
            "HashiCorp Vault Secrets Manager": {
              "Authentication": "Rasa Pro can authenticate to Vault through\n[Token authentication](https://www.vaultproject.io/docs/auth/token).\n\nBoth `expiring` and `non-expiring` (so called, root tokens) tokens are supported.\nRasa Pro will automatically renew the token if it is expiring.",
              "How to configure access to Vault": {
                "Store access credentials in environment variables": "A simple example on how to combine environment variables and `endpoints.yml` configuration file\nwould be to store access token in the environment variable and the rest of the configuration\nin the `endpoints.yml` file.\n\n```\n# environment variables\nVAULT_TOKEN=<token used to authenticate to Vault>\n```\n\n```\nsecrets_manager:\n    type: vault\n    url: \"http://localhost:1234\"\n    secrets_path: rasa-secrets  # if not set it defaults to `rasa-secrets`\n    transit_mount_point: transit # if you have enabled transit secrets engine, and you want to use it\n```"
              },
              "How to configure Tracker Store with Vault Secrets Manager": [
                "Configure Rasa to access the Vault instance",
                "Checkout the [How to configure access to Vault](#how-to-configure-access-to-vault) section for more details.",
                "Configure Rasa to use the Vault secrets manager to fetch credentials for the tracker store",
                "```\ntracker_store:\n      type: SQL\n      url: localhost:5432\n      username:\n            source: secrets_manager.vault\n            secret_key: sql_store_username\n      password:\n            source: secrets_manager.vault\n            secret_key: sql_store_password\n```"
              ]
            }
          },
          "metadata": {
            "id": "secrets-managers",
            "sidebar_label": "Secrets Managers",
            "title": "Secrets Managers",
            "description": "Safeguard credentials your service uses to authenticate to external resources.",
            "abstract": "You can store your assistant's secrets in an external credentials manager. Rasa Pro currently supports credentials manager for the Tracker Store"
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 43]"
        },
        {
          "title": "Setting up CI/CD",
          "description": "Set up a CI/CD pipeline to ensure that iterative improvements to your assistant are tested and deployed with minimum manual effort",
          "content": {
            "Overview": "Continuous Integration (CI) is the practice of merging in code changes\nfrequently and automatically testing changes as they are committed. Continuous\nDeployment (CD) means automatically deploying integrated changes to a staging\nor production environment. Together, they allow you to make more frequent improvements\nto your assistant and efficiently test and deploy those changes.\n\nThis guide will cover what should go in a CI/CD pipeline, specific to a\nRasa project. How you implement that pipeline is up to you.\nThere are many CI/CD tools out there, such as [GitHub Actions](https://github.com/features/actions),\n[GitLab CI/CD](https://docs.gitlab.com/ee/ci/), [Jenkins](https://www.jenkins.io/doc/), and\n[CircleCI](https://circleci.com/docs/). We recommend choosing a tool that integrates with\nwhatever Git repository you use.",
            "Continuous Integration (CI)": {
              "CI Pipeline Overview": "Your CI pipeline should include model training and testing as steps to streamline the deployment process.\nThe first step after saving new training data is to kick off the pipeline. This can be initiated manually\nor when you create or update a pull request.\n\nNext, you need to run various sets of test to see the impact of your changes. This includes running\ntests for data validation, NLU cross validation, and story testing. For\nmore information about testing, see [Testing Your Assistant](./testing-your-assistant.mdx).\n\nThe last step is to review the results of your test and push the changes if the tests are successful.\nOnce the new model is trained and tested, it can be deployed automatically using a Continuous\nDeployment pipeline.",
              "GitHub Actions CI Pipeline": "You can use the [Rasa Train-Test Github Action](https://github.com/RasaHQ/rasa-train-test-gha)\nin your CI pipeline to automatically perform data validation, training, and testing.\n\nAn example CI pipeline using the Github Action is shown below:\n\n```\njobs:\n  training-testing:\n    name: Training and Testing\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v1\n      - name: Rasa Train and Test GitHub Action\n        uses: RasaHQ/rasa-train-test-gha@main\n        with:\n          requirements_file: requirements.txt\n          data_validate: true\n          rasa_train: true\n          cross_validation: true\n          rasa_test: true\n          test_type: all\n          publish_summary: true\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n      - name: Upload model\n        if: github.ref == 'refs/heads/main'\n        uses: actions/upload-artifact@master\n        with:\n          name: model\n          path: models\n```\n\nIn this pipeline, the Rasa Train-Test Github Action is performing data validation, model training, and story testing\nin the first step and the model file is uploaded as an artifact in the second step.\n\nThe complete list of configurable parameters for the Rasa Train-Test Github Action is available in the repository's\n[README](https://github.com/RasaHQ/rasa-train-test-gha#input-arguments).\n\nWhen `publish_summary` is set to `true`, this action will automatically publish the model's test results to the associated\nPull Request as a comment:\n\n<img alt=\"image\" src={useBaseUrl(\"/img/train-test-github-action.png\")} />\n\nThe pull request can be approved or denied based on the evaluation results and, in many cases, you will want to automate the model's deployment if all CI checks pass. You can continue to the next section to learn more about Continuous Deployment."
            },
            "Continuous Deployment (CD)": {
              "Deploying Your Rasa Model": "If you ran [test stories](./testing-your-assistant.mdx) in your CI pipeline,\nyou'll already have a trained model. You can set up your CD pipeline to upload the trained model to your\nRasa server if the CI results are satisfactory. For example, to upload a model to Rasa X/Enterprise:\n\n```\ncurl -k -F \"model=@models/my_model.tar.gz\" \"https://example.rasa.com/api/projects/default/models?api_token={your_api_token}\"\n```\n\nIf you are using Rasa X/Enterprise, you can also [tag the uploaded model](https://rasa.com/docs/rasa-enterprise/pages/http-api/#tag/Models/paths/~1projects~1{project_id}~1models~1{model}~1tags~1{tag}/put)\nas production (or whichever deployment you want to tag if using multiple [deployment environments](https://rasa.com/docs/rasa-enterprise/enterprise/deployment-environments/#)):\n\n```\ncurl -X PUT \"https://example.rasa.com/api/projects/default/models/my_model/tags/production\"\n```\n\n:::caution updates to action code\nIf your update includes changes to both your model and your action\ncode, and these changes depend on each other in any way, you should **not**\nautomatically tag the model as `production`. You will first need to build and\ndeploy your updated action server, so that the new model won't e.g. call\nactions that don't exist in the pre-update action server.\n:::",
              "Deploying Your Action Server": "You can automate\n[building and uploading a new image for your action server](./deploy/deploy-action-server.mdx#building-an-action-server-image)\nto an image repository for each\nupdate to your action code. As noted above, be careful with\nautomatically deploying a new image tag to production if the action server\nwould be incompatible with the current production model."
            },
            "Example CI/CD pipelines": "As examples, see the CI/CD pipelines for\n[Sara](https://github.com/RasaHQ/rasa-demo/blob/main/.github/workflows/continuous_integration.yml),\nthe Rasa assistant that you can talk to in the Rasa Docs, and\n[Carbon Bot](https://github.com/RasaHQ/carbon-bot/blob/master/.github/workflows/model_ci.yml).\nBoth use [Github Actions](https://github.com/features/actions) as a CI/CD tool.\n\nThese examples are just two of many possibilities. If you have a CI/CD setup you like, please\nshare it with the Rasa community on the [forum](https://forum.rasa.com)."
          },
          "metadata": {
            "id": "setting-up-ci-cd",
            "sidebar_label": "Setting up CI/CD",
            "title": "Setting up CI/CD",
            "description": "Set up a CI/CD pipeline to ensure that iterative improvements to your assistant are tested and deployed with minimum manual effort",
            "abstract": "Even though developing a contextual assistant is different from developing traditional software, you should still follow software development best practices. Setting up a Continuous Integration (CI) and Continuous Deployment (CD) pipeline ensures that incremental updates to your bot are improving it, not harming it."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 44]"
        },
        {
          "title": "Slot Validation Actions",
          "description": null,
          "content": {
            "`action_validate_slot_mappings`": "You can use the `action_validate_slot_mappings` action to define custom extraction and / or validation of slots that\ncan be set or updated outside of a form context.\n\nThis action is called automatically at the end of the default action [`action_extract_slots`](./default-actions.mdx#action_extract_slots),\nso the name must not be changed. If you are using Rasa SDK, you should extend the Rasa SDK [`ValidationAction` class](./action-server/validation-action.mdx#how-to-subclass-validationaction).\nIf you are using a different action server, you will need to implement an action class with equivalent functionality to\nthe Rasa SDK class. Please see [the action server docs](./action-server/validation-action.mdx#validationaction-class-implementation) for details.\n\nWith this option, you do not need to specify the `action` key in the [custom slot mapping](./domain.mdx#custom-slot-mappings),\nsince the default action [`action_extract_slots`](./default-actions.mdx#action_extract_slots) runs `action_validate_slot_mappings`\nautomatically if present in the `actions` section of the domain.",
            "`validate_<form name>`": "Custom actions named `validate_<form name>` will run automatically if the form it specifies in its name is activated.\nIf you are using Rasa SDK, the custom action should inherit from the Rasa SDK [`FormValidationAction` class](./action-server/validation-action.mdx#formvalidationaction-class).\nIf you are not using Rasa SDK, you will need to implement an action or action class with equivalent functionality to the\n`FormValidationClass` in your custom action server. Please see [the action server docs](./action-server/validation-action.mdx#formvalidationaction-class-implementation) for details.",
            "Regular Custom Action": "You can use a regular custom action [custom action](./custom-actions.mdx) that returns [`slot`](./action-server/events.mdx#slot)\nevents for custom slot extraction. Use this option if neither [`action_validate_slot_mappings`](#action_validate_slot_mappings) or\n[`validate_<form name>`](#validate_form-name) meet your needs.\nFor example, if you want to reuse the same custom action explicitly in a story or rule, you should use a regular custom\naction for custom slot extraction.\nA slot validation action should only return `slot` and [`bot`](./action-server/events.mdx#bot) events.\nAny other event type will be filtered out by the default action [`action_extract_slots`](./default-actions.mdx#action_extract_slots).\nThe name of the custom action must be specified in the `action` key of the relevant [custom slot mapping](./domain.mdx#custom-slot-mappings)\nin the domain.\nNote that the action name must be listed in the domain `actions` section too.\n\n:::note Using different actions for extraction and validation\n\nYou can use both a regular custom action and `action_validate_slot_mappings` together to extract and validate a slot. For example, you can specify a regular custom action as the `action` for a `custom` slot mapping, and also add validation logic for the same slot to `action_validate_slot_mappings`. The custom action specified in the custom slot mapping will be called first, and `action_validate_slot_mappings` will be called afterwards.\n\n:::"
          },
          "metadata": {
            "id": "slot-validation-actions",
            "sidebar_label": "Slot Validation Actions",
            "title": "Slot Validation Actions",
            "abstract": "A slot validation action is a special type of custom action, designed to handle custom extraction and/or validation of slot values. This can be used to validate slots with predefined mappings or extract slots with custom mappings."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 45]"
        },
        {
          "title": "Spaces",
          "description": "Learn about Spaces for Rasa.",
          "content": {
            "When to use Spaces": "Spaces can be helpful when you are dealing with multiple domains of your business in\na single assistant. Oftentimes, this leads to multitude of forms, entities, and inform\nintents that start to overlap at some point. Form filling and inform intents are a\ntypical case of having this follow-up structure mentioned above. Another good case is\nwhen you want to be able to define different behavior for help- or clarification\nrequests based on the subdomain or process the user is in. This is technically possible\ntoday with stories, but it can be cumbersome to describe the full event horizon\nfor every interaction route.",
            "When not to use Spaces / Limitations": "Because spaces split a rasa bot into multiple parts, it is interacting with almost\nevery of the existing features of rasa. We made it work with almost all of them, but\nthere are some exceptions. This means, however, if you have created\ncustomization beyond components that are aligned with the way rasa normally\nworks, spaces might not work for you.\n\nAnother important limitation is that spaces currently do not support stories. We know\nthis is a big limitations for many existing bots. However, because stories and their\nexisting policies work with a fixed event horizon (`max_history`), they are at this\npoint not compatible with the idea of being able to describe encapsulated units of logic\nwell.\n\nCurrently, the only entity extractor that is works with the boundaries that spaces\ncreate, is an adapted version of the `CRFEntityExtractor`. You can find more on this\nin the following sections.",
            "An example spaces bot": "We have created a bot using spaces for you to look at, learn from and experiment with.\nIt features three different spaces in the financial domain. [Here is the repository](https://github.com/RasaHQ/financial-spaces-bot)",
            "How to use spaces": {
              "Training only a specific subspace": "We have added `--space` argument to the rasa train command to give you the option to\nonly train one specific subspace:\n\n`rasa train` -> trains the full assistant with all spaces\n`rasa train --space investment` -> trains an assistant only containing the investment and the main space\n\nOther commands were not adjusted as they use trained assistants as inputs."
            },
            "How do spaces work?": {
              "What happens during the assembly?": "The most important step during the assembly is the prefixing. During this step every\nintent, entity, slot, action, form, utterance that is defined in a space's domain file\nis prefixed (infixed for utterances) with this space's name. For example, an intent\n`ask_transfer_charge` in the `transfer_money` domain would become\n`transfer_money.ask_transfer_charge` and every reference of this intent would be\nadjusted. The final assistant then works on the prefixed data.\n\nAn exception to this is the main space. Anything in the main space and all its\nprimitives that are used in other spaces, will not be prefixed.\n\nAnother exception are rules. They don't have a name that can be prefixed. Instead,\nwe add a condition to each rule that it is only applicable while it's space is active.",
              "How is space activation tracked?": "A space is activated when any of their entry intents is predicted. A space can have\nmultiple entry intents. However, only a single space can be active at a given time.\nSo when space A is active and an entry intent of space B\nis predicted, space A will be deactivated and space B is active from now on.\n\nSpace activation is tracked through slots that are automatically\ngenerated during assembly.",
              "How does filter and rerank work?": "The filter and rerank component post-processes the intent ranking of the [intent\nclassification components](./components.mdx#intent-classifiers). It accesses the [tracker](./action-server/sdk-tracker.mdx)\nand checks which space are active or would be activated, in case an entry intent is at the top.\n\nIt then removes any intents from the ranking that are not possible. Further it also\nremoves any entities that are not possible given the space activation status or\nabout to be predicted entry intents.",
              "How does entity recognition work differently?": "Usually, entity recognizers in Rasa only return a single label per token in a message.\nThus, there is no ranking, that could be post-processed as in the case of intents. We\nhave built our `SpacesCRFEntityExtractor` in a way that it creates multiple extractors.\nOne for each space. Now, during the post-processing step, we can filter out the\nextractor of the spaces that are not activated.",
              "Custom Actions": "Custom actions work as before. However, the tracker, the domain, and slots\nwill be stripped of any information from other spaces before being handed to your\naction. Additionally, every event such as slot sets, will be prefixed after they are\nreturned by your action. All of this ensures that from the view point of your custom\naction, you don't need to worry about the other spaces and accidentally leaking\nor altering information. This warrants isolation between your Spaces. Note that\ncustom actions from the main spaces are not inherited by subspaces.",
              "Response Selection": "Response selection works as before. The only small difference is that in your\n`config.yaml` you'll need to specify the retrieval intent including it's final prefix.\nIf your retrieval intent is `investment_faq` in the `investment` space, then in your\nconfig you'll need to set `investment.investment_faq` as the retrieval intent. If the\nretrieval intent belongs to the main space, no prefix is added. That's why\n, in this case, during the definition of the retrieval intent, no prefixing is necessary.",
              "Lookup tables and Synonyms": "[Lookup Tables](./training-data-format.mdx#lookup-tables) and\n[Synonyms](./training-data-format.mdx#synonyms) work with spaces. However, they are not\ntruly isolated between spaces. So there can be some unanticipated interactions.\nFor synonyms, specifically:\n\n['Assume two spaces define the same synonym. Space A: \"IB\" -> \"Instant banking\".\\nSpace B: \"IB\"-> \"iron bank\". A warning is given that one value overwrites the other.', 'A similar thing can happen if \"IB\" is an entity in both spaces but only one\\ndefines it a synonym. Any entity with value IB will always be mapped to that synonym\\nno matter which space is active.']\n\nFor lookup tables no adverse interactions are known."
            }
          },
          "metadata": {
            "id": "spaces",
            "sidebar_label": "Spaces",
            "title": "Spaces",
            "description": "Learn about Spaces for Rasa.",
            "abstract": "Spaces are a way to modularize your assistant that increases isolation between subparts and classification performance for intents and entities that are only relevant in specific contexts."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 46]"
        },
        {
          "title": "Stories",
          "description": "Stories are used to teach Rasa real conversation designs to learn from providing the basis for a scalable machine learning dialogue management.",
          "content": {
            "Format": {
              "User Messages": "While writing stories, you do not have to deal with the specific contents of\nthe messages that the users send. Instead, you can take advantage of the output\nfrom the NLU pipeline, which lets you use just the combination of an intent and\nentities to refer to all the possible messages the users can send to mean the\nsame thing.\n\nIt is important to include the entities here as well because the policies learn\nto predict the next action based on a *combination* of both the intent and\nentities (you can, however, change this behavior using the\n[use_entities](./domain.mdx#ignoring-entities-for-certain-intents) attribute).",
              "Actions": "All actions executed by the bot, including [responses](./responses.mdx) are listed\nin stories under the `action` key.\n\nYou can use a response from your domain as an action by listing it as one\nin a story. Similarly, you can indicate that a story should call a custom action by including\nthe name of the custom action from the `actions` list in your domain.",
              "Events": {
                "Slot Events": "Slot events are written under `slot_was_set` in a story. If this slot is set\ninside a custom action, add the `slot_was_set` event immediately following the\ncustom action call. If your custom action resets a slot value to `None`, the\ncorresponding event for that would look like this:\n\n```\nstories:\n- story: set slot to none\n  steps:\n    # ... other story steps\n    - action: my_custom_action\n    - slot_was_set:\n      - my_slot: null\n```",
                "Form Events": "There are three kinds of events that need to be kept in mind while dealing with\nforms in stories.\n\n['A form action event (e.g. `- action: restaurant_form`) is used in the beginning when first starting a form, and also while resuming the form action when the form is already active.', 'A form activation event (e.g. `- active_loop: restaurant_form`) is used right after the first form action event.', 'A form deactivation event (e.g. `- active_loop: null`), which is used to deactivate the form.']\n\n:::note writing form stories\nIn order to get around the pitfall of forgetting to add events, the recommended\nway to write these stories is to use [interactive learning](./writing-stories.mdx#using-interactive-learning).\n\n:::"
              }
            },
            "Checkpoints and OR statements": {
              "Checkpoints": "You can use checkpoints to modularize and simplify your training\ndata. Checkpoints can be useful, but **do not overuse them**. Using\nlots of checkpoints can quickly make your example stories hard to\nunderstand, and will slow down training.\n\nHere is an example of stories that\ncontain checkpoints:\n\n```\nstories:\n- story: beginning of flow\n  steps:\n  - intent: greet\n  - action: action_ask_user_question\n  - checkpoint: check_asked_question\n\n- story: handle user affirm\n  steps:\n  - checkpoint: check_asked_question\n  - intent: affirm\n  - action: action_handle_affirmation\n  - checkpoint: check_flow_finished\n\n- story: handle user deny\n  steps:\n  - checkpoint: check_asked_question\n  - intent: deny\n  - action: action_handle_denial\n  - checkpoint: check_flow_finished\n\n- story: finish flow\n  steps:\n  - checkpoint: check_flow_finished\n  - intent: goodbye\n  - action: utter_goodbye\n```\n\n:::note\nUnlike regular stories, checkpoints are not restricted to starting with\nuser input. As long as the checkpoint is inserted at the right points\nin the main stories, the first event can be a custom action or a response\nas well.\n\n:::",
              "Or Statements": "Another way to write shorter stories, or to handle multiple intents\nor slot events the same way, is to use an `or` statement.\nFor example, if you ask the user to confirm something, and you want\nto treat the `affirm` and `thankyou` intents in the same way.\nThe story below will be converted into two stories at training time:\n\n```\nstories:\n- story:\n  steps:\n  # ... previous steps\n  - action: utter_ask_confirm\n  - or:\n    - intent: affirm\n    - intent: thankyou\n  - action: action_handle_affirmation\n```\n\nYou can also use `or` statements with slot events.\nThe following means the story requires that the current value for\nthe `name` slot is set and is either `joe` or `bob`:\n\n```\nstories:\n- story:\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - intent: tell_name\n  - or:\n    - slot_was_set:\n        - name: joe\n    - slot_was_set:\n        - name: bob\n  # ... next actions\n```\n\n`or` statements can be useful, but if you are using a\nlot of them, it is probably better to restructure your domain and/or intents.\nOverusing OR statements will slow down training."
            },
            "Test Conversation Format": "The test conversation format is a format that combines both NLU data and stories\ninto a single file for evaluation. Read more about this format in [Testing Your Assistant](./testing-your-assistant.mdx).\n\n:::caution testing only\nThis format is only used for testing and cannot be used for training.\n\n:::",
            "End-to-end Training": ":::info New in 2.2\nEnd-to-end training is an experimental feature.\nWe introduce experimental features to get feedback from our community, so we encourage you to try it out!\nHowever, the functionality might be changed or removed in the future.\nIf you have feedback (positive or negative) please share it with us on the [Rasa Forum](https://forum.rasa.com).\n\n:::\n\nWith end-to-end training, you do not have to deal with the specific\nintents of the messages that are extracted by the NLU pipeline\nor with separate `utter_` responses in the domain file.\nInstead, you can include the text of the user messages and/or bot responses directly in your stories.\nSee the [training data format](./training-data-format.mdx#end-to-end-training)\nfor detailed description of how to write end-to-end stories.\n\nYou can mix training data in the end-to-end format with labeled training data which has \n`intent`s and `action`s specified: Stories can have some steps defined by intents/actions\nand other steps defined directly by user or bot utterances.\n\nWe call it end-to-end training because policies can consume and predict actual text.\nFor end-to-end user inputs, intents classified by the NLU pipeline\nand extracted entities are ignored.\n\nOnly [Rule Policy](./policies.mdx#rule-policy)\nand [TED Policy](./policies.mdx#ted-policy) allow end-to-end training.\n\n['`RulePolicy` uses simple string matching during prediction. Namely,\\nrules based on user text will only match if the user\\ntext strings inside your rules and input during prediction are identical.', '`TEDPolicy` passes user text through an additional Neural Network to create\\nhidden representations of the text. In order to obtain robust performance you\\nneed to provide enough training stories to capture a variety of user texts for any \\nend-to-end dialogue turn.']\n\nRasa policies are trained for next utterance selection.\nThe only difference to creating `utter_` response is how `TEDPolicy` featurizes\nbot utterances.\nIn case of an `utter_` action, `TEDPolicy` sees only the name of the action, while\nif you provide actual utterance using `bot` key,\n`TEDPolicy` will featurize it as textual input depending on the NLU configuration.\nThis can help in case of similar utterances in slightly different situations.\nHowever, this can also make things harder to learn because the fact that different\nutterances have similar texts make it easier for `TEDPolicy` to confuse these utterances.\n\nEnd-to-end training requires significantly more parameters in `TEDPolicy`.\nTherefore, training an end-to-end model might require significant computational\nresources depending on how many end-to-end turns you have in your stories."
          },
          "metadata": {
            "id": "stories",
            "sidebar_label": "Stories",
            "title": "Stories",
            "description": "Stories are used to teach Rasa real conversation designs to learn from providing the basis for a scalable machine learning dialogue management.",
            "abstract": "Stories are a type of training data used to train your assistant's dialogue management model. Stories can be used to train models that are able to generalize to unseen conversation paths."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 47]"
        },
        {
          "title": "Testing Your Assistant",
          "description": null,
          "content": {
            "Validating Data and Stories": "Data validation verifies that no mistakes or major inconsistencies appear in your domain, NLU\ndata, or story data. To validate your data, have your CI run this command:\n\n```\nrasa data validate\n```\n\nIf you pass a `max_history` value to one or more policies in your `config.yml` file, provide the\nsmallest of those values as\n\n```\nrasa data validate --max-history <max_history>\n```\n\nIf data validation results in errors, training a model can also fail or yield bad performance, so it's\nalways good to run this check before training a model. By including the\n`--fail-on-warnings` flag, this step will fail on warnings indicating more minor issues.\n\n:::note\nRunning `rasa data validate` does **not** test if your [rules](./rules.mdx) are consistent with your stories.\nHowever, during training, the `RulePolicy` checks for conflicts between rules and stories. Any such conflict will abort training.\n:::\n\nTo read more about the validator and all of the available options, see [the documentation for\n`rasa data validate`](./command-line-interface.mdx#rasa-data-validate).",
            "Writing Test Stories": "Testing your trained model on test stories is the best way to have confidence in how your assistant\nwill act in certain situations. Written in a modified story\nformat, test stories allow you to provide entire conversations and test that, given certain\nuser input, your model will behave in the expected manner. This is especially\nimportant as you start introducing more complicated stories from user\nconversations.\n\nTest stories are like\nthe stories in your training data, but include the user message as well.\n\nHere are some examples:\n\n<Tabs values={[{\"label\": \"Basics\", \"value\": \"basics\"}, {\"label\": \"Button Payload\", \"value\": \"buttons\"}, {\"label\": \"Custom Actions\", \"value\": \"customactions\"}, {\"label\": \"Forms Happy Path\", \"value\": \"formshappypath\"}, {\"label\": \"Forms Unhappy Path\", \"value\": \"formsunhappypath\"}]} defaultValue=\"basics\">\n<TabItem value=\"basics\">\n\n```\nstories:\n- story: A basic story test\n  steps:\n  - user: |\n      hello\n    intent: greet\n  - action: utter_ask_howcanhelp\n  - user: |\n     show me [chinese]{\"entity\": \"cuisine\"} restaurants\n    intent: inform\n  - action: utter_ask_location\n  - user: |\n      in [Paris]{\"entity\": \"location\"}\n    intent: inform\n  - action: utter_ask_price\n```\n\n</TabItem>\n<TabItem value=\"buttons\">\n\n```\nstories:\n- story: A test where a user clicks on a button with payload\n  steps:\n  - user: |\n      hello\n    intent: greet\n  - action: utter_ask_howcanhelp\n  - user: /inform{{\"cuisine\":\"chinese\"}}\n    intent: inform\n  - action: utter_ask_location\n  - user: /inform{{\"location\":\"Paris\"}}\n    intent: inform\n  - action: utter_ask_price\n```\n\n</TabItem>\n<TabItem value=\"customactions\">\n\n```\nstories:\n- story: A test where a custom action returns events\n  steps:\n  - user: |\n      hey\n    intent: greet\n  - action: my_custom_action\n  - slot_was_set:\n    - my_slot: \"value added by custom action\"\n  - action: utter_ask_age\n  - user: |\n      thanks\n    intent: thankyou\n  - action: utter_no_worries\n```\n\n</TabItem>\n<TabItem value=\"formshappypath\">\n\n```\nstories:\n- story: A test story with a form\n  steps:\n  - user: |\n      hi\n    intent: greet\n  - action: utter_greet\n  - user: |\n      im looking for a restaurant\n    intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant_form\n  - user: |\n      [afghan](cuisine) food\n    intent: inform\n  - action: restaurant_form\n  - active_loop: null\n  - action: utter_slots_values\n  - user: |\n      thanks\n    intent: thankyou\n  - action: utter_no_worries\n```\n\n</TabItem>\n<TabItem value=\"formsunhappypath\">\n\n```\nstories:\n- story: A test story with unexpected input during a form\n  steps:\n  - user: |\n      hi\n    intent: greet\n  - action: utter_greet\n  - user: |\n      im looking for a restaurant\n    intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant_form\n  - user: |\n      How's the weather?\n    intent: chitchat\n  - action: utter_chitchat\n  - action: restaurant_form\n  - active_loop: null\n  - action: utter_slots_values\n  - user: |\n      thanks\n    intent: thankyou\n  - action: utter_no_worries\n```\n\n</TabItem>\n<TabItem value=\"entities\">\n\n```\nstories:\n- story: A basic test story with multiple entities for a single token\n  steps:\n  - user: |\n      hello\n    intent: greet\n  - action: utter_ask_howcanhelp\n  - user: |\n     show me [chinese]{\"entity\": \"cuisine\"} restaurants\n    intent: inform\n  - action: utter_ask_location\n  - user: |\n      in [Paris][{\"entity\": \"location\"}, {\"entity\": \"city\"}]\n    intent: inform\n  - action: utter_ask_price\n```\n\n</TabItem>\n</Tabs>\n\nBy default, the command will run tests on stories from any files with names starting with `test_`. You can also provide\na specific test stories file or directory with the `--stories` argument.\nYou can test your assistant against them by running:\n\n```\nrasa test\n```\n\nConversation testing is only as thorough and accurate as the test\ncases you include, so you should continue to grow your set of test stories\nas you make improvements to your assistant. A good rule of thumb to follow is that you should aim for your test stories\nto be representative of the true distribution of real conversations.\n\nSee the [CLI documentation on `rasa test`](./command-line-interface.mdx#rasa-test) for\nmore configuration options.\n\n:::caution Testing Custom Actions\n[Custom Actions](./custom-actions.mdx) are not executed as part of test stories. If your custom\nactions append any events to the conversation, this has to be reflected in your test story\n(e.g. by adding `slot_was_set` events to your test story).\n\nTo test the code of your custom actions, you should write unit tests\nfor them and include these tests in your [CI/CD pipeline](./setting-up-ci-cd.mdx).\n\n:::",
            "Evaluating an NLU Model": {
              "Using a Held-Out Test Set": "If you use the train-test\nset approach, it is best to [shuffle and split your data](./command-line-interface.mdx#rasa-data-split)\nusing `rasa data split` every time you evaluate your model, as\nopposed to using a static NLU test set, which can easily become outdated.\n\nYou can split your NLU data into train and test sets using: \n\n```\nrasa data split nlu\n```\n\nNext, you can see how well your trained NLU model predicts the\ndata from the test set you generated, using:\n\n```\nrasa test nlu\n    --nlu train_test_split/test_data.yml\n```",
              "Using Cross-Validation": "If you've made significant changes to your NLU training data (e.g.\nsplitting an intent into two intents or adding a lot of training examples), you should run a\nfull NLU evaluation using cross-validation. Cross-validation automatically creates\nmultiple train/test splits and averages the results of evaluations on each train/test split.\nThis means all your data is evaluated during cross-validation, making cross-validation the most\nthorough way to automatically test your NLU model.\n\nTo run NLU testing in cross-validation mode run: \n\n```\nrasa test nlu\n    --nlu data/nlu\n    --cross-validation\n```\n\nYou can specify the number of test/train splits used with the `-f/--folds` flag:\n\n```\nrasa test nlu\n    --nlu data/nlu\n    --cross-validation\n    --folds 5\n```\n\nNote that during cross-validation, the NLU model will be trained for each fold,\nso cross-validation with a large data set and a high number of folds can be time-consuming.\nOn a small data set, a high number of folds can result in too few examples per intent being available for each test split.\n\nOn the other hand, if you specify a low number of folds, your data will be split into much larger chunks,\nand there will be proportionally less data to train on for each fold.\n\nChoose a number of folds that balances both considerations for your dataset size.\n\n:::tip hyperparameter tuning\nTo further improve your model check out this\n[tutorial on hyperparameter tuning](https://blog.rasa.com/rasa-nlu-in-depth-part-3-hyperparameters/).\n:::",
              "Comparing NLU Pipelines": "To get the most out of your training data, you should train and evaluate your model on different pipelines\nand different amounts of training data.\n\nTo do so, pass multiple configuration files to the `rasa test` command:\n\n```\nrasa test nlu --nlu data/nlu.yml\n   --config config_1.yml config_2.yml\n```\n\nThis performs several steps:\n\n['Create a global 80% train / 20% test split from `data/nlu.yml`.', 'Exclude a certain percentage of data from the global train split.', 'Train models for each configuration on remaining training data.', 'Evaluate each model on the global test split.']\n\nThe above process is repeated with different percentages of training data in step 2\nto give you an idea of how each pipeline will behave if you increase the amount of training data.\nSince training is not completely deterministic, the whole process is repeated\nthree times for each configuration specified.\n\nA graph with the mean and standard deviations of\n[f1-scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\nacross all runs is plotted.\nThe f1-score graph, along with all train/test sets, the trained models, classification and error reports,\nwill be saved into a folder called `nlu_comparison_results`.\n\nInspecting the f1-score graph can help you understand if you have enough data for your NLU model.\nIf the graph shows that f1-score is still improving when all of the training data is used,\nit may improve further with more data. But if f1-score has plateaued when all training data is used,\nadding more data may not help.\n\nIf you want to change the number of runs or exclusion percentages, you can:\n\n```\nrasa test nlu --nlu data/nlu.yml\n  --config config_1.yml config_2.yml\n  --runs 4 --percentages 0 25 50 70 90\n```",
              "Interpreting the Output": {
                "Intent Classifiers": "The `rasa test` script will produce a report (`intent_report.json`), confusion matrix (`intent_confusion_matrix.png`)\nand confidence histogram (`intent_histogram.png`) for your intent classification model.\n\nThe report logs [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html),\n[recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) and\n[f1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) for each intent,\nas well as providing an overall average. You can save these reports as JSON files using the `--report` argument.\n\nThe confusion matrix shows which intents are mistaken for others.\nAny samples which have been incorrectly predicted are logged and saved to a file called `errors.json` for easier debugging.\n\n<div align=\"center\">\n    <img alt=\"image\" src={useBaseUrl(\"/img/intent_confusion_matrix_example.png\")} width=\"70%\" />\n</div>\n\nThe histogram allows you to visualize the confidence for all predictions,\nwith the correct and incorrect predictions being displayed by blue and red bars respectively.\nImproving the quality of your training data will move the blue histogram bars up the plot and the\nred histogram bars down the plot. It should also help in reducing the number of red histogram bars itself.\n\n<div align=\"center\">\n    <img alt=\"image\" src={useBaseUrl(\"/img/intent_histogram_example.png\")} width=\"70%\" />\n</div>",
                "Response Selectors": "`rasa test` evaluates response selectors in the same way that it evaluates intent classifiers, producing a\nreport (`response_selection_report.json`), confusion matrix (`response_selection_confusion_matrix.png`),\nconfidence histogram (`response_selection_histogram.png`) and errors (`response_selection_errors.json`).\nIf your pipeline includes multiple response selectors, they are evaluated in a single report.\n\nThe report logs precision, recall and f1 measure for\neach sub-intent of a [retrieval intent](./glossary.mdx#retrieval-intent) and provides an overall average.\nYou can save these reports as JSON files using the `--report` argument.",
                "Entity Extraction": "`rasa test` reports recall, precision, and f1-score for each entity type that\nyour trainable entity extractors are trained to recognize.\n\nOnly trainable entity extractors, such as the `DIETClassifier` and `CRFEntityExtractor` are\nevaluated by `rasa test`. Pretrained extractors like the `DucklingHTTPExtractor` are not evaluated.\n\nIf you have multiple entity extractors in your pipeline, or use some custom extractors,\nmultiple entities might be associated with the same token. In this case,\nyou can use a list notation in the test files, such as\n\n```\nstories:\n- story: A basic test story with multiple entities for a single token\n  steps:\n    - user: |\n        I like [ice cream][{\\\"entity\\\": \\\"food\\\"}, {\\\"entity\\\": \\\"desert\\\"}]\n      intent: inform\n    # ...\n```\n\n:::caution incorrect entity annotations\nIf any of your entities are incorrectly annotated, your evaluation may fail. One common problem\nis that an entity cannot stop or start inside a token.\nFor example, if you have an example for a `name` entity\nlike `[Brian](name)'s house`, this is only valid if your tokenizer splits `Brian's` into\nmultiple tokens.\n\n:::",
                "Entity Scoring": "To evaluate entity extraction we apply a simple tag-based approach. We don't consider\n[BILOU tags](nlu-training-data.mdx#bilou-entity-tagging) exactly, but only the\nentity type tags on a per token basis. For location entity like “near Alexanderplatz” we\nexpect the labels `LOC LOC` instead of the BILOU-based `B-LOC L-LOC`.\n\nOur approach is more lenient when it comes to evaluation, as it rewards\npartial extraction and does not penalize the splitting of entities.\nFor example, given the aforementioned entity “near Alexanderplatz” and a system that extracts\n“Alexanderplatz”, our approach rewards the extraction of “Alexanderplatz” and penalizes the missed out word “near”.\n\nThe BILOU-based approach, however, would label this as a complete failure since it expects Alexanderplatz\nto be labeled as a last token in an entity (`L-LOC`) instead of a single token entity (`U-LOC`). Note also that\na split extraction of “near” and “Alexanderplatz” would get full scores on our approach and zero on the\nBILOU-based one.\n\nHere's a comparison between the two scoring mechanisms for the phrase “near Alexanderplatz tonight”:\n\n|                     extracted                      |Simple tags (score) |  BILOU tags (score)   |\n|----------------------------------------------------|--------------------|-----------------------|\n|`[near Alexanderplatz](loc) [tonight](time)`        |loc loc time (3)    |B-loc L-loc U-time (3) |\n|`[near](loc) [Alexanderplatz](loc) [tonight](time)` |loc loc time (3)    |U-loc U-loc U-time (1) |\n|`near [Alexanderplatz](loc) [tonight](time)`        |O   loc time (2)    |O     U-loc U-time (1) |\n|`[near](loc) Alexanderplatz [tonight](time)`        |loc O   time (2)    |U-loc O     U-time (1) |\n|`[near Alexanderplatz tonight](loc)`                |loc loc loc  (2)    |B-loc I-loc L-loc  (1) |"
              }
            },
            "Evaluating a Dialogue Model": {
              "Interpreting the generated warnings": "The test script will also generate a warnings file called `results/stories_with_warnings.yml`.\nThis file contains all test stories for which [`action_unlikely_intent`](./default-actions.mdx#action_unlikely_intent)\nwas predicted at any conversation turn but all actions from the original story were predicted correctly.\nHowever, if a test story originally included an `action_unlikely_intent`, for example to ensure [a rule is designed to\ntrigger the conversation path after an `action_unlikely_intent`](./default-actions.mdx#customization-1) but the ensemble of\npolicies failed to do so, then the corresponding story will end up in `results/failed_test_stories.yml` as\na failed story.\n\nThe stories are sorted by the severity of `action_unlikely_intent`'s prediction.\nThis severity is calculated by [`UnexpecTEDIntentPolicy`](./policies.mdx#unexpected-intent-policy) itself at prediction time.\nThe higher the severity, the more unlikely is the intent and hence reviewing that particular\nconversation path becomes more critical.\n\nNote, that `action_unlikely_intent` is predicted by\n`UnexpecTEDIntentPolicy` which employs a machine learning based model\nunder the hood and hence can result in false warnings as well. You can choose to ignore such warnings\nif the conversation paths in these stories are already present in the training stories.",
              "Comparing Policy Configurations": "To choose a configuration for your dialogue model, or to choose hyperparameters for a\nspecific policy, you want to measure how well your dialogue model will generalize\nto conversations it hasn't seen before. Especially in the beginning\nof a project, when you don't have a lot of real conversations to train\nyour bot on, you may not want to exclude some to use as a test set.\n\nRasa has some scripts to help you choose and fine-tune your policy configuration.\nOnce you are happy with it, you can then train your final configuration on your\nfull data set.\n\nTo do this, you first have to train models for your different\nconfigurations. Create two (or more) config files including the policies you want to\ncompare, and then provide them to the train script to train your models:\n\n```\nrasa train core -c config_1.yml config_2.yml \\\n  --out comparison_models --runs 3 --percentages 0 5 25 50 70 95\n```\n\nSimilar to how the [NLU model was evaluated](./testing-your-assistant.mdx#comparing-nlu-pipelines), the above\ncommand trains the dialogue model on multiple configurations and different amounts of training data.\nFor each config file provided, Rasa will train dialogue models\nwith 0, 5, 25, 50, 70 and 95% of your training stories excluded from the training\ndata. This is repeated three times to ensure consistent results.\n\nOnce this script has finished, you can pass multiple models to the test script\nto compare the models you just trained:\n\n```\nrasa test core -m comparison_models --stories stories_folder\n  --out comparison_results --evaluate-model-directory\n```\n\nThis will evaluate each model on the stories in `stories_folder`\n(can be either training or test set) and plot some graphs\nto show you which policy performs best. Since the previous train command\nexcluded some amount of training data to train each model,\nthe above test command can measure how well your model predicts the held-out stories.\nTo compare single policies, create config files containing only one policy each.\n\n:::note\nThis training process can take a long time, so we'd suggest letting it run\nsomewhere in the background where it can't be interrupted.\n\n:::",
              "Testing Action Code": "The approach used to test your action code will depend on how it is\nimplemented. For example, if you connect to external APIs, you should write integration tests to ensure\nthat those APIs respond as expected to common inputs. However you test your action code, you should\ninclude these tests in your CI pipeline so that they run each time you make changes.\n\nIf you have any questions or problems, please share them with us in the dedicated\n[testing section on our forum](https://forum.rasa.com/tags/testing)!"
            },
            "End-To-End Testing": {
              "How to write test cases": "To write test cases, you need to create a YAML file inside the `tests` directory of your project. The name of the file\nshould be `e2e_test_cases.yml`. You can also create a subdirectory inside the `tests` directory and place your test case\nYAML files there. These files will be automatically discovered and run by Rasa Pro, however you need to provide\nthe path to the subdirectory as positional argument to the `rasa test e2e` command.\n\nEach input file must contain the `test_cases` required key. The value of this key is a list of test cases.\nEach test case must include a name given to the `test_case` key and a list of test steps given to the `steps` key.\nA step can be either one of the following:\n\n['`user`: a user message', '`bot`: a bot response', '`utter`: a domain utterance', '`slot_was_set`: a slot name and the value it was set to']\n\nYou can also add the optional `fixtures` top level key if pre-filled slots are required for setting any individual\ntest case context. The `fixtures` key is a list of fixture names (which must be unique) and each fixture name maps to a\nlist of slot key-value pairs. If one of the test cases requires a pre-filled slot, you can add the fixture name to the\ntest case definition, by adding the fixture name to the optional `fixtures` key in the test case. The slot key-value\npairs will be set before the test case is run.\n\nThe following example shows a test case file with fixtures and two test cases that make use of all available steps:\n\n```\nfixtures:\n - premium:  # name of the fixture must be provided and be unique\n   - membership_type: premium  # every fixture can contain multiple slot key-value pairs\n   - logged_in: True\n - standard:\n   - logged_in: True\n   - membership_type: standard\n\ntest_cases:\n - test_case: \"test_premium_booking\"\n   fixtures:\n    - premium  # re-use the name of the fixture provided in fixtures section\n   steps:\n    - user: \"Hi!\"\n    - bot: \"Welcome back! How can I help you?\"\n    - user: \"I want to book a trip.\"\n    - utter: utter_ask_location\n    - user: \"I would like to travel to Lisbon.\"\n    - slot_was_set:\n      - location: \"Lisbon\"\n    - utter: utter_ask_date\n    - user: \"I would like to travel on 22nd of June.\"\n    - slot_was_set:\n      - travel_date: \"2023-06-22\"\n    - bot: \"Great! I will book your trip to Lisbon on 22nd of June.\"\n    - bot: \"You saved 20% by being a premium member.\"\n\n - test_case: \"test_anonymous_booking\"\n   steps:\n    - user: \"Hi!\"\n    - bot: \"Hey! How can I help you?\"\n    - user: \"I want to book a trip.\"\n    - utter: utter_ask_location\n    - user: \"I would like to travel to Paris.\"\n    - slot_was_set:\n      - location: \"Paris\"\n    - utter: utter_ask_date\n    - user: \"I would like to travel on 2nd of April.\"\n    - slot_was_set:\n      - travel_date: \"2023-04-02\"\n    - bot: \"Great! I will book your trip to Paris on 2nd of April.\"\n    - bot: \"You can also choose to save 20% by becoming a premium member.\"\n```\n\n:::note\n\nIf you are using multiple consecutive `slot_was_set` steps in your test case, the order in which these are defined must\nmatch the order in which the slots are filled in the dialogue.\n\n:::",
              "How to run the tests": {
                "Testing custom actions": "If the test cases include custom actions, start the action server first:\n\n```\nrasa run actions && rasa test e2e\n```"
              },
              "How to interpret the output": "By default, the results are always printed to `stdout` and the command will exit with exit code `0` (if all tests passed)\nor `1` (in case of test failures).\n\nThe output style is inspired by `pytest`:\n\n['Failed test cases will be stacked, each highlighting the difference in identified mismatches in similar style to `git diff`:\\nexpected messages will be preceded by `+` prefix, while actual messages will be preceded by `-` prefix.', 'The short test summary includes a list of every failed test case name and file location in a new line.']\n\nIf `-o` flag is specified in the command, the results are also written to the `tests/e2e_results.yml` file, which will\ncontain a list of test results with the following keys:\n\n['`name`: the name of the test case', '`pass_status`: the status of the test case, either `True` or `False`', '`expected_steps`: the expected test steps', '`difference`: a list of differences between the expected and actual test steps']"
            }
          },
          "metadata": {
            "id": "testing-your-assistant",
            "sidebar_label": "Testing Your Assistant",
            "title": "Testing Your Assistant",
            "abstract": "Rasa lets you validate and test dialogues end-to-end by running through test stories. In addition, you can also test the dialogue management and the message processing (NLU) separately."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 48]"
        },
        {
          "title": "Tracker Stores",
          "description": "All conversations are stored within a tracker store. Read how Rasa provides implementations for different store types out of the box.",
          "content": {
            "Switch to root user to install packages": "USER root\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends alien libaio1 && apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*",
            "Copy in oracle instaclient": "",
            "https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html": "COPY oracle.rpm oracle.rpm",
            "Install the Python wrapper library for the Oracle drivers": "RUN pip install cx-Oracle",
            "Install Oracle client libraries": {
              "RedisTrackerStore": {
                "Configuration": "To set up Rasa with Redis the following steps are required:\n\n['Start your Redis instance', 'Add required configuration to your `endpoints.yml`:']\n\n```\ntracker_store:\n    type: redis\n    url: <url of the redis instance, e.g. localhost>\n    port: <port of your redis instance, usually 6379>\n    key_prefix: <alphanumeric value to prepend to tracker store keys>\n    db: <number of your database within redis, e.g. 0>\n    password: <password used for authentication>\n    use_ssl: <whether or not the communication is encrypted, default `false`>\n```\n\n```\nrasa run -m models --endpoints endpoints.yml\n```\n\n['If deploying your model in Docker Compose, add the service to your `docker-compose.yml`:']\n\n```\nredis:\n  image: redis:latest\n```\n\nTo route requests to the new service, make sure that the `url` in your `endpoints.yml`\nreferences the service name:\n\n```\n tracker_store:\n     type: redis\n     url: <url of the redis instance, e.g. localhost>\n     port: <port of your redis instance, usually 6379>\n     db: <number of your database within redis, e.g. 0>\n     key_prefix: <alphanumeric value to prepend to tracker store keys>\n     password: <password used for authentication>\n     use_ssl: <whether or not the communication is encrypted, default `false`>\n```\n\n['`url` (default: `localhost`): The url of your redis instance', '`port` (default: `6379`): The port which redis is running on', '`db` (default: `0`): The number of your redis database', '`key_prefix` (default: `None`): The prefix to prepend to tracker store keys. Must\\nbe alphanumeric', '`username` (default: `None`): Username used for authentication', '`password` (default: `None`): Password used for authentication\\n(`None` equals no authentication)', '`record_exp` (default: `None`): Record expiry in seconds', '`use_ssl` (default: `False`): whether or not to use SSL for transit encryption']"
              },
              "MongoTrackerStore": {
                "Configuration": {
                  "Configuration Parameters": [
                    "`url` (default: `mongodb://localhost:27017`): URL of your MongoDB",
                    "`db` (default: `rasa`): The database name which should be used",
                    "`username` (default: `0`): The username which is used for authentication",
                    "`password` (default: `None`): The password which is used for authentication",
                    "`auth_source` (default: `admin`): database name associated with the user's credentials.",
                    "`collection` (default: `conversations`): The collection name which is\nused to store the conversations"
                  ]
                }
              },
              "DynamoTrackerStore": {
                "Configuration": {
                  "Configuration Parameters": [
                    "`table_name` (default: `states`): name of the DynamoDB table",
                    "`region` (default: `us-east-1`): name of the region associated with the client"
                  ]
                }
              },
              "Custom Tracker Store": {
                "Configuration": "Put the module path to your custom tracker store and the parameters you require in your `endpoints.yml`:\n\n```\ntracker_store:\n  type: path.to.your.module.Class\n  url: localhost\n  a_parameter: a value\n  another_parameter: another value\n```\n\nIf you are deploying in Docker Compose, you have two options to add this store to Rasa:\nextending the Rasa image to include the module, or mounting the module as volume.\n\nMake sure to add the corresponding service as well. For example, mounting it as a volume would look like so:\n\n```\nrasa:\n  <existing rasa service configuration>\n  volumes:\n    - <existing volume mappings, if there are any>\n    - ./path/to/your/module.py:/app/path/to/your/module.py\ncustom-tracker-store:\n  image: custom-image:tag\n```\n\n```\ntracker_store:\n  type: path.to.your.module.Class\n  url: custom-tracker-store\n  a_parameter: a value\n  another_parameter: another value\n```"
              },
              "Fallback Tracker Store": "In case the primary tracker store configured in `endpoints.yml` becomes unavailable, the rasa agent will issue an\nerror message and fall back on the `InMemoryTrackerStore` implementation. A new dialogue session will be started for\neach turn, which will be saved separately in the `InMemoryTrackerStore` fallback.\n\nAs soon as the primary tracker store comes back up, it will replace the fallback tracker store and save the\nconversation from this point going forward. However, note that any previous states saved in the `InMemoryTrackerStore`\nfallback will be lost.\n\n:::warning Using the same redis instance as lock-store and tracker store\n\nYou must not use the same Redis instance as both lock store and tracker store.\nIf the Redis instance becomes unavailable, the conversation will hang because there is no fall back mechanism\nimplemented for the lock store (as it is for the tracker store interfaces).\n:::"
            }
          },
          "metadata": {
            "id": "tracker-stores",
            "sidebar_label": "Tracker Stores",
            "title": "Tracker Stores",
            "description": "All conversations are stored within a tracker store. Read how Rasa provides implementations for different store types out of the box.",
            "abstract": "Your assistant's conversations are stored within a tracker store. Rasa provides implementations for different store types out of the box, or you can create your own custom one."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 49]"
        },
        {
          "title": "Training Data Format",
          "description": "Description of the YAML format for training data",
          "content": {
            "Overview": {
              "High-Level Structure": "Each file can contain one or more **keys** with corresponding training\ndata. One file can contain multiple keys, but each key can only appear\nonce in a single file. The available keys are:\n\n['`version`', '`nlu`', '`stories`', '`rules`']\n\nYou should specify the `version` key in all YAML training data files.\nIf you don't specify a version key in your training data file, Rasa\nwill assume you are using the latest training data format specification supported\nby the version of Rasa you have installed.\nTraining data files with a Rasa version greater than the version you have\ninstalled on your machine will be skipped.\nCurrently, the latest training data format specification for Rasa 3.x is 3.1.",
              "Example": "Here's a short example which keeps all training data in a single file:\n\n```\nversion: \"3.1\"\n\nnlu:\n- intent: greet\n  examples: |\n    - Hey\n    - Hi\n    - hey there [Sara](name)\n\n- intent: faq/language\n  examples: |\n    - What language do you speak?\n    - Do you only handle english?\n\nstories:\n- story: greet and faq\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - intent: faq\n  - action: utter_faq\n\nrules:\n- rule: Greet user\n  steps:\n  - intent: greet\n  - action: utter_greet\n\n```\n\nTo specify your test stories, you need to put them into a separate file:\n\n```\nstories:\n- story: greet and ask language\n- steps:\n  - user: |\n      hey\n    intent: greet\n  - action: utter_greet\n  - user: |\n      what language do you speak\n    intent: faq/language\n  - action: utter_faq\n```\n\n[Test stories](#test-stories) use the same format as the story training data and should be placed\nin a separate file with the prefix `test_`.\n\n:::note The  `|` symbol\nAs shown in the above examples, the `user` and `examples` keys are followed by `|`\n(pipe) symbol. In YAML `|` identifies multi-line strings with preserved indentation.\nThis helps to keep special symbols like `\"`, `'` and others still available in the\ntraining examples.\n:::"
            },
            "NLU Training Data": {
              "Training Examples": "Training examples are grouped by [intent](glossary.mdx#intent) and listed under the\n`examples` key. Usually, you'll list one example per line as follows:\n\n```\nnlu:\n- intent: greet\n  examples: |\n    - hey\n    - hi\n    - whats up\n```\n\nHowever, it's also possible to use an extended format if you have a custom NLU component and need metadata for your examples:\n\n```\nnlu:\n- intent: greet\n  examples:\n  - text: |\n      hi\n    metadata:\n      sentiment: neutral\n  - text: |\n      hey there!\n```\n\nThe `metadata` key can contain arbitrary key-value data that is tied to an example and\naccessible by the components in the NLU pipeline.\nIn the example above, the sentiment metadata could be used by a custom component in\nthe pipeline for sentiment analysis.\n\nYou can also specify this metadata at the intent level:\n\n```\nnlu:\n- intent: greet\n  metadata:\n    sentiment: neutral\n  examples:\n  - text: |\n      hi\n  - text: |\n      hey there!\n```\n\nIn this case, the content of the `metadata` key is passed to every intent example.\n\nIf you want to specify [retrieval intents](glossary.mdx#retrieval-intent), then your NLU examples will look as follows:\n\n```\nnlu:\n- intent: chitchat/ask_name\n  examples: |\n    - What is your name?\n    - May I know your name?\n    - What do people call you?\n    - Do you have a name for yourself?\n\n- intent: chitchat/ask_weather\n  examples: |\n    - What's the weather like today?\n    - Does it look sunny outside today?\n    - Oh, do you mind checking the weather for me please?\n    - I like sunny days in Berlin.\n```\n\nAll retrieval intents have a suffix\nadded to them which identifies a particular response key for your assistant. In the\nabove example, `ask_name` and `ask_weather` are the suffixes. The suffix is separated from\nthe retrieval intent name by a `/` delimiter.\n\n:::note Special meaning of `/`\nAs shown in the above examples, the `/` symbol is reserved as a delimiter to separate\nretrieval intents from their associated response keys. Make sure not to use it in the\nname of your intents.\n:::",
              "Entities": "[Entities](glossary.mdx#entity) are structured pieces of information that can be extracted from a user's message.\n\nEntities are annotated in training examples with the entity's name.\nIn addition to the entity name, you can annotate an entity with [synonyms](nlu-training-data.mdx#synonyms), [roles, or groups](nlu-training-data.mdx#entities-roles-and-groups).\n\nIn training examples, entity annotation would look like this:\n\n```\nnlu:\n- intent: check_balance\n  examples: |\n    - how much do I have on my [savings](account) account\n    - how much money is in my [checking]{\"entity\": \"account\"} account\n    - What's the balance on my [credit card account]{\"entity\":\"account\",\"value\":\"credit\"}\n\n```\n\nThe full possible syntax for annotating an entity is:\n\n```\n[<entity-text>]{\"entity\": \"<entity name>\", \"role\": \"<role name>\", \"group\": \"<group name>\", \"value\": \"<entity synonym>\"}\n```\n\nThe keywords `role`, `group`, and `value` are optional in this notation.\nThe `value` field refers to synonyms. To understand what the labels `role` and `group` are\nfor, see the section on [entity roles and groups](./nlu-training-data.mdx#entities-roles-and-groups).",
              "Synonyms": "Synonyms normalize your training data by mapping an\nextracted entity to a value other than the literal text extracted.\nYou can define synonyms using the format:\n\n```\nnlu:\n- synonym: credit\n  examples: |\n    - credit card account\n    - credit account\n```\n\nYou can also define synonyms in-line in your training examples by\nspecifying the `value` of the entity:\n\n```\nnlu:\n- intent: check_balance\n  examples: |\n    - how much do I have on my [credit card account]{\"entity\": \"account\", \"value\": \"credit\"}\n    - how much do I owe on my [credit account]{\"entity\": \"account\", \"value\": \"credit\"}\n```\n\nRead more about synonyms on the [NLU Training Data page](./nlu-training-data.mdx#synonyms).",
              "Regular Expressions": "You can use regular expressions to improve intent classification and\nentity extraction using the [`RegexFeaturizer`](components.mdx#regexfeaturizer) and [`RegexEntityExtractor`](components.mdx#regexentityextractor) components.\n\nThe format for defining a regular expression is as follows:\n\n```\nnlu:\n- regex: account_number\n  examples: |\n    - \\d{10,12}\n```\n\nHere `account_number` is the name of the regular expression. When used as features for the `RegexFeaturizer` the name of the regular expression does not matter. When using the `RegexEntityExtractor`, the name of the regular expression should match the name of the entity you want to extract.\n\nRead more about when and how to use regular expressions with each component on the [NLU Training Data page](./nlu-training-data.mdx#regular-expressions).",
              "Lookup Tables": "Lookup tables are lists of words used to generate\ncase-insensitive regular expression patterns. The format is as follows:\n\n```\nnlu:\n- lookup: banks\n  examples: |\n    - JPMC\n    - Bank of America\n```\n\nWhen you supply a lookup table in your training data, the contents of that table\nare combined into one large regular expression. This regex is used to check\neach training example to see if it contains matches for entries in the\nlookup table.\n\nLookup table regexes are processed identically to the regular\nexpressions directly specified in the training data and can be used\neither with the [RegexFeaturizer](components.mdx#regexfeaturizer)\nor with the [RegexEntityExtractor](components.mdx#regexentityextractor).\nThe name of the lookup table is subject to the same constraints as the\nname of a regex feature.\n\nRead more about using lookup tables on the [NLU Training Data page](./nlu-training-data.mdx#lookup-tables)."
            },
            "Conversation Training Data": {
              "Stories": {
                "User Messages": "All user messages are specified with the `intent:`\nkey and an optional `entities:` key.\n\nWhile writing stories, you do not have to deal with the specific\ncontents of the messages that the users send. Instead, you can take\nadvantage of the output from the NLU pipeline, which uses\na combination of an intent and entities to refer to all possible\nmessages the users can send with the same meaning.\n\nUser messages follow the format:\n\n```\nstories:\n- story: user message structure\n  steps:\n    - intent: intent_name  # Required\n      entities:  # Optional\n      - entity_name: entity_value\n    - action: action_name\n```\n\nFor example, to represent the sentence\n`I want to check my credit balance`, where `credit` is an entity:\n\n```\nstories:\n- story: story with entities\n  steps:\n  - intent: account_balance\n    entities:\n    - account_type: credit\n  - action: action_credit_account_balance\n```\n\nIt is important to include the entities here as well because the\npolicies learn to predict the next action based on a *combination* of\nboth the intent and entities (you can, however, change this behavior\nusing the [`use_entities`](#entities) attribute).",
                "Actions": "All actions executed by the bot are specified with the `action:` key followed\nby the name of the action.\nWhile writing stories, you will encounter two types of actions:\n\n['[Responses](domain.mdx#responses): start with `utter_` and\\nsend a specific message to the user. e.g.']\n\n```\nstories:\n- story: story with a response\n  steps:\n  - intent: greet\n  - action: utter_greet\n```\n\n['[Custom actions](custom-actions.mdx): start with `action_`, run\\narbitrary code and send any number of messages (or none).']\n\n```\nstories:\n- story: story with a custom action\n  steps:\n  - intent: feedback\n  - action: action_store_feedback\n```",
                "Forms": "A [form](glossary.mdx#form) is a specific kind of custom action that contains the logic to loop over\na set of required slots and ask the user for this information. You\n[define a form](forms.mdx#defining-a-form) in the `forms` section in your domain.\nOnce defined, you should specify the [happy path](glossary.mdx#happy--unhappy-paths)\nfor a form as a [rule](forms.mdx). You should include interruptions of forms or\nother \"unhappy paths\" in stories so that the model can\ngeneralize to unseen conversation sequences.\nAs a step in a story, a form takes the following format:\n\n```\nstories:\n- story: story with a form\n  steps:\n  - intent: find_restaurant\n  - action: restaurant_form                # Activate the form\n  - active_loop: restaurant_form           # This form is currently active\n  - active_loop: null                      # Form complete, no form is active\n  - action: utter_restaurant_found\n```\n\nThe `action` step activates the form and begins looping over the required slots. The `active_loop: restaurant_form`\nstep indicates that there is a currently active form. Much like a `slot_was_set` step,\na `form` step doesn't **set** a form to active but indicates that it should already be activated.\nIn the same way, the `active_loop: null` step indicates that no form should be active before the subsequent\nsteps are taken.\n\nA form can be interrupted and remain active; in this case the interruption should come after the\n`action: <form to activate>` step and be followed by the `active_loop: <active form>` step.\nAn interruption of a form could look like this:\n\n```\nstories:\n- story: interrupted food\n  steps:\n    - intent: request_restaurant\n    - action: restaurant_form\n    - intent: chitchat\n    - action: utter_chitchat\n    - active_loop: restaurant_form\n    - active_loop: null\n    - action: utter_slots_values\n```",
                "Slots": "A slot event is specified under the key `slot_was_set:` with the\nslot name and optionally the slot's value.\n\n**[Slots](domain.mdx#slots)** act as the bots memory.\nSlots are **set** by either the default action [`action_extract_slots`](./default-actions.mdx#action_extract_slots) according to the\n[slot mappings](./domain.mdx#slot-mappings) specified in the domain, or by custom actions.\nThey are **referenced** by stories in `slot_was_set` steps. For example:\n\n```\nstories:\n- story: story with a slot\n  steps:\n  - intent: celebrate_bot\n  - slot_was_set:\n    - feedback_value: positive\n  - action: utter_yay\n```\n\nThis means the story requires that the current value for the `feedback_value`\nslot be `positive` for the conversation to continue as specified.\n\nWhether or not you need to include the slot's value depends on the\n[slot type](domain.mdx#slot-types) and whether the value can or should\ninfluence the dialogue. If the value doesn't matter, as is the case for e.g. `text` slots,\nyou can list only the slot's name:\n\n```\nstories:\n- story: story with a slot\n  steps:\n  - intent: greet\n  - slot_was_set:\n    - name\n  - action: utter_greet_user_by_name\n```\n\nThe initial value for any slot by default is `null`, and you can use it to check if the slot was not set:\n\n```\nstories:\n- story: French cuisine\n  steps:\n  - intent: inform\n  - slot_was_set:\n    - cuisine: null\n```\n\n:::note How slots work\nStories do not **set** slots. The slot must be set by the default action `action_extract_slots` if a slot mapping applies, or custom\naction **before** the `slot_was_set` step.\n:::",
                "Checkpoints": "Checkpoints are specified with the `checkpoint:` key, either at the beginning\nor the end of a story.\n\nCheckpoints are ways to connect stories together. They can be either the first\nor the last step in a story. If they are the last step in a story, that story\nwill be connected to each other story that starts with the checkpoint of the\nsame name when the model is trained. Here is an example of a story that ends\nwith a checkpoint, and one that starts with the same checkpoint:\n\n```\nstories:\n- story: story_with_a_checkpoint_1\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - checkpoint: greet_checkpoint\n\n- story: story_with_a_checkpoint_2\n  steps:\n  - checkpoint: greet_checkpoint\n  - intent: book_flight\n  - action: action_book_flight\n```\n\nCheckpoints at the beginning of stories can also be conditional on\nslots being set, for example:\n\n```\nstories:\n- story: story_with_a_conditional_checkpoint\n  steps:\n  - checkpoint: greet_checkpoint\n    # This checkpoint should only apply if slots are set to the specified value\n    slot_was_set:\n    - context_scenario: holiday\n    - holiday_name: thanksgiving\n  - intent: greet\n  - action: utter_greet_thanksgiving\n```\n\nCheckpoints can help simplify your training data and reduce redundancy in it,\nbut **do not overuse them**. Using lots of checkpoints can quickly make your\nstories hard to understand. It makes sense to use them if a sequence of steps\nis repeated often in different stories, but stories without checkpoints\nare easier to read and write.",
                "OR statement": "`or` steps are ways to handle multiple intents or slot events the same way,\nwithout writing a separate story for each intent. For example, if you ask the user to\nconfirm something, you might want to treat the `affirm` and `thankyou` intents in the\nsame way. Stories with `or` steps will be converted into multiple\nseparate stories at training time.\nFor example, the following story would be converted to two stories at training time:\n\n```\nstories:\n- story: story with OR\n  steps:\n  - intent: signup_newsletter\n  - action: utter_ask_confirm\n  - or:\n    - intent: affirm\n    - intent: thanks\n  - action: action_signup_newsletter\n```\n\nYou can also use `or` statements with slot events.\nThe following means the story requires that the current value for\nthe `name` slot is set and is either `joe` or `bob`. This story\nwould be converted to two stories at training time.\n\n```\nstories:\n- story:\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - intent: tell_name\n  - or:\n    - slot_was_set:\n        - name: joe\n    - slot_was_set:\n        - name: bob\n  # ... next actions\n```\n\nJust like checkpoints, OR statements can be useful, but if you are using a lot of them,\nit is probably better to restructure your domain and/or intents.\n\n:::warning Don't overuse\nOverusing these features (both checkpoints and OR statements) will slow down training.\n:::"
              },
              "Rules": "Rules are listed under the `rules` key and look similar to stories. A rule also has a `steps`\nkey, which contains a list of the same steps as stories do. Rules can additionally\ncontain the `conversation_started` and `conditions` keys. These are used to specify conditions\nunder which the rule should apply.\n\nA rule that with a condition looks like this:\n\n```\nrules:\n- rule: Only say `hey` when the user provided a name\n  condition:\n  - slot_was_set:\n    - user_provided_name: true\n  steps:\n  - intent: greet\n  - action: utter_greet\n```\n\nFor more information about writing rules, see [Rules](rules.mdx#writing-a-rule)."
            },
            "Test Stories": "Test stories check if a message is classified correctly as well as the action predictions.\n\nTest stories use the same format as [stories](#stories),\nexcept that user message steps can include a `user` to specify the actual\ntext and entity annotations of the user message. Here's an example of a\ntest story:\n\n```\nstories:\n- story: A basic end-to-end test\n  steps:\n  - user: |\n     hey\n    intent: greet\n  - action: utter_ask_howcanhelp\n  - user: |\n     show me [chinese]{\"entity\": \"cuisine\"} restaurants\n    intent: inform\n  - action: utter_ask_location\n  - user: |\n     in [Paris]{\"entity\": \"location\"}\n    intent: inform\n  - action: utter_ask_price\n```\n\nYou can run the tests using the following command:\n\n```\nrasa test\n```\n\nIf you want to know more about testing head over to\n[Testing Your Assistant](testing-your-assistant.mdx).",
            "End-to-end Training": ":::info New in 2.2\nEnd-to-end training is an experimental feature.\nWe introduce experimental features to get feedback from our community, so we encourage you to try it out!\nHowever, the functionality might be changed or removed in the future.\nIf you have feedback (positive or negative) please share it with us on the [Rasa Forum](https://forum.rasa.com).\n\n:::\n\nWith [end-to-end training](stories.mdx#end-to-end-training), you do not have to deal with the specific\nintents of the messages that are extracted by the NLU pipeline.\nInstead, you can put the text of the user message directly in the stories,\nby using `user` key.\n\nThese end-to-end user messages follow the format:\n\n```\nstories:\n- story: user message structure\n  steps:\n    - user: the actual text of the user message\n    - action: action_name\n```\n\nIn addition, you can add entity tags that can be extracted\nby the [TED Policy](./policies.mdx#ted-policy).\nThe syntax for entity tags is the same as in\n[the NLU training data](./training-data-format.mdx#entities).\nFor example, the following story contains the user utterance\n` I can always go for sushi`. By using the syntax from the NLU training data\n`[sushi](cuisine)`, you can mark `sushi` as an entity of type `cuisine`.\n\n```\nstories:\n- story: story with entities\n  steps:\n  - user: I can always go for [sushi](cuisine)\n  - action: utter_suggest_cuisine\n```\n\nSimilarly, you can put bot utterances directly in the stories,\nby using the `bot` key followed by the text that you want your bot to say.\n\nA story with only a bot utterance might look like this:\n\n```\nstories:\n- story: story with an end-to-end response\n  steps:\n  - intent: greet\n    entities:\n    - name: Ivan\n  - bot: Hello, a person with a name!\n```\n\nYou can also have a mixed end-to-end story:\n\n```\nstories:\n- story: full end-to-end story\n  steps:\n  - intent: greet\n    entities:\n    - name: Ivan\n  - bot: Hello, a person with a name!\n  - intent: search_restaurant\n  - action: utter_suggest_cuisine\n  - user: I can always go for [sushi](cuisine)\n  - bot: Personally, I prefer pizza, but sure let's search sushi restaurants\n  - action: utter_suggest_cuisine\n  - user: Have a beautiful day!\n  - action: utter_goodbye\n```\n\nRasa end-to-end training is fully integrated with standard Rasa approach.\nIt means that you can have mixed stories with some steps defined by actions or intents\nand other steps defined directly by user messages or bot responses."
          },
          "metadata": {
            "id": "training-data-format",
            "sidebar_label": "Training Data Format",
            "title": "Training Data Format",
            "description": "Description of the YAML format for training data",
            "abstract": "This page describes the different types of training data that go into a Rasa assistant and how this training data is structured."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 50]"
        },
        {
          "title": "Training Data Importers",
          "description": "Change the way Rasa imports training data by replacing the default importer or writing your own importer.",
          "content": {
            "RasaFileImporter (default)": "By default Rasa uses the importer `RasaFileImporter`. If you want to use it on its\nown, you don't have to specify anything in your configuration file.\nIf you want to use it together with other importers, add it to your\nconfiguration file:\n\n```\nimporters:\n- name: \"module.CustomImporter\"\n  parameter1: \"value\"\n  parameter2: \"value2\"\n- name: \"RasaFileImporter\"\n```",
            "MultiProjectImporter (experimental)": ":::info New in 1.3\nThis feature is currently experimental and might change or be removed in the future.\nShare your feedback on it in the [forum](https://forum.rasa.com) to help\nus making this feature ready for production.\n\n:::\n\nWith this importer you can train a model by combining multiple\nreusable Rasa projects.\nYou might, for example, handle chitchat with one project and greet your users with\nanother. These projects can be developed in isolation, and then combined when you train\nyour assistant.\n\nFor example, consider the following directory structure:\n\n```\n.\n├── config.yml\n└── projects\n    ├── GreetBot\n    │   ├── data\n    │   │   ├── nlu.yml\n    │   │   └── stories.yml\n    │   └── domain.yml\n    └── ChitchatBot\n        ├── config.yml\n        ├── data\n        │   ├── nlu.yml\n        │   └── stories.yml\n        └── domain.yml\n```\n\nHere the contextual AI assistant imports the `ChitchatBot` project which in turn\nimports the `GreetBot` project. Project imports are defined in the configuration files of\neach project.\n\nTo instruct Rasa to use the `MultiProjectImporter` module, you need add it to the `importers` list in your root `config.yml`.\n\n```\nimporters:\n- name: MultiProjectImporter\n```\n\nThen, in the same file, specify which projects you want to import by adding them to the `imports` list.\n\n```\nimports:\n- projects/ChitchatBot\n```\n\nThe configuration file of the `ChitchatBot` needs to reference `GreetBot`:\n\n```\nimports:\n- ../GreetBot\n```\n\nSince the `GreetBot` project does not specify further project to import, it doesn't need a `config.yml`.\n\nRasa uses paths relative from the configuration file to import projects.\nThese can be anywhere on your filesystem where file access is permitted.\n\nDuring the training process Rasa will import all required training files, combine\nthem, and train a unified AI assistant. The training data is merged at\nruntime, so no additional training data files are created.\n\n:::caution Policies and NLU Pipelines\nRasa will use the policy and NLU pipeline configuration of the root project\ndirectory during training. **Policy and NLU configurations of imported projects\nwill be ignored.**\n\n:::\n\n:::caution watch out for merging\nEqual intents, entities, slots, responses, actions and forms will be merged,\ne.g. if two projects have training data for an intent `greet`,\ntheir training data will be combined.\n\n:::",
            "Writing a Custom Importer": "If you are writing a custom importer, this importer has to implement the interface of\n[`TrainingDataImporter`](reference/rasa/shared/importers/importer.md#trainingdataimporter-objects):\n\n```\nfrom typing import Optional, Text, Dict, List, Union\n\nimport rasa\nfrom rasa.shared.core.domain import Domain\nfrom rasa.shared.nlu.interpreter import RegexInterpreter\nfrom rasa.shared.core.training_data.structures import StoryGraph\nfrom rasa.shared.importers.importer import TrainingDataImporter\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\n\n\nclass MyImporter(TrainingDataImporter):\n    \"\"\"Example implementation of a custom importer component.\"\"\"\n\n    def __init__(\n        self,\n        config_file: Optional[Text] = None,\n        domain_path: Optional[Text] = None,\n        training_data_paths: Optional[Union[List[Text], Text]] = None,\n        **kwargs: Dict\n    ):\n        \"\"\"Constructor of your custom file importer.\n\n        Args:\n            config_file: Path to configuration file from command line arguments.\n            domain_path: Path to domain file from command line arguments.\n            training_data_paths: Path to training files from command line arguments.\n            **kwargs: Extra parameters passed through configuration in configuration file.\n        \"\"\"\n\n        pass\n\n    def get_domain(self) -> Domain:\n        path_to_domain_file = self._custom_get_domain_file()\n        return Domain.load(path_to_domain_file)\n\n    def _custom_get_domain_file(self) -> Text:\n        pass\n\n    def get_stories(\n        self,\n        interpreter: \"NaturalLanguageInterpreter\" = RegexInterpreter(),\n        exclusion_percentage: Optional[int] = None,\n    ) -> StoryGraph:\n        from rasa.shared.core.training_data.story_reader.yaml_story_reader import (\n            YAMLStoryReader,\n        )\n\n        path_to_stories = self._custom_get_story_file()\n        return YAMLStoryReader.read_from_file(path_to_stories, self.get_domain())\n\n    def _custom_get_story_file(self) -> Text:\n        pass\n\n    def get_config(self) -> Dict:\n        path_to_config = self._custom_get_config_file()\n        return rasa.utils.io.read_config_file(path_to_config)\n\n    def _custom_get_config_file(self) -> Text:\n        pass\n\n    def get_nlu_data(self, language: Optional[Text] = \"en\") -> TrainingData:\n        from rasa.shared.nlu.training_data import loading\n\n        path_to_nlu_file = self._custom_get_nlu_file()\n        return loading.load_data(path_to_nlu_file)\n\n    def _custom_get_nlu_file(self) -> Text:\n        pass\n```"
          },
          "metadata": {
            "id": "training-data-importers",
            "sidebar_label": "Importers",
            "title": "Training Data Importers",
            "description": "Change the way Rasa imports training data by replacing the default importer or writing your own importer.",
            "abstract": "Rasa has built-in logic to collect and load training data written in Rasa format, but you can also customize how your training data gets imported using custom training data importers."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 51]"
        },
        {
          "title": "Tuning Your NLU Model",
          "description": null,
          "content": {
            "How to Choose a Pipeline": {
              "Sensible Starting Pipelines": "If you're starting from scratch, it's often helpful to start with pretrained word embeddings.\nPre-trained word embeddings are helpful as they already encode some kind of linguistic knowledge.\nFor example, if you have a sentence like “I want to buy apples” in your training data, and Rasa is asked to predict\nthe intent for “get pears”, your model already knows that the words “apples” and “pears” are very similar.\nThis is especially useful if you don't have enough training data.\n\nIf you are getting started with a one of [spaCy's supported languages](https://spacy.io/usage/models#languages),\nwe recommend the following pipeline:\n\n```\n```\n\nIt uses the [SpacyFeaturizer](./components.mdx#spacyfeaturizer), which provides \npre-trained word embeddings (see [Language Models](./components.mdx#language-models)).\n\nIf you don't use any pre-trained word embeddings inside your pipeline, you are not bound to a specific language\nand can train your model to be more domain specific.\n\nIf there are no word embeddings for your language or you have very domain specific terminology,\nwe recommend using the following pipeline:\n\n```\n```\n\nThis pipeline uses the [CountVectorsFeaturizer](./components.mdx#countvectorsfeaturizer) to train\non only the training data you provide. This pipeline can handle any language in which words are\nseparated by spaces. If this is not the case for your language, check out [alternatives to the \nWhitespaceTokenizer](./components.mdx#tokenizers).\n\n:::note\nIf you want to use custom components in your pipeline, see [Custom NLU Components](./components.mdx).\n\n:::",
              "Component Lifecycle": "Each component processes an input and/or creates an output. The order of the components is determined by\nthe order they are listed in the `config.yml`; the output of a component can be used by any other component that\ncomes after it in the pipeline. Some components only produce information used by other components\nin the pipeline. Other components produce `output` attributes that are returned after\nthe processing has finished.\n\nFor example, for the sentence `\"I am looking for Chinese food\"`, the output is:\n\n```\n{\n    \"text\": \"I am looking for Chinese food\",\n    \"entities\": [\n        {\n            \"start\": 8,\n            \"end\": 15,\n            \"value\": \"chinese\",\n            \"entity\": \"cuisine\",\n            \"extractor\": \"DIETClassifier\",\n            \"confidence\": 0.864\n        }\n    ],\n    \"intent\": {\"confidence\": 0.6485910906220309, \"name\": \"restaurant_search\"},\n    \"intent_ranking\": [\n        {\"confidence\": 0.6485910906220309, \"name\": \"restaurant_search\"},\n        {\"confidence\": 0.1416153159565678, \"name\": \"affirm\"}\n    ]\n}\n```\n\nThis is created as a combination of the results of the different components in the following pipeline:\n\n```\npipeline:\n  - name: WhitespaceTokenizer\n  - name: RegexFeaturizer\n  - name: LexicalSyntacticFeaturizer\n  - name: CountVectorsFeaturizer\n  - name: CountVectorsFeaturizer\n    analyzer: \"char_wb\"\n    min_ngram: 1\n    max_ngram: 4\n  - name: DIETClassifier\n  - name: EntitySynonymMapper\n  - name: ResponseSelector\n```\n\nFor example, the `entities` attribute here is created by the `DIETClassifier` component.\n\nEvery component can implement several methods from the `Component` base class; in a pipeline these different methods\nwill be called in a specific order. Assuming we added the following pipeline to our `config.yml`:\n\n```\npipeline:\n  - name: \"Component A\"\n  - name: \"Component B\"\n  - name: \"Last Component\"\n```\n\nThe image below shows the call order during the training of this pipeline:\n\nimport componentLifecycleImg from './component-lifecycle-img.png';\n\n<Image img={componentLifecycleImg} caption=\"Component Lifecycle\" alt=\"The component lifecycle during training. Components are processed in the order they're listed in the configuration file. All components are created and initialized in order before they are trained in order and then persisted in order.\" />\n\nBefore the first component is created using the `create` function, a so\ncalled `context` is created (which is nothing more than a python dict).\nThis context is used to pass information between the components. For example,\none component can calculate feature vectors for the training data, store\nthat within the context and another component can retrieve these feature\nvectors from the context and do intent classification.\n\nInitially the context is filled with all configuration values. The arrows\nin the image show the call order and visualize the path of the passed\ncontext. After all components are trained and persisted, the\nfinal context dictionary is used to persist the model's metadata.",
              "Doing Multi-Intent Classification": {
                "When to Use Multi-Intents": "Let's say you have a financial services bot and you have examples for intents `check_balances` and `transfer_money`:\n\n```\nnlu:\n- intent: check_balances\n  examples: |\n    - How much money do I have?\n    - what's my account balance?\n\n- intent: transfer_money\n  examples: |\n    - I want to transfer money to my savings account\n    - transfer money\n```\n\nHowever, your bot receives incoming messages like this one, which combine both intents:\n\n<Chat caption=\"User wants to know balance in order to transfer money\">\n<ChatUserText>How much money do I have? I want to transfer some to savings.</ChatUserText>\n</Chat>\n\nIf you see enough of these examples, you can create a new intent multi-intent `check_balances+transfer_money` and add the incoming examples to it, for example:\n\n```\nnlu:\n- intent: check_balances+transfer_money\n  examples: |\n    - How much money do I have? I want to transfer some to savings.\n    - What's the balance on my account? I need to transfer some so I want to know how much I have\n```\n\n:::note\nThe model will not predict any combination of intents for which examples are not explicitly given in training data. As accounting for every possible intent combination would result in combinatorial explosion of the number of intents, you should only add those combinations of intents for which you see enough examples coming in from real users.\n\n:::",
                "How to Use Multi-Intents for Dialogue Management": "Multi-intent classification is intended to help with the downstream task of action prediction *after* a multi-intent. There are two complementary ways to use multi intents in dialogue training data:\n\n['Add regular stories or rules for the multi-intent. For example, given the following two rules for each individual intent: ']\n\n```\nrules:\n- rule: check account balance\n  steps:\n  - intent: check_balances\n  - action: action_check_balances\n- rule: transfer money\n  steps:\n  - intent: transfer_money\n  - action: action_transfer_money\n```\n\nYou could add another rule for the multi-intent that specifies a sequence of actions to address both intents:\n\n```\nrules:\n- rule: check balances and transfer money\n  steps:\n  - intent: check_balances+transfer_money\n  - action: action_check_balances\n  - action: action_transfer_money\n```\n\n['Allow a machine-learning policy to generalize to the multi-intent scenario from single-intent stories.']\n\nWhen using a multi-intent, the intent is featurized for machine learning policies using multi-hot encoding. That means the featurization of `check_balances+transfer_money` will overlap with the featurization of each individual intent. Machine learning policies (like [TEDPolicy](./policies.mdx#ted-policy)) can then make a prediction based on the multi-intent even if it does not explicitly appear in any stories. It will typically act as if only one of the individual intents was present, however, so it is always a good idea to write a specific story or rule that deals with the multi-intent case."
              },
              "Comparing Pipelines": "Rasa gives you the tools to compare the performance of multiple pipelines on your data directly.\nSee [Comparing NLU Pipelines](./testing-your-assistant.mdx#comparing-nlu-pipelines) for more information."
            },
            "Choosing the Right Components": {
              "Tokenization": "You can process whitespace-tokenized (i.e. words are separated by spaces) languages\nwith the [WhitespaceTokenizer](./components.mdx#whitespacetokenizer). If your language is not whitespace-tokenized, you should use a different tokenizer.\nWe support a number of different [tokenizers](./components.mdx), or you can\ncreate your own [custom tokenizer](./components.mdx).\n\n:::note\nSome components further down the pipeline may require a specific tokenizer. You can find those requirements\non the individual components' `requires` parameter. If a required component is missing inside the pipeline, an\nerror will be thrown.\n\n:::",
              "Featurization": {
                "Pre-trained Embeddings": "The advantage of using pre-trained word embeddings in your pipeline is that if you have a training example like:\n“I want to buy apples”, and Rasa is asked to predict the intent for “get pears”, your model already knows that the\nwords “apples” and “pears” are very similar. This is especially useful if you don't have enough training data.\nWe support a few components that provide pre-trained word embeddings:\n\n['[MitieFeaturizer](./components.mdx#mitiefeaturizer)', '[SpacyFeaturizer](./components.mdx#spacyfeaturizer)', '[ConveRTFeaturizer](./components.mdx#convertfeaturizer)', '[LanguageModelFeaturizer](./components.mdx#languagemodelfeaturizer)']\n\nIf your training data is in English, we recommend using the [ConveRTFeaturizer](./components.mdx#convertfeaturizer).\nThe advantage of the [ConveRTFeaturizer](./components.mdx#convertfeaturizer) is that it doesn't treat each word of the user message independently, but\ncreates a contextual vector representation for the complete sentence. For example, if you\nhave a training example, like: “Can I book a car?”, and Rasa is asked to predict the intent for “I need a ride from\nmy place”, since the contextual vector representation for both examples are already very similar, the intent classified\nfor both is highly likely to be the same. This is also useful if you don't have enough training data.\n\nAn alternative to [ConveRTFeaturizer](./components.mdx#convertfeaturizer) is the [LanguageModelFeaturizer](./components.mdx#languagemodelfeaturizer) which uses pre-trained language\nmodels such as BERT, GPT-2, etc. to extract similar contextual vector representations for the complete sentence. See\n[LanguageModelFeaturizer](./components.mdx#languagemodelfeaturizer) for a full list of supported language models.\n\nIf your training data is not in English you can also use a different variant of a language model which\nis pre-trained in the language specific to your training data.\nFor example, there are chinese (`bert-base-chinese`) and japanese (`bert-base-japanese`) variants of the BERT model.\nA full list of different variants of\nthese language models is available in the\n[official documentation of the Transformers library](https://huggingface.co/models?library=tf&sort=downloads).\n\n[spacynlp](./components.mdx#spacyfeaturizer) also provides word embeddings in many different languages,\nso you can use this as another alternative, depending on the language of your training data.",
                "Supervised Embeddings": "If you don't use any pre-trained word embeddings inside your pipeline, you are not bound to a specific language\nand can train your model to be more domain specific. For example, in general English, the word “balance” is closely\nrelated to “symmetry”, but very different to the word “cash”. In a banking domain, “balance” and “cash” are closely\nrelated and you'd like your model to capture that.\nYou should only use featurizers from the category [sparse featurizers](./components.mdx#featurizers), such as\n[CountVectorsFeaturizer](./components.mdx#countvectorsfeaturizer), [RegexFeaturizer](./components.mdx#regexfeaturizer) or [LexicalSyntacticFeaturizer](./components.mdx#lexicalsyntacticfeaturizer), if you don't want to use\npre-trained word embeddings."
              },
              "Intent Classification / Response Selectors": "Depending on your data you may want to only perform intent classification, entity recognition or response selection.\nOr you might want to combine multiple of those tasks. We support several components for each of the tasks.\nWe recommend using [DIETClassifier](./components.mdx#dietclassifier) for intent classification and entity recognition\nand [ResponseSelector](./components.mdx#responseselector) for response selection.\n\nBy default all of these components consume all available features produced in the pipeline.\nHowever, sometimes it makes sense to restrict the features that are used by a specific component.\nFor example, [ResponseSelector](./components.mdx#responseselector) is likely to perform better if no features from the\n[RegexFeaturizer](./components.mdx#regexfeaturizer) or [LexicalSyntacticFeaturizer](./components.mdx#lexicalsyntacticfeaturizer) are used.\nTo achieve that, you can do the following:\nSet an alias for every featurizer in your pipeline via the option `alias`.\nBy default the alias is set the the full featurizer class name, for example, `RegexFeaturizer`.\nYou can then specify, for example, on the [ResponseSelector](./components.mdx#responseselector) via the option `featurizers` what features from\nwhich featurizers should be used.\nIf you don't set the option `featurizers` all available features will be used.\n\nHere is an example configuration file where the `DIETClassifier` is using all available features and the\n`ResponseSelector` is just using the features from the `ConveRTFeaturizer` and the `CountVectorsFeaturizer`.\n\n```\n```",
              "Entity Extraction": "Entity extraction involves parsing user messages for required pieces of information. Rasa\nprovides entity extractors for custom entities as well as pre-trained ones like dates and locations.\nHere is a summary of the available extractors and what they are best used for:\n\n|       Component          |     Requires     |                      Model                      |              Notes               |\n|--------------------------|------------------|-------------------------------------------------|----------------------------------|\n|`DIETClassifier`          | N/A              |conditional random field on top of a transformer |good for training custom entities |\n|`CRFEntityExtractor`      |sklearn-crfsuite  |conditional random field                         |good for training custom entities |\n|`SpacyEntityExtractor`    |spaCy             |averaged perceptron                              |provides pre-trained entities     |\n|`DucklingEntityExtractor` |running duckling  |context-free grammar                             |provides pre-trained entities     |\n|`MitieEntityExtractor`    |MITIE             |structured SVM                                   |good for training custom entities |\n|`EntitySynonymMapper`     |existing entities |N/A                                              |maps known synonyms               |"
            },
            "Improving Performance": {
              "Handling Class Imbalance": "Classification algorithms often do not perform well if there is a large class imbalance,\nfor example if you have a lot of training data for some intents and very little training data for others.\nTo mitigate this problem, you can use a `balanced` batching strategy.\nThis algorithm ensures that all classes are represented in every batch, or at least in\nas many subsequent batches as possible, still mimicking the fact that some classes are more frequent than others.\nBalanced batching is used by default. In order to turn it off and use a classic batching strategy include\n`batch_strategy: sequence` in your config file.\n\n```\nlanguage: \"en\"\n\npipeline:\n# - ... other components\n- name: \"DIETClassifier\"\n  batch_strategy: sequence\n```",
              "Accessing Diagnostic Data": "To gain a better understanding of what your models do, you can access intermediate results of the prediction process.\nTo do this, you need to access the `diagnostic_data` field of the [Message](./reference/rasa/shared/nlu/training_data/message.md#message-objects)\nand [Prediction](./reference/rasa/core/policies/policy.md#policyprediction-objects) objects, which contain\ninformation about attention weights and other intermediate results of the inference computation.\nYou can use this information for debugging and fine-tuning, e.g. with [RasaLit](https://github.com/RasaHQ/rasalit).\n\nAfter you've [trained a model](.//command-line-interface.mdx#rasa-train), you can access diagnostic data for DIET, \ngiven a processed message, like this:\n\n```\nnlu_diagnostic_data = message.as_dict()[DIAGNOSTIC_DATA]\n\nfor component_name, diagnostic_data in nlu_diagnostic_data.items():\n    attention_weights = diagnostic_data[\"attention_weights\"]\n    print(f\"attention_weights for {component_name}:\")\n    print(attention_weights)\n\n    text_transformed = diagnostic_data[\"text_transformed\"]\n    print(f\"\\ntext_transformed for {component_name}:\")\n    print(text_transformed)\n```\n\nAnd you can access diagnostic data for TED like this:\n\n```\nprediction = policy.predict_action_probabilities(\n    GREET_RULE, domain, RegexInterpreter()\n)\nprint(f\"{prediction.diagnostic_data.get('attention_weights')}\")\n```"
            },
            "Configuring Tensorflow": {
              "Deterministic Operations": "If you are using GPUs and have one or more sparse featurizer(s) in\nyour pipeline, and/or use any of `TEDPolicy`, `UnexpecTEDIntentPolicy`, `DIETClassifier`,\nor `ResponseSelector`, training and testing will fail if you set the environment variable\n`TF_DETERMINISTIC_OPS=1`, because there are no deterministic GPU implementations of\nunderlying tensorflow ops `tf.sparse.sparse_dense_matmul`,\n`tf.nn.sparse_softmax_cross_entropy_with_logits`,\nand `tf.math.unsorted_segment` ops. For more information see [here](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md)\n\nFor the above reasons, the models are also not guaranteed to yield the exact same\nperformance when trained on GPU across multiple runs. This even applies to situations where training is run\nmultiple times on the same training data, with the same config\nand random seeds set appropriately, while evaluating on a standard held-out test set.\nInternal experiments have shown the following fluctuation in model performance when trained and evaluated on a held-out\ntest set on a variety of datasets (experiments were run 5 times on each dataset):\n\n```\n+-----------------------+------------+------------------+------------------+-------------------+\n| Task,                 | Number of  | Average standard | Minimum standard | Maximum standard  |\n| Metric (Range)        | datasets   | deviation        | deviation        | deviation         |\n+-----------------------+------------+------------------+------------------+-------------------+\n| Intent Classification | 11         | 0.0042           | 2.4e-5           | 0.0176            |\n| Macro F1 (0-1)        |            |                  |                  |                   |\n+-----------------------+------------+------------------+------------------+-------------------+\n| Entity Recognition    | 7          | 0.0019           | 0.0007           | 0.0044            |\n| Macro F1 (0-1)        |            |                  |                  |                   |\n+-----------------------+------------+------------------+------------------+-------------------+\n| Response Selection    | 2          | 0.0098           | 0.0003           | 0.0231            |\n| Macro F1 (0-1)        |            |                  |                  |                   |\n+-----------------------+------------+------------------+------------------+-------------------+\n| Action Selection      | 5          | 0.0025           | 0.0010           | 0.0053            |\n| Macro F1 (0-1)        |            |                  |                  |                   |\n+-----------------------+------------+------------------+------------------+-------------------+\n| Conversation Success  | 5          | 0.0077           | 0.0052           | 0.0103            |\n| Accuracy (0-1)        |            |                  |                  |                   |\n+-----------------------+------------+------------------+------------------+-------------------+\n```\n\nThe above experiments were run on an Nvidia Tesla P4 GPU. You can expect similar fluctuations in\nthe model performance when you evaluate on your dataset.\nAcross different pipeline configurations tested, the fluctuation is more pronounced\nwhen you use sparse featurizers in your pipeline. You can see which featurizers are sparse [here](./components.mdx#featurizers),\nby checking the \"Type\" of a featurizer.\n\nModel performance on the above tasks should still be reproducible across multiple runs when\ntrained on a CPU and none of these have changed:\n\n['Training data', 'Test data', 'Configuration pipeline', \"Random Seed in configuration's components.\"]",
              "Optimizing CPU Performance": {
                "Parallelizing One Operation": "Set `TF_INTRA_OP_PARALLELISM_THREADS` as an environment variable to specify the maximum number of threads that can be used\nto parallelize the execution of one operation. For example, operations like `tf.matmul()` and `tf.reduce_sum` can be executed\non multiple threads running in parallel. The default value for this variable is `0` which means TensorFlow would\nallocate one thread per CPU core.",
                "Parallelizing Multiple Operations": "Set `TF_INTER_OP_PARALLELISM_THREADS` as an environment variable to specify the maximum number of threads that can be used\nto parallelize the execution of multiple **non-blocking** operations. These would include operations that do not have a\ndirected path between them in the TensorFlow graph. In other words, the computation of one operation does not affect the\ncomputation of the other operation. The default value for this variable is `0` which means TensorFlow would allocate one thread per CPU core.\n\nTo understand more about how these two options differ from each other, refer to this\n[stackoverflow thread](https://stackoverflow.com/questions/41233635/meaning-of-inter-op-parallelism-threads-and-intra-op-parallelism-threads/41233901#41233901)."
              },
              "Optimizing GPU Performance": {
                "Limiting GPU Memory Growth": "TensorFlow by default blocks all the available GPU memory for the running process. This can be limiting if you are running\nmultiple TensorFlow processes and want to distribute memory across them. To prevent Rasa from blocking all\nof the available GPU memory, set the environment variable `TF_FORCE_GPU_ALLOW_GROWTH` to `True`.",
                "Restricting Absolute GPU Memory Available": "You may want to limit the absolute amount of GPU memory that can be used by a Rasa process.\n\nFor example, say you have two visible GPUs(`GPU:0` and `GPU:1`) and you want to allocate 1024 MB from the first GPU\nand 2048 MB from the second GPU. You can do this by setting the environment variable `TF_GPU_MEMORY_ALLOC` to `\"0:1024, 1:2048\"`."
              }
            }
          },
          "metadata": {
            "id": "tuning-your-model",
            "sidebar_label": "Tuning Your NLU Model",
            "title": "Tuning Your NLU Model",
            "abstract": "Rasa will provide you with a suggested NLU config on initialization of the project, but as your project grows, it's likely that you will need to adjust your config to suit your training data."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 52]"
        },
        {
          "title": "Handling Unexpected Input",
          "description": null,
          "content": {
            "User Interjections": {
              "Contextual Interjections": "Handling contextual interjections is similar to handling [contextual conversations](contextual-conversations.mdx)\nin general.\n\nOne common case of contextual interjections is during slot filling for [form](forms.mdx), where the user\nasks “Why do you need to know that?” or \"Can you explain that?\".\nThe response should differ for each slot. For example:\n\n<Chat caption=\"A contextual interjection\">\n<ChatUserText>Hi</ChatUserText>\n<ChatBotText>Hello! I am restaurant search assistant! How can I help?</ChatBotText>\n<ChatUserText>I'm looking for a restaurant</ChatUserText>\n<ChatBotText>What cuisine?</ChatBotText>\n<ChatUserText>French</ChatUserText>\n<ChatBotText>How many people?</ChatBotText>\n<ChatUserText>Why do you need to know that?</ChatUserText>\n<ChatBotText>I need to know how many people are in your party to ensure the restaurant can accomodate you.</ChatBotText>\n<ChatBotText>How many people?</ChatBotText>\n</Chat>\n\nSince we want the `requested_slot` to influence the conversation,\nwe need to set the property `influence_conversation` of the slot `requested_slot`\nto `true`, and assign it the categorical type:\n\n```\nslots:\n  requested_slot:\n    type: categorical\n    values:\n      - cuisine\n      - num_people\n      - outdoor_seating\n      - preferences\n      - feedback\n    influence_conversation: true\n    mappings:\n    - type: custom\n```\n\nThis means that the dialogue model will pay attention to the value of the slot when making a prediction\n(read more about how [slots influence the assistant's behaviour](./domain.mdx#slots-and-conversation-behavior)).\n\nYou can then write stories for specific responses to interjections based on the value of `requested_slot`, for example:\n\n```\nstories:\n- story: cuisine interjection\n  steps:\n  - intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant_form\n  - slot_was_set:\n    - requested_slot: cuisine\n  - intent: explain\n  - action: utter_explain_cuisine\n  - action: restaurant_form\n\n- story: number of people interjection\n  steps:\n  - intent: request_restaurant\n  - action: restaurant_form\n  - active_loop: restaurant_form\n  - slot_was_set:\n    - requested_slot: num_people\n  - intent: explain\n  - action: utter_explain_num_people\n  - action: restaurant_form\n```"
            },
            "Summary": "How you handle unexpected input depends on whether the response should be\ncontext sensitive or not.\n\nFor generic interjections:\n\n['[ ] Define rules for single-turn interactions', '[ ] Use the ResponseSelector for [FAQ and chitchat interruptions](chitchat-faqs.mdx)']\n\nFor contextual interjections:\n\n['[ ] Make `requested_slot` a categorical slot (for forms)', '[ ] Write stories for context-specific responses to interjections, using slot values where applicable']"
          },
          "metadata": {
            "id": "unexpected-input",
            "sidebar_label": "Handling Unexpected Input",
            "title": "Handling Unexpected Input",
            "abstract": "One thing you can count on when building a conversational assistant is that users will say unexpected things. This page is a guide on handling unexpected input."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 53]"
        },
        {
          "title": "Writing Conversation Data",
          "description": null,
          "content": {
            "Designing Stories": "When designing stories, there are two groups of conversational interactions that need\nto be accounted for: happy and unhappy paths. Happy paths describe when the user is\nfollowing the conversation flow as you'd expect and always providing the necessary\ninformation when prompted. However, users will often deviate from happy\npaths with questions, chit chat, or other asks. We call these unhappy path.\n\nIt's important for your bot to handle unhappy paths gracefully, but it's also impossible\nto predict what path a given user might take.\nOften, developers will try to account for every possible diverging path when designing\nunhappy paths. Planning for every possible state in a state machine (many of which will never be reached)\nrequires a lot of extra work and increases training time significantly.\n\nInstead, we recommend taking a [conversation-driven development](./conversation-driven-development.mdx)\napproach when designing unhappy paths.\nConversation-Driven Development promotes sharing your bot as early as possible with test users and\ncollecting real conversation data that tells you exactly how users diverge from the\nhappy paths. From this data, you can create stories to accomplish what the user is\nrequesting and start to think about ways to guide them back into a happy path.",
            "When to Write Stories vs. Rules": "[Rules](./rules.mdx) are a type of training data used by the dialogue manager for\nhandling pieces of conversations that should always follow the same path.\n\nRules can be useful when implementing:\n\n['[One-turn interactions](./chitchat-faqs.mdx): Some messages do not require any context to answer them.\\nRules are an easy way to map intents to responses, specifying fixed answers to these messages.', '[Fallback behavior](./fallback-handoff.mdx):\\nIn combination with the [FallbackClassifier](./components.mdx#fallbackclassifier),\\nyou can write rules to respond to low-confidence user messages with a certain fallback behavior.', '[Forms](./forms.mdx): Both activating and submitting a form will often follow a fixed path.\\nYou can also write rules to handle [unexpected input](./unexpected-input.mdx) during a form.']\n\nBecause rules do not generalize to unseen conversations, you should reserve them for\nsingle-turn conversation snippets, and use stories to train on multi-turn\nconversations.\n\nAn example of a rule where the bot returns a fixed response \"utter_greet\" to a user\nmessage with intent \"greet\" would be:\n\n```\nrules:\n- rule: Greeting Rule\n  steps:\n  - intent: greet\n  - action: utter_greet\n```\n\nFor multiple-turn interactions, you should define a story, for example:\n\n```\nstories:\n - story: Greeting and ask user how they're doing\n   steps:\n   - intent: greet\n   - action: utter_greet\n   - action: utter_ask_how_doing\n   - intent: doing_great\n   - action: utter_happy\n```",
            "Managing the Conversation Flow": {
              "When to Use Slots to Influence Conversations": "Slots act as your bot’s memory. When you define a slot, you can define whether a\n[slot](domain.mdx#slots) should influence the conversation or not.\nSlots with the property `influence_conversation` set to `false` can only store\ninformation. Slots with the property `influence_conversation` set to `true` can affect\nthe dialogue flow based on the information stored in it.\n\nSlots can be set after every user message based on [slot mappings](./domain.mdx#slot-mappings).\nThey can also be set by a [custom action](./actions.mdx) run in response to a user message.\nAll slots which influence the conversation need to be added to your stories or rules.\nFor example, you can use a boolean slot set by a custom action to control the dialogue\nflow based on its value using the following stories:\n\n```\nstories:\n- story: Welcome message, premium user\n  steps:\n   - intent: greet\n   - action: action_check_profile\n   - slot_was_set:\n     - premium_account: true\n   - action: utter_welcome_premium\n\n- story: Welcome message, basic user\n  steps:\n   - intent: greet\n   - action: action_check_profile\n   - slot_was_set:\n     - premium_account: false\n   - action: utter_welcome_basic\n   - action: utter_ask_upgrade\n```\n\nIn cases where you don't want a slot to affect the conversation flow, you should\nset the slot's property `influence_conversation` to `false`. You do not need to\ninclude `slot_was_set` events for slots in your stories which do not influence the\nconversation.",
              "Implementing Branching Logic": "When writing stories, sometimes the next action will depend on a value returned in one of your custom\nactions. In these cases, it's important to find the right balance between returning slots and\nusing custom action code directly to affect what your bot does next.\n\nIn cases where a value is used only to determine the bot's response, consider embedding the\ndecision logic inside a custom action as opposed to using a featurized slot in your\nstories. This can help reduce overall complexity and make your stories easier to manage.\n\nFor example, you can convert these stories:\n\n```\nstories:\n- story: It's raining now\n  steps:\n  - intent: check_for_rain\n  - action: action_check_for_rain\n  - slot_was_set:\n    - raining: true\n  - action: utter_is_raining\n  - action: utter_bring_umbrella\n\n- story: It isn't raining now\n  steps:\n  - intent: check_for_rain\n  - action: action_check_for_rain\n  - slot_was_set:\n    - raining: false\n  - action: utter_not_raining\n  - action: utter_no_umbrella_needed\n```\n\ninto a single story:\n\n```\nstories:\n- story: check for rain\n  steps:\n  - intent: check_for_rain\n  - action: action_check_for_rain\n```\n\nwith the custom action code:\n\n```\ndef run(self, dispatcher, tracker, domain):\n    is_raining = check_rain()\n    if is_raining:\n        dispatcher.utter_message(template=\"utter_is_raining\")\n        dispatcher.utter_message(template=\"utter_bring_umbrella\")\n    else:\n        dispatcher.utter_message(template=\"utter_not_raining\")\n        dispatcher.utter_message(template=\"utter_no_umbrella_needed\")\n    return []\n```\n\nIn cases where the value is used to influence the action flow going forward,\nreturn a featurized slot to determine the stories. For example, if you want to collect\ninformation about new users, but not returning ones, your stories might look like this:\n\n```\nstories:\n- story: greet new user\n  steps:\n  - intent: greet\n  - action: check_user_status\n  - slot_was_set:\n    - new_user: true\n  - action: utter_greet\n  - action: new_user_form\n  - active_loop: new_user_form\n  - active_loop: null\n\n- story: greet returning user\n  steps:\n  - intent: greet\n  - action: check_user_status\n  - slot_was_set:\n    - new_user: false\n  - action: utter_greet\n  - action: utter_how_can_help\n```",
              "Using OR statements and Checkpoints": {
                "OR statements": "In stories where different intents or slot events are handled by your bot in the same way,\nyou can use OR statements as an alternative to creating a new story.\n\nFor example, you can merge these two stories:\n\n```\nstories:\n- story: newsletter signup\n  steps:\n  - intent: signup_newsletter\n  - action: utter_ask_confirm_signup\n  - intent: affirm\n  - action: action_signup_newsletter\n\n- story: newsletter signup, confirm via thanks\n  steps:\n  - intent: signup_newsletter\n  - action: utter_ask_confirm_signup\n  - intent: thanks\n  - action: action_signup_newsletter\n```\n\ninto a single story with an OR statement:\n\n```\nstories:\n- story: newsletter signup with OR\n  steps:\n  - intent: signup_newsletter\n  - action: utter_ask_confirm_signup\n  - or:\n    - intent: affirm\n    - intent: thanks\n  - action: action_signup_newsletter\n```\n\nAt training time, this story will be split into the two original stories.\n\n:::caution consider restructuring data\nIf you notice that you are using OR statements frequently\nin your stories, consider restructuring your intents to reduce their granularity and\nmore broadly capture user messages.\n:::",
                "Checkpoints": "Checkpoints are useful for modularizing your stories into separate blocks that are\nrepeated often. For example, if you want your bot to ask for user feedback at the end of\neach conversation flow, you can use a checkpoint to avoid having to include the feedback\ninteraction at the end of each story:\n\n```\nstories:\n- story: beginning of conversation\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - intent: goodbye\n  - action: utter_goodbye\n  - checkpoint: ask_feedback\n\n- story: user provides feedback\n  steps:\n  - checkpoint: ask_feedback\n  - action: utter_ask_feedback\n  - intent: inform\n  - action: utter_thank_you\n  - action: utter_anything_else\n\n- story: user doesn't have feedback\n  steps:\n  - checkpoint: ask_feedback\n  - action: utter_ask_feedback\n  - intent: deny\n  - action: utter_no_problem\n  - action: utter_anything_else\n```\n\n:::warning do not overuse\nCheckpoints are meant to make it easier to re-use certain sections of conversation in lots\nof different stories. We highly discourage using checkpoints inside existing checkpoints,\nas this increases training time significantly and makes your stories difficult to understand.\n:::"
              },
              "Creating Logical Breaks in Stories": "When designing conversation flows, it is often tempting to create long story\nexamples that capture a complete conversational interaction from start to finish.\nIn many cases, this will increase the number of training stories required\nto account for branching paths. Instead, consider separating your\nlonger stories into smaller conversational blocks that handle sub-tasks.\n\nA happy path story for handling a lost credit card might look like:\n\n```\nstories:\n- story: Customer loses a credit card, reviews transactions, and gets a new card\n  steps:\n  - intent: card_lost\n  - action: check_transactions\n  - slot_was_set:\n    - reviewed_transactions: [\"starbucks\"]\n  - action: utter_ask_fraudulent_transactions\n  - intent: inform\n  - action: action_update_transactions\n  - intent: affirm\n  - action: utter_confirm_transaction_dispute\n  - action: utter_replace_card\n  - action: mailing_address_form\n  - active_loop: mailing_address\n  - active_loop: null\n  - action: utter_sent_replacement\n  - action: utter_anything_else\n  - intent: affirm\n  - action: utter_help\n```\n\nHandling a lost credit card involves a series of sub-tasks, namely\nchecking spending history for fraudulent transactions, confirming a mailing\naddress for a replacement card, and then following up with the user\nwith any additional requests. In this conversation arc, there are\nseveral places where the bot prompts for user input, creating\nbranching paths that need to be accounted for.\n\nFor example, when prompted with \"utter_ask_fraudulent_transactions\",\nthe user might respond with a \"deny\" intent if none are applicable.\nThe user might also choose to respond with a \"deny\" intent when asked\nif there's anything else the bot can help them with.\n\nWe can separate out this long story into several smaller stories as:\n\n```\nstories:\n- story: Customer loses a credit card\n  steps:\n  - intent: card_lost\n  - action: utter_card_locked\n  - action: spending_history_form\n  - active_loop: spending_history_form\n  - active_loop: null\n  - slot_was_set:\n    - reviewed_transactions: [\"starbucks\"]\n  - action: utter_ask_fraudulent_transactions\n\n- story: Customer reviews transactions and gets a new card\n  steps:\n  - action: utter_ask_fraudulent_transactions\n  - intent: inform\n  - action: action_update_transactions\n  - slot_was_set:\n    - reviewed_transactions: [\"target\", \"starbucks\"]\n  - intent: affirm\n  - action: utter_confirm_transaction_dispute\n  - action: utter_replace_card\n  - action: mailing_address_form\n  - active_loop: mailing_address\n  - active_loop: null\n  - action: utter_sent_replacement\n  - action: utter_anything_else\n\n- story: Customer has something else they need help with\n  steps:\n  - action: utter_anything_else\n  - intent: affirm\n  - action: utter_help\n```"
            },
            "Handling Context Switching": {
              "Using Rules for Context Switching": "In some cases, such as single-turn interjections, you can handle context-switching\nusing [rules](./rules.mdx) instead of through [stories](./stories.mdx).\n\nConsider this conversation scenario:\n\n<Chat caption=\"A single-turn interjection\">\n<ChatUserText>Hi, I want to pay my credit card bill</ChatUserText>\n<ChatBotText>Can do! Which of these credit accounts do you want to make a payment towards?</ChatBotText>\n<ChatBotText><p>- Credit Account A</p>\n\n<p>- Credit Account B</p></ChatBotText>\n<ChatUserText>Credit account B</ChatUserText>\n<ChatBotText>How much do you want to pay?</ChatBotText>\n<ChatUserText>What's my account balance?</ChatUserText>\n<ChatBotText>Your current account balance is $1532.00</ChatBotText>\n<ChatBotText>How much do you want to pay?</ChatBotText>\n<ChatUserText>I want to pay the minimum balance</ChatUserText>\n</Chat>\n\nIn this example, the user is in the middle of paying their credit card bill, asks for\ntheir account balance, and is then guided back into the credit card payment form.\nBecause asking for the account balance should always get the same response regardless of context,\nyou can create a rule that will automatically be triggered inside of an existing flow:\n\n```\nrules:\n- rule: Check my account balance\n  steps:\n  - intent: check_account_balance\n  - action: action_get_account_balance\n```\n\nBy default, the form will continue to stay active and re-prompt for the necessary\ninformation, without having to create an additional training story.",
              "Using Stories for Context Switching": "You'll need to write additional stories for handling context switching when the\nuser's interjection requires multiple conversation turns. If you have two distinct\nconversational flows and want the user to be able to switch between the flows, you will\nneed to create stories that specify how the switching will occur and how the context\nis maintained.\n\nFor example, if you want to switch context upon a user ask and then return to the\noriginal flow after that ask is complete:\n\n<Chat caption=\"A contextual interjection\">\n<ChatUserText>Hi, I want to pay my credit card bill</ChatUserText>\n<ChatBotText>Can do! Which of these credit accounts do you want to make a payment towards?</ChatBotText>\n<ChatBotText><p>- Credit Account A</p>\n\n<p>- Credit Account B</p></ChatBotText>\n<ChatUserText>Credit account B</ChatUserText>\n<ChatBotText>How much do you want to pay?</ChatBotText>\n<ChatUserText>Actually, I want to make a transfer</ChatUserText>\n<ChatBotText>No problem!</ChatBotText>\n<ChatBotText>Who would you like to transfer money to?</ChatBotText>\n<ChatUserText>I want to send $500 to Katy Parrow</ChatUserText>\n<ChatBotText>Successfully transferred $500 to Katy Parrow.</ChatBotText>\n<ChatBotText>Would you like to pay your credit card bill now?</ChatBotText>\n</Chat>\n\nYou will need to create a story that describes this context-switching interaction:\n\n```\nstories:\n- story: Context switch from credit card payment to money transfer\n  steps:\n  - intent: pay_credit_card\n  - action: credit_card_payment_form\n  - active_loop: credit_card_payment_form\n  - intent: transfer_money                         # - user requests a money transfer\n  - active_loop: null                              # - deactivate the credit card form\n  - action: transfer_money_form                    # - switch to the money transfer form\n  - active_loop: transfer_money_form\n  - active_loop: null\n  - action: utter_continue_credit_card_payment     # - once the money transfer is completed,\n                                                   #   ask the user to return to the\n                                                   #   credit card payment form\n```"
            },
            "Managing Conversation Data Files": "You can provide training data to Rasa as\na single file or as a directory containing multiple files.\nWhen writing stories and rules, it's usually a good idea to create separate\nfiles based on the types of conversations being represented.\n\nFor example, you might create a file `chitchat.yml` for handling chitchat,\nand a `faqs.yml` file for FAQs.\nRefer to our [rasa-demo bot](https://github.com/RasaHQ/rasa-demo)\nfor examples of story file management in complex assistants.",
            "Using Interactive Learning": {
              "Command-line Interactive Learning": "The CLI command `rasa interactive` will start interactive learning on the command line.\nIf your bot has custom actions, make sure to also\n[run your action server](./action-server/running-action-server.mdx) in a separate terminal window.\n\nIn interactive mode, you will be asked to confirm every intent and action prediction\nbefore the bot proceeds. Here's an example:\n\n```\n? Next user input:  hello\n\n? Is the NLU classification for 'hello' with intent 'hello' correct?  Yes\n\n------\nChat History\n\n #    Bot                        You\n────────────────────────────────────────────\n 1    action_listen\n────────────────────────────────────────────\n 2                                    hello\n                         intent: hello 1.00\n------\n\n? The bot wants to run 'utter_greet', correct?  (Y/n)\n\n```\n\nYou'll be able to see the conversation history and slot values at each step of the conversation.\n\nIf you type ||y|| to approve a prediction, the bot will continue. If you type ||n||, you will\nbe given the chance to correct the prediction before continuing: \n\n```\n? What is the next action of the bot?  (Use arrow keys)\n » <create new action>\n   1.00 utter_greet\n   0.00 ...\n   0.00 action_back\n   0.00 action_deactivate_loop\n   0.00 action_default_ask_affirmation\n   0.00 action_default_ask_rephrase\n   0.00 action_default_fallback\n   0.00 action_listen\n   0.00 action_restart\n   0.00 action_session_start\n   0.00 action_two_stage_fallback\n   0.00 utter_cheer_up\n   0.00 utter_did_that_help\n   0.00 utter_goodbye\n   0.00 utter_happy\n   0.00 utter_iamabot\n```\n\nAt any point, you can use ||Ctrl-C|| to access the menu, allowing you to create more stories and export the \ndata from the stories you've created so far. \n\n```\n? Do you want to stop?  (Use arrow keys)\n » Continue\n   Undo Last\n   Fork\n   Start Fresh\n   Export & Quit\n```"
            }
          },
          "metadata": {
            "id": "writing-stories",
            "sidebar_label": "Writing Conversation Data",
            "title": "Writing Conversation Data",
            "abstract": "Conversation data includes the stories and rules that make up the training data for your Rasa assistant's dialogue management model. Well-written conversation data allows your assistant to reliably follow conversation paths you've laid out and generalize to unexpected paths."
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 54]"
        },
        {
          "title": "Action Server",
          "description": "Documentation section: action-server",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/action-server"
          },
          "subpages": [
            {
              "title": "Actions",
              "description": null,
              "content": {
                "Custom Action Input": {
                  "`next_action`": "The `next_action` field tells your action server what action to run. \nYour actions don't have to be implemented as classes, but they do have\nto be callable by name. \n\nIn the example case, your action server should run the action `action_tell_weather`.",
                  "`sender_id`": "The `sender_id` tells you the unique ID of the \nuser having the conversation. Its format varies according to the input channel.\nWhat it tells you about the user also depends on the input channel and how\nthe user is identified by the channel. \n\nIn the example case, the `sender_id` is not used for anything.",
                  "`tracker`": "The `tracker` contains information about the conversation, including a history of events\nand a record of all slots:\n\n['`sender_id`: The same `sender_id` as is available in the top level of the payload', \"`slots`: Each slot in your bot's domain and its value at the current time\", '`latest_message`: The attributes of the latest message', '`latest_event_time`: The timestamp at which the last event was added to the tracker', '`followup_action`: The action called was a forced follow up action', '`paused`: Whether the conversation is currently paused', '`events`: A list of all previous [events](./events.mdx)', '`latest_input_channel`: The input channel from which the last user message was received', '`active_form`: The name of the currently active form, if any', '`latest_action_name`: The name of the last action the bot executed']\n\nIn the example case, your custom action uses the value of the `location` slot (if it is set)\nto get the weather forecast. It also checks the `latest_input_channel` property\nand formats the message payload so that it will display correctly in Facebook Messenger.",
                  "`domain`": "The `domain` is a json representation of your `domain.yaml` file.\nIt is unlikely that a custom action\nwill refer to its contents, as they are static and do not indicate the state\nof the conversation. \n\nYou can control if an action should receive a domain or not.\nVisit [selective-domain](../domain.mdx#select-which-actions-should-receive-domain)",
                  "`version`": "This is the version of the Rasa server. A custom action\nis also unlikely to refer to this, although you might use it in a \nverification step if your action server\nis only compatible with certain Rasa versions."
                },
                "Custom Action Output": {
                  "`events`": "[Events](./events.mdx) are how your action server can influence the conversation.\nIn the example case, your custom action should store the maximum temperature \nin the `temperature` slot, so it needs to return a [`slot` event](./events.mdx#slot). To set the\nslot and do nothing else, your response payload would look like this:\n\n```\n    {\n        \"events\": [\n            {\n                \"event\": \"slot\",\n                \"timestamp\": null,\n                \"name\": \"temperature\",\n                \"value\": \"30\"\n            }\n        ],\n        \"responses\": []\n    }\n```\n\nNote that events will be applied to the tracker in the order you list them; with `slot`\nevents, the order won't matter, but with other event types it can.",
                  "`responses`": "A response can be of any of the response types described in the\n[documentation on rich responses](../responses.mdx#rich-responses).\nSee the response sample of the [API spec](/pages/action-server-api) for the expected formats. \n\nIn the example case, you want to send the user a message with the weather forecast. \nTo send a regular text message, the response payload would look like this:\n\n```\n    {\n        \"events\": [\n            {\n                \"event\": \"slot\",\n                \"timestamp\": null,\n                \"name\": \"temperature\",\n                \"value\": \"30\"\n            }\n        ],\n        \"responses\": [\n            {\n                \"text\": \"This is your weather forecast!\"\n            }\n        ]\n    }\n```\n\nHowever, you want to make use of your channels' specific capabilities. Since \nthe `latest_input_channel` was Facebook, you add a response with\na custom payload that will be rendered as a media message according to Facebook's API spec.\nYour response payload then looks like this:\n\n```\n    {\n        \"events\": [\n            {\n                \"event\": \"slot\",\n                \"timestamp\": null,\n                \"name\": \"temperature\",\n                \"value\": \"30\"\n            }\n        ],\n        \"responses\": [\n            {\n                \"text\": \"This is your weather forecast!\"\n            },\n            {\n                \"attachment\": {\n                    \"type\": \"template\",\n                    \"payload\": {\n                        \"template_type\": \"media\",\n                        \"elements\": [\n                            {\n                                \"media_type\": \"weather_forcast.gif\",\n                                \"attachment_id\": \"<id from facebook upload endpoint>\"\n                            }\n                        ]\n                    }\n                }\n            }\n        ]\n    }\n```\n\nWhen this response is sent back to \nthe Rasa server, Rasa will apply the `slot` event and two responses to the tracker, \nand return both messages to the user."
                },
                "Special Action Types": "There are special action types that are automatically triggered under certain circumstances, namely [default actions](../default-actions.mdx)\nand  [slot validation actions](../slot-validation-actions.mdx).\nThese special action types have predefined naming conventions that must be followed to maintain the automatic triggering behavior.\n\nYou can customize a default action by implementing a custom action with exactly the same name.\nPlease see the [docs on default actions](../default-actions.mdx) for the expected behavior of each action.\n\nSlot validation actions are run on every user turn, depending on whether a form is active or not.\nA slot validation action that should run when a form is not active must be called `action_validate_slot_mappings`.\nA slot validation action that should run when a form is active must be called `validate_<form name>`.\nThese actions are expected to return `SlotSet` events only and to behave like the Rasa SDK [`ValidationAction` class](./validation-action.mdx#validationaction-class-implementation)\nand [`FormValidationAction` class](./validation-action.mdx#formvalidationaction-class-implementation) respectively."
              },
              "metadata": {
                "id": "actions",
                "sidebar_label": "Actions",
                "title": "Actions"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 0]"
            },
            {
              "title": "Events",
              "description": null,
              "content": {
                "Event Types": {
                  "`slot`": "Sets a slot on the tracker. It can set a slot to a value, \nor reset a slot by setting its value to `null`. \n\n**Automatic Tracking**:\n\n['When a slot is filled by an entity of the same name. ']\n\nA custom action is needed\nto set any slot not auto-filled by an entity. \n\n**JSON**:\n\n```\n{\n    \"event\": \"slot\",\n    \"name\": \"departure_airport\", \n    \"value\": \"BER\"\n}\n```\n\n**Parameters**:\n\n['`name`: Name of the slot to set', '`value`: Value to set the slot to. The datatype must match the [type](../domain.mdx#slot-types)\\nof the slot']\n\n**Rasa Class**: `rasa.core.events.SlotSet`",
                  "`reset_slots`": "Resets all slots on the tracker to `null`. \n\n**Automatic Tracking**: Never\n\n**JSON**:\n\n```\n{\n    \"event\": \"reset_slots\"\n}\n```\n\n**Rasa Class**: `rasa.core.events.AllSlotsReset`",
                  "`reminder`": "Schedules an intent to be triggered at a certain time in the future. \n\n**Automatic Tracking**: Never\n\n**JSON**:\n\n```\n{\n  \"event\": \"reminder\",\n  \"intent\": \"my_intent\",\n  \"entities\": {\"entity1\": \"value1\", \"entity2\": \"value2\"},\n  \"date_time\": \"2018-09-03T11:41:10.128172\",\n  \"name\": \"my_reminder\",\n  \"kill_on_user_msg\": true,\n}\n```\n\n**Parameters**:\n\n['`intent`: Intent which the reminder will trigger ', '`entities`: Entities to send with the intent', '`date_time`: Date at which the execution of the action should be triggered. This should either be in UTC or include a timezone.', '`name`: ID of the reminder. If there are multiple reminders with the same id only the last will be run.', '`kill_on_user_msg`: Whether a user message before the trigger time will abort the reminder']\n\n**Rasa Class**: `rasa.core.events.ReminderScheduled`",
                  "`cancel_reminder`": "Cancels a scheduled reminder or reminders. \nAll reminders which match the supplied parameters will be cancelled.\n\n**Automatic Tracking**: Never\n\n**JSON**:\n\n```\n{\n  \"event\": \"cancel_reminder\",\n  \"name\": \"my_reminder\",\n  \"intent\": \"my_intent\",\n  \"entities\": [\n        {\"entity\": \"entity1\", \"value\": \"value1\"},\n        {\"entity\": \"entity2\", \"value\": \"value2\"},\n    ],\n  \"date_time\": \"2018-09-03T11:41:10.128172\",\n}\n```\n\n**Parameters**:\n\n['`intent`: Intent which the reminder will  trigger ', '`entities`: Entities to send with the intent', '`date_time`: Date at which the execution of the action should be triggered. This should either be in UTC or include a timezone.', '`name`: ID of the reminder.']\n\n**Rasa Class**: `rasa.core.events.ReminderCancelled`",
                  "`pause`": "Stops the bot from responding to user messages. The conversation will remain paused and no actions will be predicted until  the conversation is explicitly [resumed](#resume).\n\n**Automatic Tracking**: Never\n\n**JSON**:\n\n```\n{\n    \"event\": \"pause\"\n}\n```\n\n**Rasa Class**: `rasa.core.events.ConversationPaused`",
                  "`resume`": "Resume a previously paused conversation. Once this event is added to the tracker the bot will start predicting actions again. It will not predict actions for user messages received while the conversation was paused.\n\n**Automatic Tracking**: Never\n\n**JSON**:\n\n```\n{\n    \"event\": \"resume\"\n}\n```\n\n**Rasa Class**: `rasa.core.events.ConversationResumed`",
                  "`followup`": "Force a follow up action, bypassing action prediction. \n\n**Automatic Tracking**: Never\n\n**JSON**:\n\n```\n{\n    \"event\": \"followup\",\n    \"name\": \"my_action\"\n}\n```\n\n**Parameters**:\n\n['`name`: The name of the follow up action that will be executed.']\n\n**Rasa Class**: `rasa.core.events.FollowupAction`",
                  "`rewind`": "Reverts all side effects of the last user message and removes the last `user` event from the tracker. \n\n**Automatic Tracking**: \n\n**JSON**:\n\n```\n{\n    \"event\": \"rewind\"\n}\n```\n\n**Rasa Class**: `rasa.core.events.UserUtteranceReverted`",
                  "`undo`": "Undoes all side effects of the last bot action and removes the last bot action from the tracker. \n\n**Automatic Tracking**: \n\n**JSON**:\n\n```\n{\n    \"event\": \"undo\"\n}\n```\n\n**Rasa Class**: `rasa.core.events.ActionReverted`",
                  "`restart`": "Resets the tracker. After a `restart` event, there will be no conversation history and no record of the restart.\n\n**Automatic Tracking**: \n\n['When the `/restart` default intent is triggered.']\n\n**JSON**:\n\n```\n{\n    \"event\": \"restart\"\n}\n```\n\n**Rasa Class**: `rasa.core.events.Restarted`",
                  "`session_started`": "Starts a new conversation by resetting the tracker and running the default action `ActionSessionStart`. This action will by default carry over existing `SlotSet` events to a new conversation session. You can configure this behaviour in your domain file under `session_config`.\n\n**Automatic Tracking**: \n\n['Whenever a user starts a conversation with the bot for the first time.', 'Whenever a session expires (after `session_expiration_time` specified in the domain), and the user resumes their conversation']\n\nRestarting a conversation with [`restart`](#restart) event **does not** automatically cause a `session_started` event. \n\n**JSON**:\n\n```\n{\n    \"event\": \"session_started\"\n}\n```\n\n**Rasa Class**: `rasa.core.events.SessionStarted`",
                  "`user`": "The user sent a message to the bot. \n\n**Automatic Tracking**: \n\n['When the user sends a message to the bot.']\n\nThis event is not usually returned by a custom action.\n\n**JSON**:\n\n```\n{\n    \"event\": \"user\",\n    \"text\": \"Hey\",\n    \"parse_data\": {\n        \"intent\": {\n            \"name\": \"greet\",\n            \"confidence\": 0.9\n        },\n        \"entities\": []\n    },\n    \"metadata\": {},\n}\n```\n\n**Parameters**:\n\n['`text`: Text of the user message', '`parse_data`: Parsed data of user message. This is ordinarily filled by NLU.', '`metadata`: Arbitrary metadata that comes with the user message']\n\n**Rasa Class**: `rasa.core.events.UserUttered`",
                  "`bot`": "The bot sent a message to the user.\n\n**Automatic Tracking**: \n\n['Whenever `responses` are returned by a custom action', 'Whenever responses are sent to the user directly without being returned by a custom action (e.g. `utter_` actions)']\n\nThis event is not usually returned explicitly by a custom action; `responses` would be returned instead.\n\n**JSON**:\n\n```\n{\n    \"event\": \"bot\",\n    \"text\": \"Hey there!\",\n    \"data\": {}\n}\n```\n\n**Parameters**:\n\n['`text`: The text the bot sends to the user', '`data`: Any non-text elements of the bot response. The structure of `data` matches that of `responses` given in the [API spec](/pages/action-server-api).']\n\n**Rasa Class**: `rasa.core.events.BotUttered`",
                  "`action`": "Logs an action called by the bot. Only the action itself is logged; the events that the action creates are logged separately when they are applied. \n\n**Automatic Tracking**: \n\n['Any action (including custom actions and responses) that is called, even if the action does not execute successfully.']\n\nThis event is not usually returned explicitly by a custom action. \n\n**JSON**:\n\n```\n{\n    \"event\": \"action\",\n    \"name\": \"my_action\"\n}\n```\n\n**Parameters**:\n\n['`name`: Name of the action that was called']\n\n**Rasa Class**: `rasa.core.events.ActionExecuted`"
                }
              },
              "metadata": {
                "id": "events",
                "sidebar_label": "Events",
                "title": "Events"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 1]"
            },
            {
              "title": "Introduction to Rasa Action Server",
              "description": null,
              "content": {
                "How it works": "When your assistant predicts a custom action, the Rasa server sends\na `POST` request to the action server with a json payload including\nthe name of the predicted action,\nthe conversation ID, the contents of the tracker and the contents of the domain.\n\nWhen the action server finishes running a custom action, it returns a json payload\nof [responses](../responses.mdx) and [events](./events.mdx).\nSee the [API spec](/pages/action-server-api) for details about the request and response payloads.\n\nThe Rasa server then returns the responses to the user and adds the events\nto the conversation tracker.",
                "SDKs for Custom Actions": {
                  "Rasa SDK (Python)": "Rasa SDK is a Python SDK for running custom actions. Besides implementing\nthe required APIs, it offers methods for interacting with the conversation tracker\nand composing events and responses.\nIf you don't yet have an action server and don't need it\nto be in a language other than Python, using the Rasa SDK will be the easiest way\nto get started.",
                  "Other Action Servers": "If you have legacy code or existing business logic in another language,\nyou may not want to use the Rasa SDK. In this case you can write\nyour own action server in any language you want. The only requirement\nfor the action server is that it provide a `/webhook` endpoint which accepts HTTP `POST` requests\nfrom the Rasa server and returns a payload of [events](./events.mdx) and responses.\nSee the [API spec](/pages/action-server-api) for details about the required `/webhook` endpoint."
                }
              },
              "metadata": {
                "slug": "/action-server",
                "sidebar_label": "Introduction",
                "title": "Introduction to Rasa Action Server"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 2]"
            },
            {
              "title": "Knowledge Base Actions",
              "description": null,
              "content": {
                "Using `ActionQueryKnowledgeBase`": {
                  "Create a Knowledge Base": "The data used to answer the user's requests will be stored in a knowledge base.\nA knowledge base can be used to store complex data structures.\nWe suggest you get started by using the `InMemoryKnowledgeBase`.\nOnce you want to start working with a large amount of data, you can switch to a custom knowledge base\n(see [Creating Your Own Knowledge Base](./knowledge-base-actions.mdx#create-a-knowledge-base)).\n\nTo initialize an `InMemoryKnowledgeBase`, you need to provide the data in a json file.\nThe following example contains data about restaurants and hotels.\nThe json structure should contain a key for every object type, i.e. `\"restaurant\"` and `\"hotel\"`.\nEvery object type maps to a list of objects – here we have a list of 3 restaurants and a list of 3 hotels.\n\n```\n{\n    \"restaurant\": [\n        {\n            \"id\": 0,\n            \"name\": \"Donath\",\n            \"cuisine\": \"Italian\",\n            \"outside-seating\": true,\n            \"price-range\": \"mid-range\"\n        },\n        {\n            \"id\": 1,\n            \"name\": \"Berlin Burrito Company\",\n            \"cuisine\": \"Mexican\",\n            \"outside-seating\": false,\n            \"price-range\": \"cheap\"\n        },\n        {\n            \"id\": 2,\n            \"name\": \"I due forni\",\n            \"cuisine\": \"Italian\",\n            \"outside-seating\": true,\n            \"price-range\": \"mid-range\"\n        }\n    ],\n    \"hotel\": [\n        {\n            \"id\": 0,\n            \"name\": \"Hilton\",\n            \"price-range\": \"expensive\",\n            \"breakfast-included\": true,\n            \"city\": \"Berlin\",\n            \"free-wifi\": true,\n            \"star-rating\": 5,\n            \"swimming-pool\": true\n        },\n        {\n            \"id\": 1,\n            \"name\": \"Hilton\",\n            \"price-range\": \"expensive\",\n            \"breakfast-included\": true,\n            \"city\": \"Frankfurt am Main\",\n            \"free-wifi\": true,\n            \"star-rating\": 4,\n            \"swimming-pool\": false\n        },\n        {\n            \"id\": 2,\n            \"name\": \"B&B\",\n            \"price-range\": \"mid-range\",\n            \"breakfast-included\": false,\n            \"city\": \"Berlin\",\n            \"free-wifi\": false,\n            \"star-rating\": 1,\n            \"swimming-pool\": false\n        },\n    ]\n}\n```\n\nOnce the data is defined in a json file, called, for example, `data.json`, you will be able use the this data file to create your\n`InMemoryKnowledgeBase`, which will be passed to the action that queries the knowledge base.\n\nEvery object in your knowledge base should have at least the `\"name\"` and `\"id\"` fields to use the default implementation.\nIf it doesn't, you'll have to [customize your InMemoryKnowledgeBase](./knowledge-base-actions.mdx#customizing-the-inmemoryknowledgebase).",
                  "Define the NLU Data": "In this section:\n\n['we will introduce a new intent, `query_knowledge_base`', 'we will annotate `mention` entities so that our model detects indirect mentions of objects like “the\\nfirst one”', 'we will use [synonyms](../training-data-format.mdx#synonyms) extensively']\n\nFor the bot to understand that the user wants to retrieve information from the knowledge base, you need to define\na new intent. We will call it `query_knowledge_base`.\n\nWe can split requests that `ActionQueryKnowledgeBase` can handle into two categories:\n(1) the user wants to obtain a list of objects of a specific type, or (2) the user wants to know about a certain\nattribute of an object. The intent should contain lots of variations of both of these requests:\n\n```\nnlu:\n- intent: query_knowledge_base\n  examples: |\n    - what [restaurants]{\"entity\": \"object_type\", \"value\": \"restaurant\"} can you recommend?\n    - list some [restaurants]{\"entity\": \"object_type\", \"value\": \"restaurant\"}\n    - can you name some [restaurants]{\"entity\": \"object_type\", \"value\": \"restaurant\"} please?\n    - can you show me some [restaurants]{\"entity\": \"object_type\", \"value\": \"restaurant\"} options\n    - list [German](cuisine) [restaurants]{\"entity\": \"object_type\", \"value\": \"restaurant\"}\n    - do you have any [mexican](cuisine) [restaurants]{\"entity\": \"object_type\", \"value\": \"restaurant\"}?\n    - do you know the [price range]{\"entity\": \"attribute\", \"value\": \"price-range\"} of [that one](mention)?\n    - what [cuisine](attribute) is [it](mention)?\n    - do you know what [cuisine](attribute) the [last one]{\"entity\": \"mention\", \"value\": \"LAST\"} has?\n    - does the [first one]{\"entity\": \"mention\", \"value\": \"1\"} have [outside seating]{\"entity\": \"attribute\", \"value\": \"outside-seating\"}?\n    - what is the [price range]{\"entity\": \"attribute\", \"value\": \"price-range\"} of [Berlin Burrito Company](restaurant)?\n    - what about [I due forni](restaurant)?\n    - can you tell me the [price range](attribute) of [that restaurant](mention)?\n    - what [cuisine](attribute) do [they](mention) have?\n```\n\nThe above example just shows examples related to the restaurant domain.\nYou should add examples for every object type that exists in your knowledge base to the same `query_knowledge_base` intent.\n\nIn addition to adding a variety of training examples for each query type,\nyou need to specify and annotate the following entities in your training examples:\n\n['`object_type`: Whenever a training example references a specific object type from your knowledge base, the object type should\\nbe marked as an entity. Use [synonyms](../training-data-format.mdx#synonyms) to map e.g. `restaurants` to `restaurant`, the correct\\nobject type listed as a key in the knowledge base.', '`mention`: If the user refers to an object via “the first one”, “that one”, or “it”, you should mark those terms\\nas `mention`. We also use synonyms to map some of the mentions to symbols. You can learn about that\\nin [resolving mentions](./knowledge-base-actions.mdx#resolve-mentions).', '`attribute`: All attribute names defined in your knowledge base should be identified as `attribute` in the\\nNLU data. Again, use synonyms to map variations of an attribute name to the one used in the\\nknowledge base.']\n\nRemember to add those entities to your domain file (as entities and slots):\n\n```\nentities:\n  - object_type\n  - mention\n  - attribute\n\nslots:\n  object_type:\n    type: any\n    influence_conversation: false\n    mappings:\n    - type: from_entity\n      entity: object_type\n  mention:\n    type: any\n    influence_conversation: false\n    mappings:\n    - type: from_entity\n      entity: mention\n  attribute:\n    type: any\n    influence_conversation: false\n    mappings:\n    - type: from_entity\n      entity: attribute\n```\n\n<a aria-hidden=\"true\" tabIndex=\"-1\" className=\"anchor enhancedAnchor\" id=\"create-action-query-knowledge-base\"></a>",
                  "Create an Action to Query your Knowledge Base": "To create your own knowledge base action, you need to inherit `ActionQueryKnowledgeBase` and pass the knowledge\nbase to the constructor of `ActionQueryKnowledgeBase`.\n\n```\nfrom rasa_sdk.knowledge_base.storage import InMemoryKnowledgeBase\nfrom rasa_sdk.knowledge_base.actions import ActionQueryKnowledgeBase\n\nclass MyKnowledgeBaseAction(ActionQueryKnowledgeBase):\n    def __init__(self):\n        knowledge_base = InMemoryKnowledgeBase(\"data.json\")\n        super().__init__(knowledge_base)\n```\n\nWhenever you create an `ActionQueryKnowledgeBase`, you need to pass a `KnowledgeBase` to the constructor.\nIt can be either an `InMemoryKnowledgeBase` or your own implementation of a `KnowledgeBase`\n(see [Creating Your Own Knowledge Base](./knowledge-base-actions.mdx#create-a-knowledge-base)).\nYou can only pull information from one knowledge base, as the usage of multiple knowledge bases at the same time is not supported.\n\nThis is the entirety of the code for this action! The name of the action is `action_query_knowledge_base`.\nDon't forget to add it to your domain file:\n\n```\nactions:\n- action_query_knowledge_base\n```\n\n:::note\nIf you overwrite the default action name `action_query_knowledge_base`, you need to add the following three\nunfeaturized slots to your domain file: `knowledge_base_objects`, `knowledge_base_last_object`, and\n`knowledge_base_last_object_type`.\nThe slots are used internally by `ActionQueryKnowledgeBase`.\nIf you keep the default action name, those slots will be automatically added for you.\n\n:::\n\nYou also need to make sure to add a story to your stories file that includes the intent `query_knowledge_base` and\nthe action `action_query_knowledge_base`. For example:\n\n```\nstories:\n- story: knowledge base happy path\n  steps:\n  - intent: greet\n  - action: utter_greet\n  - intent: query_knowledge_base\n  - action: action_query_knowledge_base\n  - intent: goodbye\n  - action: utter_goodbye\n```\n\nThe last thing you need to do is to define the response `utter_ask_rephrase` in your domain file.\nIf the action doesn't know how to handle the user's request, it will use this response to ask the user to rephrase.\nFor example, add the following responses to your domain file:\n\n```\nresponses:\n  utter_ask_rephrase:\n  - text: \"Sorry, I'm not sure I understand. Could you rephrase it?\"\n  - text: \"Could you please rephrase your message? I didn't quite get that.\"\n```\n\nAfter adding all the relevant pieces, the action is now able to query the knowledge base."
                },
                "How It Works": {
                  "Query the Knowledge Base for Objects": "In order to query the knowledge base for any kind of object, the user's request needs to include the object type.\nLet's look at an example:\n\nCan you please name some restaurants?\n\nThis question includes the object type of interest: “restaurant.”\nThe bot needs to pick up on this entity in order to formulate a query – otherwise the action would not know what objects the user is interested in.\n\nWhen the user says something like:\n\nWhat Italian restaurant options in Berlin do I have?\n\nThe user wants to obtain a list of restaurants that (1) have Italian cuisine and (2) are located in\nBerlin. If the NER detects those attributes in the request of the user, the action will use those to filter the\nrestaurants found in the knowledge base.\n\nIn order for the bot to detect these attributes, you need to mark “Italian” and “Berlin” as entities in the NLU data:\n\n```\nintents:\n- intent: query_knowledge_base\n  examples: |\n    - What [Italian](cuisine) [restaurant](object_type) options in [Berlin](city) do I have?.\n```\n\nThe names of the attributes, “cuisine” and “city,” should be equal to the ones used in the knowledge base.\nYou also need to add those as entities and slots to the domain file.",
                  "Query the Knowledge Base for an Attribute of an Object": "If the user wants to obtain specific information about an object, the request should include both the object and\nattribute of interest.\n\n:::info New in 3.6\nThe user is not required to query the knowledge base to list any kind of object prior to this.\nThe `ActionQueryKnowledgeBase` will extract the object type from the user's request and query the knowledge base for an attribute of the object. \n:::\n\nFor example, if the user asks something like:\n\nWhat is the cuisine of Berlin Burrito Company?\n\nThe user wants to obtain the “cuisine” (attribute of interest) for the restaurant “Berlin Burrito Company” (object of\ninterest).\n\nThe attribute and object of interest should be marked as entities in the NLU training data:\n\n```\nintents:\n- intent: query_knowledge_base\n  examples: |\n    - What is the [cuisine](attribute) of [Berlin Burrito Company](restaurant)?\n```\n\nMake sure to add the object type, “restaurant,” to the domain file as entity and slot. This will support `ActionQueryKnowledgeBase`\nto extract the object type of the object the user is interested in.\n\n<a aria-hidden=\"true\" tabIndex=\"-1\" className=\"anchor enhancedAnchor\" id=\"resolve-mentions\"></a>",
                  "Resolve Mentions": "Following along from the above example, users may not always refer to restaurants by their names.\nUsers can either refer to the object of interest by its name, e.g. “Berlin Burrito Company” (representation string\nof the object), or they may refer to a previously listed object via a mention, for example:\n\nWhat is the cuisine of the second restaurant you mentioned?\n\nOur action is able to resolve these mentions to the actual object in the knowledge base.\nMore specifically, it can resolve two mention types: (1) ordinal mentions, such as “the first one”, and (2)\nmentions such as “it” or “that one”.\n\n**Ordinal Mentions**\n\nWhen a user refers to an object by its position in a list, it is called an ordinal mention. Here's an example:\n\n['User: What restaurants in Berlin do you know?', \"Bot: Found the following objects of type 'restaurant':  1: I due forni  2: PastaBar  3: Berlin Burrito Company\", 'User: Does the first one have outside seating?']\n\nThe user referred to “I due forni” by the term “the first one”.\nOther ordinal mentions might include “the second one,” “the last one,” “any,” or “3”.\n\nOrdinal mentions are typically used when a list of objects was presented to the user.\nTo resolve those mentions to the actual object, we use an ordinal mention mapping which is set in the\n`KnowledgeBase` class.\nThe default mapping looks like:\n\n```\n{\n    \"1\": lambda l: l[0],\n    \"2\": lambda l: l[1],\n    \"3\": lambda l: l[2],\n    \"4\": lambda l: l[3],\n    \"5\": lambda l: l[4],\n    \"6\": lambda l: l[5],\n    \"7\": lambda l: l[6],\n    \"8\": lambda l: l[7],\n    \"9\": lambda l: l[8],\n    \"10\": lambda l: l[9],\n    \"ANY\": lambda l: random.choice(l),\n    \"LAST\": lambda l: l[-1],\n}\n```\n\nThe ordinal mention mapping maps a string, such as “1”, to the object in a list, e.g. `lambda l: l[0]`, meaning the\nobject at index `0`.\n\nAs the ordinal mention mapping does not, for example, include an entry for “the first one”,\nit is important that you use [Entity Synonyms](../training-data-format.mdx#synonyms) to map “the first one” in your NLU data to “1”:\n\n```\nintents:\n- intent: query_knowledge_base\n  examples: |\n    - Does the [first one]{entity: \"mention\", value\": 1} have [outside seating]{entity: \"attribute\", value\": \"outside-seating\"}\n```\n\nThe NER detects “first one” as a `mention` entity, but puts “1” into the `mention` slot.\nThus, our action can take the `mention` slot together with the ordinal mention mapping to resolve “first one” to\nthe actual object “I due forni”.\n\nYou can overwrite the ordinal mention mapping by calling the function `set_ordinal_mention_mapping()` on your\n`KnowledgeBase` implementation (see [Customizing the InMemoryKnowledgeBase](./knowledge-base-actions.mdx#customizing-the-inmemoryknowledgebase)).\n\n**Other Mentions**\n\nTake a look at the following conversation:\n\n['User: What is the cuisine of PastaBar?', 'Bot: PastaBar has an Italian cuisine.', 'User: Does it have wifi?', 'Bot: Yes.', 'User: Can you give me an address?']\n\nIn the question “Does it have wifi?”, the user refers to “PastaBar” by the word “it”.\nIf the NER detected “it” as the entity `mention`, the knowledge base action would resolve it to the last mentioned\nobject in the conversation, “PastaBar”.\n\nIn the next input, the user refers indirectly to the object “PastaBar” instead of mentioning it explicitly.\nThe knowledge base action would detect that the user wants to obtain the value of a specific attribute, in this case, the address.\nIf no mention or object was detected by the NER, the action assumes the user is referring to the most recently\nmentioned object, “PastaBar”.\n\nYou can disable this behavior by setting `use_last_object_mention` to `False` when initializing the action."
                },
                "Customization": {
                  "Customizing `ActionQueryKnowledgeBase`": "You can overwrite the following two functions of `ActionQueryKnowledgeBase` if you'd like to customize what the bot\nsays to the user:\n\n['`utter_objects()`', '`utter_attribute_value()`']\n\n`utter_objects()` is used when the user has requested a list of objects.\nOnce the bot has retrieved the objects from the knowledge base, it will respond to the user by default with a message, formatted like:\n\nFound the following objects of type 'restaurant':\n1: I due forni\n2: PastaBar\n3: Berlin Burrito Company\n\nOr, if no objects are found,\n\nI could not find any objects of type 'restaurant'.\n\nIf you want to change the utterance format, you can overwrite the method `utter_objects()` in your action.\n\nThe function `utter_attribute_value()` determines what the bot utters when the user is asking for specific information about\nan object.\n\nIf the attribute of interest was found in the knowledge base, the bot will respond with the following utterance:\n\n'Berlin Burrito Company' has the value 'Mexican' for attribute 'cuisine'.\n\nIf no value for the requested attribute was found, the bot will respond with\n\nDid not find a valid value for attribute 'cuisine' for object 'Berlin Burrito Company'.\n\nIf you want to change the bot utterance, you can overwrite the method `utter_attribute_value()`.\n\n:::note\nThere is a [tutorial](https://blog.rasa.com/integrating-rasa-with-knowledge-bases/) on our blog about\nhow to use knowledge bases in custom actions. The tutorial explains the implementation behind\n`ActionQueryKnowledgeBase` in detail.\n\n:::",
                  "Creating Your Own Knowledge Base Actions": "`ActionQueryKnowledgeBase` should allow you to easily get started with integrating knowledge bases into your actions.\nHowever, the action can only handle two kind of user requests:\n\n['the user wants to get a list of objects from the knowledge base', 'the user wants to get the value of an attribute for a specific object']\n\nThe action is not able to compare objects or consider relations between objects in your knowledge base.\nFurthermore, resolving any mention to the last mentioned object in the conversation might not always be optimal.\n\nIf you want to tackle more complex use cases, you can write your own custom action.\nWe added some helper functions to `rasa_sdk.knowledge_base.utils`\n([link to code](https://github.com/RasaHQ/rasa-sdk/tree/main/rasa_sdk/knowledge_base/) )\nto help you when implement your own solution.\nWe recommend using `KnowledgeBase` interface so that you can still use the `ActionQueryKnowledgeBase`\nalongside your new custom action.\n\nIf you write a knowledge base action that tackles one of the above use cases or a new one, be sure to tell us about\nit on the [forum](https://forum.rasa.com)!\n\n<a aria-hidden=\"true\" tabIndex=\"-1\" className=\"anchor enhancedAnchor\" id=\"customizing-the-inmemoryknowledgebase\"></a>",
                  "Customizing the `InMemoryKnowledgeBase`": "The class `InMemoryKnowledgeBase` inherits `KnowledgeBase`.\nYou can customize your `InMemoryKnowledgeBase` by overwriting the following functions:\n\n['`get_key_attribute_of_object()`: To keep track of what object the user was talking about last, we store the value\\nof the key attribute in a specific slot. Every object should have a key attribute that is unique,\\nsimilar to the primary key in a relational database. By default, the name of the key attribute for every object type\\nis set to `id`. You can overwrite the name of the key attribute for a specific object type by calling\\n`set_key_attribute_of_object()`.', \"`get_representation_function_of_object()`: Let's focus on the following restaurant:\", '```\\n{\\n    \"id\": 0,\\n    \"name\": \"Donath\",\\n    \"cuisine\": \"Italian\",\\n    \"outside-seating\": true,\\n    \"price-range\": \"mid-range\"\\n}\\n```', \"When the user asks the bot to list any Italian restaurant, it doesn't need all of the details of the restaurant.\\nInstead, you want to provide a meaningful name that identifies the restaurant – in most cases, the name of the object will do.\\nThe function `get_representation_function_of_object()` returns a lambda function that maps the\\nabove restaurant object to its name.\", '```\\nlambda obj: obj[\"name\"]\\n```', 'This function is used whenever the bot is talking about a specific object, so that the user is presented a meaningful\\nname for the object.', 'By default, the lambda function returns the value of the `\"name\"` attribute of the object.\\nIf your object does not have a `\"name\"` attribute , or the `\"name\"` of an object is\\nambiguous, you should set a new lambda function for that object type by calling\\n`set_representation_function_of_object()`.', '`set_ordinal_mention_mapping()`: The ordinal mention mapping is needed to resolve an ordinal mention, such as\\n“second one,” to an object in a list. By default, the ordinal mention mapping looks like this:', '```\\n{\\n    \"1\": lambda l: l[0],\\n    \"2\": lambda l: l[1],\\n    \"3\": lambda l: l[2],\\n    \"4\": lambda l: l[3],\\n    \"5\": lambda l: l[4],\\n    \"6\": lambda l: l[5],\\n    \"7\": lambda l: l[6],\\n    \"8\": lambda l: l[7],\\n    \"9\": lambda l: l[8],\\n    \"10\": lambda l: l[9],\\n    \"ANY\": lambda l: random.choice(l),\\n    \"LAST\": lambda l: l[-1],\\n}\\n```', 'You can overwrite it by calling the function `set_ordinal_mention_mapping()`.\\nIf you want to learn more about how this mapping is used, check out [Resolve Mentions](./knowledge-base-actions.mdx#resolve-mentions).']\n\nSee the [example bot](https://github.com/RasaHQ/rasa/blob/main/examples/knowledgebasebot/actions/actions.py) for an\nexample implementation of an `InMemoryKnowledgeBase` that uses the method `set_representation_function_of_object()`\nto overwrite the default representation of the object type “hotel.”\nThe implementation of the `InMemoryKnowledgeBase` itself can be found in the\n[rasa-sdk](https://github.com/RasaHQ/rasa-sdk/tree/main/rasa_sdk/knowledge_base/) package.\n\n<a aria-hidden=\"true\" tabIndex=\"-1\" className=\"anchor enhancedAnchor\" id=\"custom-knowledge-base\"></a>",
                  "Creating Your Own Knowledge Base": "If you have more data or if you want to use a more complex data structure that, for example, involves relations between\ndifferent objects, you can create your own knowledge base implementation.\nJust inherit `KnowledgeBase` and implement the methods `get_objects()`, `get_object()`, `get_object_types()` and\n`get_attributes_of_object()`. The [knowledge base code](https://github.com/RasaHQ/rasa-sdk/tree/main/rasa_sdk/knowledge_base/)\nprovides more information on what those methods should do.\n\nYou can also customize your knowledge base further, by adapting the methods mentioned in the section\n[Customizing the InMemoryKnowledgeBase](./knowledge-base-actions.mdx#customizing-the-inmemoryknowledgebase).\n\n:::note\nWe wrote a [blog post](https://blog.rasa.com/set-up-a-knowledge-base-to-encode-domain-knowledge-for-rasa/)\nthat explains how you can set up your own knowledge base.\n\n:::"
                }
              },
              "metadata": {
                "id": "knowledge-bases",
                "sidebar_label": "Knowledge Base Actions",
                "title": "Knowledge Base Actions",
                "abstract": "Leverage information from knowledge bases inside conversations using `ActionQueryKnowledgeBase` in open source bot framework Rasa."
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 3]"
            },
            {
              "title": "Running a Rasa SDK Action Server",
              "description": null,
              "content": {
                "root": [
                  "There are two ways to run the action server, depending on whether you \nare using an environment with \n`rasa` installed or not:",
                  "If `rasa` is installed, you can run the action server using a `rasa` command:",
                  "```\nrasa run actions\n```",
                  "Alternatively you can make your assistant listen on a specific address using the `SANIC_HOST` environment\nvariable:",
                  "```\nSANIC_HOST=192.168.69.150 rasa run actions\n```",
                  "If `rasa` is not installed, you can run the action server directly as a python module:",
                  "```\npython -m rasa_sdk --actions actions\n```",
                  "Running the action server directly as a python module allows for `SANIC_HOST` too:",
                  "```\nSANIC_HOST=192.168.69.150 python -m rasa_sdk --actions actions\n```",
                  "Using the command above, `rasa_sdk` will expect to find your actions \nin a file called `actions.py`\nor in a package directory called `actions`. \nYou can specify a different actions module or package with the \n`--actions` flag.",
                  "The full list of options for running the action server with either command is:",
                  "```\n```"
                ]
              },
              "metadata": {
                "id": "running-action-server",
                "sidebar_label": "Running a Rasa SDK Server",
                "title": "Running a Rasa SDK Action Server"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 4]"
            },
            {
              "title": "Sanic Extensions",
              "description": null,
              "content": {
                "Step-by-step guide on creating your own Sanic extension in rasa_sdk": {
                  "Create the rasa_sdk_plugins package": "Create a package in your action server project which you must name `rasa_sdk_plugins`. Rasa SDK will try to instantiate this package in your project to start plugins.\nIf no plugins are found, it will print a debug log that there are no plugins in your project.",
                  "Register modules containing the hooks": "Create the package `rasa_sdk_plugins` and initialize the hooks by creating an `__init__.py` file where the plugin manager will look for the module where the hooks are implemented:\n\n```\ndef init_hooks(manager: pluggy.PluginManager) -> None:\n    \"\"\"Initialise hooks into rasa sdk.\"\"\"\n    import sys\n    import rasa_sdk_plugins.your_module\n\n    logger.info(\"Finding hooks\")\n    manager.register(sys.modules[\"rasa_sdk_plugins.your_module\"])\n```",
                  "Implement your hook": "Implement the hook `attach_sanic_app_extensions`. This hook forwards the app object created by Sanic in the `rasa_sdk` and allows you to create additional routes, middlewares, listeners and background tasks. Here's an example of this implementation that creates a listener.\n\nIn your `rasa_sdk_plugins.your_module.py`:\n\n```\nfrom __future__ import annotations\n\nimport logging\nimport pluggy\n\nfrom asyncio import AbstractEventLoop\nfrom functools import partial\n\n\nlogger = logging.getLogger(__name__)\nhookimpl = pluggy.HookimplMarker(\"rasa_sdk\")\n\n\n@hookimpl  # type: ignore[misc]\ndef attach_sanic_app_extensions(app: Sanic) -> None:\n    logger.info(\"hook called\")\n    app.register_listener(\n        partial(before_server_start),\n        \"before_server_start\",\n    )\n\n\nasync def before_server_start(app: Sanic, loop: AbstractEventLoop):\n    logger.info(\"BEFORE SERVER START\")\n```"
                }
              },
              "metadata": {
                "id": "sanic-extensions",
                "sidebar_label": "Sanic Extensions",
                "title": "Sanic Extensions"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 5]"
            },
            {
              "title": "Actions",
              "description": null,
              "content": {
                "Methods": {
                  "Action.name": "Defines the action's name. The name returned by this method is the one used in your bot's domain.\n\n['**Returns**:', 'Name of action', '**Return type**: ', '`str`']",
                  "Action.run": {
                    "**Parameters**": [
                      "**dispatcher** – the dispatcher which is used to\nsend messages back to the user. Use\n`dispatcher.utter_message()` or any other\n`rasa_sdk.executor.CollectingDispatcher`\nmethod. See the [documentation for the dispatcher](sdk-dispatcher.mdx)",
                      "**tracker** – the state tracker for the current\nuser. You can access slot values using\n`tracker.get_slot(slot_name)`, the most recent user message\nis `tracker.latest_message.text` and any other\n`rasa_sdk.Tracker` property. See the [documentation for the tracker](sdk-tracker.mdx).",
                      "**domain** – the bot's domain"
                    ],
                    "**Returns**": "A list of `rasa_sdk.events.Event` instances. See the [documentation for events](sdk-events.mdx).",
                    "**Return type**": "`List`[`Dict`[`str`, `Any`]]"
                  }
                },
                "Example": "In a restaurant bot, if the user says “show me a Mexican restaurant”,\nyour bot could execute the action `ActionCheckRestaurants`,\nwhich might look like this:\n\n```\nfrom typing import Text, Dict, Any, List\nfrom rasa_sdk import Action\nfrom rasa_sdk.events import SlotSet\n\nclass ActionCheckRestaurants(Action):\n   def name(self) -> Text:\n      return \"action_check_restaurants\"\n\n   def run(self,\n           dispatcher: CollectingDispatcher,\n           tracker: Tracker,\n           domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n\n      cuisine = tracker.get_slot('cuisine')\n      q = \"select * from restaurants where cuisine='{0}' limit 1\".format(cuisine)\n      result = db.query(q)\n\n      return [SlotSet(\"matches\", result if result is not None else [])]\n```\n\nThis action queries a database to find restaurants matching\nthe requested cuisine, and uses the list of restaurants found \nto set the value of the `matches` slot."
              },
              "metadata": {
                "id": "sdk-actions",
                "sidebar_label": "Actions",
                "title": "Actions"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 6]"
            },
            {
              "title": "Dispatcher",
              "description": null,
              "content": {
                "CollectingDispatcher": {
                  "CollectingDispatcher.utter_message": {
                    "**Parameters**": "The `utter_message` method takes the following optional arguments.\nPassing no arguments will result in an empty message being returned to the user.\nPassing multiple arguments will result in a rich response (e.g. text and buttons) being returned to the user.\n\n['`text`: The text to return to the user.']\n\n```\ndispatcher.utter_message(text = \"Hey there\")\n```\n\n['`image`: An image URL or file path that will be used to display an image to the user.']\n\n```\ndispatcher.utter_message(image = \"https://i.imgur.com/nGF1K8f.jpg\")\n```\n\n['`json_message`: A custom json payload as a dictionary. It can be used to send [channel specific responses](../responses.mdx).\\nThe following example would return a date picker in Slack:']\n\n```\ndate_picker = {\n  \"blocks\":[\n    {\n      \"type\": \"section\",\n      \"text\":{\n        \"text\": \"Make a bet on when the world will end:\",\n        \"type\": \"mrkdwn\"\n      },\n      \"accessory\":\n      {\n        \"type\": \"datepicker\",\n        \"initial_date\": \"2019-05-21\",\n        \"placeholder\":\n        {\n          \"type\": \"plain_text\",\n          \"text\": \"Select a date\"\n        }\n      }\n    }\n  ]\n}\ndispatcher.utter_message(json_message = date_picker)\n```\n\n['`response`: The name of a response to return to the user. This response should\\nbe specified in your assistants [domain](../domain.mdx).']\n\n```\ndispatcher.utter_message(response = \"utter_greet\")\n```\n\n['`attachment`: A URL or file path of an attachment to return to the user.']\n\n```\ndispatcher.utter_message(attachment = \"\")\n```\n\n[\"`buttons`: A list of buttons to return to the user.\\nEach button is a dictionary and should have a `title` and a `payload` key.\\nA button can include other keys, but these will only be used if\\na specific channel looks for them.\\nThe button's `payload` will be sent as a user message if the user\\nclicks the button.\"]\n\n```\ndispatcher.utter_message(buttons = [\n                {\"payload\": \"/affirm\", \"title\": \"Yes\"},\n                {\"payload\": \"/deny\", \"title\": \"No\"},\n            ])\n```\n\n[\"`elements`: These are specific to using Facebook as a messaging channel. For details\\nof expected format see [Facebook's documentation](https://developers.facebook.com/docs/messenger-platform/send-messages/template/generic/)\", '`**kwargs`: arbitrary keyword arguments, which can be\\nused to specify values for [variable interpolation in response variations](../responses.mdx). For example,\\ngiven the following response:']\n\n```\nresponses:\n  utter_greet_name:\n  - text: Hi {name}!\n```\n\nYou could specify the name with:\n\n```\ndispatcher.utter_message(response = \"utter_greet_name\", name = \"Aimee\")\n```",
                    "**Return type**": "`None`"
                  }
                }
              },
              "metadata": {
                "id": "sdk-dispatcher",
                "sidebar_label": "Dispatcher",
                "title": "Dispatcher"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 7]"
            },
            {
              "title": "Events",
              "description": null,
              "content": {
                "Event Classes": {
                  "SlotSet": "```\nrasa_sdk.events.SlotSet(\n    key: Text, \n    value: Any = None, \n    timestamp: Optional[float] = None\n)\n```\n\n**Underlying event**: [`slot`](events.mdx#slot)\n\n**Parameters**:\n\n['`key`: Name of the slot to set', '`value`: Value to set the slot to. The datatype must match the [type](../domain.mdx#slot-types)\\nof the slot', '`timestamp`: Optional timestamp of the event']\n\n**Example**\n\n```\nevt = SlotSet(key = \"name\", value = \"Mary\")\n```",
                  "AllSlotsReset": "```\nrasa_sdk.events.AllSlotsReset(timestamp: Optional[float] = None)\n```\n\n**Underlying event**: [`reset_slots`](events.mdx#reset_slots)\n\n**Parameters**:\n\n['`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = AllSlotsReset()\n```",
                  "ReminderScheduled": "```\nrasa_sdk.events.ReminderScheduled(\n    intent_name: Text,\n    trigger_date_time: datetime.datetime,\n    entities: Optional[Union[List[Dict[Text, Any]], Dict[Text, Text]]] = None,\n    name: Optional[Text] = None,\n    kill_on_user_message: bool = True,\n    timestamp: Optional[float] = None,\n)\n```\n\n**Underlying event**: [`reminder`](events.mdx#reminder)\n\n**Parameters**:\n\n['`intent_name`: Intent which the reminder will trigger ', '`trigger_date_time`: Datetime at which the execution of the action should be triggered. ', '`entities`: Entities to send with the intent', '`name`: ID of the reminder. If there are multiple reminders with the same id only the last will be run.', '`kill_on_user_message`: Whether a user message before the trigger time will abort the reminder', '`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nfrom datetime import datetime\n\nevt = ReminderScheduled(\n    intent_name = \"EXTERNAL_dry_plant\",\n    trigger_date_time = datetime(2020, 9, 15, 0, 36, 0, 851609),\n    entities = [{\"name\": \"plant\",\"value\":\"orchid\"}], \n    name = \"remind_water_plants\",\n)\n```",
                  "ReminderCancelled": "```\nReminderCancelled(\n    name: Optional[Text] = None,\n    intent_name: Optional[Text] = None,\n    entities: Optional[Union[List[Dict[Text, Any]], Dict[Text, Text]]] = None,\n    timestamp: Optional[float] = None,\n) \n```\n\n**Underlying event**: [`cancel_reminder`](events.mdx#cancel_reminder)\n\n**Parameters**:\n\n['`name`: ID of the reminder.', '`intent_name`: Intent which the reminder triggers ', '`entities`: Entities sent with the intent', '`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = ReminderCancelled(name = \"remind_water_plants\") \n```",
                  "ConversationPaused": "```\nConversationPaused(timestamp: Optional[float] = None)\n```\n\n**Underlying event**: [`pause`](events.mdx#slot)\n\n**Parameters**:\n\n['`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = ConversationPaused()\n```",
                  "ConversationResumed": "```\nConversationResumed(timestamp: Optional[float] = None)\n```\n\n**Underlying event**: [`resume`](events.mdx#resume)\n\n**Parameters**:\n\n['`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = ConversationResumed()\n```",
                  "FollowupAction": "```\nFollowupAction(\n    name: Text, \n    timestamp: Optional[float] = None\n)\n```\n\n**Underlying event**: [`followup`](events.mdx#followup)\n\n**Parameters**:\n\n['`name`: The name of the follow up action that will be executed.', '`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = FollowupAction(name = \"action_say_goodbye\")\n```",
                  "UserUtteranceReverted": "```\nUserUtteranceReverted(timestamp: Optional[float] = None)\n```\n\n**Underlying event**: [`rewind`](events.mdx#rewind)\n\n**Parameters**:\n\n['`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = UserUtteranceReverted()\n```",
                  "ActionReverted": "```\nActionReverted(timestamp: Optional[float] = None)\n```\n\n**Underlying event**: [`undo`](events.mdx#undo)\n\n**Parameters**:\n\n['`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = ActionReverted()\n```",
                  "Restarted": "```\nRestarted(timestamp: Optional[float] = None) \n```\n\n**Underlying event**: [`restart`](events.mdx#restart)\n\n**Parameters**:\n\n['`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = Restarted()\n```",
                  "SessionStarted": "```\nSessionStarted(timestamp: Optional[float] = None)\n```\n\n**Underlying event**: [`session_started`](events.mdx#session_started)\n\n**Parameters**:\n\n['`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = SessionStarted()\n```",
                  "UserUttered": "```\nUserUttered(\n    text: Optional[Text],\n    parse_data: Optional[Dict[Text, Any]] = None,\n    timestamp: Optional[float] = None,\n    input_channel: Optional[Text] = None,\n)\n```\n\n**Underlying event**: [`user`](events.mdx#user)\n\n**Parameters**:\n\n['`text`: Text of the user message', '`parse_data`: Parsed data of user message. This is ordinarily filled by NLU.', '`input_channel`: The channel on which the message was received', '`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = UserUttered(text = \"Hallo bot\")\n```",
                  "BotUttered": "```\nBotUttered(\n    text: Optional[Text] = None,\n    data: Optional[Dict[Text, Any]] = None,\n    metadata: Optional[Dict[Text, Any]] = None,\n    timestamp: Optional[float] = None,\n)\n```\n\n**Underlying event**: [`bot`](events.mdx#bot)\n\n**Parameters**:\n\n['`text`: The text the bot sends to the user', '`data`: Any non-text elements of the bot response. The structure of `data` matches that of `responses` given in the [API spec](/pages/action-server-api).', '`metadata`: Arbitrary key-value metadata', '`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = BotUttered(text = \"Hallo user\")\n```",
                  "ActionExecuted": "```\nActionExecuted(\n    action_name,\n    policy=None,\n    confidence: Optional[float] = None,\n    timestamp: Optional[float] = None,\n)\n```\n\n**Underlying event**: [`action`](events.mdx#action)\n\n**Parameters**:\n\n['`action_name`: Name of the action that was called', '`policy`: The policy used to predict the action', '`confidence`: The confidence with which the action was predicted', '`timestamp`: Optional timestamp of the event']\n\n**Example**:\n\n```\nevt = ActionExecuted(\"action_greet_user\")\n```"
                }
              },
              "metadata": {
                "id": "sdk-events",
                "sidebar_label": "Events",
                "title": "Events"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 8]"
            },
            {
              "title": "Tracker",
              "description": null,
              "content": {
                "Attributes": "The following are available as attributes of a `Tracker` object:\n\n['`sender_id` - The unique ID of person talking to the bot.', '`slots` - The list of slots that can be filled as defined in the\\n“ref”domains.', '`latest_message` - A dictionary containing the attributes of the latest\\nmessage: `intent`, `entities` and `text`.', '`events` - A list of all previous events.', '`active_loop` - The name of the currently active loop.', '`latest_action_name` - The name of the last action the bot executed.']",
                "Methods": {
                  "Tracker.current_state": "Return the current tracker state as an object.\n\n['**Return type**']\n\n`Dict[str, Any]`",
                  "Tracker.is_paused": "State whether the tracker is currently paused.\n\n['**Return type**']\n\n`bool`",
                  "Tracker.get_latest_entity_values": "Get entity values found for the passed entity type and optional role and\ngroup in latest message.\nIf you are only interested in the first entity of a given type use:\n\n```\nnext(tracker.get_latest_entity_values(“my_entity_name”), None)\n```\n\nIf no entity is found, then `None` is the default result.\n\n['**Parameters**', ['`entity_type` – the entity type of interest', '`entity_role` – optional entity role of interest', '`entity_group` – optional entity group of interest']]\n\n['**Returns**']\n\nList of entity values.\n\n['**Return type**']\n\n`Iterator[str]`",
                  "Tracker.get_latest_input_channel": "Get the name of the input_channel of the latest UserUttered event\n\n['**Return type**']\n\n`Optional[str]`",
                  "Tracker.events_after_latest_restart": "Return a list of events after the most recent restart.\n\n['**Return type**']\n\n`List[Dict]`",
                  "Tracker.get_slot": "Retrieves the value of a slot.\n\n['**Parameters**', ['`key` – the name of the slot of which to retrieve the value']]\n\n['**Return type**']\n\n`Optional[Any]`",
                  "Tracker.get_intent_of_latest_message": "Retrieves the user's latest intent.\n\n['**Parameters**', ['`skip_fallback_intent` (default: `True`) – Optionally skip the `nlu_fallback` intent and return the next highest ranked.']]\n\n['**Returns**']\n\nThe intent of the latest message if available.\n\n['**Return type**']\n\n`Optional[Text]`"
                }
              },
              "metadata": {
                "id": "sdk-tracker",
                "sidebar_label": "Tracker",
                "title": "Tracker"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 9]"
            },
            {
              "title": "Slot Validation Actions",
              "description": null,
              "content": {
                "`ValidationAction` class": {
                  "How to subclass `ValidationAction`": {
                    "Validation of Slots with Predefined Mappings": "To validate slots with a predefined mapping, you must write functions named `validate_<slot_name>`.\n\nIn the following example, the value for slot `location` is capitalized only if the extracted value is of type string:\n\n```\nfrom typing import Text, Any, Dict\n\nfrom rasa_sdk import Tracker, ValidationAction\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom rasa_sdk.types import DomainDict\n\n\nclass ValidatePredefinedSlots(ValidationAction):\n    def validate_location(\n        self,\n        slot_value: Any,\n        dispatcher: CollectingDispatcher,\n        tracker: Tracker,\n        domain: DomainDict,\n    ) -> Dict[Text, Any]:\n        \"\"\"Validate location value.\"\"\"\n        if isinstance(slot_value, str):\n            # validation succeeded, capitalize the value of the \"location\" slot\n            return {\"location\": slot_value.capitalize()}\n        else:\n            # validation failed, set this slot to None\n            return {\"location\": None}\n```",
                    "Extraction of Custom Slot Mappings": "To define custom extraction code, write an `extract_<slot_name>` method for every slot with a\ncustom slot mapping.\n\nThe following example shows the implementation of a custom action that extracts the slot `count_of_insults` to keep\ntrack of the user's attitude.\n\n```\nfrom typing import Dict, Text, Any\n\nfrom rasa_sdk import Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nfrom rasa_sdk.forms import ValidationAction\n\n\nclass ValidateCustomSlotMappings(ValidationAction):\n    async def extract_count_of_insults(\n        self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict\n    ) -> Dict[Text, Any]:\n        intent_of_last_user_message = tracker.get_intent_of_latest_message()\n        current_count_of_insults = tracker.get_slot(\"count_of_insults\")\n        if intent_of_last_user_message == \"insult\":\n           current_count_of_insults += 1\n\n        return {\"count_of_insults\": current_count_of_insults}\n```"
                  },
                  "`ValidationAction` class implementation": {
                    "Methods": {
                      "ValidationAction.name": "Defines the action's name: this must be hardcoded as `action_validate_slot_mappings`.\n\n['**Returns**:', 'Name of action', '**Return type**:', '`str`']",
                      "ValidationAction.run": {
                        "**Parameters**": [
                          "**dispatcher** – the dispatcher which is used to\nsend messages back to the user. Use\n`dispatcher.utter_message()` or any other\n`rasa_sdk.executor.CollectingDispatcher`\nmethod. See the [documentation for the dispatcher](./sdk-dispatcher.mdx)",
                          "**tracker** – the state tracker for the current\nuser. You can access slot values using\n`tracker.get_slot(slot_name)`, the most recent user message\nis `tracker.latest_message.text` and any other\n`rasa_sdk.Tracker` property. See the [documentation for the tracker](./sdk-tracker.mdx).",
                          "**domain** – the bot's domain"
                        ],
                        "**Returns**": "A list of `rasa_sdk.events.Event` instances. See the [documentation for events](./sdk-events.mdx).",
                        "**Return type**": "`List`[`Dict`[`str`, `Any`]]"
                      },
                      "ValidationAction.required_slots": {
                        "**Returns**": "A list of slot names of type `Text`."
                      },
                      "ValidationAction.get_extraction_events": "```\nasync ValidationAction.get_extraction_events(dispatcher, tracker, domain)\n```\n\nThe `get_extraction_events` method will gather the list of slot names via `required_slots` method call and then loop\nthrough every slot name to run the `extract_<slot name>` method if available.",
                      "**Returns**": "A list of `rasa_sdk.events.SlotSet` instances. See the [documentation for SlotSet events](./sdk-events.mdx#slotset).",
                      "ValidationAction.get_validation_events": {
                        "**Returns**": "A list of `rasa_sdk.events.SlotSet` instances. See the [documentation for SlotSet events](./sdk-events.mdx#slotset)."
                      }
                    }
                  }
                },
                "`FormValidationAction` class": {
                  "`FormValidationAction` class implementation": {
                    "Methods": {
                      "FormValidationAction.name": "The method `name` will raise a `NotImplementedError` exception if the bot custom action subclassing `FormValidationAction`\ndoes not return a custom name which follows this naming convention: `validate_<form name>`.",
                      "FormValidationAction.required_slots": "The method `required_slots` will return the `domain_slots` which is a list of all slot names included in the form's\n`required_slots`. `domain_slots` is returned by the `domain_slots` method, which only takes\n`Domain` as an argument.",
                      "FormValidationAction.next_requested_slot": "The method `next_requested_slot` will set the value of `REQUESTED_SLOT` to the next unset slot only if the\n`required_slots` method was overridden by the custom action subclassing `FormValidationAction`.\n\nIf users didn't override `required_slots` then we'll let the `FormAction` within Rasa Open\nSource request the next slot, and the method will return `None`.\n\nThe parameters the method requires are:\n\n['[dispatcher](./sdk-dispatcher.mdx)', '[tracker](./sdk-tracker.mdx)', \"the bot's domain\"]",
                      "FormValidationAction.run": "The original implementation of the `ValidationAction.run` method is extended to add a call to the `next_requested_slot`\nmethod. The output of the `next_requested_slot` method call (if not `None`) is added to the list of events that `run`\nmethod returns."
                    }
                  }
                }
              },
              "metadata": {
                "id": "validation-action",
                "sidebar_label": "Slot Validation Actions",
                "title": "Slot Validation Actions",
                "abstract": "Learn how `ValidationAction` class is implemented in the Rasa SDK."
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 55, \"subpages\", 10]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 55]"
        },
        {
          "title": "Connectors",
          "description": "Documentation section: connectors",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/connectors"
          },
          "subpages": [
            {
              "title": "Audiocodes VoiceAI Connect",
              "description": "Build a Rasa Voice Bot on Audiocodes VoiceAI Connect",
              "content": {
                "Getting Credentials": "To get credentials, create a bot on the [VoiceAI connect portal](https://voiceaiconnect.audiocodes.io/).\n\n['Select **Bots** in the left sidebar.', 'Click on the **+** sign to create a new bot.', 'Select **Rasa** as the Bot Framework', 'Set the bot URL and choose a token value.']\n\n:::info Setting the bot URL with a tunneling solution when testing locally\n\nVisit this [section](../messaging-and-voice-channels.mdx#testing-channels-on-your-local-machine) to learn how to generate\nthe required bot URL when testing the channel on your local machine.\n\n:::",
                "Setting credentials": "The token value chosen above will be used in the `credentials.yml`:\n\n```\nrasa_plus.channels.audiocodes.AudiocodesInput:\n    token: <token>\n```\n\nYou can also specify optional parameters:\n\n| Parameter       | Default value    | Description                                                                                                                 |\n| --------------- | ---------------- | --------------------------------------------------------------------------------------------------------------------------- |\n| `token`         | No default value | The token to authenticate calls between your Rasa assistant and VoiceAI connect                                             |\n| `use_websocket` | `true`           | If `true`, Rasa will send messages through a web socket. If set to `false`, Rasa will send messages through http API calls. |\n| `keep_alive`    | 120              | In seconds. For each ongoing conversation, VoiceAI Connect will periodically verify the conversation is still active on the Rasa side.        |\n\nThen restart your Rasa server to make the new channel endpoint available.",
                "Usage": {
                  "Receiving messages from a user": "When a user speaks on the phone, VoiceAI Connect will send a text message (after it is processed by the speech-to-text engine) to your assistant like any other channel.\nThis message will be interpreted by the NLU and you can then drive the conversation with rules, stories and forms.",
                  "Sending messages to a user": "Your bot will respond with text messages like with any other channel. The text-to-speech engine will convert the text and deliver it as a voice message to the user.\n\nHere is an example:\n\n```\nutter_greet:\n    - text: 'Hello! isn’t every life and every work beautiful?'\n```\n\n:::note \nOnly text messages are allowed. Images, attachments, and buttons cannot be used with a voice channel. \n:::",
                  "Handling conversation events": "Non-voice events can also be handled by the bot. Here are a few examples:\n\n| Event   | intent             | Description                                                                                                                                                                                                                                                                                                                                |\n| ------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `start` | `vaig_event_start` | VoiceAI will send this intent when it picks-up a phone call. In general, the response to that intent is a welcome or greeting message. [Call context](https://techdocs.audiocodes.com/voice-ai-connect/#VAIG_Combined/call-initiation.htm?TocPath=Bot%2520integration%257CBasic%2520behavior%257C_____1) will be provided through entities |\n| `end`   | `vaig_event_end`   | VoiceAI will send this intent when a call ends. You can use that to call an action that updates the call information.                                                                                                                                                                                                            |\n| `DTMF`  | `vaig_event_DTMF`  | VoiceAI will send this intent when receiving a DTMF tone (i.e user presses digit on the keyboard of the phone). The digit(s) sent will be passed in the `value` entity                                                                                                                                                                     |\n\nThe general pattern is that for every `event` sent, the bot will receive the `vaig_event_<event>` intent, with context information in entities.\n\nHere is a simple rule to send a greeting message when a call to the bot is initiated:\n\n```\n- rule: New call\n  steps:\n      - intent: vaig_event_start\n      - action: utter_greet\n```\n\nCheck the [VoiceAI Connect documentation](https://techdocs.audiocodes.com/voice-ai-connect/#VAIG_Combined/voiceai_connect.htm?TocPath=VoiceAI%2520Connect%257C_____0) for an exhaustive list of events.",
                  "Configuring calls": {
                    "Example: changing a pin code": "In this example we create a flow to allow a user to change a pin code.\n\n```\n- rule: Set pin code\n  steps:\n  # User says \"I want to change my pin code\"\n  - intent: set_pin_code\n  # Send the noUserInput configuration event\n  - action: utter_config_no_user_input\n  # Send the DTMF format configuration event\n  - action: utter_config_dtmf_pin_code \n  # A standard Rasa form to collect the pin code from the user\n  - action: pin_code_form \n  - ...\n```\n\nIn the domain, we can add the `utter_config_<config_event>` responses:\n\n[`noUserInput` event](https://techdocs.audiocodes.com/voice-ai-connect/#VAIG_Combined/inactivity-detection.htm?TocPath=Bot%2520integration%257CReceiving%2520notifications%257C_____3)\n\n```\n  utter_config_no_user_input:\n  - custom:\n      type: event\n      name: config\n      sessionParams:\n        # If user stays silent for 5 seconds or more, the notification will be sent\n        userNoInputTimeoutMS: 5000\n        # If you want to allow for more than one notification during a call\n        userNoInputRetries: 2\n        # Enable the noUserInput notification\n        userNoInputSendEvent: true\n```\n\n[`DTMF` event](https://techdocs.audiocodes.com/voice-ai-connect/#VAIG_Combined/receive-dtmf.htm?TocPath=Bot%2520integration%257CReceiving%2520notifications%257C_____2)\n\n```\n   utter_config_dtmf_pin_code:\n  - custom:\n      type: event\n      name: config\n      sessionParams:\n        # Enable grouped collection (i.e will send all digits in a single payload)\n        dtmfCollect: true\n        # If more than 5 secs have passed since a digit was pressed, \n        # the input is considered completed and will be sent to the bot\n        dtmfCollectInterDigitTimeoutMS: 5000\n        # If 6 digits are collected, VoiceAI will send those 6 digits\n        # even if the user keeps pressing buttons\n        dtmfCollectMaxDigits: 6\n        # If the user presses '#' the input is considered complete\n        dtmfCollectSubmitDigit: \"#\"\n```\n\nNow you can configure the `pin_code` slot in the `pin_code_form` to extract the pin code from the `value` entity with the `vaig_event_DTMF` intent:\n\n```\n  pin_code:\n    type: text\n    influence_conversation: false\n    mappings:\n    - type: from_entity\n      entity: value\n      intent: vaig_event_DTMF\n      not_intent: vaig_event_noUserInput\n      conditions:\n      - active_loop: pin_code_form\n        requested_slot: pin_code\n```\n\nNotice how `vaig_event_noUserInput` was declared in the `not_intent` field. \n\nSince the `vaig_event_noUserInput` intent is sent by VoiceAI Connect when the user stays silent as per our configuration,\nwe must deactivate the form so we can pick up the conversation from a rule or a story and gracefully handle the failure.\n\nIn the following example, we simply cancel the current flow if we receive the `vaig_event_noUserInput` intent (i.e. user stays silent) while the `pin_code_form` loop is active.\n\n```\n- rule: Set pin code - happy path\n  steps:\n  - intent: set_pin_code\n  - action: utter_config_no_user_input\n  - action: utter_config_dtmf_pin_code\n  - action: pin_code_form\n  - active_loop: pin_code_form\n  - active_loop: null\n  - slot_was_set:\n    - requested_slot: null\n  - action: utter_pin_code_changed\n  - action: action_pin_code_cleanup\n\n- rule: Set pin code - no response - cancel.\n  condition:\n  - active_loop: pin_code_form\n  steps:\n  - intent: vaig_event_noUserInput\n  - action: utter_cancel_set_pin_code\n  - action: action_deactivate_loop\n  - active_loop: null\n```"
                  }
                }
              },
              "metadata": {
                "id": "audiocodes-voiceai-connect",
                "sidebar_label": "Audiocodes VoiceAI Connect",
                "title": "Audiocodes VoiceAI Connect",
                "description": "Build a Rasa Voice Bot on Audiocodes VoiceAI Connect"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 0]"
            },
            {
              "title": "Cisco Webex Teams",
              "description": "Build a Rasa Chat Bot on Cisco Webex",
              "content": {
                "Getting Credentials": "**How to get the Cisco Webex Teams credentials:**\n\nYou need to set up a bot. Check out the [Cisco Webex for Developers\ndocumentation](https://developer.webex.com/docs/bots) for information\nabout how to create your bot.\n\nAfter you have created the bot through Cisco Webex Teams, you need to create a\nroom in Cisco Webex Teams. Then add the bot in the room the same way you would\nadd a person in the room.\n\nYou need to note down the room ID for the room you created. This room ID will\nbe used in `room` variable in the `credentials.yml` file.\n\nPlease follow this link below to find the room ID\n`https://developer.webex.com/endpoint-rooms-get.html`\n\nIn the OAuth & Permissions section, add the URL of the Rasa endpoint\nthat Webex should forward the messages to. The endpoint for receiving Cisco Webex Teams messages\nis `http://<host>:<port>/webhooks/webexteams/webhook`, replacing\nthe host and port with the appropriate values from your running Rasa server.",
                "Running on Cisco Webex Teams": "Add the Webex Teams credentials to your `credentials.yml`:\n\n```\nwebexteams:\n  access_token: \"YOUR-BOT-ACCESS-TOKEN\"\n  room: \"YOUR-CISCOWEBEXTEAMS-ROOM-ID\"\n```\n\nRestart your Rasa server\nto make the new channel endpoint available for Cisco Webex Teams to send messages to.\n\n:::note\nIf you do not set the `room` keyword\nargument, messages will by delivered back to\nthe user who sent them.\n\n:::"
              },
              "metadata": {
                "id": "cisco-webex-teams",
                "sidebar_label": "Cisco Webex Teams",
                "title": "Cisco Webex Teams",
                "description": "Build a Rasa Chat Bot on Cisco Webex"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 1]"
            },
            {
              "title": "Custom Connectors",
              "description": "Deploy and Run a Rasa Chat Bot on a custom chat interface",
              "content": {
                "The `name` method": "The `name` method defines the url prefix for the connector's webhook. It also defines the channel name you should use \nin any [channel specific response variations](../responses.mdx#channel-specific-response-variations) and the name you\nshould pass to the `output_channel` query parameter on the [trigger intent endpoint](https://www.rasa.com/docs/rasa/pages/http-api#operation/triggerConversationIntent).\n\nFor example, if your custom channel is named `myio`, you would define the `name` method as:\n\n```\nfrom rasa.core.channels.channel import InputChannel\n\nclass MyIO(InputChannel):\n    def name() -> Text:\n        \"\"\"Name of your custom channel.\"\"\"\n        return \"myio\"\n```\n\nYou would write a response variation specific to the `myio` channel  as:\n\n```\nresponses:\n  utter_greet:\n    - text: Hi! I'm the default greeting.\n    - text: Hi! I'm the custom channel greeting\n      channel: myio\n```\n\nThe webhook you give to the custom channel to call would be\n`http://<host>:<port>/webhooks/myio/webhook`, replacing \nthe host and port with the appropriate values from your running Rasa server.",
                "The `blueprint` method": "The `blueprint` method\nneeds to create a [sanic blueprint](https://sanicframework.org/en/guide/best-practices/blueprints.html#overview)\nthat can be attached to a sanic server. \nYour blueprint should have at least the two routes: `health` on the route `/`,\nand `receive` on the route `/webhook` (see example custom channel below).\n\nAs part of your implementation of the `receive` endpoint, you will need to tell\nRasa to handle the user message. You do this by calling \n\n```\n    on_new_message(\n      rasa.core.channels.channel.UserMessage(\n        text,\n        output_channel,\n        sender_id\n      )\n    )\n```\n\nCalling `on_new_message` will send the user message to the [`handle_message`](https://github.com/RasaHQ/rasa/blob/c922253fe890bb4903329d4ade764e0711d384ec/rasa/core/agent.py#L511_) method.\n\nSee more details on the `UserMessage` object [here](https://www.rasa.com/docs/rasa/reference/rasa/core/channels/channel#usermessage-objects).\n\nThe `output_channel` argument refers to an output channel implementing the \n[`OutputChannel`](https://www.rasa.com/docs/rasa/reference/rasa/core/channels/channel#outputchannel-objects) class. You can\neither implement your own output channel class with the methods for your particular chat channel\n(e.g. methods to send text and images) or you can use the\n[`CollectingOutputChannel`](https://www.rasa.com/docs/rasa/reference/rasa/core/channels/channel#collectingoutputchannel-objects) \nto collect the bot responses Rasa creates while the bot is processing your messages and return\nthem as part of your endpoint response. This is the way the `RestInput`\nchannel is implemented. For examples on how to create and use your own output\nchannel, take a look at the implementations of the other\noutput channels, e.g. the `SlackBot` in `rasa.core.channels.slack`.\n\nHere is a simplified example of a custom channel connector that makes use of the `CollectingOutputChannel`:\n\n```\nimport asyncio\nimport inspect\nfrom sanic import Sanic, Blueprint, response\nfrom sanic.request import Request\nfrom sanic.response import HTTPResponse\nfrom typing import Text, Dict, Any, Optional, Callable, Awaitable, NoReturn\n\nimport rasa.utils.endpoints\nfrom rasa.core.channels.channel import (\n    InputChannel,\n    CollectingOutputChannel,\n    UserMessage,\n)\n\nclass MyIO(InputChannel):\n    def name() -> Text:\n        \"\"\"Name of your custom channel.\"\"\"\n        return \"myio\"\n\n    def blueprint(\n        self, on_new_message: Callable[[UserMessage], Awaitable[None]]\n    ) -> Blueprint:\n\n        custom_webhook = Blueprint(\n            \"custom_webhook_{}\".format(type(self).__name__),\n            inspect.getmodule(self).__name__,\n        )\n\n        @custom_webhook.route(\"/\", methods=[\"GET\"])\n        async def health(request: Request) -> HTTPResponse:\n            return response.json({\"status\": \"ok\"})\n\n        @custom_webhook.route(\"/webhook\", methods=[\"POST\"])\n        async def receive(request: Request) -> HTTPResponse:\n            sender_id = request.json.get(\"sender\") # method to get sender_id \n            text = request.json.get(\"text\") # method to fetch text\n            input_channel = self.name() # method to fetch input channel\n            metadata = self.get_metadata(request) # method to get metadata\n\n            collector = CollectingOutputChannel()\n            \n            # include exception handling\n\n            await on_new_message(\n                UserMessage(\n                    text,\n                    collector,\n                    sender_id,\n                    input_channel=input_channel,\n                    metadata=metadata,\n                )\n            )\n\n            return response.json(collector.messages)\n\n        return custom_webhook\n```",
                "Metadata on messages": "If you need to use extra information from your front end in your custom\nactions, you can pass this information using the `metadata` key of your user\nmessage. This information will accompany the user message through the Rasa\nserver into the action server when applicable, where you can find it stored in\nthe `tracker`. Message metadata will not directly affect NLU classification\nor action prediction. \n\nThe `InputChannel` class's default implementation of `get_metadata` **ignores all metadata**.\nTo extract metadata in a custom connector, implement the `get_metadata` method.\nThe `SlackInput` channel provides one example of a `get_metadata` method that extracts metadata according to the channel's response format.",
                "Credentials for Custom Channels": "To use a custom channel, you need to supply credentials for it in a credentials configuration file\ncalled `credentials.yml`. \nThis credentials file has to contain the **module path** (not the channel name) of your custom channel and\nany required configuration parameters. \n\nFor example, for a custom connector class called `MyIO` saved in a file `addons/custom_channel.py`, \nthe module path would be `addons.custom_channel.MyIO`, and the credentials could look like:\n\n```\naddons.custom_channel.MyIO:\n  username: \"user_name\"\n  another_parameter: \"some value\"\n```\n\nTo make the Rasa\nserver aware of your custom channel, specify the path to `credentials.yml` to the Rasa server at startup with the command line argument `--credentials` .",
                "Testing the Custom Connector Webhook": "To test your custom connector, you can `POST` messages to the webhook using a json body with the following format:\n\n```\n{\n  \"sender\": \"test_user\",  // sender ID of the user sending the message\n  \"message\": \"Hi there!\",\n  \"metadata\": {}  // optional, any extra info you want to add for processing in NLU or custom actions\n}\n```\n\nFor a locally running Rasa server, the curl request would look like this:\n\n```\ncurl --request POST \\\n     --url http://localhost:5005/webhooks/myio/webhook \\\n     --header 'Content-Type: application/json' \\\n     --data '{\n            \"sender\": \"test_user\",\n            \"message\": \"Hi there!\",\n            \"metadata\": {}\n          }'\n```"
              },
              "metadata": {
                "id": "custom-connectors",
                "sidebar_label": "Custom Connectors",
                "title": "Custom Connectors",
                "description": "Deploy and Run a Rasa Chat Bot on a custom chat interface"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 2]"
            },
            {
              "title": "Facebook Messenger",
              "description": "Build a Rasa Chat Bot on Facebook Messenger",
              "content": {
                "Facebook Setup": {
                  "Getting Credentials": "**How to get the Facebook credentials:**\nYou need to set up a Facebook app and a page.\n\n['To create the app head over to\\n[Facebook for Developers](https://developers.facebook.com/)\\nand click on **My Apps** → **Add New App**.', 'Go onto the dashboard for the app and under **Products**,\\nfind the **Messenger** section and click **Set Up**. Scroll down to\\n**Token Generation** and click on the link to create a new page for your\\napp.', 'Create your page and select it in the dropdown menu for the\\n**Token Generation**. The shown **Page Access Token** is the\\n`page-access-token` needed later on.', 'Locate the **App Secret** in the app dashboard under **Settings** → **Basic**.\\nThis will be your `secret`.', 'Use the collected `secret` and `page-access-token` in your\\n`credentials.yml`, and add a field called `verify` containing\\na string of your choice. Start `rasa run` with the\\n`--credentials credentials.yml` option.', 'Set up a **Webhook** and select at least the **messaging** and\\n**messaging_postback** subscriptions. Insert your callback URL, which will\\nlook like `https://<host>:<port>/webhooks/facebook/webhook`, replacing\\nthe host and port with the appropriate values from your running Rasa server.', 'Insert the **Verify Token** which has to match the `verify`\\nentry in your `credentials.yml`.']\n\n:::note configure https\nFacebook Messenger only forwards\nmessages to endpoints via `https`, so take appropriate measures to add\nit to your setup. For local testing of your bot, see [Testing Channels on Your Local Machine](../messaging-and-voice-channels.mdx#testing-channels-on-your-local-machine).\n:::\n\nFor more detailed steps, visit the\n[Messenger docs](https://developers.facebook.com/docs/graph-api/webhooks).",
                  "Running On Facebook Messenger": "Add the Facebook credentials to your `credentials.yml`:\n\n```\nfacebook:\n  verify: \"rasa-bot\"\n  secret: \"3e34709d01ea89032asdebfe5a74518\"\n  page-access-token: \"EAAbHPa7H9rEBAAuFk4Q3gPKbDedQnx4djJJ1JmQ7CAqO4iJKrQcNT0wtD\"\n```\n\nRestart your Rasa server\nto make the new channel endpoint available for Facebook Messenger to send messages to."
                },
                "Supported response attachments": "In addition to typical text, image, and custom responses, the Facebook Messenger\nchannel supports the following additional response attachments:\n\n['[Buttons](https://developers.facebook.com/docs/messenger-platform/send-messages/buttons)\\nare structured the same as other Rasa buttons. Facebook API limits the amount of\\nbuttons you can sent in a message to 3. If more than 3 buttons are provided in a\\nmessage, Rasa will ignore all provided buttons.', \"[Quick Replies](https://developers.facebook.com/docs/messenger-platform/send-messages/quick-replies)\\nprovide a way to present a set of up to 13 buttons in-conversation that contain a\\ntitle and optional image, and appear prominently above the composer. You can also\\nuse quick replies to request a person's email address or phone number.\", \"```\\nutter_fb_quick_reply_example:\\n   - text: Hello World!\\n     quick_replies:\\n       - title: Text quick reply\\n         payload: /example_intent\\n       - title: Image quick reply\\n         payload: /example_intent\\n         image_url: http://example.com/img/red.png\\n       # below are Facebook provided quick replies\\n       # the title and payload will be filled\\n       # with the user's information from their profile\\n       - content_type: user_email\\n         title:\\n         payload:\\n       - content_type: user_phone_number\\n         title:\\n         payload:\\n```\"]\n\n:::note\nBoth Quick Reply and Button titles in Facebook Messenger have a character limit of\n\n['Titles longer than 20 characters will be truncated.']\n\n:::\n\n['[Elements](https://developers.facebook.com/docs/messenger-platform/send-messages/template/generic)\\nprovide a way to create a horizontally scrollable list up to 10 content elements that\\nintegrate buttons, images, and more alongside text a single message.', '```\\nutter_fb_element_example:\\n   - text: Hello World!\\n     elements:\\n       - title: Element Title 1\\n         subtitle: Subtitles are supported\\n         buttons: # note the button limit still applies here\\n           - title: Example button A\\n             payload: /example_intent\\n           - title: Example button B\\n             payload: /example_intent\\n           - title: Example button C\\n             payload: /example_intent\\n       - title: Element Title 2\\n         image_url: http://example.com/img/red.png\\n         buttons:\\n           - title: Example button D\\n             payload: /example_intent\\n           - title: Example button E\\n             payload: /example_intent\\n           - title: Example button F\\n             payload: /example_intent\\n```']"
              },
              "metadata": {
                "id": "facebook-messenger",
                "sidebar_label": "Facebook Messenger",
                "title": "Facebook Messenger",
                "description": "Build a Rasa Chat Bot on Facebook Messenger"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 3]"
            },
            {
              "title": "Google Hangouts Chat",
              "description": "Build a Rasa Chat Bot on Google Hangouts Chat",
              "content": {
                "Hangouts Chat Setup": {
                  "Running On Hangouts Chat": "Add the Hangouts credentials to your `credentials.yml`:\n\n```\nhangouts:\n  # no credentials required here\n```\n\nIf you want to use OAuth2, add the project id obtained from the Google Developer Console:\n\n```\nhangouts:\n  project_id: \"12345678901\"\n```\n\nRestart your Rasa server\nto make the new channel endpoint available for Google Hangouts to send messages to.",
                  "Cards and Interactive Cards": "There are two ways in which Hangouts Chat will display bot messages, either as text or card. For each received\nrequest, your bot will send all messages in one response. If one of those messages is a card (e.g. an image),\nall other messages are converted to card format as well.\n\nInteractive cards trigger the `CARD_CLICKED` event for user interactions, e.g. when a button is clicked. When\ncreating an interactive card, e.g. via `dispatcher.utter_button_message()` in your `actions.py`, you can\nspecify a payload for each button that is going to be returned with the `CARD_CLICKED` event and extracted\nby the `HangoutsInput` channel (for example\n`buttons=[{\"text\":\"Yes!\", \"payload\":\"/affirm\"}, {\"text\":\"Nope.\", \"payload\":\"/deny\"}])`.\nUpdating cards is not yet supported.\n\nFor more detailed information on cards, visit the\n[Hangouts docs](https://developers.google.com/hangouts/chat/reference).",
                  "Other Hangouts Chat Events": "Except for `MESSAGE` and `CARD_CLICKED`, Hangouts Chat knows two other event types, `ADDED_TO_SPACE` and\n`REMOVED_FROM_SPACE`, which are triggered when your bot is added or removed from a direct message or chat room\nspace. The default intent names for these events can be modified in the `HangoutsInput` constructor method."
                }
              },
              "metadata": {
                "id": "hangouts",
                "sidebar_label": "Google Hangouts Chat",
                "title": "Google Hangouts Chat",
                "description": "Build a Rasa Chat Bot on Google Hangouts Chat"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 4]"
            },
            {
              "title": "Mattermost",
              "description": "Build a Rasa Chat Bot on Mattermost",
              "content": {
                "Getting Credentials": "Mattermost now uses bot accounts for better security.  So you can use their guide to create\nyour bot to get your token required for the credentials.yml file.\n\nFor more information on creating a bot account please see\n[Bot Creation](https://docs.mattermost.com/developer/bot-accounts.html#bot-account-creation).\n\nFor information on converting existing user account into bot account please see\n[User Conversion](https://docs.mattermost.com/developer/bot-accounts.html#how-do-i-convert-an-existing-account-to-a-bot-account).\n\n**How to set up the outgoing webhook:**\n\n['To create the Mattermost outgoing webhook, login to your Mattermost\\nteam site and go to **Main Menu > Integrations > Outgoing Webhooks**.', 'Click **Add outgoing webhook**.', \"Fill out the details including the channel you want the bot in.\\nYou will need to ensure the **trigger words** section is set up\\nwith `@yourbotname` so that the bot doesn't trigger on everything\\nthat is said.\", 'The **Content Type** must be set to `application/json`.', 'Make sure **trigger when** is set to value\\n**first word matches a trigger word exactly**.', 'Add the Callback URL, which will\\nlook like `http://<host>:<port>/webhooks/mattermost/webhook`, replacing\\nthe host and port with the appropriate values from your running Rasa server.']\n\nFor more detailed steps, visit the\n[Mattermost docs](https://docs.mattermost.com/guides/developer.html).",
                "Running on Mattermost": "Add the Mattermost credentials to your `credentials.yml`:\n\n```\nmattermost:\n  url: \"https://chat.example.com/api/v4\"\n  token: \"xxxxx\" #  the token for the bot account from creating the bot step.\n  webhook_url: \"https://server.example.com/webhooks/mattermost/webhook\"  # this should match the callback url from step 6\n```\n\nRestart your Rasa server\nto make the new channel endpoint available for Mattermost to send messages to."
              },
              "metadata": {
                "id": "mattermost",
                "sidebar_label": "Mattermost",
                "title": "Mattermost",
                "description": "Build a Rasa Chat Bot on Mattermost"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 5]"
            },
            {
              "title": "Microsoft Bot Framework",
              "description": "Build a Rasa Chat Bot on Microsoft Bot Framework",
              "content": {
                "Running on Microsoft Bot Framework": "Add the Botframework credentials to your `credentials.yml`:\n\n```\nbotframework:\n  app_id: \"MICROSOFT_APP_ID\"\n  app_password: \"MICROSOFT_APP_PASSWORD\"\n```\n\nRestart your Rasa server\nto make the new channel endpoint available for Microsoft Bot Framework to send messages to."
              },
              "metadata": {
                "id": "microsoft-bot-framework",
                "sidebar_label": "Microsoft Bot Framework",
                "title": "Microsoft Bot Framework",
                "description": "Build a Rasa Chat Bot on Microsoft Bot Framework"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 6]"
            },
            {
              "title": "RocketChat",
              "description": "Build a Rasa Chat Bot on Rocketchat",
              "content": {
                "Getting Credentials": "**How to set up Rocket.Chat:**\n\n['Create a user that will be used to post messages, and set its\\ncredentials at credentials file.', 'Create a Rocket.Chat outgoing webhook by logging in as admin to\\nRocket.Chat and going to\\n**Administration > Integrations > New Integration**.', 'Select **Outgoing Webhook**.', 'Set **Event Trigger** section to value **Message Sent**.', \"Fill out the details, including the channel you want the bot\\nlisten to. Optionally, it is possible to set the\\n**Trigger Words** section with `@yourbotname` so that the bot\\ndoesn't trigger on everything that is said.\", 'In the **URLs** section, set the URL to \\n`http://<host>:<port>/webhooks/rocketchat/webhook`, replacing\\nthe host and port with the appropriate values from your running Rasa server.']\n\nFor more information on the Rocket.Chat Webhooks, see the\n[Rocket.Chat Guide](https://docs.rocket.chat/use-rocket.chat/workspace-administration/integrations).",
                "Running on RocketChat": "Add the RocketChat credentials to your `credentials.yml`:\n\n```\nrocketchat:\n  user: \"yourbotname\"\n  password: \"YOUR_PASSWORD\"\n  server_url: \"https://demo.rocket.chat\"\n```\n\nRestart your Rasa server\nto make the new channel endpoint available for RocketChat to send messages to."
              },
              "metadata": {
                "id": "rocketchat",
                "sidebar_label": "RocketChat",
                "title": "RocketChat",
                "description": "Build a Rasa Chat Bot on Rocketchat"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 7]"
            },
            {
              "title": "Slack",
              "description": "Build a Rasa Chat Bot on Slack",
              "content": {
                "Sending Messages": "Create a new file `credentials.yml` in the root folder of your Rasa\nproject (if you've used `rasa init` this file should already exist and you can\njust edit it). Add the following lines to the file:\n\n```\nslack:\n  slack_channel: \"CA003L0XZ\"    # channel ID, not a channel name!\n  slack_token: \"xoxb-XXX\"       # token obtained in the next step\n  slack_signing_secret: \"YYY\"   # secret obtained in the next step\n```\n\nThe `slack_channel` can be a channel or an individual person that the bot should\nlisten to for communications, in addition to the default behavior of listening\nfor direct messages and app mentions, i.e. *@app_name*.\nTo get a channel id, right click on the channel in Slack and choose **Copy Link**.\nThe id will be the last component in the URL.\n\nIn the next couple steps, you'll create a Slack App to get the values for\n`slack_token` and `slack_signing_secret`:\n\n['To create the app go to [Your Apps](https://api.slack.com/apps \"The Your Apps section of your Slack interface\") and click\\non **Create New App**.', '<Image img={createAppImg} caption=\"Create New App\" alt=\"Create New Slack App Screenshot\" max-width=\"500px\"/>', \"Fill out your **App Name** and select the **Development Workspace** where\\nyou'll play around and build your app.\", 'Head over to **OAuth & Permissions** and scroll down to **Scopes**. Scopes give\\nyour app permission to do things in your workspace.', 'To get started, you should at least add the following scopes:', ['`app_mentions:read`,', '`channels:history`,', '`chat:write`,', '`groups:history`,', '`im:history`,', '`mpim:history` and', '`reactions:write`.'], '<Image img={scopesImg} caption=\"Set up Slack Permissions\" alt=\"Set up Slack Permissions Screenshot\" max-width=\"500px\" />', 'In Slacks API documentation you can find a\\n[list and explanation of all available scopes](https://api.slack.com/scopes).', 'On the **OAuth & Permissions** page, click **Install App to Workspace** to add\\nthe bot to your workspace.', '<Image img={installAppImg} caption=\"Install Slack App\" alt=\"Install Slack App Screenshot\" max-width=\"500px\"/>', \"Once added, Slack will show you a **Bot User OAuth Access Token** which you'll\\nneed to add to your `credentials.yml` as the value for `slack_token`:\", '```\\nslack:\\n  slack_channel: \"your-channel\" # choose a channel for your bot\\n  slack_token: \"xoxb-XXX\"       # token obtained in the next step\\n  slack_signing_secret: \"YYY\"   # secret obtained in the next step\\n```', 'The token should start with `xoxb`.', 'Head over to **Basic Information** to gather the **Signing Secret**.', '<Image img={secretImg} caption=\"Signing Secret\" alt=\"Signing Secret Screenshot\" max-width=\"500px\"/>', 'Copy the signing secret into your `credentials.yml` as the value for `slack_signing_secret`:', '```\\nslack:\\n  slack_channel: \"your-channel\" # choose a channel for your bot\\n  slack_token: \"xoxb-XXX\"       # token obtained in the next step\\n  slack_signing_secret: \"YYY\"   # secret obtained in the next step\\n```']\n\nThis setup will allow your bot to send messages. Now let's head over to the setup\nfor receiving and reacting to messages.",
                "Receiving Messages": "Before continuing, make sure you have configured a Slack App for [Sending Messages](./slack.mdx#sending-messages) and have added Slack credentials to your `credentials.yml` file.\n\nTo receive messages, you will need a publicly available URL for Slack to reach\nyour bot and tell you about new messages. If you are running locally, you can\n[test channels using ngrok](../messaging-and-voice-channels.mdx#testing-channels-on-your-local-machine)\n\n['To configure your bot to receive messages, your bot needs to be running.\\nStart your bot e.g. using', '```\\nrasa run\\n```', 'If you are running locally, make sure ngrok (or another tool to retrieve a public\\nurl) is running as well.', 'To send messages directly to your bot using the slack UI, head to **App Home**,\\nscroll to the bottom and select the checkbox for\\n`Allow users to send Slash commands and messages from the messages tab.`', 'You might have to quit the Slack app and re-open it before your changes take effect.', '<Image img={appHomeImg} caption=\"Messages Tab\" alt=\"Allow users to send Slash commands and messages from the messages tab\" max-width=\"500px\"/>', 'Configure the webhook by heading to **Event Subscriptions** and\\nturning **Enable Events** on.', 'As a request URL enter the public url of your bot and append `/webhooks/slack/webhook`, e.g.\\n`https://<host>/webhooks/slack/webhook` replacing `<host>` with your URL. If you\\nare using ngrok, your url should look like `https://92832de0.ngrok.io/webhooks/slack/webhook`.', \"You won't be able to use a `localhost` url.\", '<Image img={requestUrlImg} caption=\"Request URL\" alt=\"Request URL Screenshot\" max-width=\"500px\"/>', \"As a last step, you'll need to **Subscribe to bot events** on the same page.\\nYou'll need to add the following events:\", ['`message.channels`,', '`message.groups`,', '`message.im` and', '`message.mpim`.'], '<Image img={eventsImg} caption=\"Subscribe to Bot Events\" alt=\"Subscribe to Bot Events Screenshot\" max-width=\"500px\"/>', \"Make sure to hit **Save Changes** at the bottom of the page after you've added these events.\", \"(If you didn't grant all required permissions to your app while setting up\\nsending of messages, you'll be prompted to **reinstall your app** which you will\\nneed to do. Otherwise, Slack will confirm your change with a **Success!**)\"]\n\n:::note invite to channels\nAs per [Slack docs](https://api.slack.com/authentication/basics#calling), make\nsure you invite your bot to a channel it should be accessing. You can do this\nby using `/invite` in the channel.\n:::\n\nYour bot is now ready to go and will receive webhook notifications about new messages.",
                "Optional: Interactive Components": "After you've completed [Sending Messages](./slack.mdx#sending-messages) and\n[Receiving Messages](./slack.mdx#receiving-messages) your bot is ready to go. If you\nwant to use Slack's [interactive components](https://api.slack.com/block-kit/interactivity) (buttons or menus), you'll need to do\nsome additional configuration:\n\nOpen the **Interactivity & Shortcuts** page and toggle **Interactivity** to be on.\nAfterwards, you'll need to enter the same url into the **Request URL** field which\nyou used in Step 2 of [Receiving Messages](./slack.mdx#receiving-messages), e.g. `https://<host>/webhooks/slack/webhook`.\n\n<Image img={interactivityImg} caption=\"Enabling Interactivity\" alt=\"Interactivity Screenshot\" max-width=\"500px\"/>",
                "Additional Slack Options": "Here is a complete overview of all the configuration parameters for the Slack\nconnection:\n\n```\nslack:\n  slack_channel: \"CA003L0XZ\"    # channel ID, not a channel name!\n  slack_token: \"xoxb-XXX\"       # token obtained in the next step\n  slack_signing_secret: \"YYY\"   # secret obtained in the next step\n  proxy: \"http://myProxy.online\"  # Proxy Server to route your traffic through. This configuration is optional. Only HTTP proxies are supported\n  slack_retry_reason_header: \"x-slack-retry-reason\"  # Slack HTTP header name indicating reason that slack send retry request. This configuration is optional.\n  slack_retry_number_header: \"x-slack-retry-num\"  # Slack HTTP header name indicating the attempt number. This configuration is optional.\n  errors_ignore_retry: None  # Any error codes given by Slack included in this list will be ignored. Error codes are listed [here](https://api.slack.com/events-api#errors).\n  use_threads: False  # If set to True, bot responses will appear as a threaded message in Slack. This configuration is optional and set to False by default.\n  conversation_granularity: \"sender\" # sender allows 1 conversation per user (across channels), channel allows 1 conversation per user per channel, thread allows 1 conversation per user per thread. This configuration is optional and set to sender by default.\n```\n\nMake sure to restart your Rasa server after changing the\n`credentials.yml` for the changes to take effect."
              },
              "metadata": {
                "id": "slack",
                "sidebar_label": "Slack",
                "title": "Slack",
                "description": "Build a Rasa Chat Bot on Slack"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 8]"
            },
            {
              "title": "Telegram",
              "description": "Build a Rasa Chat Bot on Telegram",
              "content": {
                "Getting Credentials": "**How to get the Telegram credentials:**\nYou need to set up a Telegram bot.\n\n['To create the bot, go to [Bot Father](https://web.telegram.org/#/im?p=@BotFather),\\nenter `/newbot` and follow the instructions. The URL that Telegram should send messages to will look like\\n`http://<host>:<port>/webhooks/telegram/webhook`, replacing\\nthe host and port with the appropriate values from your running Rasa server.', 'At the end you should get your `access_token` and the username you\\nset will be your `verify`.', \"If you want to use your bot in a group setting, it's advisable to\\nturn on group privacy mode by entering `/setprivacy`. Then the bot\\nwill only listen when a user's message starts with `/bot`.\"]\n\nFor more information, check out the [Telegram HTTP API](https://core.telegram.org/bots/api).",
                "Running on Telegram": "Add the Telegram credentials to your `credentials.yml`:\n\n```\ntelegram:\n  access_token: \"490161424:AAGlRxinBRtKGb21_rlOEMtDFZMXBl6EC0o\"\n  verify: \"your_bot\"\n  webhook_url: \"https://your_url.com/webhooks/telegram/webhook\"\n```\n\nRestart your Rasa server\nto make the new channel endpoint available for Telegram to send messages to.\n\n:::note Handling `/start` message\n\nAt the beginning of a conversation, the user will press the 'Start' button in Telegram.\nThis will trigger a message with the content */start* to be sent.\nMake sure your bot can handle this intro message by designing a specific intent in the nlu training data file.\nThen add this `start` intent to the domain alongside a story or rule to handle it.\n\n:::",
                "Supported Response Attachments": "In addition to standard `text:` responses, this channel also supports the following components from the [Telegram API](https://core.telegram.org/bots/api/#message):\n\n['`button` arguments:', ['button_type: inline | vertical | reply'], '`custom` arguments:', ['photo', 'audio', 'document', 'sticker', 'video', 'video_note', 'animation', 'voice', 'media', 'latitude, longitude (location)', 'latitude, longitude, title, address (venue)', 'phone_number', 'game_short_name', 'action']]\n\nExamples:\n\n```\n\n  utter_ask_transfer_form_confirm:\n  - buttons:\n    - payload: /affirm\n      title: Yes\n    - payload: /deny\n      title: No, cancel the transaction\n    button_type: vertical\n    text: Would you like to transfer {currency}{amount_of_money} to {PERSON}?\n    image: \"https://i.imgur.com/nGF1K8f.jpg\"\n```\n\n```\n  utter_giraffe_sticker:\n  - text: Here's my giraffe sticker!\n    custom:\n      sticker: \"https://github.com/TelegramBots/book/raw/master/src/docs/sticker-fred.webp\"\n```"
              },
              "metadata": {
                "id": "telegram",
                "sidebar_label": "Telegram",
                "title": "Telegram",
                "description": "Build a Rasa Chat Bot on Telegram"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 9]"
            },
            {
              "title": "Twilio Voice",
              "description": "Deploy a Rasa IVR assistant via the Twilio Voice connector",
              "content": {
                "Running on Twilio": {
                  "Connect to a Twilio Phone Number": "To forward calls from Twilio to your Rasa assistant the webhook for your phone number needs to be updated. Go to the [Phone\nNumbers](https://www.twilio.com/console/phone-numbers/incoming) section of your Twilio account and select the phone\nnumber you want to connect to Rasa. Find the `Voice & Fax` section on the screen. Under the `A CALL COMES IN` section\nadd the webhook URL (e.g. `https://<host>:<port>/webhooks/twilio_voice/webhook`) for your Rasa bot replacing the host\nand port with the appropriate values for your deployment. Click `Save` at the bottom of the page.\n\n<Image img={twilioWebhook} caption=\"Set Twilio Webhook\" alt=\"Screenshot of Twilio Console to Set Webhook\" max-width=\"500px\"/>",
                  "Connect to a Twilio SIP Domain": "You can also connect your Twilio Voice channel to a Twilio SIP domain. You can follow the\n[Twilio Sending SIP](https://www.twilio.com/docs/voice/api/sending-sip) guide to forward SIP requests to your Twilio SIP\ndomain. Once your SIP domain is configured you will have to forward incoming calls to Rasa. In the Twilio console go to\nthe [SIP Domains](https://console.twilio.com/develop/voice/manage/sip-domains?frameUrl=/console/voice/sip/endpoints)\nsection. Select the SIP domain you would like to use. Find the `Call Control Configuration` section and add the\nwebhook URL (e.g. `https://<host>:<port>/webhooks/twilio_voice/webhook`) for your Rasa bot replacing the host and port\nwith the appropriate values for your deployment. Click `Save` at the bottom of the page.\n\n<Image img={twilioSipWebhook} caption=\"Set Twilio SIP Domain Webhook\" alt=\"Screenshot of Twilio Console to Set Webhook\" max-width=\"500px\"/>"
                },
                "Configure Channel in Rasa": "In your `credentials.yml` file make sure the `twilio_voice` channel is added. Within `credentials.yml` there are a\nnumber of parameters you can set to control the behavior of your assistant. An example with definitions of each\nparameter is below. Unlike the Twilio text channel there is no need to add your Twilio\ncredentials for the voice channel. Note, changing values for `enhanced` and `assistant_voice` can result in added costs\nfrom Twilio. Review the documentation below for details about these settings.\n\n```\ntwilio_voice:\n  initial_prompt: \"hello\"\n  assistant_voice: \"woman\"\n  reprompt_fallback_phrase: \"I didn't get that could you repeat?\"\n  speech_timeout: \"5\"\n  speech_model: \"default\"\n  enhanced: \"false\"\n```",
                "Parameter Definitions": {
                  "Initial Prompt": "When Twilio receives a new call and forwards this to Rasa, Twilio does not provide a user message. In this\ncase Rasa will act as if the user sent the message \"Hello\". This behavior can be configured in your\n`credentials.yml` file by setting the `initial_prompt` parameter to the desired input. The `initial_prompt` value will\nbe sent to Rasa and the response will be spoken to the user to start the voice conversation. How you greet\na user via a voice channel may differ from a text channel. You should review your responses and consider creating\n[channel-specific variations](../responses.mdx#channel-specific-response-variations) where\nnecessary.",
                  "Assistant Voice": "You can add personality to your assistant by specifying the type of voice your assistant should speak with. In the\n`credentials.yml` file you can add an option for `assistant_voice` to specify the type of voice of your assistant. For a\nlist of supported voices you can check the [Twilio documentation](https://www.twilio.com/docs/voice/twiml/say#voice). By\ndefault Twilio's `woman` voice will be used. Note that you will incur additional charges for using any of the `Polly`\nvoices. The Twilio documentation has details about\n[Polly pricing](https://www.twilio.com/docs/voice/twiml/say/text-speech#pricing).",
                  "Reprompt Fallback Phrase": "Unlike text channels where users can review previous messages in the conversation and take their time to\nreply, with voice channels users can get confused when there is a pause in the conversation. When a long pause is\ndetected during a conversation Rasa will repeat the last utterance from the bot to re-prompt the user for a response. If\nthe previous bot utterance cannot be identified then the message defined in `reprompt_fallback_phrase` will be sent. By\ndefault this is set to \"I'm sorry I didn't get that could you rephrase\".",
                  "Speech Timeout": "How long Twilio will collect speech from the caller. This parameter must be set to a number or \"auto\". If a number is\nprovided the assistant will collect speech for the specified amount of time. If `speech_timeout` is set to \"auto\" then\nTwilio will collect speech until a pause is detected. The default setting of this parameter is \"auto\". You can find more\ndetails about `speech_timeout` in the\n[Twilio documentation](https://www.twilio.com/docs/voice/twiml/gather#speechtimeout). Note that `speech_timeout=\"auto\"`\nis only compatible with `speech_model='numbers_and_commands'`. An error will be raised if an incompatible\n`speech_model` is used with the `speech_timeout`.",
                  "Speech Model": "Adjusting the `speech_model` parameter can help Twilio with the accuracy of its speech-to-text transformation. Valid\nvalues are `default`, `numbers_and_commands`, and `phone_call`. The default setting is \"default\". You can find more\ndetails about `speech_model` in the\n[Twilio documentation](https://www.twilio.com/docs/voice/twiml/gather#speechmodel).",
                  "Enhanced": "Setting `enhanced` to `true` will allow you to use Twilio's premium speech model to improve the accuracy of\ntranscription results. Note, setting this parameter to `true` will result in higher Twilio transcription costs. This\nparameter is only supported if you also have set the `speech_model` parameter to `phone_call`. By default `enhanced` is\nset to \"false\". You can find more about the `enhanced` speech model option in the\n[Twilio documentation](https://www.twilio.com/docs/voice/twiml/gather#enhanced)."
                },
                "Custom Voice Responses": "It is highly recommended that you provide voice\n[channel specific responses](../responses.mdx#channel-specific-response-variations) for all\nresponses containing images, emojis and/or abbreviations. Visual media like images and emojis do not translate well to\nvoice applications. Having voice alternatives to those responses lets you tailor the user experience to the voice\nchannel. If a response is detected as containing either an image or emoji a warning will be raised. The image or emoji\nwill be removed from the response and only any accompanying text will be included in the response back to the user.\n\nIn addition to reviewing responses containing images and emojis you should also review all other responses for their\napplicability to voice channels. Short-hand abbreviations like \"e.g.\" do not translate well. Voice responses should use\nthe long-hand versions of any abbreviations. Interactions with users can differ between text and voice channels.\nReviewing all responses in the scope of a voice channel and providing voice specific responses can make for a better\nuser experience."
              },
              "metadata": {
                "id": "twilio-voice",
                "sidebar_label": "Twilio Voice",
                "title": "Twilio Voice",
                "description": "Deploy a Rasa IVR assistant via the Twilio Voice connector"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 10]"
            },
            {
              "title": "Twilio",
              "description": "Deploy a Rasa assistant through text message or WhatsApp via the Twilio connector",
              "content": {
                "Getting Credentials": {
                  "Connecting to WhatsApp": "You can deploy a Rasa assistant to WhatsApp through Twilio. However, to do so, you have\nto have a [WhatsApp Business](https://www.whatsapp.com/business/) profile. Associate\nyour Whatsapp Business profile with the phone number you purchased through Twilio to\naccess the [Twilio API for WhatsApp](https://www.twilio.com/docs/whatsapp/api).\n\nAccording to the [Twilio API documentation](https://www.twilio.com/docs/whatsapp/api#using-phone-numbers-with-whatsapp),\nthe phone number you use should be prefixed with whatsapp: in the `credentials.yml` described below."
                },
                "Running on Twilio": {
                  "Receiving Location Data from Whatsapp with Twilio connector": "This is how you can receive location data (WhatsApp Location) from a user on this channel:\n\n['Create an intent named \"locationData\" and define two entities and slots for Latitude and Longitude respectively.']\n\n```\nintents:\n  - locationData\n\nslots:\n   Latitude:\n     type: text\n     mappings:\n     - type: from_entity\n       entity: Latitude\n\n   Longitude:\n     type: text\n     mappings:\n     - type: from_entity\n       entity: Longitude\n\nentities:\n  - Latitude\n  - Longitude\n\n```\n\n['When the user sends a location message, the locationData intent will be triggered and the slots will be set from the entities. Note that you do not need to provide training data for the entities as they are handled by the channel.']"
                }
              },
              "metadata": {
                "id": "twilio",
                "sidebar_label": "Twilio",
                "title": "Twilio",
                "description": "Deploy a Rasa assistant through text message or WhatsApp via the Twilio connector"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 11]"
            },
            {
              "title": "Your Own Website",
              "description": "Deploy and Run a Rasa Chat Bot on a Website",
              "content": {
                "REST Channels": {
                  "RestInput": {
                    "Request and Response Format": "After making the `rest` input channel available, you can `POST` messages to\n`http://<host>:<port>/webhooks/rest/webhook`, with the following format:\n\n```\n{\n  \"sender\": \"test_user\",  // sender ID of the user sending the message\n  \"message\": \"Hi there!\"\n}\n```\n\nThe response from Rasa will be a JSON body of bot responses, for example:\n\n```\n[\n  {\"text\": \"Hey Rasa!\"}, {\"image\": \"http://example.com/image.jpg\"}\n]\n```"
                  },
                  "CallbackInput": {
                    "Request and Response Format": "After making the `callback` input available, you can `POST` messages to\n`http://<host>:<port>/webhooks/callback/webhook` with the following format:\n\n```\n{\n  \"sender\": \"test_user\",  // sender ID of the user sending the message\n  \"message\": \"Hi there!\"\n}\n```\n\nIf successful, the response will be `success`. Once Rasa is ready to send a\nmessage to the user, it will call the `url` specified in your `credentials.yml` with a `POST`\nrequest containing a JSON body of the bot's responses:\n\n```\n[\n  {\"text\": \"Hey Rasa!\"}, {\"image\": \"http://example.com/image.jpg\"}\n]\n```"
                  }
                },
                "Websocket Channel": {
                  "JWT Authentication": "The SocketIO channel can be optionally configured to perform JWT authentication on connect\nby defining the `jwt_key` and optional `jwt_method` in the `credentials.yml` file. \n\n```\nsocketio:\n  user_message_evt: user_uttered\n  bot_message_evt: bot_uttered\n  session_persistence: true\n  jwt_key: my_public_key\n  jwt_method: HS256\n```\n\nWhen initially requesting the connection, the client should pass in an encoded payload\nas a JSON object under the key `token`:\n\n```\n{\n  \"token\": \"jwt_encoded_payload\"\n}\n```",
                  "Chat Widget": "Once you've set up your SocketIO channel, you can use the official Rasa Chat Widget on any webpage.\nJust paste the following into your site HTML and paste the URL of your Rasa instance into\nthe `data-websocket-url` attribute\n\n```\n<div id=\"rasa-chat-widget\" data-websocket-url=\"https://your-rasa-url-here/\"></div>\n<script src=\"https://unpkg.com/@rasahq/rasa-chat\" type=\"application/javascript\"></script>\n```\n\nFor more information, including how to fully customize the widget for your website, you can check out the [full documentation](https://chat-widget-docs.rasa.com/).\n\nAlternatively, if you want to embed the widget in a React app, there is\n[a library in the NPM package repository](https://www.npmjs.com/package/@rasahq/rasa-chat)."
                }
              },
              "metadata": {
                "id": "your-own-website",
                "sidebar_label": "Your Own Website",
                "title": "Your Own Website",
                "description": "Deploy and Run a Rasa Chat Bot on a Website"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 56, \"subpages\", 12]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 56]"
        },
        {
          "title": "Deploy",
          "description": "Documentation section: deploy",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/deploy"
          },
          "subpages": [
            {
              "title": "Deploy Rasa Action Server",
              "description": null,
              "content": {
                "Installation Requirements": [
                  "Check that you have installed the Kubernetes or OpenShift command line\ninterface (CLI). You can check this using the following command:",
                  "<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">",
                  "```\nkubectl version --short --client\n\n# The output should be similar to this\n# Client Version: v1.19.11\n```",
                  "</TabItem>\n<TabItem value=\"openshift\">",
                  "```\noc version --client\n\n# The output should be similar to this\n# Client Version: 4.7.13\n```",
                  "</TabItem>\n</Tabs>",
                  "If this command resulted in an error, please install the\n[Kubernetes CLI](https://kubernetes.io/docs/tasks/tools/install-kubectl/) or the\n[OpenShift CLI](https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli)\ndepending on the cluster you’re using.",
                  "Make sure that the Kubernetes / OpenShift CLI is correctly connected to\nyour cluster. You can do so by using the following commands:",
                  "<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">",
                  "```\nkubectl version --short\n\n# The output should be similar to this\n# Client Version: v1.19.11\n# Server Version: v1.19.10\n```",
                  "</TabItem>\n<TabItem value=\"openshift\">",
                  "```\noc version\n\n# The output should be similar to this\n# Client Version: 4.7.13\n# Kubernetes Version: v1.20.0+df9c838\n```",
                  "</TabItem>\n</Tabs>",
                  "If you get an error when executing the command, you are not connected to your\ncluster. To get the command to connect to the cluster please consult your cluster’s\nadmin or the documentation of your cloud provider.",
                  "Make sure you have the [Helm CLI](https://helm.sh/docs/intro/install/)\ninstalled. To check this, run:",
                  "```\nhelm version --short\n\n# The output should be similar to this\n# v3.6.0+g7f2df64\n```",
                  "If this command leads to an error, please install the\n[Helm CLI](https://helm.sh/docs/intro/install/).",
                  "In case you are using a version `<3.5` of Helm, please update to Helm version\n`>=3.5`."
                ],
                "1. Installation": {
                  "a. Create Namespace": "We recommend installing Rasa Action Server in a separate\n[namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)\nto avoid interfering with existing cluster deployments. To create a new namespace\nrun the following command:\n\n<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">\n\n```\nkubectl create namespace <your namespace>\n```\n\n</TabItem>\n<TabItem value=\"openshift\">\n\n```\noc create namespace <your namespace>\n```\n\n</TabItem>\n</Tabs>",
                  "b. Deploy Rasa Action Server": "Run the following commands:\n\n```\n# Add the repository which contains the Rasa Action Server Helm chart\nhelm repo add rasa https://helm.rasa.com\n\n# Deploy Rasa Action Server\nhelm install \\\n    --namespace <your namespace> \\\n    <release name> \\\n    rasa/rasa-action-server\n```",
                  "c. Access Rasa Action Server": "By default the Rasa Action Server deployment is exposed via the `rasa-action-server` (`<release name>`) service and accessible only within a Kubernetes cluster. You can get\nthe IP address using this command:\n\n<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">\n\n```\n  export SERVICE_PORT=$(kubectl get --namespace <your namespace> -o jsonpath=\"{.spec.ports[0].port}\" services <release name>)\n  kubectl port-forward --namespace <your namespace> svc/<release name> ${SERVICE_PORT}:${SERVICE_PORT} &\n```\n\n</TabItem>\n<TabItem value=\"openshift\">\n\n```\n  export SERVICE_PORT=$(oc get --namespace <your namespace> -o jsonpath=\"{.spec.ports[0].port}\" services <release name>)\n  oc port-forward --namespace <your namespace> svc/<release name> ${SERVICE_PORT}:${SERVICE_PORT} &\n```\n\n</TabItem>\n</Tabs>\n\nYou can then access the deployment on `http://127.0.0.1:${SERVICE_PORT}`\n\n:::note\nThe Rasa Action Server Helm chart uses the [rasa-x-demo](https://github.com/RasaHQ/rasa-x-demo) Docker image as default. In the [Building an Action Server Image](#building-an-action-server-image)\nyou can learn how to build and use your custom Action Server image.\n:::"
                },
                "Building an Action Server Image": {
                  "Automating your Action Server Image Builds": {
                    "Manually Building an Action Server": "To create your image:\n\n['Make sure your actions are defined in `actions/actions.py`. The `rasa/rasa-sdk`\\nimage will automatically look for the actions in this file.', 'If your actions have any extra dependencies, create a list of them in a file,\\n`actions/requirements-actions.txt`.', \"Create a file named `Dockerfile` in your project directory,\\nin which you'll extend the official SDK image, copy over your code, and add any custom dependencies (if necessary).\\nFor example:\", '<pre><code parentName=\"pre\" className=\"language-python\">\\n{`# Extend the official Rasa SDK image\\nFROM rasa/rasa-sdk:${variables.rasa_sdk_version}', 'Use subdirectory as working directory', 'WORKDIR /app', 'Copy any additional custom requirements, if necessary (uncomment next line)', 'COPY actions/requirements-actions.txt ./', 'Change back to root user to install dependencies', 'USER root', 'Install extra requirements for actions code, if necessary (uncomment next line)', 'RUN pip install -r requirements-actions.txt', 'Copy actions folder to working directory', 'COPY ./actions /app/actions', \"By best practices, don't run the code with root user\", 'USER 1001`}</code></pre>']\n\nYou can then build the image via the following command:\n\n```\ndocker build . -t <account_username>/<repository_name>:<custom_image_tag>\n```\n\nThe `<custom_image_tag>` should reference how this image will be different from others. For\nexample, you could version or date your tags, as well as create different tags that have different code for production\nand development servers. You should create a new tag any time you update your code and want to re-deploy it."
                  },
                  "Using your Custom Action Server Image": "If you're building this image to make it available from another server, you should push the image to a cloud repository.\n\nThis documentation assumes you are pushing your images to [DockerHub](https://hub.docker.com/).\nDockerHub will let you host multiple public repositories and\none private repository for free. Be sure to first [create an account](https://hub.docker.com/signup/)\nand [create a repository](https://hub.docker.com/signup/) to store your images. You could also push images to\na different Docker registry, such as [Google Container Registry](https://cloud.google.com/container-registry),\n[Amazon Elastic Container Registry](https://aws.amazon.com/ecr/), or\n[Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/).\n\nYou can push the image to DockerHub via:\n\n```\ndocker login --username <account_username> --password <account_password>\ndocker push <account_username>/<repository_name>:<custom_image_tag>\n```\n\nTo authenticate and push images to a different container registry, please refer to the documentation of\nyour chosen container registry."
                },
                "Setting a Custom Action Server Image": "In order to use a Custom Action Server image along with the Rasa Action Server deployment, you have to\nuse the following values for your deployment.\n\n```\n# values.yaml\nimage:\n  name: \"image_name\"\n  tag: \"image_tag\"\n```\n\nthen upgrade your deployment by executing the command:\n\n```\nhelm upgrade --namespace <namespace> --reuse-values \\\n  -f values.yaml <release name> rasa/rasa-action-server\n```",
                "2. Connect Rasa Action Server to a Rasa deployment": "If you have deployed your assistant using the Rasa Helm chart, and you have deployed your Rasa Action Server as well.\nNow it's time to connect them together. You can do this easily by following the steps:\n\na. Create a `rasa-values.yaml` file which will include configuration for the Rasa deployment.\n\n```\n# rasa-values.yaml\nrasa-action-server:\n  external:\n    # -- Determine if external URL is used\n    enabled: true\n    # -- URL to Rasa Action Server\n    url: \"http://rasa-action-server/webhook\"\n```\n\nThe configuration above tells Rasa which Rasa Action Server to use. In the example the `http://rasa-action-server/webhook` URL is used,\nthe URL indicates that Rasa Action Server is deployed with release name `rasa-action-server`, and is located in the same namespace as Rasa server deployment.\n\nb. Upgrade the Rasa deployment\n\n```\nhelm upgrade -n <namespace> --reuse-values -f rasa-values.yaml \\\n  <release name> rasa/rasa\n```"
              },
              "metadata": {
                "id": "deploy-action-server",
                "sidebar_label": "Deploy Action Server",
                "title": "Deploy Rasa Action Server",
                "abstract": "This page explains how to build an Action Server Image and deploy a Rasa Action Server using Helm. The second of three steps in deploying your Rasa assistant."
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 57, \"subpages\", 0]"
            },
            {
              "title": "Deploy Rasa Pro Services",
              "description": "Deploy Rasa Rasa Pro Services in production",
              "content": {
                "Rasa Pro Services Setup": {
                  "Prerequisites": {
                    "System Requirements": "The minimum hardware requirements include information about the\nrequirements you need to install and use Rasa Pro Services. Hardware\nrequirements are dependent on the average number of conversations and\nexpected workload. Your exact needs may be more, depending on your workload.\n\nThe following is the recommended minimum hardware guidance:\n\n['CPU: 2 vCPU', 'Memory: 4 GB', 'Disk space: 10 GB']\n\nThese requirements correspond to the `t3.medium` instance type on AWS.",
                    "License": "Running Rasa Pro Services requires a valid license. You need to supply the\nlicense as an environment variable to the service."
                  },
                  "Installation and Configuration": "Rasa Plus streams data to Kafka which gets consumed by Rasa Pro Services.\nThese different services need to be run and configured to be able to communicate.\n\n['Configure Rasa Plus to stream data to Kafka. The configuration for\\nKafka should be put in a configuration\\nfile `endpoints.yml`:', '```\\nevent_broker:\\n  type: kafka\\n  partition_by_sender: True\\n  topic: rasa-events\\n  url: <BROKER URL>\\n```', 'The `<BROKER_URL>` needs to be replaced with an address of a bootstrap\\nserver of your Kafka cluster. Configuration examples for different\\nsecurity protocols and other parameters can be found in the\\n[Kafka Event Broker](../event-brokers.mdx#kafka-event-broker)\\ndocumentation.', 'Rasa Pro Services is a docker image. Authenticate to pull\\nthe docker images:', '```\\ngcloud auth configure-docker europe-west3-docker.pkg.dev\\n```', 'Pull the Rasa Pro Services image to your machine:', '```\\ndocker pull europe-west3-docker.pkg.dev/rasa-releases/rasa-pro/rasa-pro\\n```', ':::note', 'For a more complete walkthrough on how to manage the authentication\\ncredentials and licenses in a production environment, see the\\n[Rasa Plus Deployment Guide](/docs/rasa/deploy/deploy-rasa#4-deploy-rasa-assistant) (select the **Rasa Plus** tab).', ':::', 'Run the Rasa Pro Services docker image and configure it to receive data\\nfrom your Rasa Plus assistant connected to Kafka:', '```\\ndocker run \\\\\\n    -e RASA_PRO_LICENSE=<your_license> \\\\\\n    -e KAFKA_BROKER_ADDRESS=<BROKER_URL> \\\\\\n    -e KAFKA_TOPIC=rasa-events \\\\\\n    europe-west3-docker.pkg.dev/rasa-releases/rasa-pro/rasa-pro\\n```', 'For production deployments, we recommend to us\\n[Amazon Elastic Container Service](https://docs.docker.com/cloud/ecs-integration/).']",
                  "Docker Container Configuration (Reference)": "The Rasa Pro Services docker container supports configuration through\nseveral environment variables. The following table lists the available\nenvironment variables:\n\n| Environment Variable      | Description                                                                                                           | Default            |\n| :------------------------ | :-------------------------------------------------------------------------------------------------------------------- | :----------------- |\n| `RASA_PRO_LICENSE`        | **Required**. The license key for Rasa Pro Services.                                                                  |                    |\n| `KAFKA_BROKER_ADDRESS`    | **Required**. The address of the Kafka broker.                                                                        |                    |\n| `KAFKA_TOPIC`             | **Required**. The topic Rasa Plus publishes events to and Rasa Pro consumes from.                                     | `rasa_core_events` |\n| `LOGGING_LEVEL`           | Set the log level of the application. Valid levels are DEBUG, INFO, WARNING, ERROR, CRITICAL. (Available from 3.0.2)  | `INFO`             |\n| `RASA_ANALYTICS_DB_URL`   | The URL of the data lake to store analytics data in.                                                                  |                    |\n| `KAFKA_SASL_MECHANISM`    | The SASL mechanism to use for authentication. Valid mechanisms are                                                    | `PLAIN`            |\n| `KAFKA_SASL_USERNAME`     | The username for SASL authentication.                                                                                 |                    |\n| `KAFKA_SASL_PASSWORD`     | The password for SASL authentication.                                                                                 |                    |\n| `KAFKA_SECURITY_PROTOCOL` | The security protocol to use for communication with Kafka. Supported mechanisms are `PLAINTEXT` and `SASL_PLAINTEXT`. | `PLAINTEXT`        |\n| `KAFKA_SSL_CA_LOCATION`   | The filepath for SSL CA Certificate that will be used to connect with Kafka (Available from `3.1.0b1`)                |                    |",
                  "Healthcheck Endpoint": "Rasa Pro Services exposes a health check endpoint at `/healthcheck`.\nThe endpoint will return a `200` status code if the service is healthy.\nIf any other status code is returned or if the endpoint is not reachable\nthe service is unhealthy.\n\nExample response for `/healthcheck`:\n\n```\n{\n  \"details\": {\n    \"analytics-consumer\": {\n      \"alive\": 1,\n      \"isHealthy\": true\n    }\n  },\n  \"isHealthy\": true\n}\n```",
                  "Connect to a secured Kafka instance": "The connection to a secured Kafka instance can be configured by setting the\nfollowing environment variables on the Rasa Pro docker container:\n`KAFKA_SASL_MECHANISM`, `KAFKA_SASL_USERNAME`, `KAFKA_SASL_PASSWORD` and\n`KAFKA_SECURITY_PROTOCOL`. A detailed description of the parameters can\nbe found in the\n[environment variable reference](#docker-container-configuration-reference).\n\nThe environment variables need to be configured upon startup of the Rasa Pro\ndocker container.\n\nUsing Kafka `truststores` and `keystores` is currently not supported.",
                  "Upgrading Versions": "To upgrade to the latest version of Rasa Pro Services, you must follow these steps:\n\n['Read the changelog documentation on breaking changes.', 'Download a new docker container and run it.']\n\n:::note Container Start-up\nNote that the container might take some time to start-up as it is running\ndatabase schema migrations as part of its start-up.\n\nIf the migrations have failed, the container will be shut down.\n:::"
                }
              },
              "metadata": {
                "id": "deploy-rasa-pro-services",
                "sidebar_label": "Deploy Rasa Pro Services",
                "title": "Deploy Rasa Pro Services",
                "description": "Deploy Rasa Rasa Pro Services in production"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 57, \"subpages\", 1]"
            },
            {
              "title": "Deploy Rasa",
              "description": "Deploy a Rasa assistant on Kubernetes/Openshift using Helm",
              "content": {
                "Installation Requirements": [
                  "Check that you have installed the Kubernetes or OpenShift command line\ninterface (CLI). You can check this using the following command:",
                  "<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">",
                  "```\nkubectl version --short --client\n\n# The output should be similar to this\n# Client Version: v1.19.11\n```",
                  "</TabItem>\n<TabItem value=\"openshift\">",
                  "```\noc version --client\n\n# The output should be similar to this\n# Client Version: 4.7.13\n```",
                  "</TabItem>\n</Tabs>",
                  "If this command resulted in an error, please install the\n[Kubernetes CLI](https://kubernetes.io/docs/tasks/tools/install-kubectl/) or the\n[OpenShift CLI](https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli)\ndepending on the cluster you’re using.",
                  "Make sure that the Kubernetes / OpenShift CLI is correctly connected to\nyour cluster. You can do so by using the following commands:",
                  "<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">",
                  "```\nkubectl version --short\n\n# The output should be similar to this\n# Client Version: v1.19.11\n# Server Version: v1.19.10\n```",
                  "</TabItem>\n<TabItem value=\"openshift\">",
                  "```\noc version\n\n# The output should be similar to this\n# Client Version: 4.7.13\n# Kubernetes Version: v1.20.0+df9c838\n```",
                  "</TabItem>\n</Tabs>",
                  "If you get an error when executing the command, you are not connected to your\ncluster. To get the command to connect to the cluster please consult your cluster’s\nadmin or the documentation of your cloud provider.",
                  "Make sure you have the [Helm CLI](https://helm.sh/docs/intro/install/)\ninstalled. To check this, run:",
                  "```\nhelm version --short\n\n# The output should be similar to this\n# v3.6.0+g7f2df64\n```",
                  "If this command leads to an error, please install the\n[Helm CLI](https://helm.sh/docs/intro/install/).",
                  "In case you are using a version `<3.5` of Helm, please update to Helm version\n`>=3.5`."
                ],
                "Installation": {
                  "1. Create Namespace": "We recommend installing Rasa in a separate\n[namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)\nto avoid interfering with existing cluster deployments. To create a new namespace\nrun the following command:\n\n<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">\n\n```\nkubectl create namespace <your namespace>\n```\n\n</TabItem>\n<TabItem value=\"openshift\">\n\n```\noc create namespace <your namespace>\n```\n\n</TabItem>\n</Tabs>",
                  "2. Create Values File": "Prepare an empty file called `rasa-values.yml` which will include all your custom\nconfiguration for the installation with Helm.\n\nAll available values you can find in [the Rasa Helm Chart repository](https://github.com/RasaHQ/helm-charts/tree/main/charts/rasa#values).\n\n:::note\nThe default configuration of the Rasa chart deploys a Rasa Server, downloads a model, and serves the downloaded model.\nVisit [the Rasa Helm Chart repository](https://github.com/RasaHQ/helm-charts/tree/main/charts/rasa#quick-start) to check out more examples of configuration.\n\n:::",
                  "3. Loading an initial model": "The first time you install Rasa, you may not have a model server available yet, or you may want a lightweight model for testing the deployment.\nFor this purpose, you can choose between training or downloading an initial model. By default, the Rasa chart downloads an example model from GitHub.\nTo use this option, you don't have to change anything.\n\nIf you want to define an existing model to download from a URL you define instead, update your `rasa-values.yaml` with the URL according to the following configuration:\n\n```\napplicationSettings:\n  initialModel: \"https://github.com/RasaHQ/rasa-x-demo/blob/master/models/model.tar.gz?raw=true\"\n```\n\n:::note\nThe URL for the initial model download has to point to a tar.gz file and must not require authentication.\n\n:::\n\nIf you want to train an initial model you can do this by setting the `applicationSettings.trainInitialModel` to `true`.\nIt creates a init container that trains a model based on data located in the `/app` directory. If the `/app` directory is empty it creates a new project.\nYou can find an example that shows how to download data files from a git repository and train an initial model in the Rasa Helm Charts [examples](https://github.com/RasaHQ/helm-charts/blob/main/examples/rasa/train-model-helmfile.yaml).",
                  "4. Deploy Rasa Assistant": {
                    "Security Patch Releases": "Beginning with Rasa Plus 3.4.0, we release a new Docker image for supported versions each day at 0800 UTC. These images have OS security patches applied and include the date of generation in their tag in the format `YYYYMMDD`. \nFor example, to use the 3.4.2 image generated on the 1st of February 2023, use the tag `3.4.2-20230201`. These images can optionally be used as a drop-in replacement for the same version of Rasa Plus, but with all OS security updates up to that date applied.",
                    "Step 1: image pull secret": "You will need to create a pull secret to access the `rasa-plus` image. You should have\nbeen provided with a JSON credentials file to access the registry.\nThe file content should look similar to this:\n\n```\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"rasa-platform\",\n  \"private_key_id\": \"somerandomcharacters\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n\\nPBTu1lAJDLo136ZGTdMKi+/TuRqrIMg/..................................................................................................................................\\nsjAsuAH4Iz1XdfdenzGnyBZH\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"company@rasa-platform.iam.gserviceaccount.com\",\n  \"client_id\": \"12345678910123\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/company%40rasa-platform.iam.gserviceaccount.com\"\n}\n```\n\nCreate the pull secret by running the corresponding command for your cluster environment:\n\n```\n# kubernetes\nkubectl --namespace <your namespace> \\\n    create secret docker-registry rasa-plus-pull-secret \\\n    --docker-server=https://europe-west3-docker.pkg.dev \\\n    --docker-email=company@rasa-platform.iam.gserviceaccount.com \\ # this value needs to be updated with correct client email\n    --docker-username=_json_key \\\n    --docker-password=\"$(cat <your-json-registry-creds-file>.json)\"\n```\n\n```\n# openshift\noc --namespace <your namespace> \\\n    create secret docker-registry rasa-plus-pull-secret \\\n    --docker-server=https://europe-west3-docker.pkg.dev \\\n    --docker-email=company@rasa-platform.iam.gserviceaccount.com \\ # this value needs to be updated with correct client email\n    --docker-username=_json_key \\\n    --docker-password=\"$(cat <your-json-registry-creds-file>.json)\"\n```",
                    "Step 2: store license in a secret": "Step a: Save the license string to a file, for example:\n\n```\necho \"<rasaplus-license-key-you-were-emailed>\" > license.txt\n```\n\nStep b: Create a secret to hold the license key:\n\n```\nkubectl -n <your-namespace> create secret generic rasapro-license --from-file=licensekey=./license.txt\n```\n\nStep c: Remove the file on disk, for example:\n\n```\n/bin/rm license.txt\n```",
                    "Step 3: update `values.yml`": "For the [Rasa Helm Chart](https://github.com/RasaHQ/helm-charts/tree/main/charts/rasa), update as follows:\n\n```\nregistry: europe-west3-docker.pkg.dev/rasa-releases/rasa-plus/rasa-plus\ncommand:\n  - rasa\nimage:\n  name: rasa-plus\n  tag: 0.1.1\n  pullSecrets:\n    - name: rasa-plus-pull-secret\nextraEnv:\n  - name: RASA_PRO_LICENSE\n    valueFrom:\n      secretKeyRef:\n        name: rasapro-license\n        key: licensekey\n```\n\nFor the [Rasa X/Enterprise Helm Chart](https://github.com/RasaHQ/rasa-x-helm), update as follows:\n\n```\nimages:\n  imagePullSecrets:\n    - name: rasa-plus-pull-secret\nrasa:\n  command:\n  - rasa\n  name: europe-west3-docker.pkg.dev/rasa-releases/rasa-plus/rasa-plus\n  tag: 0.1.1\n  extraEnv:\n    - name: RASA_PRO_LICENSE\n      valueFrom:\n        secretKeyRef:\n          name: rasapro-license\n          key: licensekey\n```\n\n</TabItem>\n</Tabs>\n\n:::note\n**OpenShift only**: If the deployment fails and `oc get events` returns\n`1001 is not an allowed group spec.containers[0].securityContext.securityContext.runAsUser`,\nre-run the installation command with the following values:\n\n```\npostgresql:\n  volumePermissions:\n    securityContext:\n      runAsUser: \"auto\"\n  securityContext:\n    enabled: false\n  shmVolume:\n    chmod:\n      enabled: false\nnginx:\n  image:\n    name: nginxinc/nginx-unprivileged\n    port: 8080\n```\n\nThen wait until the deployment is ready. If you want to check on its status, the following command\nwill block until the Rasa deployment is ready:\n\n<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">\n\n```\nkubectl --namespace <your namespace> \\\n    wait \\\n    --for=condition=available \\\n    --timeout=20m \\\n    --selector app.kubernetes.io/instance=<release name> \\\n    deployment\n```\n\n</TabItem>\n<TabItem value=\"openshift\">\n\n```\noc --namespace <your namespace> \\\n    wait \\\n    --for=condition=available \\\n    --timeout=20m \\\n    --selector app.kubernetes.io/instance=<release name> \\\n    deployment\n```\n\n</TabItem>\n</Tabs>\n\n:::"
                  },
                  "5. Access Rasa Assistant": "By default the Rasa deployment is exposed via the `rasa` (`<release name>`) service and accessible only within a Kubernetes cluster.\nTo access Rasa Assistant by using `kubectl port-forward`, use these commands:\n\n<Tabs groupId=\"kubernetes-dist\" values={[{\"label\": \"Kubernetes\", \"value\": \"kubernetes\"}, {\"label\": \"OpenShift\", \"value\": \"openshift\"}]} defaultValue=\"kubernetes\">\n<TabItem value=\"kubernetes\">\n\n```\n  export SERVICE_PORT=$(kubectl get --namespace <your namespace> -o jsonpath=\"{.spec.ports[0].port}\" services <release name>)\n  kubectl port-forward --namespace <your namespace> svc/<release name> ${SERVICE_PORT}:${SERVICE_PORT} &\n```\n\n</TabItem>\n<TabItem value=\"openshift\">\n\n```\n  export SERVICE_PORT=$(oc get --namespace <your namespace> -o jsonpath=\"{.spec.ports[0].port}\" services <release name>)\n  oc port-forward --namespace <your namespace> svc/<release name> ${SERVICE_PORT}:${SERVICE_PORT} &\n```\n\n</TabItem>\n</Tabs>\n\nYou can then access the deployment on `http://127.0.0.1:${SERVICE_PORT}`\n\nThe other option is to expose your deployment on `NodePort` and access it directly.\n\n['Prepare configuration that switch the rasa service to `NodePort`.']\n\n```\n# rasa-values.yaml\nservice:\n  type: \"NodePort\"\n```\n\n['Upgrade deployment.']\n\n```\nhelm upgrade --namespace <NAMESPACE> --reuse-values -f rasa-values.yaml <RELEASE NAME> rasa/rasa\n```\n\n['Get the node port and address for the rasa service']\n\n```\nexport NODE_PORT=$(kubectl get --namespace <NAMESPACE> -o jsonpath=\"{.spec.ports[0].nodePort}\" services <RELEASE NAME>)\n\n$ curl http://127.0.0.1:${NODE_PORT}\nHello from Rasa: 2.8.7\n```\n\nVisit [the Rasa Helm Chart README](https://github.com/RasaHQ/helm-charts/tree/main/charts/rasa#exposing-the-rasa-deployment-to-the-public) to learn other ways to expose your deployment."
                },
                "Next Steps": [
                  "Visit [the Rasa Helm Chart repository](https://github.com/RasaHQ/helm-charts/tree/main/charts/rasa) where you can find examples of configuration"
                ]
              },
              "metadata": {
                "id": "deploy-rasa",
                "sidebar_label": "Deploy Rasa",
                "title": "Deploy Rasa",
                "description": "Deploy a Rasa assistant on Kubernetes/Openshift using Helm",
                "abstract": "This page explains how to deploy Rasa Open Source and Rasa Plus using Helm."
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 57, \"subpages\", 2]"
            },
            {
              "title": "Deploying a Rasa Assistant",
              "description": "How to deploy your Rasa Assistant with Kubernetes/Openshift",
              "content": {
                "When to Deploy Your Assistant": "The best time to deploy your assistant and make it available to test users is once it can handle the most\nimportant happy paths or is what we call a [minimum viable assistant](../glossary.mdx). Then you can use incoming\nconversations to inform further development of your assistant.",
                "Recommended Deployment Method": {
                  "Cluster Requirements": "To install the Rasa Helm Chart, you need an existing\n[Kubernetes cluster](https://kubernetes.io/) or [OpenShift cluster](https://www.openshift.com/).\nIf you don't have one yet, you can get a managed cluster from a cloud provider like:\n\n['[Google Cloud](https://cloud.google.com/kubernetes-engine),', '[DigitalOcean](https://www.digitalocean.com/products/kubernetes/),', '[Microsoft Azure](https://azure.microsoft.com/en-us/services/kubernetes-service/), or', '[Amazon EKS](https://aws.amazon.com/eks/).']"
                },
                "Alternative Deployment Methods": "The following deployment methods are not suited to a production deployment, but can be useful for development and testing:\n\n['[Running an assistant locally on the command line](../command-line-interface.mdx#rasa-run)', '[Developing an assistant in a Docker container](../docker/building-in-docker.mdx)', '[Deploying an assistant with Docker Compose](../docker/deploying-in-docker-compose.mdx)']"
              },
              "metadata": {
                "id": "introduction",
                "sidebar_label": "Introduction",
                "title": "Deploying a Rasa Assistant",
                "description": "How to deploy your Rasa Assistant with Kubernetes/Openshift",
                "abstract": "This section explains when and how to deploy an assistant built with Rasa. It will allow you to make your assistant available to users and set you up with a production-ready environment."
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 57, \"subpages\", 3]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 57]"
        },
        {
          "title": "Docker",
          "description": "Documentation section: docker",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/docker"
          },
          "subpages": [
            {
              "title": "Building a Rasa Assistant in Docker",
              "description": "Learn how to build a Rasa assistant in Docker.",
              "content": {
                "Installing Docker": "If you're not sure if you have Docker installed, you can check by running:\n\n```\ndocker -v\n# Docker version 18.09.2, build 6247962\n```\n\nIf Docker is installed on your machine, the output should show you your installed\nversions of Docker. If the command doesn't work, you'll have to install Docker.\nSee [Docker Installation](https://docs.docker.com/install/) for details.",
                "Setting up your Rasa Project": "Just like starting a project from scratch, you'll use the `rasa init` command to create a project.\nThe only difference is that you'll be running Rasa inside a Docker container, using\nthe image `rasa/rasa`. To initialize your project, run:\n\n<pre><code parentName=\"pre\" className=\"language-bash\">\n{`docker run -v $(pwd):/app rasa/rasa:${variables.release}-full init --no-prompt`}</code></pre>\n\nWhat does this command mean?\n\n<ul>\n  <li><inlineCode>-v $(pwd):/app</inlineCode> mounts your current working directory to the working directory\n    in the Docker container. This means that files you create on your computer will be\n    visible inside the container, and files created in the container will\n    get synced back to your computer.</li>\n  <li><inlineCode>rasa/rasa</inlineCode> is the name of the docker image to run. '{variables.release}-full' is the name of the tag,\n    which specifies the version and dependencies.</li>\n\n  <li>the Docker image has the <inlineCode>rasa</inlineCode> command as its entrypoint, which means you don't\n    have to type <inlineCode>rasa init</inlineCode>, just <inlineCode>init</inlineCode> is enough.</li>\n</ul>\n\nRunning this command will produce a lot of output. What happens is:\n\n['a Rasa project is created', \"an initial model is trained using the project's training data.\"]\n\nTo check that the command completed correctly, look at the contents of your working directory:\n\n```\nls -1\n```\n\nThe initial project files should all be there, as well as a `models` directory that contains your trained model.\n\n:::note\nIf you run into permission errors, it may be because the `rasa/rasa` images\nrun as user `1001` as a best practice, to avoid giving the container `root` permissions.\nHence, all files created by these containers will be owned by user `1001`. See the [Docker documentation](https://docs.docker.com/reference/cli/docker/container/run/)\nif you want to run the containers as a different user.\n\n:::",
                "Talking to Your Assistant": "To talk to your newly-trained assistant, run this command:\n\n<pre><code parentName=\"pre\" className=\"language-bash\">\n{`docker run -it -v $(pwd):/app rasa/rasa:${variables.release}-full shell`}</code></pre>\n\nThis will start a shell where you can chat to your assistant.\nNote that this command includes the flags `-it`, which means that you are running\nDocker interactively, and you are able to give input via the command line.\nFor commands which require interactive input, like `rasa shell` and `rasa interactive`,\nyou need to pass the `-it` flags.",
                "Training a Model": "If you edit any training data or edit the `config.yml` file, you'll need to\nretrain your Rasa model. You can do so by running:\n\n<pre><code parentName=\"pre\" className=\"language-bash\">\n{`docker run -v $(pwd):/app rasa/rasa:${variables.release}-full train --domain domain.yml --data data --out models`}</code></pre>\n\nHere's what's happening in that command:\n\n<ul>\n  <li><inlineCode>-v $(pwd):/app</inlineCode>: Mounts your project directory into the Docker\n  container so that Rasa can train a model on your training data</li>\n\n  <li>rasa/rasa:{variables.release}-full: Use the Rasa image with the tag '{variables.release}-full'</li>\n\n  <li><inlineCode>train</inlineCode>: Execute the <inlineCode>rasa train</inlineCode> command within the container. For more\n  information see <a href={useBaseUrl(\"/command-line-interface\")}>Command Line Interface</a>.</li>\n</ul>\n\nIn this case, we've also passed values for the location of the domain file, training\ndata, and the models output directory to show how these can be customized.\nYou can also leave these out, since we are passing the default values.",
                "Customizing your Model": {
                  "Choosing a Tag": "<p>\n  All <inlineCode>rasa/rasa</inlineCode> image tags start with a version number. The current version is  {variables.release}. The tags are:\n</p>\n\n['`{version}`', '`{version}-full`', '`{version}-spacy-en`', '`{version}-spacy-de`', '`{version}-spacy-it`', '`{version}-mitie-en`']\n\nThe `{version}-full` tag includes all possible pipeline dependencies, allowing you to change your `config.yml`\nas you like without worrying about missing dependencies. The plain `{version}` tag includes all the\ndependencies you need to run the default pipeline created by `rasa init`.\n\nTo keep images as small as possible, we also publish different tags of the `rasa/rasa` image\nwith different dependencies installed. See [Additional Dependencies](../installation/installing-rasa-open-source.mdx#additional-dependencies) for more dependency information\nspecific to your pipeline. For example, if you are using components with pre-trained word vectors from spaCy or\nMITIE, you should choose the corresponding tag.\n\nIf your model has a dependency that is not included in any of the tags (for example, a different spaCy language model),\nyou can build a docker image that extends the `rasa/rasa` image.\n\n:::note\nYou can see a list of all the versions and tags of the Rasa\nDocker image on [DockerHub](https://hub.docker.com/r/rasa/rasa/).\n\n:::\n\n:::caution\nThe `latest` tags correspond to the build of the latest stable version.\n\n:::",
                  "Adding Custom Components": "If you are using a custom NLU component or policy in your `config.yml`, you have to add the module file to your\nDocker container. You can do this by either mounting the file or by including it in your\nown custom image (e.g. if the custom component or policy has extra dependencies). Make sure\nthat your module is in the Python module search path by setting the\nenvironment variable `PYTHONPATH=$PYTHONPATH:<directory of your module>`.",
                  "Adding Custom Actions": "To create more sophisticated assistants, you will want to use [Custom Actions](../actions.mdx#custom-actions).\nContinuing the example from above, you might want to add an action which tells\nthe user a joke to cheer them up.\n\nBuild a custom action using the Rasa SDK by editing `actions/actions.py`, for example:\n\n```\nimport requests\nimport json\nfrom rasa_sdk import Action\n\n\nclass ActionJoke(Action):\n  def name(self):\n    return \"action_joke\"\n\n  def run(self, dispatcher, tracker, domain):\n    request = requests.get('http://api.icndb.com/jokes/random').json()  # make an api call\n    joke = request['value']['joke']  # extract a joke from returned json response\n    dispatcher.utter_message(text=joke)  # send the message back to the user\n    return []\n```\n\nIn `data/stories.yml`, replace `utter_cheer_up` in with the custom action `action_joke`\ntell your bot to use this new action.\n\nIn `domain.yml`, add a section for custom actions, including your new action:\n\n```\nactions:\n  - action_joke\n```\n\nAfter updating your domain and stories, you have to retrain your model:\n\n<pre><code parentName=\"pre\" className=\"language-bash\">\n{`docker run -v $(pwd):/app rasa/rasa:${variables.release}-full train`}</code></pre>\n\nYour actions will run on a separate server from your Rasa server. First create a network to connect the two containers:\n\n```\ndocker network create my-project\n```\n\nYou can then run the actions with the following command:\n\n<pre><code parentName=\"pre\" className=\"language-bash\">\n{`docker run -d -v $(pwd)/actions:/app/actions --net my-project --name action-server rasa/rasa-sdk:${variables.rasa_sdk_version}`}</code></pre>\n\nHere's what's happening in that command:\n\n['`-d`: Runs the container in detached mode so that you can run the rasa container in the same window.', '`-v $(pwd):/app`: Mounts your project directory into the Docker\\ncontainer so that the action server can run the code in the `actions` folder', '`net my-project`: Run the server on a specific network so that the rasa container can find it', '`--name action-server`: Gives the server a specific name for the rasa server to reference', '<code>rasa/rasa-sdk:{variables.rasa_sdk_version}</code> : Uses the Rasa SDK image with the tag {`${variables.rasa_sdk_version}`}']\n\nBecause the action server is running in detached mode, if you want to stop the container,\ndo it with `docker stop action-server`. You can also run `docker ps` at any time to see all\nof your currently running containers.\n\nTo instruct the Rasa server to use the action server, you have to tell Rasa its location.\nAdd this endpoint to your `endpoints.yml`, referencing the `--name` you gave the server\n(in this example, `action-server`):\n\n```\naction_endpoint:\n  url: \"http://action-server:5055/webhook\"\n```\n\nNow you can talk to your bot again via the `shell` command:\n\n<pre><code parentName=\"pre\" className=\"language-bash\">\n{`docker run -it -v $(pwd):/app -p 5005:5005 --net my-project rasa/rasa:${variables.release}-full shell`}</code></pre>\n\n:::note\nIf you stop and restart the `action-server` container, you might see an error like this:\n\n```\ndocker: Error response from daemon: Conflict. The container name \"/action-server\" is\nalready in use by container \"f7ffc625e81ad4ad54cf8704e6ad85123c71781ca0a8e4b862f41c5796c33530\".\nYou have to remove (or rename) that container to be able to reuse that name.\n```\n\nIf that happens, it means you have a (stopped) container with the name already. You can remove it via:\n\n```\ndocker rm action-server\n```\n\n:::"
                },
                "Deploying your Assistant": "Work on your bot until you have a minimum viable assistant that can handle your happy paths. After\nthat, you'll want to deploy your model to get feedback from real test users. To do so, you can deploy the\nmodel you created via one of our [recommended deployment methods](../deploy/introduction.mdx#recommended-deployment-method)."
              },
              "metadata": {
                "id": "building-in-docker",
                "sidebar_label": "Building a Rasa Assistant in Docker",
                "title": "Building a Rasa Assistant in Docker",
                "description": "Learn how to build a Rasa assistant in Docker."
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 58, \"subpages\", 0]"
            },
            {
              "title": "Deploying a Rasa Assistant in Docker Compose",
              "description": "Use Docker Compose to deploy a Rasa assistant",
              "content": {
                "Installing Docker": "If you're not sure if you have Docker installed, you can check by running:\n\n```\ndocker -v && docker-compose -v\n# Docker version 18.09.2, build 6247962\n# docker-compose version 1.23.2, build 1110ad01\n```\n\nIf Docker is installed on your machine, the output should show you your installed\nversions of Docker and Docker Compose. If the command doesn't work, you'll have to\ninstall Docker.\nSee [Docker Installation](https://docs.docker.com/install/) for details.",
                "Configuring Channels": "To run your AI assistant in production, don't forget to configure your required\n[Messaging and Voice Channels](../messaging-and-voice-channels.mdx) in `credentials.yml`. For example, to add a\nREST channel, uncomment this section in the `credentials.yml`:\n\n```\nrest:\n  # you don't need to provide anything here - this channel doesn't\n  # require any credentials\n```\n\nThe REST channel will open your bot up to incoming requests at the `/webhooks/rest/webhook` endpoint.",
                "Using Docker Compose to Run Multiple Services": "Docker Compose provides an easy way to run multiple containers together without\nhaving to run multiple commands or configure networks. This is essential when you\nwant to deploy an assistant that also has an action server.\n\nStart by creating a file called `docker-compose.yml`:\n\n```\ntouch docker-compose.yml\n```\n\nAdd the following content to the file:\n\n<pre><code parentName=\"pre\" className=\"language-yaml\">\n{`version: '3.0'\nservices:\n  rasa:\n    image: rasa/rasa:${variables.release}-full\n    ports:\n      - 5005:5005\n    volumes:\n      - ./:/app\n    command:\n      - run`}</code></pre>\n\nThe file starts with the version of the Docker Compose specification that you\nwant to use.\nEach container is declared as a `service` within the `docker-compose.yml`.\nThe first service is the `rasa` service, which runs your Rasa server.\n\nTo add the action server, add the image of your action server code. To learn how to deploy\nan action server image, see [Building an Action Server Image](../deploy/deploy-action-server.mdx#building-an-action-server-image).\n\n<pre><code parentName=\"pre\" className=\"language-yaml\">\n{`version: '3.0'\nservices:\n  rasa:\n    image: rasa/rasa:${variables.release}-full\n    ports:\n      - 5005:5005\n    volumes:\n      - ./:/app\n    command:\n      - run\n  app:\n    image: <image:tag>\n    expose: 5055`}</code></pre>\n\nThe `expose: 5005` is what allows the `rasa` service to reach the `app` service on that port.\nTo instruct the `rasa` service to send its action requests to that endpoint, add it to your `endpoints.yml`:\n\n```\naction_endpoint:\n  url: http://app:5055/webhook\n```\n\nTo run the services configured in your `docker-compose.yml` execute:\n\n```\ndocker-compose up\n```\n\nYou should then be able to interact with your bot via requests to port 5005, on the webhook endpoint that\ncorresponds to a [configured channel](#configuring-channels):\n\n```\ncurl -XPOST http://localhost:5005/webhooks/rest/webhook \\\n  -H \"Content-type: application/json\" \\\n  -d '{\"sender\": \"test\", \"message\": \"hello\"}'\n```",
                "Configuring a Tracker Store": "By default, all conversations are saved in memory. This means that all\nconversations are lost as soon as you restart the Rasa server.\nIf you want to persist your conversations, you can use a different\n[Tracker Store](../tracker-stores.mdx).\n\nTo add a tracker store to a Docker Compose deployment, you need to add a new\nservice to your `docker-compose.yml` and modify the `endpoints.yml` to add\nthe new tracker store, pointing to your new service. More information about how\nto do so can be found in the tracker store documentation:\n\n['[SQLTrackerStore](../tracker-stores.mdx#sqltrackerstore)', '[RedisTrackerStore](../tracker-stores.mdx#redistrackerstore)', '[MongoTrackerStore](../tracker-stores.mdx#mongotrackerstore)', '[Custom Tracker Store](../tracker-stores.mdx#custom-tracker-store)']"
              },
              "metadata": {
                "id": "deploying-in-docker-compose",
                "sidebar_label": "Deploying a Rasa Assistant in Docker Compose",
                "title": "Deploying a Rasa Assistant in Docker Compose",
                "description": "Use Docker Compose to deploy a Rasa assistant"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 58, \"subpages\", 1]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 58]"
        },
        {
          "title": "Installation",
          "description": "Documentation section: installation",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/installation"
          },
          "subpages": [
            {
              "title": "Setting up your environment",
              "description": "How to set up your environment before installing Rasa",
              "content": {
                "1. Python Environment Setup": "Check if your Python environment is already configured:\n\n```\npython3 --version\npip3 --version\n```\n\n:::note Python3 Supported Versions\n\nCurrently, rasa supports the following Python versions: `3.7`, `3.8`, `3.9` and `3.10`.\nNote that Python `3.10` is only supported for versions `3.4.x` and upwards.\nAdditionally, rasa installation on Apple Silicon with Python `3.10` is not functional in `3.4.x` but will be supported starting from `3.5.x`.\n\n:::\n\nIf these packages are already installed, these commands should display version\nnumbers for each step, and you can skip to the next step.\n\nOtherwise, proceed with the instructions below to install them.\n\n<Tabs values={[{\"label\": \"Ubuntu\", \"value\": \"ubuntu\"}, {\"label\": \"macOS\", \"value\": \"macos\"}, {\"label\": \"Windows\", \"value\": \"windows\"}]}  groupId=\"operating-systems\" defaultValue=\"ubuntu\">\n<TabItem value=\"ubuntu\">\n\nFetch the relevant packages using `apt`, and install virtualenv using `pip`.\n\n```\nsudo apt update\nsudo apt install python3-dev python3-pip\n```\n\n</TabItem>\n<TabItem value=\"macos\">\n\nInstall the [Homebrew](https://brew.sh) package manager if you haven't already.\n\nOnce you're done, you can install Python3.\n\n```\nbrew update\nbrew install python\n```\n\n</TabItem>\n<TabItem value=\"windows\">\n\nMake sure the Microsoft VC++ Compiler is installed, so python can compile\nany dependencies. You can get the compiler from <a className=\"reference external\"\nhref=\"https://visualstudio.microsoft.com/visual-cpp-build-tools/\"\ntarget=\"_blank\">Visual Studio</a>. Download the installer and select\nVC++ Build tools in the list.Install [Python 3](https://www.python.org/downloads/windows/) (64-bit version) for Windows.\n\n```\nC:\\> pip3 install -U pip\n```\n\n</TabItem>\n</Tabs>",
                "2. Virtual Environment Setup": "This step is optional, but we strongly recommend isolating python projects\nusing virtual environments. Tools like\n[virtualenv](https://virtualenv.pypa.io/en/latest/) and\n[virtualenvwrapper](https://virtualenvwrapper.readthedocs.io/en/latest/) provide\nisolated Python environments, which are cleaner than installing packages system-wide\n(as they prevent dependency conflicts). They also let you install packages\nwithout root privileges.\n\n<Tabs values={[{\"label\": \"Ubuntu\", \"value\": \"ubuntu\"}, {\"label\": \"macOS\", \"value\": \"macos\"}, {\"label\": \"Windows\", \"value\": \"windows\"}]}  groupId=\"operating-systems\" defaultValue=\"ubuntu\">\n<TabItem value=\"ubuntu\">\n\nCreate a new virtual environment by choosing a Python interpreter and making a `./venv` directory to hold it:\n\n```\npython3 -m venv ./venv\n```\n\nActivate the virtual environment:\n\n```\nsource ./venv/bin/activate\n```\n\n</TabItem>\n<TabItem value=\"macos\">\n\nCreate a new virtual environment by choosing a Python interpreter and making a `./venv` directory to hold it:\n\n```\npython3 -m venv ./venv\n```\n\nActivate the virtual environment:\n\n```\nsource ./venv/bin/activate\n```\n\n</TabItem>\n<TabItem value=\"windows\">\n\nCreate a new virtual environment by choosing a Python interpreter and making a `.\\\\venv` directory to hold it:\n\n```\nC:\\> python3 -m venv ./venv\n```\n\nActivate the virtual environment:\n\n```\nC:\\> .\\venv\\Scripts\\activate\n```\n\n</TabItem>\n</Tabs>",
                "M1 / M2 (Apple Silicon) Limitations": "Rasa installations on Apple Silicon _don't_ use [Apple Metal](https://developer.apple.com/metal/) by default.\nWe found that using the GPU on Apple Silicon increased training time for the\n[`DIETClassifier`](../components.mdx#dietclassifier) and [`TEDPolicy`](../policies.mdx#ted-policy) dramatically.\nYou can, however, install the optional dependency to test it yourself\nor try it with other components using: `pip3 install 'rasa[metal]'`.\n\nCurrently, not all of Rasa's dependencies support Apple Silicon natively. This leads\nto the following restrictions:\n\n['You can not run Duckling as a docker container on Apple Silicon. If you want\\nto use the [duckling entity extractor](../components.mdx#ducklingentityextractor) we recommend a cloud deployment of\\nduckling. Progress on this can be tracked on the\\n[Duckling project](https://github.com/facebook/duckling/issues/695).', \"Rasa on Apple Silicon does not support the [`ConveRTFeaturizer` component](../components.mdx#convertfeaturizer) or pipelines\\ncontaining it. The component relies on `tensorflow-text` which currently\\nisn't available for Apple Silicon. Progress on this can be tracked on the\\n[Tensorflow Text project](https://github.com/tensorflow/text/issues/823).\"]"
              },
              "metadata": {
                "sidebar_label": "Setting up your environment",
                "title": "Setting up your environment",
                "description": "How to set up your environment before installing Rasa"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 59, \"subpages\", 0]"
            },
            {
              "title": "Installing Rasa Open Source",
              "description": "Install Rasa Open Source on premises to enable local and customizable Natural Language Understanding and Dialogue Management.",
              "content": {
                "Install Rasa Open Source": "<Tabs values={[{\"label\": \"Ubuntu / macOS / Windows\", \"value\": \"ubuntu/macos/windows\"}]} defaultValue=\"ubuntu/macos/windows\">\n<TabItem value=\"ubuntu/macos/windows\">\n\nFirst make sure your `pip` version is up to date:\n\n```\npip3 install -U pip\n```\n\nTo install Rasa Open Source:\n\n```\npip3 install rasa\n```\n\n</TabItem>\n</Tabs>\n\n:::note Telemetry reporting\nWhen you run Rasa Open Source for the first time, you’ll see a\nmessage notifying you about anonymous usage data that is being collected.\nYou can read more about how that data is pulled out and what it is used for in the\n[telemetry documentation](../telemetry/telemetry.mdx).\n:::\n\n**Congratulations! You have successfully installed Rasa Open Source!**\n\nYou can now create a new project with:\n\n```\nrasa init\n```\n\nYou can learn about the most important Rasa commands in the [Command Line Interface](../command-line-interface.mdx).",
                "Building from Source": "If you want to use the development version of Rasa Open Source, you can get it from GitHub:\n\n```\ncurl -sSL https://install.python-poetry.org | python3 -\ngit clone https://github.com/RasaHQ/rasa.git\ncd rasa\npoetry install\n```",
                "Additional dependencies": {
                  "Python 3.10 requirements": "_If you are using Linux_, installing `rasa[full]` could result in a failure while installing `tokenizers` and\n`cryptography`.\n\nIn order to resolve it, you must follow these steps to install a Rust compiler:\n\n```\napt install rustc && apt install cargo\n```\n\nAfter initializing the Rust compiler, you should restart the console and check its installation:\n\n```\nrustc --version\n```\n\nIn case the PATH variable had not been automatically setup, run:\n\n```\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\n```\n\n_If you are using macOS_, note that installing `rasa[full]` (either via pip or from source) could result in a failure\nwhile installing `tokenizers` (issue described in depth [here](https://github.com/huggingface/tokenizers/issues/1050)).\n\nIn order to resolve it, you must follow these steps to install a Rust compiler:\n\n```\nbrew install rustup\nrustup-init\n```\n\nAfter initializing the Rust compiler, you should restart the console and check its installation:\n\n```\nrustc --version\n```\n\nIn case the PATH variable had not been automatically setup, run:\n\n```\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\n```",
                  "Dependencies for spaCy": "For more information on spaCy models, check out the [spaCy docs](https://spacy.io/usage/models).\n\nYou can install it with the following commands:\n\n```\npip3 install 'rasa[spacy]'\npython3 -m spacy download en_core_web_md\n```\n\n:::tip Using `zsh`?\n\nIn zsh, square brackets are interpreted as patterns on the command line.\nTo run commands with square brackets, you can either enclose the arguments\nwith square brackets in quotes, like `pip3 install 'rasa[spacy]'`, or escape\nthe square brackets using backslashes, like `pip3 install rasa\\[spacy\\]`.\nWe recommend using the former method (`pip3 install 'rasa[spacy]'`) in our\ndocumentation because it works as expected across any shell\n\n:::\n\nThis will install Rasa Open Source as well as spaCy and its language model\nfor the English language, but many other languages are available too.\nWe recommend using at least the \"medium\" sized models (`_md`) instead of the spaCy's\ndefault small `en_core_web_sm` model. Small models require less\nmemory to run, but will likely reduce intent classification performance.",
                  "Dependencies for MITIE": "First, run\n\n```\npip3 install git+https://github.com/mit-nlp/MITIE.git\npip3 install 'rasa[mitie]'\n```\n\nand then download the\n[MITIE models](https://github.com/mit-nlp/MITIE/releases/download/v0.4/MITIE-models-v0.2.tar.bz2).\nThe file you need is `total_word_feature_extractor.dat`. Save this\nanywhere. If you want to use MITIE, you need to\ntell it where to find this file (in this example it was saved in the\n`data` folder of the project directory)."
                },
                "Upgrading Versions": "To upgrade your installed version of Rasa Open Source to the latest version from PyPI:\n\n```\npip3 install --upgrade rasa\n```\n\nTo download a specific version, specify the version number:\n\n```\npip3 install rasa==3.0\n```"
              },
              "metadata": {
                "sidebar_label": "Installing Rasa Open Source",
                "title": "Installing Rasa Open Source",
                "description": "Install Rasa Open Source on premises to enable local and customizable Natural Language Understanding and Dialogue Management."
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 59, \"subpages\", 1]"
            },
            {
              "title": "Rasa Pro",
              "description": "Documentation section: rasa-pro",
              "content": {},
              "metadata": {
                "type": "directory",
                "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/installation/rasa-pro"
              },
              "subpages": [
                {
                  "title": "Rasa Pro Installation",
                  "description": "Install Rasa Pro in your production environment to create an enterprise ready Rasa deployment.",
                  "content": {
                    "Rasa Pro Setup": {
                      "Python Package Installation": {
                        "Authentication Set-Up": "To authenticate you need use a service account key file provided by Rasa to authenticate with Google Cloud.\n\nAuthenticate with GCP using the service account key.\n\n```\ngcloud auth activate-service-account --key-file=service-account.json\n```\n\nSet up keyring to allow Pip to authenticate with GCP Artifact Registry by installing keyring and then the backend that supports GCP Artifact Registry\n\n```\npip install keyring\npip install keyrings.google-artifactregistry-auth\n```\n\nVerify that the backends have been installed correctly\n\n```\nkeyring --list-backends\n```\n\nThe results should include `ChainerBackend` and `GooglePythonAuth`.",
                        "Installing with `pip`": "Enter the following settings to the `.pypirc` file. This can be found:\n\n['Linux and MacOS: `$HOME/.pypirc`', 'Windows: `%USERPROFILE%\\\\.pypirc`']\n\n```\n[distutils]\nindex-servers =\n    rasa-plus-py\n\n[rasa-plus-py]\nrepository: https://europe-west3-python.pkg.dev/rasa-releases/rasa-plus-py/\n\n```\n\nNext, add these specific settings to the pip configuration file. The location for this depends on whether you want to update the per-user file or the file specific to a virtual environment that you are using.\n\nFor the file associated with your operating system user:\n\n['Linux: `$HOME/.config/pip/pip.conf` or `$HOME/.pip/pip.conf`', 'MacOS: `/Library/Application Support/pip/pip.conf` or `$HOME/.config/pip/pip.conf`', 'Windows: `%APPDATA%\\\\pip\\\\pip.ini` or `%USERPROFILE%\\\\pip\\\\pip.ini`']\n\nFor virtual environments:\n\n['Linux and macOS: `$VIRTUAL_ENV/pip.conf`', 'Windows: `%VIRTUAL_ENV%\\\\pip.ini`']\n\n```\n[global]\nextra-index-url = https://europe-west3-python.pkg.dev/rasa-releases/rasa-plus-py/simple/\n\n```\n\nFinally, you should be able to run `pip install rasa-plus`.",
                        "Installing with `poetry`": "To install `rasa-plus` with `poetry`, you will need to associate the Artifact Registry URL with `rasa-plus` before installing it.\nNote that you must upgrade poetry to the latest minor (`1.2.0`) in order for `poetry` to work with the GCP authentication set-up.\nProceed with the following steps:\n\n['Run `poetry self add \"keyrings.google-artifactregistry-auth\"`', 'Add this section to `pyproject.toml`:']\n\n```\n[[tool.poetry.source]]\nname = \"rasa-plus\"\nurl = \"https://europe-west3-python.pkg.dev/rasa-releases/rasa-plus-py/simple\"\ndefault = false\nsecondary = true\n\n```\n\n['Run `poetry install`.']"
                      },
                      "Docker Image Installation": "The Rasa Pro Docker image is named `rasa-plus`. The Docker images are hosted on our GCP Artifact Registry.\nAs a prerequisite, you will need:\n\n['to [install](https://cloud.google.com/sdk/docs/install) the Google Cloud CLI.', 'to verify that the user or service account you are using has the required permissions to access the repository.']\n\nTo authenticate you need use a service account key file provided by Rasa to authenticate with Google Cloud.\n\n```\ngcloud auth activate-service-account --key-file=${KEYFILE}\ngcloud auth configure-docker europe-west3-docker.pkg.dev\ndocker pull europe-west3-docker.pkg.dev/rasa-releases/rasa-plus/rasa-plus\n```",
                      "Using An Intermediate Repository": "If you are using your own intermediate repository to cache libraries or dependencies (such as Artifactory or Nexus Repository Manager), you may need to generate a set of static credentials that allow you to authenticate with GCP Artifact Registry.\n\nAs a prerequisite, you will need:\n\n['to [install](https://cloud.google.com/sdk/docs/install) the Google Cloud CLI.', 'to verify that the user or service account you are using has the required permissions to access the repository.']\n\nTo generate your credentials, run:\n\n```\ngcloud artifacts print-settings python \\\n    --project=rasa-releases \\\n    --repository=rasa-plus-py \\\n    --location=europe-west3 \\\n    --json-key=service-account.json\n```\n\nYour credentials can be found in the output. The username will be `_json_key_base64` and the password will be a long, base64 encoded string.",
                      "Runtime Configuration": "Rasa Pro will look for your license in the env var `RASA_PRO_LICENSE`.\nYou can set this env var temporarily in your terminal, but it is recommended\nto set it persistently so that you don't have to set it every time you run\nRasa Pro.\n\nBash:\n\n```\n## Temporary\nexport RASA_PRO_LICENSE=<your-license-string>\n\n## Persistent\necho \"export RASA_PRO_LICENSE=<your-license-string>\" >> ~/.bashrc\n## If you're using a different flavor of bash e.g. Zsh, replace .bashrc with your shell's initialization script e.g. ~/.zshrc\n```\n\nWindows Powershell:\n\n```\n## Temporary\n$env: RASA_PRO_LICENSE=<your-license-string>\n\n## Persistent\n[System.Environment]::SetEnvironmentVariable('RASA_PRO_LICENSE','<your-license-string>')\n```\n\nThen you can use the [`rasa` CLI](https://rasa.com/docs/rasa/command-line-interface) as usual, for example:\n\n```\nrasa init\n```\n\n:::note\n\nIf you run `rasa train` or `rasa run` after you’ve enabled tracing using Jaeger as a backend in your local development environment,\nyou might come across this error `OSError: [Errno 40] Message too long`.\n\nThis could be caused by the OS of your local development environment restricting UDP packet size.\nYou can find out the current UDP packet size by running `sysctl net.inet.udp.maxdgram` on macOS.\nYou can increase UDP packet size by running `sudo sysctl -w net.inet.udp.maxdgram=65535`.\n\n:::\n\nCongratulations! You have now successfully installed Rasa Pro on your development environment.\nTo learn more about production deployments, visit:\n\n['[Deploying Rasa Pro in production](../../deploy/deploy-rasa)', '[Deploying Rasa Pro Services in production](../../deploy/deploy-rasa-pro-services)']"
                    }
                  },
                  "metadata": {
                    "sidebar_label": "Rasa Pro Installation",
                    "title": "Rasa Pro Installation",
                    "description": "Install Rasa Pro in your production environment to create an enterprise ready Rasa deployment."
                  },
                  "subpages": [],
                  "path": "[\"subpages\", 1, \"subpages\", 59, \"subpages\", 2, \"subpages\", 0]"
                },
                {
                  "title": "Rasa Pro Artifacts",
                  "description": "artifacts that ship with Rasa Pro.",
                  "content": {
                    "root": [
                      "import useBaseUrl from \"@docusaurus/useBaseUrl\";\nimport RasaProLabel from \"@theme/RasaProLabel\";\nimport RasaProBanner from \"@theme/RasaProBanner\";",
                      "<RasaProLabel />",
                      "<RasaProBanner />",
                      [
                        "**Rasa Pro**, a drop-in replacement for Rasa Open Source",
                        "**Rasa Pro Services**, flexible infrastructure and APIs on top of Rasa\nOpen Source. Rasa Pro Services should be deployed alongside, but separately\nfrom your production assistant."
                      ],
                      "Rasa Pro includes the Rasa Plus Python package, which is a drop-in replacement for Rasa Open Source that includes all the functionality of Rasa Open Source as well as additional features.\nRasa Pro features are built on a plugin architecture that is seamlessly integrated with Rasa Open Source.",
                      "For example, in the below diagram, tracing is run as a hook implementation in Rasa Plus, whose specification is defined and registered in Rasa Open Source.",
                      "<img alt=\"image\" src={useBaseUrl(\"/img/rasa-plus-architecture.png\")} />",
                      "The hook configures the tracing backend as specified in the `endpoints.yml` and instruments model training and message handling actions.\nThe hook is then called within the Rasa Open Source main module, which is the entry point for the Rasa Open Source command line.",
                      "The plugin architecture enables Rasa Pro to continue enhancing the Rasa Open Source argument parser while maintaining the same runnable command name (`rasa`)."
                    ]
                  },
                  "metadata": {
                    "sidebar_label": "Rasa Pro Artifacts",
                    "title": "Rasa Pro Artifacts",
                    "description": "artifacts that ship with Rasa Pro."
                  },
                  "subpages": [],
                  "path": "[\"subpages\", 1, \"subpages\", 59, \"subpages\", 2, \"subpages\", 1]"
                }
              ],
              "path": "[\"subpages\", 1, \"subpages\", 59, \"subpages\", 2]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 59]"
        },
        {
          "title": "Migrate From",
          "description": "Documentation section: migrate-from",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/migrate-from"
          },
          "subpages": [
            {
              "title": "Rasa as open source alternative to Facebook’s Wit.ai - Migration Guide",
              "description": "Open source alternative to Facebook's Wit.ai for conversational bots and NLP",
              "content": {
                "Step 1: Export your Training Data from Wit.ai": "Navigate to your app's setting page by clicking the **Settings** item\nin the **Management** section of the left navigation bar.\nScroll down to **Export your data** and hit the button **Download .zip with your data**.\n\nThis will download a file with a `.zip` extension. Unzip this file to create a folder.\nThe files you want from your download are located in the **utterances** directory.",
                "Step 2: Create a Rasa Project": "To create a Rasa project, run:\n\n```\nrasa init\n```\n\nThis will create a directory called `data`.\nRemove the files in this directory, and move the content of the `utterances` directory\nto `data`.\n\n```\nrm -rf data/\nmv /path/to/utterances data/\n```",
                "Step 3: Train your NLU model": "To train a model using your Wit data, run:\n\n```\nrasa train nlu\n```",
                "Step 4: Test your NLU model": "Let's see how your NLU model will interpret some test messages.\nTo start a testing session, run:\n\n```\nrasa shell nlu\n```\n\nThis will prompt your for input.\nType a test message and press 'Enter'.\nThe output of your NLU model will be printed to the screen.\nYou can keep entering messages and test as many as you like.\nPress 'control + C' to quit.",
                "Step 5: Start a Server with your NLU Model": "To start a server with your NLU model, run:\n\n```\nrasa run nlu\n```\n\nThis will start a server listening on port 5005.\n\nTo send a request to the server, run:\n\n```\ncurl 'localhost:5005/model/parse?emulation_mode=wit' -d '{\"text\": \"hello\"}'\nThe `emulation_mode` parameter tells Rasa that you want your json\nresponse to have the same format as you would get from wit.ai.\nYou can also leave it out to get the result in the usual Rasa format.\n\nJoin the [Rasa Community Forum](https://forum.rasa.com/) and let us know how your migration went!\n```"
              },
              "metadata": {
                "id": "facebook-wit-ai-to-rasa",
                "sidebar_label": "Rasa as open source alternative to Facebook’s Wit.ai - Migration Guide",
                "title": "Rasa as open source alternative to Facebook’s Wit.ai - Migration Guide",
                "description": "Open source alternative to Facebook's Wit.ai for conversational bots and NLP"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 60, \"subpages\", 0]"
            },
            {
              "title": "Rasa as open source alternative to Google Dialogflow - Migration Guide",
              "description": "Open source alternative to Google Dialogflow for conversational bots and NLP",
              "content": {
                "Step 1: Export your data from Dialogflow": "Navigate to your agent's settings by clicking the gear icon.\n\nimport dialogflowExport1 from './dialogflow_export.png';\n\n<Image img={dialogflowExport1} width=\"240\" caption=\"Selecting settings\" alt=\"In Dialogflow, the gear icon that indicates settings sits next to the dropdown menu for choosing between projects.\" />\n\nClick on the 'Export and Import' tab and click on the 'Export as ZIP' button.\n\nimport dialogflowExport2 from './dialogflow_export_2.png';\n\n<Image img={dialogflowExport2} caption=\"Selecting Export and Import\" alt=\"The Settings page will let you select from multiple headers, of which you want to select Export and Import.\" />\n\nThis will download a file with a `.zip` extension. Unzip this file to create a folder.",
                "Step 2: Create a Rasa Project": "To create a Rasa project, run:\n\n```\nrasa init\n```\n\nThis will create a directory called `data`.\nRemove the files in this directory, and\nmove your unzipped folder into this directory.\n\n```\nrm -r data/*\nmv testagent data/\n```",
                "Step 3: Train your NLU model": "To train a model using your Dialogflow data, run:\n\n```\nrasa train nlu\n```",
                "Step 4: Test your NLU model": "Let's see how your NLU model will interpret some test messages.\nTo start a testing session, run:\n\n```\nrasa shell nlu\n```\n\nThis will prompt your for input.\nType a test message and press 'Enter'.\nThe output of your NLU model will be printed to the screen.\nYou can keep entering messages and test as many as you like.\nPress 'control + C' to quit.",
                "Step 5: Start a Server with your NLU Model": "To start a server with your NLU model, run:\n\n```\nrasa run --enable-api\n```\n\nThis will start a server listening on port 5005.\n\nTo send a request to the server, run:\n\n```\ncurl 'localhost:5005/model/parse?emulation_mode=dialogflow' -d '{\"text\": \"hello\"}'\n```\n\nThe `emulation_mode` parameter tells Rasa that you want your JSON response to have the same format as you would\nget from the Dialogflow `sessions.detectIntent` endpoint (the format is\ndescribed [here](https://cloud.google.com/dialogflow/es/docs/reference/rest/v2/DetectIntentResponse)).\nYou can also leave it out to get the result in the usual Rasa format.",
                "Terminology:": "The words `intent`, `entity`, and `utterance` have the same meaning in Rasa as they do in Dialogflow.\nIn Dialogflow, there is a concept called `Fulfillment`. In Rasa we call this a [Custom Action](../actions.mdx#custom-actions).\n\nJoin the [Rasa Community Forum](https://forum.rasa.com/) and let us know how your migration went!"
              },
              "metadata": {
                "sidebar_label": "Rasa as open source alternative to Google Dialogflow - Migration Guide",
                "title": "Rasa as open source alternative to Google Dialogflow - Migration Guide",
                "description": "Open source alternative to Google Dialogflow for conversational bots and NLP"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 60, \"subpages\", 1]"
            },
            {
              "title": "Rasa as open source alternative to IBM Watson - Migration Tips",
              "description": "Open source alternative to IBM Watson for conversational bots and NLP",
              "content": {
                "root": [
                  "There is no support for IBM Watson yet. However, a group of community members is working on a way\nto use <a className=\"reference external\" href=\"https://developer.ibm.com/tutorials/learn-how-to-export-import-a-watson-assistant-workspace/\" target=\"_blank\">exported IBM Watson workspaces</a>\nin Rasa. If you're interested in that, check out our <a className=\"reference external\" href=\"https://forum.rasa.com/\" target=\"_blank\">Community Forum</a>."
                ]
              },
              "metadata": {
                "id": "ibm-watson-to-rasa",
                "sidebar_label": "Rasa as open source alternative to IBM Watson - Migration Tips",
                "title": "Rasa as open source alternative to IBM Watson - Migration Tips",
                "description": "Open source alternative to IBM Watson for conversational bots and NLP"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 60, \"subpages\", 2]"
            },
            {
              "title": "Rasa as open source alternative to Microsoft LUIS - Migration Guide",
              "description": "Open source alternative to Microsoft LUIS for conversational bots and NLP",
              "content": {
                "Step 1: Export your Training Data from LUIS": "Go to your list of [LUIS conversation apps](https://www.luis.ai/conversations/applications) and select the application\nyou want to export.\n\nimport luisImage from './luis_export.png';\n\n<Image img={luisImage} caption=\"Export menu\" alt=\"Opening the menu reveals the 'Export to JSON' option.\" />\n\nSelect 'Export' > 'Export as JSON'. This will download a file with a `.json` extension that can be imported directly into Rasa.",
                "Step 2: Create a Rasa Project": "To create a Rasa project, run:\n\n```\nrasa init\n```\n\nThis will create a directory called `data`.\nRemove the files in this directory, and\nmove your json file into this directory.\n\n```\nrm -r data/*\nmv /path/to/file.json data/\n```",
                "Step 3: Train your NLU model": "To train a model using your LUIS data, run:\n\n```\nrasa train nlu\n```",
                "Step 4: Test your NLU model": "Let's see how your NLU model will interpret some test messages.\nTo start a testing session, run:\n\n```\nrasa shell nlu\n```\n\nThis will prompt your for input.\nType a test message and press 'Enter'.\nThe output of your NLU model will be printed to the screen.\nYou can keep entering messages and test as many as you like.\nPress 'control + C' to quit.",
                "Step 5: Start a Server with your NLU Model": "To start a server with your NLU model, run:\n\n```\nrasa run\n```\n\nThis will start a server listening on port 5005.\n\nTo send a request to the server, run:\n\n```\ncurl 'localhost:5005/model/parse?emulation_mode=luis' -d '{\"text\": \"hello\"}'\nThe `emulation_mode` parameter tells Rasa that you want your json\nresponse to have the same format as you would get from LUIS.\nYou can also leave it out to get the result in the usual Rasa format.\n\n## Terminology:\n\nThe words `intent`, `entity`, `role`, and `utterance` have the same meaning in Rasa as they do\nin LUIS.\nLUIS's `patterns` feature is very similar to Rasa NLU's [regex features](./training-data-format.mdx#regular-expressions)\nLUIS's `phrase lists` feature does not currently have an equivalent in Rasa NLU.\n\nJoin the [Rasa Community Forum](https://forum.rasa.com/) and let us know how your migration went!\n```"
              },
              "metadata": {
                "id": "microsoft-luis-to-rasa",
                "sidebar_label": "Rasa as open source alternative to Microsoft LUIS - Migration Guide",
                "title": "Rasa as open source alternative to Microsoft LUIS - Migration Guide",
                "description": "Open source alternative to Microsoft LUIS for conversational bots and NLP"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 60, \"subpages\", 3]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 60]"
        },
        {
          "title": "Monitoring",
          "description": "Documentation section: monitoring",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/monitoring"
          },
          "subpages": [
            {
              "title": "Load Testing Guidelines",
              "description": "Information about how best to scale up your bot to support parallel user activity\nand how you can use tracing to help debug issues.\n",
              "content": {
                "Overview": {
                  "Some recommendations to improve latency": [
                    "Sanic Workers must be mapped 1:1 to CPU for both Rasa Pro and Rasa Action Server",
                    "Create `async` actions to avoid any blocking I/O",
                    "`enable_selective_domain: true` : Domain is only sent for actions that needs it. This massively trims the payload between the two pods.",
                    "Consider using compute efficient machines on cloud which are optimized for high performance computing such as the C5 instances on AWS.\nHowever, as they are low on memory, models need to be trained lightweight."
                  ],
                  "Debugging bot related issues while scaling up": "To test the Rasa [HTTP-API](https://rasa.com/docs/rasa/pages/http-api) ability to handle a large number of concurrent user activity we used the Rasa Pro [tracing](./tracing.mdx) capability\nalong with a tracing backend or collector, such as Jaeger, to collect traces for the bot under test.\n\n:::note\n\nOur team is currently in the process of running additional performance-related tests. More information will be added here as we progress.\n\n:::"
                }
              },
              "metadata": {
                "sidebar_label": "Load Testing Guidelines",
                "title": "Load Testing Guidelines",
                "description": "Information about how best to scale up your bot to support parallel user activity\nand how you can use tracing to help debug issues.\n"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 61, \"subpages\", 0]"
            },
            {
              "title": "Tracing",
              "description": "Resolve performance issues faster and identify bottlenecks through OpenTelemetry-based tracing",
              "content": {
                "Tracing": {
                  "Supported Tracing Backends/Collectors": "To trace requests in Rasa Pro, you can either use\n[Jaeger](https://www.jaegertracing.io/) as a backend, or use\nthe [OTEL Collector (OpenTelemetry Collector)](https://opentelemetry.io/docs/collector/).\nto collect traces and then send them to the backend of your choice.\nSee [Configuring a Tracing Backend or Collector](#configuring-a-tracing-backend-or-collector)\nfor instructions.",
                  "Enabling / Disabling": "Tracing is automatically enabled in Rasa Pro by\n[configuring a supported tracing backend](#configuring-a-tracing-backend-or-collector).\nNo further action is required to enable tracing.\n\nYou can disable tracing by leaving the `tracing:` configuration key empty\nin your endpoints file.",
                  "Action Server": "The trace context is sent along with requests to the custom action server\nusing the [W3C Trace Context Specification](https://www.w3.org/TR/trace-context/).\nYou can use this trace context to continue tracing the request through\nyour custom action code. See [traced events](#traced-events) for\ndetails on what attributes are made available as part of the trace context."
                },
                "Configuring a Tracing Backend or Collector": {
                  "Jaeger": "To configure a Jaeger tracing backend, specify the `type` as `jaeger`.\n\n```\ntracing:\n  type: jaeger\n  host: localhost\n  port: 6831\n  service_name: rasa\n  sync_export: ~\n```",
                  "OTEL Collector": "Collectors are components that collect traces in a vendor-agnostic way and then forward them to various backends.\nFor example, the OpenTelemetry Collector (OTEL) can collect traces from multiple different components and instrumentation libraries, and then export them to multiple different backends e.g. jaeger.\n\nTo configure an OTEL Collector, specify the `type` as `otlp`.\n\n```\ntracing:\n  type: otlp\n  endpoint: my-otlp-host:4318\n  insecure: false\n  service_name: rasa\n  root_certificates: ./tests/unit/tracing/fixtures/ca.pem\n```"
                },
                "Traced Events": {
                  "Model Training": {
                    "`GraphTrainer` Attributes": "The following attributes can be inspected during training of `GraphTrainer`:\n\n['`training_type` of model configuration:', ['`\"NLU\"`', '`\"CORE\"`', '`\"BOTH\"`', '`\"END-TO-END\"`'], '`language` of model configuration', '`recipe_name` used in the `config.yml` file', '`output_filename`: the location where the packaged model is saved', '`is_finetuning`: boolean argument, if `True` enables incremental training']",
                    "`GraphNode` Attributes": "The following attributes are captured during the training (as well as prediction during message handling) of every graph node:\n\n['`node_name`', '`component_class`', '`fn_name`: method of component class that gets called']"
                  },
                  "Message Handling": {
                    "`Agent` Attributes": "Tracing the `Agent` instance handling a message captures the following attributes:\n\n['`input_channel`: the name of the channel connector', '`sender_id`: the conversation id', '`model_id`: a unique identifier for the model', '`model_name`: the model name']",
                    "`MessageProcessor` Attributes": "The following `MessageProcessor` attributes are extracted during the tracing:\n\n['`number_of_events`: number of events in tracker', '`action_name`: the name of the predicted and executed action', '`sender_id`: the conversation id of the `DialogueStateTracker` object', '`message_id`: the unique message id']\n\nThe latter three attributes are also injected in the trace context that gets passed to the requests made to the custom action server.",
                    "`TrackerStore` & `LockStore` Attributes": "Observable `TrackerStore` and `LockStore` attributes include:\n\n['`number_of_streamed_events`: number of new events to stream', '`broker_class`: the `EventBroker` on which the new events are published', '`lock_store_class`: Name of lock store used to lock conversations while messages are actively processed']"
                  }
                }
              },
              "metadata": {
                "sidebar_label": "Tracing",
                "title": "Tracing",
                "description": "Resolve performance issues faster and identify bottlenecks through OpenTelemetry-based tracing"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 61, \"subpages\", 1]"
            },
            {
              "title": "Analytics",
              "description": "Documentation section: analytics",
              "content": {},
              "metadata": {
                "type": "directory",
                "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/monitoring/analytics"
              },
              "subpages": [
                {
                  "title": "Data structure reference",
                  "description": null,
                  "content": {
                    "Database Table Overview": "<!-- the diagram is generated from a database that has the correct schema\n     you need to connect to a live database, e.g. the one connected to our\n     example metabase. afterwards, you can use\n     https://app.trevor.io/datasources/617ec3a0-526f-4295-ad5a-3a31f0f4c027#m=ap\n     and its database map to create screenshots for all tables or the individual\n     tables -->\n\n<div align=\"center\">\n  <img\n    alt=\"An overview of the components of Rasa Pro.\"\n    src={useBaseUrl(\"/img/analytics/analytics-er-db.png\")}\n    width=\"100%\"\n  />\n</div>",
                    "Common Terms": [
                      "**a sender** is a user who is talking to the assistant through a channel.\nA user might have multiple senders if they use multiple channels, e.g.\ncommunicating with the assistant through a website and through a\nchannel integrated into a mobile app.",
                      "**a session** is a conversation between a sender and the assistant.\nA session is started when a sender sends a message to the assistant and\nends when the session either has been timed out or explicitly ended.\nIf a session is interrupted by a longer period of inactivity new\nactivity will trigger a new session to be created ([configurable\nthrough the session timeout](../../domain.mdx#session-configuration)).",
                      "**a turn** always starts with a message from a sender and ends right before\nthe next message from the sender. A turn can also end with a session\nbeing timed out or explicitly ended. A turn will usually contain at least\none bot response."
                    ],
                    "Tables": {
                      "rasa_sender": {
                        "`id` sender identifier": "The unique identifier of the sender is generated by Analytics. Sender\ngets a different, generated id assigned. The `id` differs from the `sender_id`\nused by the Rasa channels, the `sender_id` in Rasa is the\n`sender_key` in Analytics.\n\n['Type: `varchar(36)`', 'Example: `a78783c4-bef7-4e55-9ec7-5afb4420f19a`']",
                        "`sender_key` Rasa channel sender identifier": "The unique identifier used by the Rasa channel to identify this sender. The\n`sender_key` is specific to the channel implementation in Rasa and\nthe format depends on the channel.\n\n['Type: `varchar(255)`', 'Example: `fb26ba0a9d8b4bd99e2b8716acb19e4b`']",
                        "`channel` Rasa channel name": "Name of the channel that is used for this sender. The channel names are\ndefined in the implementation of the respective Rasa channel.\n\n['Type: `varchar(255)`', 'Example: `socket.io`']",
                        "`first_seen` first contact with this sender": "The date and time of the first contact with this sender. Corresponds to the\ntime of the first event of the first session created for this sender.\n\n['Type: `DateTime`', 'Example: `2022-06-28 02:15:49.326936`']",
                        "`last_seen` latest contact with this sender": "The date and time of the last contact with this sender. Corresponds to the\ntime of the latest event of the latest session created for this sender.\n\n['Type: `DateTime`', 'Example: `2022-10-28 02:15:49.326936`']\n\n[]"
                      },
                      "rasa_session": {
                        "`id` session identifier": "The unique identifier of the session. Every session gets a different,\ngenerated id assigned.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`sender_id` sender who started the session": "The unique identifier of the sender who started the session. It is a\nforeign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`timestamp` creation date time": "The timestamp when the session was created. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-28 02:15:49.326936`']",
                        "`start_sequence_number` start of the session": "The sequence number of the first event in this session. All events belong\nto exactly one session. The start sequence number is always smaller or equal\nto the `end_sequence_number`. The difference between start and end sequence\nnumbers does not equal the number of events in this session since\nsequence numbers are incremented across multiple conversations.\n\n['Type: `Integer`', 'Example: `78`']",
                        "`end_sequence_number` end of the session": "The sequence number of the last event in the session.\n\n['Type: `Integer`', 'Example: `91`']\n\n[]"
                      },
                      "rasa_turn": {
                        "`id` session identifier": "The unique identifier of the turn. Every turn gets a different\ngenerated id assigned.\n\n['Type: `varchar(36)`', 'Example: `ffa5d0cd-f5a6-45a4-9506-ba7ffd76edf1`']",
                        "`sender_id` sender who started the turn": "The unique identifier of the sender who started the turn. It is a\nforeign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this turn is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`start_sequence_number` start of the turn": "The sequence number of the first event in this turn. All events belong\nto exactly one session. The start sequence number is always smaller or equal\nto the `end_sequence_number`. The difference between start and end sequence\nnumbers does not equal the number of events in this session since\nsequence numbers are incremented across multiple conversations.\n\n['Type: `Integer`', 'Example: `79`']",
                        "`end_sequence_number` end of the turn": "The sequence number of the last event in this turn.\n\n['Type: `Integer`', 'Example: `82`']\n\n[]"
                      },
                      "rasa_event": {
                        "`id` event identifier": "The unique identifier of the event. Every event gets different,\ngenerated id assigned.\n\n['Type: `varchar(36)`', 'Example: `f5adcd16-b18d-4c5c-95f0-1747b20cb0e6`']",
                        "`sender_id` sender whose conversation the event belongs to": "The unique identifier of the sender whose conversation this event is part of.\nIt is a foreign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this event is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`timestamp` creation date time": "The timestamp when the event was created. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-28 02:15:49.326936`']",
                        "`event_type` kind of event": "The type of the event. The event type is a string and can be one of the\nfollowing:\n\n['`user`: The user sent a message to the assistant.', '`bot`: The assistant sent a message to the user.', '`action`: The assistant executed an action.', '`session_started`: A new session was started.', '`action_execution_rejected`: An action failed to execute.', '`active_loop`: The assistant is currently in a loop.', '`slot`: A slot was set.', '`followup`: A follow-up action was triggered.', '`loop_interrupted`: A loop was interrupted.', '`pause`: A session is paused, e.g. because the session was handed over\\nto a human agent.', '`restart`: A session was restarted. This will trigger a new session to\\nbe started. The state of the assistant will be reset.', '`rewind`: The assistant rewinds to a previous state.', '`user_featurization`: The assistant featurized the user input.']\n\nThe event type defines how the event is interpreted and how the event\naffects the conversation. For example, the `user` event type will\nbe interpreted as a user message and the `bot` event type will be\ninterpreted as a bot response.\n\n['Type: `varchar(255)`', 'Example: `action`']",
                        "`model_id` model identifier": "The identifier of the Rasa model that was running as part of the assistant\nwhen this event was created.\n\n['Type: `varchar(255)`', 'Example: `75a985b7b86d442ca013d61ea4781b22`']",
                        "`environment` name of the assistant environment": "The name of the environment of the assistant that created this event.\nThe environment is a string that is set up during the start of the assistant,\n\n['Type: `varchar(255)`', 'Example: `production`']",
                        "`sequence_number` start of the event": "The sequence number of the event. The events of a session always have\nincreasing sequence numbers. Sequence numbers are not guaranteed to be\nsequential for events following one another. But sequence numbers can\nbe used to order the events of a session.\n\n['Type: `Integer`', 'Example: `78`']\n\n[]"
                      },
                      "rasa_bot_message": {
                        "`id` bot message identifier": "The unique identifier of the bot message is generated by Analytics.\n\n['Type: `varchar(36)`', 'Example: `2f2e5384-1bfa-4b53-90a7-c75e5f20b117`']",
                        "`event_id` id of the event of this message": "The unique identifier of the event that created this bot message.\nIt is a foreign key to the [`rasa_event.id`](#rasa_event) column.\n\n['Type: `varchar(36)`', 'Example: `f5adcd16-b18d-4c5c-95f0-1747b20cb0e6`']",
                        "`sender_id` sender whose conversation the message belongs to": "The unique identifier of the sender whose conversation this message is part of.\nIt is a foreign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this message is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`timestamp` creation date time": "The timestamp when the message was created. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-28 02:15:49.326936`']",
                        "`template_name` name of the template used to generate the message": "The name of the template that Rasa used to generate the bot message. Might\nbe empty if the message was not generated from a template but a custom\naction.\n\n['Type: `varchar(255)`', 'Example: `utter_greet`']",
                        "`text` message content": "The text of the bot message.\n\n['Type: `varchar(65535)`', 'Example: `Ok, what can I help you with?`']",
                        "`model_id` model identifier": "The identifier of the Rasa model that was running as part of the assistant\nwhen this message was created.\n\n['Type: `varchar(255)`', 'Example: `75a985b7b86d442ca013d61ea4781b22`']",
                        "`sequence_number` start of the event": "The sequence number of the message. The events of a session always have\nincreasing sequence numbers. The sequence number of this message is the same\nas the one of the underlying event.\n\n['Type: `Integer`', 'Example: `78`']\n\n[]"
                      },
                      "rasa_user_message": {
                        "`id` user message identifier": "The unique identifier of the user message is generated by Analytics.\n\n['Type: `varchar(36)`', 'Example: `49fdd79e-976b-47c2-ab27-a4c3d743a1c9`']",
                        "`event_id` id of the event of this message": "The unique identifier of the event that created this user message.\nIt is a foreign key to the [`rasa_event.id`](#rasa_event) column.\n\n['Type: `varchar(36)`', 'Example: `f5adcd16-b18d-4c5c-95f0-1747b20cb0e6`']",
                        "`sender_id` sender whose conversation the message belongs to": "The unique identifier of the sender whose conversation this message is part of.\nIt is a foreign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this message is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`intent` classification of the text": "The name of the intent that Rasa classified the text as. One of the intents\nin the domain used to train the model.\n\n['Type: `varchar(255)`', 'Example: `book_flight`']",
                        "`retrieval_intent` classification of the text": "The name of the retrieval intent that Rasa classified the text as. Only\npopulated if there is a configured retrieval intent.\n\n['Type: `varchar(255)`', 'Example: `book_flight/faq`']",
                        "`confidence` certainty the model predicted for classifications": "The confidence of the ML model's intent prediction. The confidence is a\nvalue between 0 and 1. The higher the value, the more certain the model is\nthat the intent is correct.\n\n['Type: `Float`', 'Example: `0.8798527419567108`']",
                        "`text` message content": "The text of the user message.\n\n['Type: `varchar(65535)`', 'Example: `I want to book a flight.`']",
                        "`timestamp` creation date time": "The timestamp when the message was created. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-28 02:15:49.326936`']",
                        "`model_id` model identifier": "The identifier of the Rasa model that was running as part of the assistant\nwhen this message was created.\n\n['Type: `varchar(255)`', 'Example: `75a985b7b86d442ca013d61ea4781b22`']",
                        "`sequence_number` start of the event": "The sequence number of the message. The events of a session always have\nincreasing sequence numbers. The sequence number of this message is the same\nas the one of the underlying event.\n\n['Type: `Integer`', 'Example: `78`']",
                        "`message_id` unique id for the message text": "A unique id that identifies the text of the message.\n\n['Type: `varchar(255)`', 'Example: `7cdb5700ac9c493aa46987b77d91c363`']\n\n[]"
                      },
                      "rasa_action": {
                        "`id` action identifier": "The unique identifier of the action execution is generated by Analytics.\n\n['Type: `varchar(36)`', 'Example: `bd074dc7-e745-4db6-86d0-75b0af7bc067`']",
                        "`event_id` id of the event of this action execution": "The unique identifier of the event that created this action execution.\nIt is a foreign key to the [`rasa_event.id`](#rasa_event) column.\n\n['Type: `varchar(36)`', 'Example: `f5adcd16-b18d-4c5c-95f0-1747b20cb0e6`']",
                        "`sender_id` sender whose conversation triggered this action execution": "The unique identifier of the sender whose conversation triggered this action\nexecution. It is a foreign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this action execution is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`name` name of the executed action": "The name of the action that Rasa has predicted and executed. One of the actions\nin the domain used to train the model.\n\n['Type: `varchar(255)`', 'Example: `action_book_flight`']",
                        "`confidence` ML models certainty of the predicted action": "The confidence of ML model's action prediction. The confidence is a\nvalue between 0 and 1. The higher the value, the more certain the model is\nthat the action is correct.\n\n['Type: `Float`', 'Example: `0.9398527419567108`']",
                        "`policy` name of the policy that predicted the action": "The name of the policy that predicted this action. The policy is a component\nin the Rasa assistant that makes a prediction. The policy can be a rule\npolicy, a memoization policy, or an ML policy.\n\n['Type: `varchar(255)`', 'Example: `policy_2_TEDPolicy`']",
                        "`timestamp` creation date time": "The timestamp when the action was executed. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-28 02:15:49.326936`']",
                        "`model_id` model identifier": "The identifier of the Rasa model that was running as part of the assistant\nwhen this action was executed.\n\n['Type: `varchar(255)`', 'Example: `75a985b7b86d442ca013d61ea4781b22`']",
                        "`sequence_number` start of the event": "The sequence number of the executed action. The events of a session always have\nincreasing sequence numbers. The sequence number of this executed action is\nthe same as the one of the underlying event.\n\n['Type: `Integer`', 'Example: `78`']\n\n[]"
                      },
                      "rasa_slot": {
                        "`id` slot change identifier": "The unique identifier of this change in slot values is generated by Analytics.\n\n['Type: `varchar(36)`', 'Example: `a793d284-b5b9-4cef-be8a-bc0f58c70c28`']",
                        "`event_id` id of the event that triggered this slot change": "The unique identifier of the event that triggered this change\nin the slot value. It is a foreign key to\nthe [`rasa_event.id`](#rasa_event) column.\n\n['Type: `varchar(36)`', 'Example: `f5adcd16-b18d-4c5c-95f0-1747b20cb0e6`']",
                        "`sender_id` sender whose conversation triggered this slot change": "The unique identifier of the sender whose conversation triggered this\nslot change. It is a foreign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this slot change is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`slot_path` path of the slot": "A path to the slot that was changed. The path identifies the slot by its\nname, the sender and the session. The path is a string that looks like\n`<sender_id>/<session_id>/<slot_name>`.\n\n['Type: `varchar(255)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53/63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1/email`']",
                        "`name` name of the slot": "The name of the changed slot. The name of the slot is the same\nas the name of the slot in the domain.\n\n['Type: `varchar(255)`', 'Example: `email`']",
                        "`value` new slot value": "The new value of the slot for the session. The value is a dumped\nJSON object.\n\n['Type: `varchar(65535)`', 'Example: `john@example.com`']",
                        "`timestamp` creation date time": "The timestamp when the slot value was changed. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-28 02:15:49.326936`']",
                        "`sequence_number` start of the event": "The sequence number of the slot change. The events of a session always have\nincreasing sequence numbers. The sequence number of the slot change is\nthe same as the one of the underlying event.\n\n['Type: `Integer`', 'Example: `78`']\n\n[]"
                      },
                      "rasa_session_slot_state": {
                        "`id` path of the slot": "A path to the slot. The path identifies the slot by its\nname, the sender and the session. The path is a string that looks like\n`<sender_id>/<session_id>/<slot_name>`.\n\n['Type: `varchar(255)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53/63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1/email`']",
                        "`sender_id` sender whose conversation this slot is part of": "The unique identifier of the sender whose conversation this slot is part of.\nIt is a foreign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this slot is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`name` name of the slot": "The name of the slot. The name of the slot is the same\nas the name of the slot in the domain.\n\n['Type: `varchar(255)`', 'Example: `email`']",
                        "`value` last value of the slot in the session": "The value of the slot at the end of the session. The value is a dumped\nJSON object. If a slot is changed multiple times during a session,\nthe value is set to the last change.\n\n['Type: `varchar(65535)`', 'Example: `john@example.com`']",
                        "`timestamp` creation date time": "Time of the last update of the slot in this session. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-21 02:15:49.326936`']"
                      },
                      "rasa_patterns": {
                        "`id` pattern identifier": "The unique identifier of the rasa pattern is generated by Analytics.\n\n['Type: `varchar(36)`', 'Example: `bd074dc7-e745-4db6-86d0-75b0af7bc067`']",
                        "`name` pattern name": "Name of the pattern\n\n['Type: `varchar()`', 'Example: `registration success`']",
                        "`description` pattern description": "Description of the pattern\n\n['Type: `varchar()`', 'Example: `This marker identifies successful account registration in the chat`']",
                        "`config` pattern configuration": "Pattern configuration dictionary stored as an escaped string\n\n['Type: `varchar()`', 'Example: `\"{\\'or\\': [{\\'intent\\': \\'mood_unhappy\\'},{\\'intent\\': \\'mood_great\\'}]}\"`']",
                        "`is_active` soft-delete flag": "Only patterns with `is_active==True` are processed during real-time analysis\n\n['Type: `boolean`']",
                        "`created_at` creation date time": "Time of creation of this pattern. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-21 02:15:49.326936`']",
                        "`updated_at` update date time": "Time of the last update of the pattern in this session. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-21 02:15:49.326936`']"
                      },
                      "rasa_markers": {
                        "`id` marker identifier": "The unique identifier of the extracted rasa marker is generated by Analytics.\n\n['Type: `varchar(36)`', 'Example: `bd074dc7-e745-4db6-86d0-75b0af7bc067`']",
                        "`pattern_id` pattern which was applied in this marker": "The unique identifier of the pattern which was applied in this marker. It is\na foreign key to the [`rasa_patterns.id`](#rasa_patterns) column\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`sender_id` sender identifier": "The unique identifier of the sender whose conversation this marker is part of.\nIt is a foreign key to the [`rasa_sender.id`](#rasa_sender) column.\n\n['Type: `varchar(36)`', 'Example: `9e4ebded-f232-4cc5-af78-d98daa0c1a53`']",
                        "`session_id` session identifier": "The unique identifier of the session this marker is part of.\nIt is a foreign key to the [`rasa_session.id`](#rasa_session) column.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`event_id` event identifier": "The unique identifier of the event from event broker where this marker was applied.\nNote that a marker can be applied across multiple events, this is the ID of the last event in the sequence.\n\n['Type: `varchar(36)`', 'Example: `63b150a6-21a3-4e6c-bb24-5ab6ddc30cf1`']",
                        "`num_preceding_user_turns` Number of Proeeding User turns": "an integer indicating the number of user turns preceding the event at which the marker applied.\n\n['Type: `integer`', 'Example: `4`']",
                        "`created_at` creation date time": "Time of creation of this marker. The timestamp is a UTC.\n\n['Type: `DateTime`', 'Example: `2022-06-21 02:15:49.326936`']"
                      }
                    },
                    "Internal Tables": "Internal tables are used to store information about the assistant and\nthe events that are sent to the assistant. They are not meant to be\nqueried directly but are required for the functioning of Analytics. They\nare a private API that is used by the Analytics service internally\nand might change without notice.\n\nInternal tables:\n\n['`_rasa_raw_event`', '`alembic_version`']"
                  },
                  "metadata": {
                    "id": "data-structure-reference",
                    "sidebar_label": "Data structure reference",
                    "title": "Data structure reference",
                    "abstract": "Overview of the data structure created by the Analytics pipeline. Description of all tables and attributes that can be used to build analytics dashboards."
                  },
                  "subpages": [],
                  "path": "[\"subpages\", 1, \"subpages\", 61, \"subpages\", 2, \"subpages\", 0]"
                },
                {
                  "title": "Example queries",
                  "description": null,
                  "content": {
                    "Number of sessions per month": "A common high-level usage metric of your assistant is the number of\nsessions per month. Here is how it would look as an SQL query:\n\n```\nSELECT\n  date_trunc('month', \"public\".\"rasa_session\".\"timestamp\") AS \"first_seen\",\n  count(*) AS \"count\"\nFROM \"public\".\"rasa_session\"\nGROUP BY 1\nORDER BY 1 ASC\n```\n\n<figure align=\"center\">\n  <img\n    alt=\"Number of sessions per month visualized in Metabase.\"\n    src={useBaseUrl(\"/img/analytics/graph-number-sessions-month.png\")}\n    width=\"100%\"\n  />\n  <figcaption>Number of sessions per month visualized in Metabase.</figcaption>\n</figure>",
                    "Number of sessions per channel": "If you're connecting your assistant to multiple channels, it could be\nuseful to look at the number of sessions per channel, let's say per week.\nThe query you would need for this metric is:\n\n```\nSELECT\n  \"public\".\"rasa_sender\".\"channel\" AS \"channel\",\n  \"public\".\"rasa_sender\".\"first_seen\" AS \"timestamp\",\n  count(distinct \"public\".\"rasa_sender\".\"sender_key\") AS \"count\"\nFROM \"public\".\"rasa_sender\"\nGROUP BY 1, 2\nORDER BY 1 ASC, 2 ASC\n```\n\n<figure align=\"center\">\n  <img\n    alt=\"Number of sessions per channel visualized in Metabase.\"\n    src={useBaseUrl(\"/img/analytics/graph-number-sessions-channel.png\")}\n    width=\"100%\"\n  />\n  <figcaption>\n    The number of sessions per channel as visualized in Metabase.\n  </figcaption>\n</figure>",
                    "Top N intents": "To improve your assistant, you could look into the variety of intents\nyour users express. The query below selects the top 5 intents which\ncould help you have a good perspective on that topic:\n\n```\nSELECT\n  \"public\".\"rasa_user_message\".\"intent\" AS \"intent\",\n  count(*) AS \"count\"\nFROM \"public\".\"rasa_user_message\"\nGROUP BY 1\nORDER BY 2 DESC, 1 ASC\nLIMIT 5\n```\n\n<figure align=\"center\">\n  <img\n    alt=\"Top 5 intents visualized in Metabase.\"\n    src={useBaseUrl(\"/img/analytics/graph-top-5-intents.png\")}\n    width=\"100%\"\n  />\n  <figcaption>Top 5 intents visualized in Metabase.</figcaption>\n</figure>\n\nMoreover, you can look for the intent distribution over time:\n\n```\nSELECT\n  \"public\".\"rasa_user_message\".\"intent\" AS \"intent\",\n  date_trunc('month', \"public\".\"rasa_user_message\".\"timestamp\") AS \"timestamp\",\n  count(*) AS \"count\" FROM \"public\".\"rasa_user_message\"\nGROUP BY 1, 2\nORDER BY 1 ASC, 2 ASC\n```\n\n<figure align=\"center\">\n  <img\n    alt=\"Intent distribution over time visualized in Metabase.\"\n    src={useBaseUrl(\"/img/analytics/graph-intent-distribution.png\")}\n    width=\"100%\"\n  />\n  <figcaption>Intent distribution over time visualized in Metabase.</figcaption>\n</figure>",
                    "Escalation rate": "The escalation rate or human hand-off rate is a measure of the number of\nconversations the assistant passes to a human agent. This metric can\nhelp you gain a better understanding of what happens during a conversation.\nLet's say you have an intent named `handoff_to_support`. You'll get the\nescalation rate over time with this sample query:\n\n```\nWITH \"sessions\" AS (\n    SELECT\n        \"public\".\"rasa_user_message\".\"session_id\" AS \"session_id\",\n        date_trunc('month', \"public\".\"rasa_user_message\".\"timestamp\") AS \"timestamp\",\n        (\n          CASE \"public\".\"rasa_user_message\".\"intent\"\n            WHEN 'handoff_to_support'\n            THEN 1 ELSE 0\n          END\n        ) AS \"has_handoff_to_support\"\n    FROM \"public\".\"rasa_user_message\"\n),\n\"sessions_with_handoff\" AS (\n    SELECT\n      \"session_id\",\n      \"timestamp\",\n      SUM(\"has_handoff_to_support\") AS \"has_handoff_to_support\"\n    FROM \"sessions\"\n    GROUP BY 1, 2\n)\nSELECT\n  \"timestamp\",\n  SUM(\"has_handoff_to_support\") / count(*) AS \"escalation_rate\"\nFROM \"sessions_with_handoff\"\nGROUP BY 1 ASC\nORDER BY 1 ASC\n```\n\n<figure align=\"center\">\n  <img\n    alt=\"Escalation rate visualized in Metabase.\"\n    src={useBaseUrl(\"/img/analytics/graph-escalation-rate.png\")}\n    width=\"100%\"\n  />\n  <figcaption>Escalation rate visualized in Metabase.</figcaption>\n</figure>",
                    "Abandonment rate": "Abandonment rate can be defined in many different custom ways,\nhowever here we'll define it as a session ending without a user message\nafter a specific message was uttered by the bot, e.g. `utter_ask_name`.\nYou could adapt the metric to detect sessions ending without a user\nmessage after a specific set of intents. The SQL query would look like this:\n\n```\nWITH \"sessions\" AS (\n    SELECT\n        DISTINCT ON (\"public\".\"rasa_event\".\"session_id\") \"public\".\"rasa_event\".\"session_id\",\n        \"public\".\"rasa_event\".\"timestamp\" AS \"timestamp\",\n        (\n            CASE\n                WHEN \"public\".\"rasa_bot_message\".\"template_name\" = 'utter_ask_name'\n                THEN 1 ELSE 0\n            END\n        ) AS \"is_abandonned\"\n    FROM \"public\".\"rasa_event\"\n    INNER JOIN \"public\".\"rasa_bot_message\"\n      ON \"public\".\"rasa_event\".\"id\" = \"public\".\"rasa_bot_message\".\"event_id\"\n    WHERE \"public\".\"rasa_event\".\"event_type\" = 'bot'\n    ORDER BY 1, 2 DESC\n)\nSELECT\n  date_trunc('month', \"timestamp\") AS \"timestamp\",\n  SUM(\"is_abandonned\")::float / count(*) AS \"abandonment_rate\"\nFROM \"sessions\"\nGROUP BY 1\nORDER BY 1 ASC\n```\n\n<figure align=\"center\">\n  <img\n    alt=\"Abandonment rate visualized in Metabase.\"\n    src={useBaseUrl(\"/img/analytics/graph-abandonment-rate.png\")}\n    width=\"100%\"\n  />\n  <figcaption>Abandonment rate visualized in Metabase.</figcaption>\n</figure>"
                  },
                  "metadata": {
                    "id": "example-queries",
                    "sidebar_label": "Example queries",
                    "title": "Example queries",
                    "description": null
                  },
                  "subpages": [],
                  "path": "[\"subpages\", 1, \"subpages\", 61, \"subpages\", 2, \"subpages\", 1]"
                },
                {
                  "title": "Getting started with Analytics",
                  "description": null,
                  "content": {
                    "Types of metrics": "Metrics collected from your assistant can broadly be categorized as\n\n['User Analytics: Who are the users of the assistant, and how do they feel\\nabout it?  Examples: demographics, channels, sentiment analysis', 'Usage Analytics:  How is the assistant’s overall health and what kind of\\ntraffic is coming to it?  Examples:  total number of sessions, time per\\nsession, errors and error rates', 'Conversation Analytics: What happened during the conversation?\\nExamples: number of messages sent, abandonment depth, number of topics\\nintroduced by user, top N intents', 'Business Analytics: How is the assistant performing with regard to business goals?\\nExamples: ROI of assistant per LoB, time comparison of assistant vs agent, containment rate']\n\nIn this version of the Analytics pipeline, measurement of the following metrics\nis possible\n\n| Metric | Category | Meaning |\n|--------|----------|---------|\n| Number of conversations| Usage Analytics | Total number of conversations |\n| Number of users | Usage Analytics | Total number of users |\n| Number of sessions | Usage Analytics | Gross traffic to assistant |\n| Channels used | Usage Analytics | Sessions by channel |\n| User session count | User Analytics | Total number of user sessions or average sessions per user |\n| Top N intents | Conversation Analytics | Top intents across all users |\n| Avg response time | Conversation Analytics | Average response time for assistant |\n| Containment rate | Business Analytics | % of conversations handled purely by assistant (not handed to human agent |\n| Abandonment rate | Business Analytics | % of abandoned conversations |\n| Escalation rate | Business Analytics | % of conversations escalated to human agent |\n\nFor examples of how you can extract these metrics,\nsee [Example queries](./example-queries.mdx).",
                    "Prerequisites": [
                      "A production deployment of Kafka is required to set up Rasa Pro.\nWe recommend using [Amazon Managed Streaming for\nApache Kafka](https://aws.amazon.com/msk/).",
                      "A production deployment of a data lake needs to be connected to\nthe data pipeline. Rasa Pro directly supports the following data lakes:",
                      [
                        "[PostgreSQL](https://aws.amazon.com/rds/postgresql/) (\n**recommended**. All PostgreSQL >= 11.0 are supported)",
                        "[Amazon Redshift](https://aws.amazon.com/redshift/)"
                      ],
                      "Virtually any other data lakes can be configured to sync with your deployment of PostgreSQL.\nYou can find additional instructions on how to connect your PostgreSQL deployment to either [BigQuery](#bigquery)\nor [Snowflake](#snowflake) in the [Connect a data warehouse step](#2-connect-a-data-warehouse).",
                      "We recommend managed deployments of your data lake to minimize maintenance\nefforts."
                    ],
                    "1. Connect an assistant": "To connect an assistant to Rasa Pro Services, you need to connect the assistant\nto an event broker. The assistant will stream all events to the event broker,\nwhich will then be consumed by Rasa Pro Services.\n\nThe configuration of the assistant is the first step of\n[Installation and Configuration](./deploy/deploy-rasa-pro-services.mdx/#installation-and-configuration).\nNo additional configuration is required to connect the assistant to the\nAnalytics pipeline. After the assistant is deployed, the Analytics pipeline\nwill receive the data from the assistant and persist it to your\ndata warehouse which will be configured in the next step.",
                    "2. Connect a data warehouse": {
                      "PostgreSQL": "You can use Amazon Relational Database Service (RDS) to create a PostgreSQL\nDB instance which is the environment that will run your PostgreSQL database.\n\nFirst, you must set up Amazon RDS by completing the instructions listed\n[here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SettingUp.html).\nNext, create the PostgreSQL DB instance. You can follow one of the\nfollowing instruction sets:\n\n['the AWS _Easy create_ instructions listed in the [**Creating a PostgreSQL DB instance** section](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html#CHAP_GettingStarted.Creating.PostgreSQL)', 'the AWS [_Standard create_ instructions](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateDBInstance.html)']\n\nTo build the DB URL in the DBAPI format specified in the\n[**Connect an assistance** section](#connect-an-assistant) you must enter\nthe database credentials. You must obtain the database username and password\nafter you select a database authentication option during the process of\nthe PostgreSQL DB instance creation:\n\n['**Password authentication** to use database credentials only, in which\\ncase you must enter a username for the master username, as well as generate\\nthe master password.', '[**Password and IAM DB authentication**](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html)\\nto use IAM users and roles for the authentication of database users.', '[**Password and Kerberos authentication**](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/postgresql-kerberos.html)']\n\nFinally, when running the Analytics pipeline Docker container, set the\n[environment variable `RASA_ANALYTICS_DB_URL`](../../deploy/deploy-rasa-pro-services.mdx#docker-container-configuration-reference)\nto the PostgreSQL Amazon RDS DB instance URL.",
                      "Redshift": {
                        "Streaming from PostgreSQL to Redshift": "If you meet the [prerequisites](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites)\nfor using an Amazon Redshift database as a target, you will need to implement two steps:\n\n['configure the PostgreSQL source for AWS Database Migration Service\\n(DMS) by following these [instructions](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html)', 'configure the Redshift target for AWS DMS following the instructions\\n[here](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html).']",
                        "Direct connection": "You can get started on enabling the direct connection with Redshift by\nfollowing these resources on creating an [Amazon Redshift cluster](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html).\nBefore you do so, you should take into account that streaming historical\ndata directly to Redshift could take much longer than streaming directly\nto or via the PostgreSQL RDS instance.\n\nYou must update the `RASA_ANALYTICS_DB_URL` to the Redshift cluster DB URL\nwhich must follow the following format:\n\n```\nredshift://<USER>:<PASSWORD>@<AWS URL>:5439/<DB NAME>\n```\n\nFor example:\n\n```\nredshift://awsuser:4324312adfaGQ@analytics.cp1yucixmagz.us-east-1.redshift.amazonaws.com:5439/analytics\n```\n\n:::caution Redshift write performance\n\nAs a result of performance considerations, we strongly advise against\nchoosing a direct connection to Redshift. Redshift is a great data lake for\nanalytics but lacks the necessary write performance to directly stream data to it.\n\n:::"
                      },
                      "BigQuery": "To stream data from PostgreSQL to BigQuery, you can use [Datastream for BigQuery](https://cloud.google.com/datastream/docs).\nDatastream for BigQuery supports several [PostgreSQL deployment types](https://cloud.google.com/datastream/docs/configure-your-source-postgresql-database#overview),\nincluding [CloudSQL](https://cloud.google.com/sql/docs/postgres).\n\nBefore you begin, make sure to check the Datastream [prerequisites](https://cloud.google.com/datastream/docs/before-you-begin), as well as\nadditional Datastream networking connectivity [requirements](https://cloud.google.com/datastream/docs/quickstart-replication-to-bigquery#requirements).\n\nYou can closely follow [this quickstart guide](https://cloud.google.com/datastream/docs/quickstart-replication-to-bigquery) on replicating data from PostgreSQL CloudSQL to BigQuery with Datastream.\n\nAlternatively, you can deep dive into the following Datastream set-up guides:\n\n['[Configure your source PostgreSQL database](https://cloud.google.com/datastream/docs/configure-your-source-postgresql-database)', 'Optional: use [customer-managed encryption keys](https://cloud.google.com/datastream/docs/use-cmek)', 'Create a [connection profile for PostgreSQL database](https://cloud.google.com/datastream/docs/create-connection-profiles#cp4postgresdb)', 'Create a [stream](https://cloud.google.com/datastream/docs/create-a-stream)']",
                      "Snowflake": "You can sync your PostgreSQL deployment manually or via an automated [partner solution](https://docs.snowflake.com/en/user-guide/ecosystem-etl.html).\n\nThe instructions for manual sync include the following steps:\n\n['Extract data from PostgreSQL to file using `COPY INTO` [command](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location.html).\\nYou should also explore the Snowflake [data loading best practices](https://docs.snowflake.com/en/user-guide/data-load-considerations.html) before extraction.', 'Stage the extracted data files to either internal or external locations such as AWS S3, Google Cloud Storage or Microsoft Azure.', 'Copy staged files to Snowflake tables using `COPY INTO` [command](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html).\\nYou can decide to use [bulk data loading](https://docs.snowflake.com/en/user-guide/data-load-bulk.html) into Snowflake or to load continuously using [Snowpipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe.html).\\nAlternatively you can also benefit from [this plugin](https://cdap.atlassian.net/wiki/spaces/DOCS/pages/694157985/Cloud+Storage+to+Snowflake+Action) to load data to an existing Snowflake table.']"
                    },
                    "3. Ingest past conversations (optional)": "When Analytics is connected to your Kafka instance, it will consume\nall prior events on the Kafka topic and ingest them into the database.\nKafka has a retention policy for events on a [topic which defaults to\n7 days](https://docs.confluent.io/platform/current/installation/configuration/topic-configs.html#topicconfigs_retention.ms).\n\nIf you want to process events from conversations that are older than the\nretention policy configured for the Rasa topic, you can manually\ningest events from past conversations.\n\nManually ingesting data from past conversations requires a connection to the\ntracker store. The tracker store contains past conversations and a\nconnection to the Kafka cluster. Use the `rasa export` command to export\nthe events stored in the tracker store to Kafka:\n\n```\nrasa export --endpoints endpoints.yml\n```\n\nConfigure the export to read from your production tracker store\nand write to Kafka as an event broker, e.g.\n\n```\n  tracker_store:\n      type: SQL\n      dialect: \"postgresql\"\n      url: \"localhost\"\n      db: \"tracker\"\n      username: postgres\n      password: password\n\n  event_broker:\n      type: kafka\n      topic: rasa-events\n      url: localhost:29092\n      partition_by_sender: true\n```\n\n:::note\n\nRunning manual ingestion of past events multiple times will result in\nduplicated events. There is currently no deduplication implemented in\nAnalytics. Every ingested event will be stored in the database,\neven if it was processed previously.\n:::",
                    "4. Connect a BI Solution": {
                      "Example: Metabase": "Metabase is a free and open-source business intelligence platform. It\nprovides a simple interface to query and visualize data. Metabase can\nbe connected to PostgreSQL or Redshift databases.\n\n['[Connecting Metabase to PostgreSQL](https://www.metabase.com/data_sources/postgresql)', '[Connecting Metabase to Redshift](https://www.metabase.com/data_sources/amazon-redshift)']",
                      "Example: Tableau": "Tableau is a business intelligence platform. It provides a flexible interface\nto build business intelligence dashboards. Tableau can be connected to\nPostgreSQL or Redshift databases.\n\n['[Connecting Tableau to PostgreSQL](https://help.tableau.com/current/pro/desktop/en-us/examples_postgresql.htm)', '[Connecting Tableau to Redshift](https://help.tableau.com/current/pro/desktop/en-us/examples_amazonredshift.htm)']"
                    }
                  },
                  "metadata": {
                    "id": "getting-started-with-analytics",
                    "sidebar_label": "Getting started",
                    "title": "Getting started with Analytics",
                    "description": null,
                    "abstract": "Visualise and process Rasa assistant metrics in the tooling of choice."
                  },
                  "subpages": [],
                  "path": "[\"subpages\", 1, \"subpages\", 61, \"subpages\", 2, \"subpages\", 2]"
                },
                {
                  "title": "Real-Time Analysis of Markers",
                  "description": null,
                  "content": {
                    "Defining Markers": "Please consult the [Markers](../../markers.mdx/#defining-markers) section of Rasa documentation\nfor details about defining markers.",
                    "Enable Real-time Processing": {
                      "Configuring the CLI command": "Visit our [CLI page](./command-line-interface.mdx#rasa-marker-upload)\nfor more information about the command line arguments available."
                    },
                    "How are Markers processed?": "The markers YAML file describes the pattern of events for marker extraction.\nOnce the YAML files are uploaded, the patterns to be used for marker extraction are\nstored in the `rasa_patterns` table. As the Kafka Consumer starts receiving events\nfrom the Rasa Assistant, it starts analyzing them for markers. The Pipeline\nprocesses all the events from the Kafka Event Broker and identifies points of\ninterest in the conversation that match the marker. The extracted markers are then stored\nin the `rasa_marker` table.\n\nThe evaluation of Markers in the Pipeline is similar to the `rasa evaluate markers`\ncommand which can be used to process Markers from the conversations in Tracker Store.\nRead more about it [here](../../markers.mdx/#extracting-markers)\n\nExtracted markers are added to the `rasa_markers` table in the database\nimmediately once they are processed. Each row in this table contains\nthe foreign key identifiers for the pattern, session, sender and\nlast event when marker was extracted along with `num_preceding_user_turns`\nwhich tracks the number of turns preceding the event at which the marker applied.\nCheck out the [Data Structure Reference](data-structure-reference.mdx/#rasa-markers) page for the\ndatabase schema of relevant tables."
                  },
                  "metadata": {
                    "id": "realtime-markers",
                    "sidebar_label": "Real-Time Markers",
                    "title": "Real-Time Analysis of Markers",
                    "hide_table_of_contents": false
                  },
                  "subpages": [],
                  "path": "[\"subpages\", 1, \"subpages\", 61, \"subpages\", 2, \"subpages\", 3]"
                }
              ],
              "path": "[\"subpages\", 1, \"subpages\", 61, \"subpages\", 2]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 61]"
        },
        {
          "title": "Telemetry",
          "description": "Documentation section: telemetry",
          "content": {},
          "metadata": {
            "type": "directory",
            "path": "/home/anhnh/CodeWikiBench/data/rasa/original/docs/docs/telemetry"
          },
          "subpages": [
            {
              "title": "Rasa Telemetry",
              "description": null,
              "content": {
                "How to opt-out": "You can opt out of telemetry reporting at any time by running the command:\n\n```\nrasa telemetry disable\n```\n\nor by defining `RASA_TELEMETRY_ENABLED=false` as an environment variable.\nIf you want to enable reporting again, you can run:\n\n```\nrasa telemetry enable\n```",
                "Why do we use telemetry reporting?": "**Anonymous** telemetry data allow us to prioritize our research efforts\nand feature development based on usage. We want to collect aggregated\ninformation on usage and reliability so that we can ensure a high-quality product.\n\nSo how will we use the reported telemetry data? Here are some examples\nof what we use the data for:\n\n['We will be able to know which languages, pipelines and policies are used.\\nThis will enable us to direct our research efforts towards text and\\ndialogue handling projects that will have the biggest impact for our users.', 'We will be able to know data set sizes and general structure (e.g. the number\\nof intents). This allows us to better test our software on different types\\nof data sets and optimize the frameworks performance.', 'We will be able to get more detail on the types of errors you are running\\ninto while building an assistant (e.g. initialization, training, etc.).\\nThis will let us improve the quality of our framework and better focus our\\ntime on solving more common, frustrating issues.']",
                "What about sensitive data?": "Your sensitive data never leaves your machine. We:\n\n[\"**don't** report any personal identifiable information\", \"**don't** report your training data\", \"**don't** report any messages your assistant receives or sends\"]\n\n:::note Inspect what is reported\nYou can view all the telemetry information that is reported\nby defining the environment variable `RASA_TELEMETRY_DEBUG=true`, for example when running the train command:\n\n```\nRASA_TELEMETRY_DEBUG=true rasa train\n```\n\nWhen you set `RASA_TELEMETRY_DEBUG` no information will be sent to any server,\ninstead it will be logged to the commandline as a json dump for you to inspect.\n:::",
                "What do we report?": "Rasa reports aggregated usage details, command invocations, performance\nmeasurements and errors.\nWe use the telemetry data to better understand usage patterns. The reported data\nwill directly allow us to better decide how to design future features\nand prioritize current work.\n\nSpecifically, we collect the following information for all telemetry events:\n\n['Type of the reported event (e.g. *Training Started*)', 'Rasa machine ID: This is generated with a UUID and stored in the global Rasa\\nconfig at `~/.config/rasa/global.yml` and sent as `metrics_id`', 'One-way hash of the current working directory or a hash of the git remote', 'General OS level information (operating system, number of CPUs, number of\\nGPUs and whether the command is run inside a CI)', 'Current Rasa and Python version', 'Whether the command is run inside a Docker container', 'Hash of the license (if you are using Rasa Pro)']\n\nHere is an example report that shows the data reported to Rasa after running\n`rasa train`:\n\n```\n{\n  \"userId\": \"38d23c36c9be443281196080fcdd707d\",\n  \"event\": \"Training Started\",\n  \"properties\": {\n    \"language\": \"en\",\n    \"num_intent_examples\": 68,\n    \"num_entity_examples\": 0,\n    \"num_actions\": 17,\n    \"num_templates\": 6,\n    \"num_conditional_response_variations\": 5,\n    \"num_slot_mappings\": 10,\n    \"num_custom_slot_mappings\": 2,\n    \"num_conditional_slot_mappings\": 3,\n    \"num_slots\": 0,\n    \"num_forms\": 0,\n    \"num_intents\": 6,\n    \"num_entities\": 0,\n    \"num_story_steps\": 5,\n    \"num_lookup_tables\": 0,\n    \"num_synonyms\": 0,\n    \"num_regexes\": 0,\n    \"metrics_id\": \"38d23c36c9be443281196080fcdd707d\"\n  },\n  \"context\": {\n    \"os\": {\n      \"name\": \"Darwin\",\n      \"version\": \"19.4.0\"\n    },\n    \"ci\": false,\n    \"project\": \"a0a7178e6e5f9e6484c5cfa3ea4497ffc0c96d0ad3f3ad8e9399a1edd88e3cf4\",\n    \"python\": \"3.7.5\",\n    \"rasa_open_source\": \"2.0.0\",\n    \"cpu\": 16,\n    \"docker\": false,\n    \"license_hash\": \"t1a7170e6e5f9e6484c5cfa3ea4497ffc0c96a0ad3f3ad8e9399adadd88e3cf5\"\n  }\n}\n```\n\nWe **cannot identify individual users** from the dataset. It is anonymized and\nuntraceable back to the user."
              },
              "metadata": {
                "id": "telemetry",
                "sidebar_label": "Rasa Telemetry",
                "title": "Rasa Telemetry",
                "abstract": "Rasa uses telemetry to report anonymous usage information. This information\nis essential to help improve Rasa for all users.\n"
              },
              "subpages": [],
              "path": "[\"subpages\", 1, \"subpages\", 62, \"subpages\", 0]"
            }
          ],
          "path": "[\"subpages\", 1, \"subpages\", 62]"
        }
      ],
      "path": "[\"subpages\", 1]"
    }
  ]
}