{
  "title": "logstash",
  "description": "Documentation for logstash",
  "content": {},
  "metadata": {
    "type": "root",
    "path": "/home/anhnh/CodeWiki-Benchmarking-System/data/logstash/original/docs"
  },
  "subpages": [
    {
      "title": "Extend",
      "description": "Documentation section: extend",
      "content": {},
      "metadata": {
        "type": "directory",
        "path": "/home/anhnh/CodeWiki-Benchmarking-System/data/logstash/original/docs/extend"
      },
      "subpages": [
        {
          "title": "How to write a Logstash codec plugin [codec-new-plugin]",
          "description": null,
          "content": {
            "Get started [_get_started_2]": {
              "Create a GitHub repo for your new plugin [_create_a_github_repo_for_your_new_plugin_2]": "Each Logstash plugin lives in its own GitHub repository. To create a new repository for your plugin:\n\n['Log in to GitHub.', 'Click the **Repositories** tab. You’ll see a list of other repositories you’ve forked or contributed to.', 'Click the green **New** button in the upper right.', 'Specify the following settings for your new repo:', ['**Repository name**\\u2009—\\u2009a unique name of the form `logstash-codec-pluginname`.', '**Public or Private**\\u2009—\\u2009your choice, but the repository must be Public if you want to submit it as an official plugin.', '**Initialize this repository with a README**\\u2009—\\u2009enables you to immediately clone the repository to your computer.'], 'Click **Create Repository**.']",
              "Use the plugin generator tool [_use_the_plugin_generator_tool_2]": "You can create your own Logstash plugin in seconds! The `generate` subcommand of `bin/logstash-plugin` creates the foundation for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you can start adding custom code to process data with Logstash.\n\nFor more information, see [Generating plugins](/reference/plugin-generator.md)",
              "Copy the codec code [_copy_the_codec_code]": "Alternatively, you can use the examples repo we host on github.com\n\n['**Clone your plugin.** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``codec-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash``-codec-MYPLUGINNAME.git`'], '`cd logstash-codec-MYPLUGINNAME`'], '**Clone the codec plugin example and copy it to your plugin branch.**', 'You don’t want to include the example .git directory or its contents, so delete it before you copy the example.', ['`cd /tmp`', '`git clone https://github.com/logstash-plugins/logstash``-codec-example.git`', '`cd logstash-codec-example`', '`rm -rf .git`', '`cp -R * /path/to/logstash-codec-mypluginname/`'], '**Rename the following files to match the name of your plugin.**', ['`logstash-codec-example.gemspec`', '`example.rb`', '`example_spec.rb`', '```\\ncd /path/to/logstash-codec-mypluginname\\nmv logstash-codec-example.gemspec logstash-codec-mypluginname.gemspec\\nmv lib/logstash/codecs/example.rb lib/logstash/codecs/mypluginname.rb\\nmv spec/codecs/example_spec.rb spec/codecs/mypluginname_spec.rb\\n```']]\n\nYour file structure should look like this:\n\n```\n$ tree logstash-codec-mypluginname\n├── Gemfile\n├── LICENSE\n├── README.md\n├── Rakefile\n├── lib\n│   └── logstash\n│       └── codecs\n│           └── mypluginname.rb\n├── logstash-codec-mypluginname.gemspec\n└── spec\n    └── codecs\n        └── mypluginname_spec.rb\n```\n\nFor more information about the Ruby gem file structure and an excellent walkthrough of the Ruby gem creation process, see [http://timelessrepo.com/making-ruby-gems](http://timelessrepo.com/making-ruby-gems)",
              "See what your plugin looks like [_see_what_your_plugin_looks_like_2]": "Before we dive into the details, open up the plugin file in your favorite text editor and take a look.\n\n```\nrequire \"logstash/codecs/base\"\nrequire \"logstash/codecs/line\"\n\n# Add any asciidoc formatted documentation here\nclass LogStash::Codecs::Example < LogStash::Codecs::Base\n\n  # This example codec will append a string to the message field\n  # of an event, either in the decoding or encoding methods\n  #\n  # This is only intended to be used as an example.\n  #\n  # input {\n  #   stdin { codec => example }\n  # }\n  #\n  # or\n  #\n  # output {\n  #   stdout { codec => example }\n  # }\n  config_name \"example\"\n\n  # Append a string to the message\n  config :append, :validate => :string, :default => ', Hello World!'\n\n  public\n  def register\n    @lines = LogStash::Codecs::Line.new\n    @lines.charset = \"UTF-8\"\n  end\n\n  public\n  def decode(data)\n    @lines.decode(data) do |line|\n      replace = { \"message\" => line[\"message\"].to_s + @append }\n      yield LogStash::Event.new(replace)\n    end\n  end # def decode\n\n  public\n  def encode(event)\n    @on_event.call(event, event.get(\"message\").to_s + @append + NL)\n  end # def encode\n\nend # class LogStash::Codecs::Example\n```"
            },
            "Coding codec plugins [_coding_codec_plugins]": {
              "`require` Statements [_require_statements_2]": "Logstash codec plugins require parent classes defined in `logstash/codecs/base` and logstash/namespace:\n\n```\nrequire \"logstash/codecs/base\"\nrequire \"logstash/namespace\"\n```\n\nOf course, the plugin you build may depend on other code, or even gems. Just put them here along with these Logstash dependencies."
            },
            "Plugin Body [_plugin_body_2]": {
              "`class` Declaration [_class_declaration_2]": "The codec plugin class should be a subclass of `LogStash::Codecs::Base`:\n\n```\nclass LogStash::Codecs::Example < LogStash::Codecs::Base\n```\n\nThe class name should closely mirror the plugin name, for example:\n\n```\nLogStash::Codecs::Example\n```",
              "`config_name` [_config_name_2]": "```\n  config_name \"example\"\n```\n\nThis is the name your plugin will call inside the codec configuration block.\n\nIf you set `config_name \"example\"` in your plugin code, the corresponding Logstash configuration block would need to look like this:"
            },
            "Configuration Parameters [_configuration_parameters_2]": "```\n  config :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\nThe configuration, or `config` section allows you to define as many (or as few) parameters as are needed to enable Logstash to process events.\n\nThere are several configuration attributes:\n\n['`:validate` - allows you to enforce passing a particular data type to Logstash for this configuration option, such as `:string`, `:password`, `:boolean`, `:number`, `:array`, `:hash`, `:path` (a file-system path), `uri`, `:codec` (since 1.2.0), `:bytes`.  Note that this also works as a coercion in that if I specify \"true\" for boolean (even though technically a string), it will become a valid boolean in the config.  This coercion works for the `:number` type as well where \"1.2\" becomes a float and \"22\" is an integer.', '`:default` - lets you specify a default value for a parameter', '`:required` - whether or not this parameter is mandatory (a Boolean `true` or', '`:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable. `false`)', '`:deprecated` - informational (also a Boolean `true` or `false`)', '`:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.']",
            "Plugin Methods [_plugin_methods_2]": {
              "`register` Method [_register_method_2]": "```\n  public\n  def register\n  end # def register\n```\n\nThe Logstash `register` method is like an `initialize` method. It was originally created to enforce having `super` called, preventing headaches for newbies. (Note: It may go away in favor of `initialize`, in conjunction with some enforced testing to ensure `super` is called.)\n\n`public` means the method can be called anywhere, not just within the class. This is the default behavior for methods in Ruby, but it is specified explicitly here anyway.\n\nYou can also assign instance variables here (variables prepended by `@`). Configuration variables are now in scope as instance variables, like `@message`",
              "`decode` Method [_decode_method]": "```\n  public\n  def decode(data)\n    @lines.decode(data) do |line|\n      replace = { \"message\" => line[\"message\"].to_s + @append }\n      yield LogStash::Event.new(replace)\n    end\n  end # def decode\n```\n\nThe codec’s `decode` method is where data coming in from an input is transformed into an event.  There are complex examples like the [collectd](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/lib/logstash/codecs/collectd.rb#L386-L484) codec, and simpler examples like the [spool](https://github.com/logstash-plugins/logstash-codec-spool/blob/main/lib/logstash/codecs/spool.rb#L11-L16) codec.\n\nThere must be a `yield` statement as part of the `decode` method which will return decoded events to the pipeline.",
              "`encode` Method [_encode_method]": "```\n  public\n  def encode(event)\n    @on_event.call(event, event.get(\"message\").to_s + @append + NL)\n  end # def encode\n```\n\nThe `encode` method takes an event and serializes it (*encodes*) into another format.  Good examples of `encode` methods include the simple [plain](https://github.com/logstash-plugins/logstash-codec-plain/blob/main/lib/logstash/codecs/plain.rb#L39-L46) codec, the slightly more involved [msgpack](https://github.com/logstash-plugins/logstash-codec-msgpack/blob/main/lib/logstash/codecs/msgpack.rb#L38-L46) codec, and even an [avro](https://github.com/logstash-plugins/logstash-codec-avro/blob/main/lib/logstash/codecs/avro.rb#L38-L45) codec.\n\nIn most cases, your `encode` method should have an `@on_event.call()` statement. This call will output data per event in the described way."
            },
            "Building the Plugin [_building_the_plugin_2]": {
              "External dependencies [_external_dependencies_2]": "A `require` statement in Ruby is used to include necessary code. In some cases your plugin may require additional files.  For example, the collectd plugin [uses](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/lib/logstash/codecs/collectd.rb#L148) the `types.db` file provided by collectd.  In the main directory of your plugin, a file called `vendor.json` is where these files are described.\n\nThe `vendor.json` file contains an array of JSON objects, each describing a file dependency. This example comes from the [collectd](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/vendor.json) codec plugin:\n\n```\n[{\n        \"sha1\": \"a90fe6cc53b76b7bdd56dc57950d90787cb9c96e\",\n        \"url\": \"http://collectd.org/files/collectd-5.4.0.tar.gz\",\n        \"files\": [ \"/src/types.db\" ]\n}]\n```\n\n['`sha1` is the sha1 signature used to verify the integrity of the file referenced by `url`.', '`url` is the address from where Logstash will download the file.', '`files` is an optional array of files to extract from the downloaded file. Note that while tar archives can use absolute or relative paths, treat them as absolute in this array.  If `files` is not present, all files will be uncompressed and extracted into the vendor directory.']\n\nAnother example of the `vendor.json` file is the [`geoip` filter](https://github.com/logstash-plugins/logstash-filter-geoip/blob/main/vendor.json)\n\nThe process used to download these dependencies is to call `rake vendor`.  This will be discussed further in the testing section of this document.\n\nAnother kind of external dependency is on jar files.  This will be described in the \"Add a `gemspec` file\" section.",
              "Deprecated features [_deprecated_features_2]": "As a plugin evolves, an option or feature may no longer serve the intended purpose, and the developer may want to *deprecate* its usage. Deprecation warns users about the option’s status, so they aren’t caught by surprise when it is removed in a later release.\n\n{{ls}} 7.6 introduced a *deprecation logger* to make handling those situations easier. You can use the [adapter](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support) to ensure that your plugin can use the deprecation logger while still supporting older versions of {{ls}}. See the [readme](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support/blob/main/README.md) for more information and for instructions on using the adapter.\n\nDeprecations are noted in the `logstash-deprecation.log` file in the `log` directory.",
              "Add a Gemfile [_add_a_gemfile_2]": "Gemfiles allow Ruby’s Bundler to maintain the dependencies for your plugin. Currently, all we’ll need is the Logstash gem, for testing, but if you require other gems, you should add them in here.\n\n::::{tip}\nSee [Bundler’s Gemfile page](http://bundler.io/gemfile.html) for more details.\n::::\n\n```\nsource 'https://rubygems.org'\ngemspec\ngem \"logstash\", :github => \"elastic/logstash\", :branch => \"master\"\n```"
            },
            "Add a `gemspec` file [_add_a_gemspec_file_2]": {
              "Runtime and Development Dependencies [_runtime_and_development_dependencies_2]": "At the bottom of the `gemspec` file is a section with a comment: `Gem dependencies`.  This is where any other needed gems must be mentioned. If a gem is necessary for your plugin to function, it is a runtime dependency. If a gem are only used for testing, then it would be a development dependency.\n\n::::{note}\nYou can also have versioning requirements for your dependencies—​including other Logstash plugins:\n\n```\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\n```\n\nThis gemspec has a runtime dependency on the logstash-core-plugin-api and requires that it have a version number greater than or equal to version 1.60 and less than or equal to version 2.99.\n\n::::\n\n::::{important}\nAll plugins have a runtime dependency on the `logstash-core-plugin-api` gem, and a development dependency on `logstash-devutils`.\n::::",
              "Jar dependencies [_jar_dependencies_2]": "In some cases, such as the [Elasticsearch output plugin](https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/main/logstash-output-elasticsearch.gemspec#L22-L23), your code may depend on a jar file.  In cases such as this, the dependency is added in the gemspec file in this manner:\n\n```\n  # Jar dependencies\n  s.requirements << \"jar 'org.elasticsearch:elasticsearch', '5.0.0'\"\n  s.add_runtime_dependency 'jar-dependencies'\n```\n\nWith these both defined, the install process will search for the required jar file at [http://mvnrepository.com](http://mvnrepository.com) and download the specified version."
            },
            "Document your plugin [_document_your_plugin_2]": "Documentation is an important part of your plugin. All plugin documentation is rendered and placed in the [Logstash Reference](/reference/index.md) and the [Versioned plugin docs](logstash-docs-md://vpr/integration-plugins.md).\n\nSee [Document your plugin](/extend/plugin-doc.md) for tips and guidelines.",
            "Add Tests [_add_tests_2]": "Logstash loves tests. Lots of tests. If you’re using your new codec plugin in a production environment, you’ll want to have some tests to ensure you are not breaking any existing functionality.\n\n::::{note}\nA full exposition on RSpec is outside the scope of this document. Learn more about RSpec at [http://rspec.info](http://rspec.info)\n::::\n\nFor help learning about tests and testing, look in the `spec/codecs/` directory of several other similar plugins.",
            "Clone and test! [_clone_and_test_2]": "Now let’s start with a fresh clone of the plugin, build it and run the tests.\n\n['**Clone your plugin into a temporary location** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``codec-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash-``codec-MYPLUGINNAME.git`'], '`cd logstash-codec-MYPLUGINNAME`']]\n\nThen, you’ll need to install your plugins dependencies with bundler:\n\n```\nbundle install\n```\n\n::::{important}\nIf your plugin has an external file dependency described in `vendor.json`, you must download that dependency before running or testing.  You can do this by running:\n\n```\nrake vendor\n```\n\n::::\n\nAnd finally, run the tests:\n\n```\nbundle exec rspec\n```\n\nYou should see a success message, which looks something like this:\n\n```\nFinished in 0.034 seconds\n1 example, 0 failures\n```\n\nHooray! You’re almost there! (Unless you saw failures…​ you should fix those first).",
            "Building and Testing [_building_and_testing_2]": {
              "Build [_build_2]": "You already have all the necessary ingredients, so let’s go ahead and run the build command:\n\n```\ngem build logstash-codec-example.gemspec\n```\n\nThat’s it!  Your gem should be built and be in the same path with the name\n\n```\nlogstash-codec-mypluginname-0.1.0.gem\n```\n\nThe `s.version` number from your gemspec file will provide the gem version, in this case, `0.1.0`.",
              "Test installation [_test_installation_2]": "You should test install your plugin into a clean installation of Logstash. Download the latest version from the [Logstash downloads page](https://www.elastic.co/downloads/logstash/).\n\n['Untar and cd in to the directory:', '```\\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\\ntar xzvf logstash-9.0.0.tar.gz\\ncd logstash-9.0.0\\n```', 'Using the plugin tool, we can install the gem we just built.', ['Replace `/my/logstash/plugins` with  the correct path to the gem for your environment, and `0.1.0` with the correct version number from the gemspec file.', '```\\nbin/logstash-plugin install /my/logstash/plugins/logstash-codec-example/logstash-codec-example-0.1.0.gem\\n```', 'After running this, you should see feedback from Logstash that it was successfully installed:', \"```\\nvalidating /my/logstash/plugins/logstash-codec-example/logstash-codec-example-0.1.0.gem >= 0\\nValid logstash plugin. Continuing...\\nSuccessfully installed 'logstash-codec-example' with version '0.1.0'\\n```\", '::::{tip}\\nYou can also use the Logstash plugin tool to determine which plugins are currently available:', '```\\nbin/logstash-plugin list\\n```', 'Depending on what you have installed, you might see a short or long list of plugins: inputs, codecs, filters and outputs.', '::::'], 'Now try running Logstash with a simple configuration passed in via the command-line, using the `-e` flag.', '::::{note}\\nYour results will depend on what your codec plugin is designed to do.\\n::::']\n\n```\nbin/logstash -e 'input { stdin{ codec => example{}} } output {stdout { codec => rubydebug }}'\n```\n\nThe example codec plugin will append the contents of `append` (which by default appends \", Hello World!\")\n\nAfter starting Logstash, type something, for example \"Random output string\". The resulting output message field contents should be, \"Random output string, Hello World!\":\n\n```\nRandom output string\n{\n       \"message\" => \"Random output string, Hello World!\",\n      \"@version\" => \"1\",\n    \"@timestamp\" => \"2015-01-27T19:17:18.932Z\",\n          \"host\" => \"cadenza\"\n}\n```\n\nFeel free to experiment and test this by changing the `append` parameter:\n\n```\nbin/logstash -e 'input { stdin{ codec => example{ append => \", I am appending this! }} } output {stdout { codec => rubydebug }}'\n```\n\nCongratulations! You’ve built, deployed and successfully run a Logstash codec."
            },
            "Submitting your plugin to [RubyGems.org](http://rubygems.org) and [logstash-plugins](https://github.com/logstash-plugins) [_submitting_your_plugin_to_rubygems_orghttprubygems_org_and_logstash_pluginshttpsgithub_comlogstash_plugins_2]": {
              "Licensing [_licensing_2]": "Logstash and all its plugins are licensed under [Apache License, version 2 (\"ALv2\")](https://github.com/elasticsearch/logstash/blob/main/LICENSE). If you make your plugin publicly available via [RubyGems.org](http://rubygems.org), please make sure to have this line in your gemspec:\n\n[\"`s.licenses = ['Apache License (2.0)']`\"]",
              "Publishing to [RubyGems.org](http://rubygems.org) [_publishing_to_rubygems_orghttprubygems_org_2]": "To begin, you’ll need an account on RubyGems.org\n\n['[Sign-up for a RubyGems account](https://rubygems.org/sign_up).']\n\nAfter creating an account, [obtain](http://guides.rubygems.org/rubygems-org-api/#api-authorization) an API key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials` to store your API key. These credentials will be used to publish the gem. Replace `username` and `password` with the credentials you created at RubyGems.org:\n\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\nBefore proceeding, make sure you have the right version in your gemspec file and commit your changes.\n\n[\"`s.version = '0.1.0'`\"]\n\nTo publish version 0.1.0 of your new logstash gem:\n\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n::::{note}\nExecuting `rake publish_gem`:\n\n[\"Reads the version from the gemspec file (`s.version = '0.1.0'`)\", 'Checks in your local repository if a tag exists for that version. If the tag already exists, it aborts the process. Otherwise, it creates a new version tag in your local repository.', 'Builds the gem', 'Publishes the gem to RubyGems.org']\n\n::::\n\nThat’s it! Your plugin is published! Logstash users can now install your plugin by running:\n\n```\nbin/logstash-plugin install logstash-codec-mypluginname\n```"
            },
            "Contributing your source code to [logstash-plugins](https://github.com/logstash-plugins) [_contributing_your_source_code_to_logstash_pluginshttpsgithub_comlogstash_plugins_2]": {
              "Benefits [_benefits_2]": "Some of the many benefits of having your plugin in the logstash-plugins repository are:\n\n['**Discovery.** Your plugin will appear in the [Logstash Reference](/reference/index.md), where Logstash users look first for plugins and documentation.', '**Documentation.** Your plugin documentation will automatically be added to the [Logstash Reference](/reference/index.md).', '**Testing.** With our testing infrastructure, your plugin will be continuously tested against current and future releases of Logstash.  As a result, users will have the assurance that if incompatibilities arise, they will be quickly discovered and corrected.']",
              "Acceptance Guidelines [_acceptance_guidelines_2]": [
                "**Code Review.** Your plugin must be reviewed by members of the community for coherence, quality, readability, stability and security.",
                "**Tests.** Your plugin must contain tests to be accepted.  These tests are also subject to code review for scope and completeness.  It’s ok if you don’t know how to write tests — we will guide you. We are working on publishing a guide to creating tests for Logstash which will make it easier.  In the meantime, you can refer to [http://betterspecs.org/](http://betterspecs.org/) for examples."
              ]
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/codec-new-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 0]"
        },
        {
          "title": "Logstash Plugins Community Maintainer Guide [community-maintainer]",
          "description": null,
          "content": {
            "Contribution Guidelines [_contribution_guidelines]": "For general guidance around contributing to Logstash Plugins, see the [*Contributing to Logstash*](/extend/index.md) section.",
            "Document Goals [_document_goals]": "To help make the Logstash plugins community participation easy with positive feedback.\n\nTo increase diversity.\n\nTo reduce code review, merge and release dependencies on the core team by providing support and tools to the Community and Maintainers.\n\nTo support the natural life cycle of a plugin.\n\nTo codify the roles and responsibilities of: Maintainers and Contributors with specific focus on patch testing, code review, merging and release.",
            "Development Workflow [_development_workflow]": {
              "Terminology [_terminology_2]": "A \"Contributor\" is a role a person assumes when providing a patch. Contributors will not have commit access to the repository. They need to sign the Elastic [Contributor License Agreement](https://www.elastic.co/contributor-agreement) before a patch can be reviewed. Contributors can add themselves to the plugin Contributor list.\n\nA \"Maintainer\" is a role a person assumes when maintaining a plugin and keeping it healthy, including triaging issues, and reviewing and merging patches.",
              "Patch Requirements [_patch_requirements]": "A patch is a minimal and accurate answer to exactly one identified and agreed upon problem. It must conform to the [code style guidelines](https://github.com/elastic/logstash/blob/main/STYLE.md) and must include RSpec tests that verify the fitness of the solution.\n\nA patch will be automatically tested by a CI system that will report on the Pull Request status.\n\nA patch CLA will be automatically verified and reported on the Pull Request status.\n\nA patch commit message has a single short (less than 50 character) first line summarizing the change, a blank second line, and any additional lines as necessary for change explanation and rationale.\n\nA patch is mergeable when it satisfies the above requirements and has been reviewed positively by at least one other person.",
              "Development Process [_development_process]": "A user will log an issue on the issue tracker describing the problem they face or observe with as much detail as possible.\n\nTo work on an issue, a Contributor forks the plugin repository and then works on their forked repository and submits a patch by creating a pull request back to the plugin.\n\nMaintainers must not merge patches where the author has not signed the CLA.\n\nBefore a patch can be accepted it should be reviewed. Maintainers should merge accepted patches without delay.\n\nMaintainers should not merge their own patches except in exceptional cases, such as non-responsiveness from other Maintainers or core team for an extended period (more than 2 weeks).\n\nReviewer’s comments should not be based on personal preferences.\n\nThe Maintainers should label Issues and Pull Requests.\n\nMaintainers should involve the core team if help is needed to reach consensus.\n\nReview non-source changes such as documentation in the same way as source code changes.",
              "Branch Management [_branch_management]": "The plugin has a main branch that always holds the latest in-progress version and should always build.  Topic branches should kept to the minimum.",
              "Changelog Management [_changelog_management]": {
                "Detailed format of https://www.elastic.co/guide/en/logstash/current/CHANGELOG.html [_detailed_format_of_changelog_md]": "Sharing a similar format of https://www.elastic.co/guide/en/logstash/current/CHANGELOG.html in plugins ease readability for users. Please see following annotated example and see a concrete example in [logstash-filter-date](https://raw.githubusercontent.com/logstash-plugins/logstash-filter-date/main/https://www.elastic.co/guide/en/logstash/current/CHANGELOG.html).\n\n```\n## 1.0.x                              <1>\n - change description                 <2>\n - tag: change description            <3>\n - tag1,tag2: change description      <4>\n - tag: Multi-line description        <5>\n   must be indented and can use\n   additional markdown syntax\n                                      <6>\n## 1.0.0                              <7>\n[...]\n```\n\n['Latest version is the first line of https://www.elastic.co/guide/en/logstash/current/CHANGELOG.html. Each version identifier should be a level-2 header using `##`', 'One change description is described as a list item using a dash `-` aligned under the version identifier', 'One change can be tagged by a word and suffixed by `:`.<br> Common tags are `bugfix`, `feature`, `doc`, `test` or `internal`.', 'One change can have multiple tags separated by a comma and suffixed by `:`', 'A multi-line change description must be properly indented', 'Please take care to **separate versions with an empty line**', 'Previous version identifier']"
              },
              "Continuous Integration [_continuous_integration]": "Plugins are setup with automated continuous integration (CI) environments and there should be a corresponding badge on each Github page.  If it’s missing, please contact the Logstash core team.\n\nEvery Pull Request opened automatically triggers a CI run.  To conduct a manual run, comment “Jenkins, please test this.” on the Pull Request."
            },
            "Versioning Plugins [_versioning_plugins]": {
              "Changing the version [_changing_the_version]": "Version can be changed in the Gemspec, which needs to be associated with a changelog entry. Following this, we can publish the gem to RubyGem.org manually. At this point only the core developers can publish a gem.",
              "Labeling [_labeling]": "Labeling is a critical aspect of maintaining plugins. All issues in GitHub should be labeled correctly so it can:\n\n['Provide good feedback to users/developers', 'Help prioritize changes', 'Be used in release notes']\n\nMost labels are self explanatory, but here’s a quick recap of few important labels:\n\n['`bug`: Labels an issue as an unintentional defect', '`needs details`: If a the issue reporter has incomplete details, please ask them for more info and label as needs details.', '`missing cla`: Contributor License Agreement is missing and patch cannot be accepted without it', '`adopt me`: Ask for help from the community to take over this issue', '`enhancement`: New feature, not a bug fix', '`needs tests`: Patch has no tests, and cannot be accepted without unit/integration tests', '`docs`: Documentation related issue/PR']"
            },
            "Logging [_logging]": "Although it’s important not to bog down performance with excessive logging, debug level logs can be immensely helpful when diagnosing and troubleshooting issues with Logstash.  Please remember to liberally add debug logs wherever it makes sense as users will be forever gracious.\n\n```\n@logger.debug(\"Logstash loves debug logs!\", :actions => actions)\n```",
            "Contributor License Agreement (CLA) Guidance [_contributor_license_agreement_cla_guidance]": "Why is a [CLA](https://www.elastic.co/contributor-agreement) required?\n:   We ask this of all Contributors in order to assure our users of the origin and continuing existence of the code. We are not asking Contributors to assign copyright to us, but to give us the right to distribute a Contributor’s code without restriction.\n\nPlease make sure the CLA is signed by every Contributor prior to reviewing PRs and commits.\n:   Contributors only need to sign the CLA once and should sign with the same email as used in Github. If a Contributor signs the CLA after a PR is submitted, they can refresh the automated CLA checker by pushing another comment on the PR after 5 minutes of signing.",
            "Need Help? [_need_help]": "Ping @logstash-core on Github to get the attention of the Logstash core team.",
            "Community Administration [_community_administration]": "The core team is there to support the plugin Maintainers and overall ecosystem.\n\nMaintainers should propose Contributors to become a Maintainer.\n\nContributors and Maintainers should follow the Elastic Community [Code of Conduct](https://www.elastic.co/community/codeofconduct).  The core team should block or ban \"bad actors\"."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/community-maintainer.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 1]"
        },
        {
          "title": "Contribute To Core",
          "description": null,
          "content": {
            "Extending Logstash core [contribute-to-core]": "We also welcome contributions and bug fixes to the Logstash core feature set.\n\nPlease read through our [contribution](https://github.com/elastic/logstash/blob/main/CONTRIBUTING.md) guide, and the Logstash [readme](https://github.com/elastic/logstash/blob/main/README.md) document."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/contribute-to-core.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 2]"
        },
        {
          "title": "Contributing a patch to a Logstash plugin [contributing-patch-plugin]",
          "description": null,
          "content": {
            "Input plugins [contrib-patch-input]": {
              "Input API [input-api]": "`#register() -> nil`\n:   Required. This API sets up resources for the plugin, typically the connection to the external source.\n\n`#run(queue) -> nil`\n:   Required. This API fetches or listens for source data, typically looping until stopped. Must handle errors inside the loop. Pushes any created events to the queue object specified in the method argument. Some inputs may receive batched data to minimize the external call overhead.\n\n`#stop() -> nil`\n:   Optional. Stops external connections and cleans up."
            },
            "Codec plugins [contrib-patch-codec]": {
              "Codec API [codec-api]": "`#register() -> nil`\n:   Identical to the API of the same name for input plugins.\n\n`#decode(data){|event| block} -> nil`\n:   Must be implemented. Used to create an Event from the raw data given in the method argument. Must handle errors. The caller must provide a Ruby block. The block is called with the created Event.\n\n`#encode(event) -> nil`\n:   Required.  Used to create a structured data object from the given Event. May handle errors. This method calls a block that was previously stored as @on_event with two arguments: the original event and the data object."
            },
            "Filter plugins [contrib-patch-filter]": {
              "Filter API [filter-api]": "`#register() -> nil`\n:   Identical to the API of the same name for input plugins.\n\n`#filter(event) -> nil`\n:   Required. May handle errors. Used to apply a mutation function to the given event."
            },
            "Output plugins [contrib-patch-output]": {
              "Output API [output-api]": "`#register() -> nil`\n:   Identical to the API of the same name for input plugins.\n\n`#receive(event) -> nil`\n:   Required. Must handle errors. Used to prepare the given event for transmission to the external destination. Some outputs may buffer the prepared events to batch transmit to the destination."
            },
            "Process [patch-process]": "A bug or feature is identified. An issue is created in the plugin repository. A patch is created and a pull request (PR) is submitted. After review and possible rework the PR is merged and the plugin is published.\n\nThe [Community Maintainer Guide](/extend/community-maintainer.md) explains, in more detail, the process of getting a patch accepted, merged and published.  The Community Maintainer Guide also details the roles that contributors and maintainers are expected to perform.",
            "Testing methodologies [test-methods]": {
              "Test driven development [tdd]": "Test driven development (TDD) describes a methodology for using tests to guide evolution of source code. For our purposes, we are use only a part of it. Before writing the fix, we create tests that illustrate the bug by failing. We stop when we have written enough code to make the tests pass and submit the fix and tests as a patch. It is not necessary to write the tests before the fix, but it is very easy to write a passing test afterwards that may not actually verify that the fault is really fixed especially if the fault can be triggered via multiple execution paths or varying input data.",
              "RSpec framework [rspec]": "Logstash uses Rspec, a Ruby testing framework, to define and run the test suite. What follows is a summary of various sources.\n\n```\n 2 require \"logstash/devutils/rspec/spec_helper\"\n 3 require \"logstash/plugin\"\n 4\n 5 describe \"outputs/riemann\" do\n 6   describe \"#register\" do\n 7     let(:output) do\n 8       LogStash::Plugin.lookup(\"output\", \"riemann\").new(configuration)\n 9     end\n10\n11     context \"when no protocol is specified\" do\n12       let(:configuration) { Hash.new }\n13\n14       it \"the method completes without error\" do\n15         expect {output.register}.not_to raise_error\n16       end\n17     end\n18\n19     context \"when a bad protocol is specified\" do\n20       let(:configuration) { {\"protocol\" => \"fake\"} }\n21\n22       it \"the method fails with error\" do\n23         expect {output.register}.to raise_error\n24       end\n25     end\n26\n27     context \"when the tcp protocol is specified\" do\n28       let(:configuration) { {\"protocol\" => \"tcp\"} }\n29\n30       it \"the method completes without error\" do\n31         expect {output.register}.not_to raise_error\n32       end\n33     end\n34   end\n35\n36   describe \"#receive\" do\n37     let(:output) do\n38       LogStash::Plugin.lookup(\"output\", \"riemann\").new(configuration)\n39     end\n40\n41     context \"when operating normally\" do\n42       let(:configuration) { Hash.new }\n43       let(:event) do\n44         data = {\"message\"=>\"hello\", \"@version\"=>\"1\",\n45                 \"@timestamp\"=>\"2015-06-03T23:34:54.076Z\",\n46                 \"host\"=>\"vagrant-ubuntu-trusty-64\"}\n47         LogStash::Event.new(data)\n48       end\n49\n50       before(:example) do\n51         output.register\n52       end\n53\n54       it \"should accept the event\" do\n55         expect { output.receive event }.not_to raise_error\n56       end\n57     end\n58   end\n59 end\n```\n\n```\ndescribe(string){block} -> nil\ndescribe(Class){block} -> nil\n```\n\nWith RSpec, we are always describing the plugin method behavior. The describe block is added in logical sections and can accept either an existing class name or a string. The string used in line 5 is the plugin name. Line 6 is the register method, line 36 is the receive method. It is a RSpec convention to prefix instance methods with one hash and class methods with one dot.\n\n```\ncontext(string){block} -> nil\n```\n\nIn RSpec, context blocks define sections that group tests by a variation.  The string should start with the word `when` and then detail the variation. See line 11.  The tests in the content block should should only be for that variation.\n\n```\nlet(symbol){block} -> nil\n```\n\nIn RSpec, `let` blocks define resources for use in the test blocks. These resources are reinitialized for every test block. They are available as method calls inside the test block. Define `let` blocks in `describe` and `context` blocks, which scope the `let` block and any other nested blocks. You can use other `let` methods defined later within the `let` block body. See lines 7-9, which define the output resource and use the configuration method, defined with different variations in lines 12, 20 and 28.\n\n```\nbefore(symbol){block} -> nil - symbol is one of :suite, :context, :example, but :all and :each are synonyms for :suite and :example respectively.\n```\n\nIn RSpec, `before` blocks are used to further set up any resources that would have been initialized in a `let` block. You cannot define `let` blocks inside `before` blocks.\n\nYou can also define `after` blocks, which are typically used to clean up any setup activity performed by a `before` block.\n\n```\nit(string){block} -> nil\n```\n\nIn RSpec, `it` blocks set the expectations that verify the behavior of the tested code. The string should not start with *it* or *should*, but needs to express the outcome of the expectation.  When put together the texts from the enclosing describe, `context` and `it` blocks should form a fairly readable sentence, as in lines 5, 6, 11 and 14:\n\n```\noutputs/riemann\n#register when no protocol is specified the method completes without error\n```\n\nReadable code like this make the goals of tests easy to understand.\n\n```\nexpect(object){block} -> nil\n```\n\nIn RSpec, the expect method verifies a statement that compares an actual result to an expected result. The `expect` method is usually paired with a call to the `to` or `not_to` methods. Use the block form when expecting errors or observing for changes. The `to` or `not_to` methods require a `matcher` object that encapsulates the expected value. The argument form of the `expect` method encapsulates the actual value. When put together the whole line tests the actual against the expected value.\n\n```\nraise_error(error class|nil) -> matcher instance\nbe(object) -> matcher instance\neq(object) -> matcher instance\neql(object) -> matcher instance\n  for more see http://www.relishapp.com/rspec/rspec-expectations/docs/built-in-matchers\n```\n\nIn RSpec, a matcher is an object generated by the equivalent method call (be, eq) that will be used to evaluate the expected against the actual values."
            },
            "Putting it all together [all-together]": "This example fixes an [issue](https://github.com/logstash-plugins/logstash-output-zeromq/issues/9) in the ZeroMQ output plugin. The issue does not require knowledge of ZeroMQ.\n\nThe activities in this example have the following prerequisites:\n\n['A minimal knowledge of Git and Github. See the [Github boot camp](https://help.github.com/categories/bootcamp/).', 'A text editor.', 'A JRuby [runtime](https://www.ruby-lang.org/en/documentation/installation/#managers) [environment](https://howistart.org/posts/ruby/1). The `chruby` tool manages Ruby versions.', 'JRuby 1.7.22 or later.', 'The `bundler` and `rake` gems installed.', 'ZeroMQ [installed](http://zeromq.org/intro:get-the-software).']\n\n['In Github, fork the ZeroMQ [output plugin repository](https://github.com/logstash-plugins/logstash-output-zeromq).', 'On your local machine, [clone](https://help.github.com/articles/fork-a-repo/) the fork to a known folder such as `logstash/`.', 'Open the following files in a text editor:', ['`logstash-output-zeromq/lib/logstash/outputs/zeromq.rb`', '`logstash-output-zeromq/lib/logstash/util/zeromq.rb`', '`logstash-output-zeromq/spec/outputs/zeromq_spec.rb`'], 'According to the issue, log output in server mode must indicate `bound`. Furthermore, the test file contains no tests.', '::::{note}\\nLine 21 of `util/zeromq.rb` reads `@logger.info(\"0mq: #{server? ? \\'connected\\' : \\'bound\\'}\", :address => address)`\\n::::', 'In the text editor, require `zeromq.rb` for the file `zeromq_spec.rb` by adding the following lines:', '```\\nrequire \"logstash/outputs/zeromq\"\\nrequire \"logstash/devutils/rspec/spec_helper\"\\n```', 'The desired error message should read:', \"```\\nLogStash::Outputs::ZeroMQ when in server mode a 'bound' info line is logged\\n```\", 'To properly generate this message, add a `describe` block with the fully qualified class name as the argument, a context block, and an `it` block.', '```\\ndescribe LogStash::Outputs::ZeroMQ do\\n  context \"when in server mode\" do\\n    it \"a \\'bound\\' info line is logged\" do\\n    end\\n  end\\nend\\n```', 'To add the missing test, use an instance of the ZeroMQ output and a substitute logger. This example uses an RSpec feature called *test doubles* as the substitute logger.', 'Add the following lines to `zeromq_spec.rb`, after `describe LogStash::Outputs::ZeroMQ do` and before `context \"when in server mode\" do`:', '```\\n  let(:output) { described_class.new(\"mode\" => \"server\", \"topology\" => \"pushpull\" }\\n  let(:tracer) { double(\"logger\") }\\n```', 'Add the body to the `it` block. Add the following five lines after the line `context \"when in server mode\" do`:', '```\\n      allow(tracer).to receive(:debug)<1>\\n      output.logger = logger<2>\\n      expect(tracer).to receive(:info).with(\"0mq: bound\", {:address=>\"tcp://127.0.0.1:2120\"})<3>\\n      output.register<4>\\n      output.do_close<5>\\n```']\n\n['Allow the double to receive `debug` method calls.', 'Make the output use the test double.', 'Set an expectation on the test to receive an `info` method call.', 'Call `register` on the output.', 'Call `do_close` on the output so the test does not hang.']\n\nAt the end of the modifications, the relevant code section reads:\n\n```\nrequire \"logstash/outputs/zeromq\"\nrequire \"logstash/devutils/rspec/spec_helper\"\n\ndescribe LogStash::Outputs::ZeroMQ do\n  let(:output) { described_class.new(\"mode\" => \"server\", \"topology\" => \"pushpull\") }\n  let(:tracer) { double(\"logger\") }\n\n  context \"when in server mode\" do\n    it \"a ‘bound’ info line is logged\" do\n      allow(tracer).to receive(:debug)\n      output.logger = tracer\n      expect(tracer).to receive(:info).with(\"0mq: bound\", {:address=>\"tcp://127.0.0.1:2120\"})\n      output.register\n      output.do_close\n    end\n  end\nend\n```\n\nTo run this test:\n\n['Open a terminal window', 'Navigate to the cloned plugin folder', 'The first time you run the test, run the command `bundle install`', 'Run the command `bundle exec rspec`']\n\nAssuming all prerequisites were installed correctly, the test fails with output similar to:\n\n```\nUsing Accessor#strict_set for specs\nRun options: exclude {:redis=>true, :socket=>true, :performance=>true, :couchdb=>true, :elasticsearch=>true,\n:elasticsearch_secure=>true, :export_cypher=>true, :integration=>true, :windows=>true}\n\nLogStash::Outputs::ZeroMQ\n  when in server mode\n    a ‘bound’ info line is logged (FAILED - 1)\n\nFailures:\n\n  1) LogStash::Outputs::ZeroMQ when in server mode a ‘bound’ info line is logged\n     Failure/Error: output.register\n       Double \"logger\" received :info with unexpected arguments\n         expected: (\"0mq: bound\", {:address=>\"tcp://127.0.0.1:2120\"})\n              got: (\"0mq: connected\", {:address=>\"tcp://127.0.0.1:2120\"})\n     # ./lib/logstash/util/zeromq.rb:21:in `setup'\n     # ./lib/logstash/outputs/zeromq.rb:92:in `register'\n     # ./lib/logstash/outputs/zeromq.rb:91:in `register'\n     # ./spec/outputs/zeromq_spec.rb:13:in `(root)'\n     # /Users/guy/.gem/jruby/1.9.3/gems/rspec-wait-0.0.7/lib/rspec/wait.rb:46:in `(root)'\n\nFinished in 0.133 seconds (files took 1.28 seconds to load)\n1 example, 1 failure\n\nFailed examples:\n\nrspec ./spec/outputs/zeromq_spec.rb:10 # LogStash::Outputs::ZeroMQ when in server mode a ‘bound’ info line is logged\n\nRandomized with seed 2568\n```\n\nTo correct the error, open the `util/zeromq.rb` file in your text editor and swap the positions of the words `connected` and `bound` on line 21. Line 21 now reads:\n\n```\n@logger.info(\"0mq: #{server? ? 'bound' : 'connected'}\", :address => address)\n```\n\nRun the test again with the `bundle exec rspec` command.\n\nThe test passes with output similar to:\n\n```\nUsing Accessor#strict_set for specs\nRun options: exclude {:redis=>true, :socket=>true, :performance=>true, :couchdb=>true, :elasticsearch=>true, :elasticsearch_secure=>true, :export_cypher=>true, :integration=>true, :windows=>true}\n\nLogStash::Outputs::ZeroMQ\n  when in server mode\n    a ‘bound’ info line is logged\n\nFinished in 0.114 seconds (files took 1.22 seconds to load)\n1 example, 0 failures\n\nRandomized with seed 45887\n```\n\n[Commit](https://help.github.com/articles/fork-a-repo/#next-steps) the changes to git and Github.\n\nYour pull request is visible from the [Pull Requests](https://github.com/logstash-plugins/logstash-output-zeromq/pulls) section of the original Github repository. The plugin maintainers review your work, suggest changes if necessary, and merge and publish a new version of the plugin."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/contributing-patch-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 3]"
        },
        {
          "title": "Create Logstash plugins [contributing-java-plugin]",
          "description": null,
          "content": {
            "Process overview [_process_overview]": {
              "Let’s get started [_lets_get_started]": "Here are the example repos:\n\n['[Input plugin example](https://github.com/logstash-plugins/logstash-input-java_input_example)', '[Codec plugin example](https://github.com/logstash-plugins/logstash-codec-java_codec_example)', '[Filter plugin example](https://github.com/logstash-plugins/logstash-filter-java_filter_example)', '[Output plugin example](https://github.com/logstash-plugins/logstash-output-java_output_example)']\n\nHere are the instructions:\n\n['[How to write a Java input plugin](/extend/java-input-plugin.md)', '[How to write a Java codec plugin](/extend/java-codec-plugin.md)', '[How to write a Java filter plugin](/extend/java-filter-plugin.md)', '[How to write a Java output plugin](/extend/java-output-plugin.md)']"
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/contributing-java-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 4]"
        },
        {
          "title": "How to write a Logstash filter plugin [filter-new-plugin]",
          "description": null,
          "content": {
            "Get started [_get_started_3]": {
              "Create a GitHub repo for your new plugin [_create_a_github_repo_for_your_new_plugin_3]": "Each Logstash plugin lives in its own GitHub repository. To create a new repository for your plugin:\n\n['Log in to GitHub.', 'Click the **Repositories** tab. You’ll see a list of other repositories you’ve forked or contributed to.', 'Click the green **New** button in the upper right.', 'Specify the following settings for your new repo:', ['**Repository name**\\u2009—\\u2009a unique name of the form `logstash-filter-pluginname`.', '**Public or Private**\\u2009—\\u2009your choice, but the repository must be Public if you want to submit it as an official plugin.', '**Initialize this repository with a README**\\u2009—\\u2009enables you to immediately clone the repository to your computer.'], 'Click **Create Repository**.']",
              "Use the plugin generator tool [_use_the_plugin_generator_tool_3]": "You can create your own Logstash plugin in seconds! The `generate` subcommand of `bin/logstash-plugin` creates the foundation for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you can start adding custom code to process data with Logstash.\n\nFor more information, see [Generating plugins](/reference/plugin-generator.md)",
              "Copy the filter code [_copy_the_filter_code]": "Alternatively, you can use the examples repo we host on github.com\n\n['**Clone your plugin.** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``filter-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash``-filter-MYPLUGINNAME.git`'], '`cd logstash-filter-MYPLUGINNAME`'], '**Clone the filter plugin example and copy it to your plugin branch.**', 'You don’t want to include the example .git directory or its contents, so delete it before you copy the example.', ['`cd /tmp`', '`git clone https://github.com/logstash-plugins/logstash``-filter-example.git`', '`cd logstash-filter-example`', '`rm -rf .git`', '`cp -R * /path/to/logstash-filter-mypluginname/`'], '**Rename the following files to match the name of your plugin.**', ['`logstash-filter-example.gemspec`', '`example.rb`', '`example_spec.rb`', '```\\ncd /path/to/logstash-filter-mypluginname\\nmv logstash-filter-example.gemspec logstash-filter-mypluginname.gemspec\\nmv lib/logstash/filters/example.rb lib/logstash/filters/mypluginname.rb\\nmv spec/filters/example_spec.rb spec/filters/mypluginname_spec.rb\\n```']]\n\nYour file structure should look like this:\n\n```\n$ tree logstash-filter-mypluginname\n├── Gemfile\n├── LICENSE\n├── README.md\n├── Rakefile\n├── lib\n│   └── logstash\n│       └── filters\n│           └── mypluginname.rb\n├── logstash-filter-mypluginname.gemspec\n└── spec\n    └── filters\n        └── mypluginname_spec.rb\n```\n\nFor more information about the Ruby gem file structure and an excellent walkthrough of the Ruby gem creation process, see [http://timelessrepo.com/making-ruby-gems](http://timelessrepo.com/making-ruby-gems)",
              "See what your plugin looks like [_see_what_your_plugin_looks_like_3]": "Before we dive into the details, open up the plugin file in your favorite text editor and take a look.\n\n```\nrequire \"logstash/filters/base\"\nrequire \"logstash/namespace\"\n\n# Add any asciidoc formatted documentation here\n# This example filter will replace the contents of the default\n# message field with whatever you specify in the configuration.\n#\n# It is only intended to be used as an example.\nclass LogStash::Filters::Example < LogStash::Filters::Base\n\n  # Setting the config_name here is required. This is how you\n  # configure this filter from your Logstash config.\n  #\n  # filter {\n  #   example { message => \"My message...\" }\n  # }\n  config_name \"example\"\n\n  # Replace the message with this value.\n  config :message, :validate => :string, :default => \"Hello World!\"\n\n\n  public\n  def register\n    # Add instance variables\n  end # def register\n\n  public\n  def filter(event)\n\n    if @message\n      # Replace the event message with our message as configured in the\n      # config file.\n      event.set(\"message\", @message)\n    end\n\n    # filter_matched should go in the last line of our successful code\n    filter_matched(event)\n  end # def filter\n\nend # class LogStash::Filters::Example\n```"
            },
            "Coding filter plugins [_coding_filter_plugins]": {
              "`require` Statements [_require_statements_3]": "Logstash filter plugins require parent classes defined in `logstash/filters/base` and logstash/namespace:\n\n```\nrequire \"logstash/filters/base\"\nrequire \"logstash/namespace\"\n```\n\nOf course, the plugin you build may depend on other code, or even gems. Just put them here along with these Logstash dependencies."
            },
            "Plugin Body [_plugin_body_3]": {
              "`class` Declaration [_class_declaration_3]": "The filter plugin class should be a subclass of `LogStash::Filters::Base`:\n\n```\nclass LogStash::Filters::Example < LogStash::Filters::Base\n```\n\nThe class name should closely mirror the plugin name, for example:\n\n```\nLogStash::Filters::Example\n```",
              "`config_name` [_config_name_3]": "```\n  config_name \"example\"\n```\n\nThis is the name your plugin will call inside the filter configuration block.\n\nIf you set `config_name \"example\"` in your plugin code, the corresponding Logstash configuration block would need to look like this:"
            },
            "Configuration Parameters [_configuration_parameters_3]": "```\n  config :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\nThe configuration, or `config` section allows you to define as many (or as few) parameters as are needed to enable Logstash to process events.\n\nThere are several configuration attributes:\n\n['`:validate` - allows you to enforce passing a particular data type to Logstash for this configuration option, such as `:string`, `:password`, `:boolean`, `:number`, `:array`, `:hash`, `:path` (a file-system path), `uri`, `:codec` (since 1.2.0), `:bytes`.  Note that this also works as a coercion in that if I specify \"true\" for boolean (even though technically a string), it will become a valid boolean in the config.  This coercion works for the `:number` type as well where \"1.2\" becomes a float and \"22\" is an integer.', '`:default` - lets you specify a default value for a parameter', '`:required` - whether or not this parameter is mandatory (a Boolean `true` or', '`:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable. `false`)', '`:deprecated` - informational (also a Boolean `true` or `false`)', '`:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.']",
            "Plugin Methods [_plugin_methods_3]": {
              "`register` Method [_register_method_3]": "```\n  public\n  def register\n  end # def register\n```\n\nThe Logstash `register` method is like an `initialize` method. It was originally created to enforce having `super` called, preventing headaches for newbies. (Note: It may go away in favor of `initialize`, in conjunction with some enforced testing to ensure `super` is called.)\n\n`public` means the method can be called anywhere, not just within the class. This is the default behavior for methods in Ruby, but it is specified explicitly here anyway.\n\nYou can also assign instance variables here (variables prepended by `@`). Configuration variables are now in scope as instance variables, like `@message`",
              "`filter` Method [_filter_method]": "```\n  public\n  def filter(event)\n\n    if @message\n      # Replace the event message with our message as configured in the\n      # config file.\n      event.set(\"message\", @message)\n    end\n\n  # filter_matched should go in the last line of our successful code\n  filter_matched(event)\nend # def filter\n```\n\nThe plugin’s `filter` method is where the actual filtering work takes place! Inside the `filter` method you can refer to the event data using the `Event` object. Event is the main object that encapsulates data flow internally in Logstash and provides an [API](/reference/event-api.md) for the plugin developers to interact with the event’s content.\n\nThe `filter` method should also handle any [event dependent configuration](/reference/event-dependent-configuration.md) by explicitly calling the `sprintf` method available in Event class. For example:\n\n```\nfield_foo = event.sprintf(field)\n```\n\nNote that configuration variables are now in scope as instance variables, like `@message`\n\n```\n  filter_matched(event)\n```\n\nCalling the `filter_matched` method upon successful execution of the plugin will ensure that any fields or tags added through the Logstash configuration for this filter will be handled correctly. For example, any `add_field`, `remove_field`, `add_tag` and/or `remove_tag` actions will be performed at this time.\n\nEvent methods such as `event.cancel` are now available to control the workflow of the event being processed."
            },
            "Building the Plugin [_building_the_plugin_3]": {
              "External dependencies [_external_dependencies_3]": "A `require` statement in Ruby is used to include necessary code. In some cases your plugin may require additional files.  For example, the collectd plugin [uses](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/lib/logstash/codecs/collectd.rb#L148) the `types.db` file provided by collectd.  In the main directory of your plugin, a file called `vendor.json` is where these files are described.\n\nThe `vendor.json` file contains an array of JSON objects, each describing a file dependency. This example comes from the [collectd](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/vendor.json) codec plugin:\n\n```\n[{\n        \"sha1\": \"a90fe6cc53b76b7bdd56dc57950d90787cb9c96e\",\n        \"url\": \"http://collectd.org/files/collectd-5.4.0.tar.gz\",\n        \"files\": [ \"/src/types.db\" ]\n}]\n```\n\n['`sha1` is the sha1 signature used to verify the integrity of the file referenced by `url`.', '`url` is the address from where Logstash will download the file.', '`files` is an optional array of files to extract from the downloaded file. Note that while tar archives can use absolute or relative paths, treat them as absolute in this array.  If `files` is not present, all files will be uncompressed and extracted into the vendor directory.']\n\nAnother example of the `vendor.json` file is the [`geoip` filter](https://github.com/logstash-plugins/logstash-filter-geoip/blob/main/vendor.json)\n\nThe process used to download these dependencies is to call `rake vendor`.  This will be discussed further in the testing section of this document.\n\nAnother kind of external dependency is on jar files.  This will be described in the \"Add a `gemspec` file\" section.",
              "Deprecated features [_deprecated_features_3]": "As a plugin evolves, an option or feature may no longer serve the intended purpose, and the developer may want to *deprecate* its usage. Deprecation warns users about the option’s status, so they aren’t caught by surprise when it is removed in a later release.\n\n{{ls}} 7.6 introduced a *deprecation logger* to make handling those situations easier. You can use the [adapter](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support) to ensure that your plugin can use the deprecation logger while still supporting older versions of {{ls}}. See the [readme](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support/blob/main/README.md) for more information and for instructions on using the adapter.\n\nDeprecations are noted in the `logstash-deprecation.log` file in the `log` directory.",
              "Add a Gemfile [_add_a_gemfile_3]": "Gemfiles allow Ruby’s Bundler to maintain the dependencies for your plugin. Currently, all we’ll need is the Logstash gem, for testing, but if you require other gems, you should add them in here.\n\n::::{tip}\nSee [Bundler’s Gemfile page](http://bundler.io/gemfile.html) for more details.\n::::\n\n```\nsource 'https://rubygems.org'\ngemspec\ngem \"logstash\", :github => \"elastic/logstash\", :branch => \"master\"\n```"
            },
            "Add a `gemspec` file [_add_a_gemspec_file_3]": {
              "Runtime and Development Dependencies [_runtime_and_development_dependencies_3]": "At the bottom of the `gemspec` file is a section with a comment: `Gem dependencies`.  This is where any other needed gems must be mentioned. If a gem is necessary for your plugin to function, it is a runtime dependency. If a gem are only used for testing, then it would be a development dependency.\n\n::::{note}\nYou can also have versioning requirements for your dependencies—​including other Logstash plugins:\n\n```\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\n```\n\nThis gemspec has a runtime dependency on the logstash-core-plugin-api and requires that it have a version number greater than or equal to version 1.60 and less than or equal to version 2.99.\n\n::::\n\n::::{important}\nAll plugins have a runtime dependency on the `logstash-core-plugin-api` gem, and a development dependency on `logstash-devutils`.\n::::",
              "Jar dependencies [_jar_dependencies_3]": "In some cases, such as the [Elasticsearch output plugin](https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/main/logstash-output-elasticsearch.gemspec#L22-L23), your code may depend on a jar file.  In cases such as this, the dependency is added in the gemspec file in this manner:\n\n```\n  # Jar dependencies\n  s.requirements << \"jar 'org.elasticsearch:elasticsearch', '5.0.0'\"\n  s.add_runtime_dependency 'jar-dependencies'\n```\n\nWith these both defined, the install process will search for the required jar file at [http://mvnrepository.com](http://mvnrepository.com) and download the specified version."
            },
            "Document your plugin [_document_your_plugin_3]": "Documentation is an important part of your plugin. All plugin documentation is rendered and placed in the [Logstash Reference](/reference/index.md) and the [Versioned plugin docs](logstash-docs-md://vpr/integration-plugins.md).\n\nSee [Document your plugin](/extend/plugin-doc.md) for tips and guidelines.",
            "Add Tests [_add_tests_3]": "Logstash loves tests. Lots of tests. If you’re using your new filter plugin in a production environment, you’ll want to have some tests to ensure you are not breaking any existing functionality.\n\n::::{note}\nA full exposition on RSpec is outside the scope of this document. Learn more about RSpec at [http://rspec.info](http://rspec.info)\n::::\n\nFor help learning about tests and testing, look in the `spec/filters/` directory of several other similar plugins.",
            "Clone and test! [_clone_and_test_3]": "Now let’s start with a fresh clone of the plugin, build it and run the tests.\n\n['**Clone your plugin into a temporary location** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``filter-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash-``filter-MYPLUGINNAME.git`'], '`cd logstash-filter-MYPLUGINNAME`']]\n\nThen, you’ll need to install your plugins dependencies with bundler:\n\n```\nbundle install\n```\n\n::::{important}\nIf your plugin has an external file dependency described in `vendor.json`, you must download that dependency before running or testing.  You can do this by running:\n\n```\nrake vendor\n```\n\n::::\n\nAnd finally, run the tests:\n\n```\nbundle exec rspec\n```\n\nYou should see a success message, which looks something like this:\n\n```\nFinished in 0.034 seconds\n1 example, 0 failures\n```\n\nHooray! You’re almost there! (Unless you saw failures…​ you should fix those first).",
            "Building and Testing [_building_and_testing_3]": {
              "Build [_build_3]": "You already have all the necessary ingredients, so let’s go ahead and run the build command:\n\n```\ngem build logstash-filter-example.gemspec\n```\n\nThat’s it!  Your gem should be built and be in the same path with the name\n\n```\nlogstash-filter-mypluginname-0.1.0.gem\n```\n\nThe `s.version` number from your gemspec file will provide the gem version, in this case, `0.1.0`.",
              "Test installation [_test_installation_3]": "You should test install your plugin into a clean installation of Logstash. Download the latest version from the [Logstash downloads page](https://www.elastic.co/downloads/logstash/).\n\n['Untar and cd in to the directory:', '```\\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\\ntar xzvf logstash-9.0.0.tar.gz\\ncd logstash-9.0.0\\n```', 'Using the plugin tool, we can install the gem we just built.', ['Replace `/my/logstash/plugins` with  the correct path to the gem for your environment, and `0.1.0` with the correct version number from the gemspec file.', '```\\nbin/logstash-plugin install /my/logstash/plugins/logstash-filter-example/logstash-filter-example-0.1.0.gem\\n```', 'After running this, you should see feedback from Logstash that it was successfully installed:', \"```\\nvalidating /my/logstash/plugins/logstash-filter-example/logstash-filter-example-0.1.0.gem >= 0\\nValid logstash plugin. Continuing...\\nSuccessfully installed 'logstash-filter-example' with version '0.1.0'\\n```\", '::::{tip}\\nYou can also use the Logstash plugin tool to determine which plugins are currently available:', '```\\nbin/logstash-plugin list\\n```', 'Depending on what you have installed, you might see a short or long list of plugins: inputs, codecs, filters and outputs.', '::::'], 'Now try running Logstash with a simple configuration passed in via the command-line, using the `-e` flag.', '::::{note}\\nYour results will depend on what your filter plugin is designed to do.\\n::::']\n\n```\nbin/logstash -e 'input { stdin{} } filter { example {} } output {stdout { codec => rubydebug }}'\n```\n\nTest your filter by sending input through `stdin` and output (after filtering) through `stdout` with the `rubydebug` codec, which enhances readability.\n\nIn the case of the example filter plugin, any text you send will be replaced by the contents of the `message` configuration parameter, the default value being \"Hello World!\":\n\n```\nTesting 1, 2, 3\n{\n       \"message\" => \"Hello World!\",\n      \"@version\" => \"1\",\n    \"@timestamp\" => \"2015-01-27T19:17:18.932Z\",\n          \"host\" => \"cadenza\"\n}\n```\n\nFeel free to experiment and test this by changing the `message` parameter:\n\n```\nbin/logstash -e 'input { stdin{} } filter { example { message => \"This is a new message!\"} } output {stdout { codec => rubydebug }}'\n```\n\nCongratulations! You’ve built, deployed and successfully run a Logstash filter."
            },
            "Submitting your plugin to [RubyGems.org](http://rubygems.org) and [logstash-plugins](https://github.com/logstash-plugins) [_submitting_your_plugin_to_rubygems_orghttprubygems_org_and_logstash_pluginshttpsgithub_comlogstash_plugins_3]": {
              "Licensing [_licensing_3]": "Logstash and all its plugins are licensed under [Apache License, version 2 (\"ALv2\")](https://github.com/elasticsearch/logstash/blob/main/LICENSE). If you make your plugin publicly available via [RubyGems.org](http://rubygems.org), please make sure to have this line in your gemspec:\n\n[\"`s.licenses = ['Apache License (2.0)']`\"]",
              "Publishing to [RubyGems.org](http://rubygems.org) [_publishing_to_rubygems_orghttprubygems_org_3]": "To begin, you’ll need an account on RubyGems.org\n\n['[Sign-up for a RubyGems account](https://rubygems.org/sign_up).']\n\nAfter creating an account, [obtain](http://guides.rubygems.org/rubygems-org-api/#api-authorization) an API key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials` to store your API key. These credentials will be used to publish the gem. Replace `username` and `password` with the credentials you created at RubyGems.org:\n\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\nBefore proceeding, make sure you have the right version in your gemspec file and commit your changes.\n\n[\"`s.version = '0.1.0'`\"]\n\nTo publish version 0.1.0 of your new logstash gem:\n\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n::::{note}\nExecuting `rake publish_gem`:\n\n[\"Reads the version from the gemspec file (`s.version = '0.1.0'`)\", 'Checks in your local repository if a tag exists for that version. If the tag already exists, it aborts the process. Otherwise, it creates a new version tag in your local repository.', 'Builds the gem', 'Publishes the gem to RubyGems.org']\n\n::::\n\nThat’s it! Your plugin is published! Logstash users can now install your plugin by running:\n\n```\nbin/logstash-plugin install logstash-filter-mypluginname\n```"
            },
            "Contributing your source code to [logstash-plugins](https://github.com/logstash-plugins) [_contributing_your_source_code_to_logstash_pluginshttpsgithub_comlogstash_plugins_3]": {
              "Benefits [_benefits_3]": "Some of the many benefits of having your plugin in the logstash-plugins repository are:\n\n['**Discovery.** Your plugin will appear in the [Logstash Reference](/reference/index.md), where Logstash users look first for plugins and documentation.', '**Documentation.** Your plugin documentation will automatically be added to the [Logstash Reference](/reference/index.md).', '**Testing.** With our testing infrastructure, your plugin will be continuously tested against current and future releases of Logstash.  As a result, users will have the assurance that if incompatibilities arise, they will be quickly discovered and corrected.']",
              "Acceptance Guidelines [_acceptance_guidelines_3]": [
                "**Code Review.** Your plugin must be reviewed by members of the community for coherence, quality, readability, stability and security.",
                "**Tests.** Your plugin must contain tests to be accepted.  These tests are also subject to code review for scope and completeness.  It’s ok if you don’t know how to write tests — we will guide you. We are working on publishing a guide to creating tests for Logstash which will make it easier.  In the meantime, you can refer to [http://betterspecs.org/](http://betterspecs.org/) for examples."
              ]
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/filter-new-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 5]"
        },
        {
          "title": "Contribute to Logstash [contributing-to-logstash]",
          "description": null,
          "content": {
            "Add a plugin [add-plugin]": "Plugins can be developed and deployed independently of the Logstash core. Here are some documents to guide you through the process of coding, deploying, and sharing your plugin:\n\n['Write a new plugin', ['[How to write a Logstash input plugin](/extend/input-new-plugin.md)', '[How to write a Logstash codec plugin](/extend/codec-new-plugin.md)', '[How to write a Logstash filter plugin](/extend/filter-new-plugin.md)', '[How to write a Logstash output plugin](/extend/output-new-plugin.md)', '[Community Maintainer’s Guide](/extend/community-maintainer.md)'], '[Document your plugin](/extend/plugin-doc.md)', '[Publish your plugin to RubyGems.org](/extend/publish-plugin.md)', '[List your plugin](/extend/plugin-listing.md)', 'Contribute a patch', ['[Contributing a patch to a Logstash plugin](/extend/contributing-patch-plugin.md)', '[Extending Logstash core](/extend/contribute-to-core.md)']]\n\nPlugin Shutdown APIs [shutdown-apis]\n\nYou have three options for shutting down a plugin: `stop`, `stop?`, and `close`.\n\n['Call the `stop` method from outside the plugin thread. This method signals the plugin to stop.', 'The `stop?` method returns `true` when the `stop` method has already been called for that plugin.', 'The `close` method performs final bookkeeping and cleanup after the plugin’s `run` method and the plugin’s thread both exit. The `close` method is a a new name for the method known as `teardown` in previous versions of Logstash.']\n\nThe `shutdown`, `finished`, `finished?`, `running?`, and `terminating?` methods are redundant and no longer present in the Plugin Base class.\n\nSample code for the plugin shutdown APIs is [available](https://github.com/logstash-plugins/logstash-input-example/blob/main/lib/logstash/inputs/example.rb)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 6]"
        },
        {
          "title": "How to write a Logstash input plugin [input-new-plugin]",
          "description": null,
          "content": {
            "Get started [_get_started]": {
              "Create a GitHub repo for your new plugin [_create_a_github_repo_for_your_new_plugin]": "Each Logstash plugin lives in its own GitHub repository. To create a new repository for your plugin:\n\n['Log in to GitHub.', 'Click the **Repositories** tab. You’ll see a list of other repositories you’ve forked or contributed to.', 'Click the green **New** button in the upper right.', 'Specify the following settings for your new repo:', ['**Repository name**\\u2009—\\u2009a unique name of the form `logstash-input-pluginname`.', '**Public or Private**\\u2009—\\u2009your choice, but the repository must be Public if you want to submit it as an official plugin.', '**Initialize this repository with a README**\\u2009—\\u2009enables you to immediately clone the repository to your computer.'], 'Click **Create Repository**.']",
              "Use the plugin generator tool [_use_the_plugin_generator_tool]": "You can create your own Logstash plugin in seconds! The `generate` subcommand of `bin/logstash-plugin` creates the foundation for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you can start adding custom code to process data with Logstash.\n\nFor more information, see [Generating plugins](/reference/plugin-generator.md)",
              "Copy the input code [_copy_the_input_code]": "Alternatively, you can use the examples repo we host on github.com\n\n['**Clone your plugin.** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``input-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash``-input-MYPLUGINNAME.git`'], '`cd logstash-input-MYPLUGINNAME`'], '**Clone the input plugin example and copy it to your plugin branch.**', 'You don’t want to include the example .git directory or its contents, so delete it before you copy the example.', ['`cd /tmp`', '`git clone https://github.com/logstash-plugins/logstash``-input-example.git`', '`cd logstash-input-example`', '`rm -rf .git`', '`cp -R * /path/to/logstash-input-mypluginname/`'], '**Rename the following files to match the name of your plugin.**', ['`logstash-input-example.gemspec`', '`example.rb`', '`example_spec.rb`', '```\\ncd /path/to/logstash-input-mypluginname\\nmv logstash-input-example.gemspec logstash-input-mypluginname.gemspec\\nmv lib/logstash/inputs/example.rb lib/logstash/inputs/mypluginname.rb\\nmv spec/inputs/example_spec.rb spec/inputs/mypluginname_spec.rb\\n```']]\n\nYour file structure should look like this:\n\n```\n$ tree logstash-input-mypluginname\n├── Gemfile\n├── LICENSE\n├── README.md\n├── Rakefile\n├── lib\n│   └── logstash\n│       └── inputs\n│           └── mypluginname.rb\n├── logstash-input-mypluginname.gemspec\n└── spec\n    └── inputs\n        └── mypluginname_spec.rb\n```\n\nFor more information about the Ruby gem file structure and an excellent walkthrough of the Ruby gem creation process, see [http://timelessrepo.com/making-ruby-gems](http://timelessrepo.com/making-ruby-gems)",
              "See what your plugin looks like [_see_what_your_plugin_looks_like]": "Before we dive into the details, open up the plugin file in your favorite text editor and take a look.\n\n```\nrequire \"logstash/inputs/base\"\nrequire \"logstash/namespace\"\nrequire \"stud/interval\"\nrequire \"socket\" # for Socket.gethostname\n\n# Add any asciidoc formatted documentation here\n# Generate a repeating message.\n#\n# This plugin is intended only as an example.\n\nclass LogStash::Inputs::Example < LogStash::Inputs::Base\n  config_name \"example\"\n\n  # If undefined, Logstash will complain, even if codec is unused.\n  default :codec, \"plain\"\n\n  # The message string to use in the event.\n  config :message, :validate => :string, :default => \"Hello World!\"\n\n  # Set how frequently messages should be sent.\n  #\n  # The default, `1`, means send a message every second.\n  config :interval, :validate => :number, :default => 1\n\n  public\n  def register\n    @host = Socket.gethostname\n  end # def register\n\n  def run(queue)\n    Stud.interval(@interval) do\n      event = LogStash::Event.new(\"message\" => @message, \"host\" => @host)\n      decorate(event)\n      queue << event\n    end # loop\n  end # def run\n\nend # class LogStash::Inputs::Example\n```"
            },
            "Coding input plugins [_coding_input_plugins]": {
              "`require` Statements [_require_statements]": "Logstash input plugins require parent classes defined in `logstash/inputs/base` and logstash/namespace:\n\n```\nrequire \"logstash/inputs/base\"\nrequire \"logstash/namespace\"\n```\n\nOf course, the plugin you build may depend on other code, or even gems. Just put them here along with these Logstash dependencies."
            },
            "Plugin Body [_plugin_body]": {
              "`class` Declaration [_class_declaration]": "The input plugin class should be a subclass of `LogStash::Inputs::Base`:\n\n```\nclass LogStash::Inputs::Example < LogStash::Inputs::Base\n```\n\nThe class name should closely mirror the plugin name, for example:\n\n```\nLogStash::Inputs::Example\n```",
              "`config_name` [_config_name]": "```\n  config_name \"example\"\n```\n\nThis is the name your plugin will call inside the input configuration block.\n\nIf you set `config_name \"example\"` in your plugin code, the corresponding Logstash configuration block would need to look like this:\n\n```\ninput {\n  example {...}\n}\n```"
            },
            "Configuration Parameters [_configuration_parameters]": "```\n  config :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\nThe configuration, or `config` section allows you to define as many (or as few) parameters as are needed to enable Logstash to process events.\n\nThere are several configuration attributes:\n\n['`:validate` - allows you to enforce passing a particular data type to Logstash for this configuration option, such as `:string`, `:password`, `:boolean`, `:number`, `:array`, `:hash`, `:path` (a file-system path), `uri`, `:codec` (since 1.2.0), `:bytes`.  Note that this also works as a coercion in that if I specify \"true\" for boolean (even though technically a string), it will become a valid boolean in the config.  This coercion works for the `:number` type as well where \"1.2\" becomes a float and \"22\" is an integer.', '`:default` - lets you specify a default value for a parameter', '`:required` - whether or not this parameter is mandatory (a Boolean `true` or', '`:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable. `false`)', '`:deprecated` - informational (also a Boolean `true` or `false`)', '`:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.']",
            "Plugin Methods [_plugin_methods]": {
              "`register` Method [_register_method]": "```\n  public\n  def register\n  end # def register\n```\n\nThe Logstash `register` method is like an `initialize` method. It was originally created to enforce having `super` called, preventing headaches for newbies. (Note: It may go away in favor of `initialize`, in conjunction with some enforced testing to ensure `super` is called.)\n\n`public` means the method can be called anywhere, not just within the class. This is the default behavior for methods in Ruby, but it is specified explicitly here anyway.\n\nYou can also assign instance variables here (variables prepended by `@`). Configuration variables are now in scope as instance variables, like `@message`",
              "`run` Method [_run_method]": "The example input plugin has the following `run` Method:\n\n```\n  def run(queue)\n    Stud.interval(@interval) do\n      event = LogStash::Event.new(\"message\" => @message, \"host\" => @host)\n      decorate(event)\n      queue << event\n    end # loop\n  end # def run\n```\n\nThe `run` method is where a stream of data from an input becomes an event.\n\nThe stream can be plain or generated as with the [heartbeat](https://github.com/logstash-plugins/logstash-input-heartbeat/blob/main/lib/logstash/inputs/heartbeat.rb#L43-L61) input plugin.  In these cases, though no codec is used, [a default codec](https://github.com/logstash-plugins/logstash-input-heartbeat/blob/main/lib/logstash/inputs/heartbeat.rb#L17) must be set in the code to avoid errors.\n\nHere’s another example `run` method:\n\n```\n  def run(queue)\n    while true\n      begin\n        # Based on some testing, there is no way to interrupt an IO.sysread nor\n        # IO.select call in JRuby.\n        data = $stdin.sysread(16384)\n        @codec.decode(data) do |event|\n          decorate(event)\n          event.set(\"host\", @host) if !event.include?(\"host\")\n          queue << event\n        end\n      rescue IOError, EOFError, LogStash::ShutdownSignal\n        # stdin closed or a requested shutdown\n        break\n      end\n    end # while true\n    finished\n  end # def run\n```\n\nIn this example, the `data` is being sent to the codec defined in the configuration block to `decode` the data stream and return an event.\n\nIn both examples, the resulting `event` is passed to the `decorate` method:\n\n```\n      decorate(event)\n```\n\nThis applies any tags you might have set in the input configuration block. For example, `tags => [\"tag1\", \"tag2\"]`.\n\nAlso in both examples, the `event`, after being \"decorated,\" is appended to the queue:\n\n```\n      queue << event\n```\n\nThis inserts the event into the pipeline.\n\n::::{tip}\nBecause input plugins can range from simple to complex, it is helpful to see more examples of how they have been created:\n\n['[syslog](https://github.com/logstash-plugins/logstash-input-syslog/blob/main/lib/logstash/inputs/syslog.rb)', '[zeromq](https://github.com/logstash-plugins/logstash-input-zeromq/blob/main/lib/logstash/inputs/zeromq.rb)', '[stdin](https://github.com/logstash-plugins/logstash-input-stdin/blob/main/lib/logstash/inputs/stdin.rb)', '[tcp](https://github.com/logstash-plugins/logstash-input-tcp/blob/main/lib/logstash/inputs/tcp.rb)']\n\nThere are many more more examples in the [logstash-plugin github repository](https://github.com/logstash-plugins?query=logstash-input).\n\n::::"
            },
            "Building the Plugin [_building_the_plugin]": {
              "External dependencies [_external_dependencies]": "A `require` statement in Ruby is used to include necessary code. In some cases your plugin may require additional files.  For example, the collectd plugin [uses](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/lib/logstash/codecs/collectd.rb#L148) the `types.db` file provided by collectd.  In the main directory of your plugin, a file called `vendor.json` is where these files are described.\n\nThe `vendor.json` file contains an array of JSON objects, each describing a file dependency. This example comes from the [collectd](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/vendor.json) codec plugin:\n\n```\n[{\n        \"sha1\": \"a90fe6cc53b76b7bdd56dc57950d90787cb9c96e\",\n        \"url\": \"http://collectd.org/files/collectd-5.4.0.tar.gz\",\n        \"files\": [ \"/src/types.db\" ]\n}]\n```\n\n['`sha1` is the sha1 signature used to verify the integrity of the file referenced by `url`.', '`url` is the address from where Logstash will download the file.', '`files` is an optional array of files to extract from the downloaded file. Note that while tar archives can use absolute or relative paths, treat them as absolute in this array.  If `files` is not present, all files will be uncompressed and extracted into the vendor directory.']\n\nAnother example of the `vendor.json` file is the [`geoip` filter](https://github.com/logstash-plugins/logstash-filter-geoip/blob/main/vendor.json)\n\nThe process used to download these dependencies is to call `rake vendor`.  This will be discussed further in the testing section of this document.\n\nAnother kind of external dependency is on jar files.  This will be described in the \"Add a `gemspec` file\" section.",
              "Deprecated features [_deprecated_features]": "As a plugin evolves, an option or feature may no longer serve the intended purpose, and the developer may want to *deprecate* its usage. Deprecation warns users about the option’s status, so they aren’t caught by surprise when it is removed in a later release.\n\n{{ls}} 7.6 introduced a *deprecation logger* to make handling those situations easier. You can use the [adapter](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support) to ensure that your plugin can use the deprecation logger while still supporting older versions of {{ls}}. See the [readme](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support/blob/main/README.md) for more information and for instructions on using the adapter.\n\nDeprecations are noted in the `logstash-deprecation.log` file in the `log` directory.",
              "Add a Gemfile [_add_a_gemfile]": "Gemfiles allow Ruby’s Bundler to maintain the dependencies for your plugin. Currently, all we’ll need is the Logstash gem, for testing, but if you require other gems, you should add them in here.\n\n::::{tip}\nSee [Bundler’s Gemfile page](http://bundler.io/gemfile.html) for more details.\n::::\n\n```\nsource 'https://rubygems.org'\ngemspec\ngem \"logstash\", :github => \"elastic/logstash\", :branch => \"master\"\n```"
            },
            "Add a `gemspec` file [_add_a_gemspec_file]": {
              "Runtime and Development Dependencies [_runtime_and_development_dependencies]": "At the bottom of the `gemspec` file is a section with a comment: `Gem dependencies`.  This is where any other needed gems must be mentioned. If a gem is necessary for your plugin to function, it is a runtime dependency. If a gem are only used for testing, then it would be a development dependency.\n\n::::{note}\nYou can also have versioning requirements for your dependencies—​including other Logstash plugins:\n\n```\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\n```\n\nThis gemspec has a runtime dependency on the logstash-core-plugin-api and requires that it have a version number greater than or equal to version 1.60 and less than or equal to version 2.99.\n\n::::\n\n::::{important}\nAll plugins have a runtime dependency on the `logstash-core-plugin-api` gem, and a development dependency on `logstash-devutils`.\n::::",
              "Jar dependencies [_jar_dependencies]": "In some cases, such as the [Elasticsearch output plugin](https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/main/logstash-output-elasticsearch.gemspec#L22-L23), your code may depend on a jar file.  In cases such as this, the dependency is added in the gemspec file in this manner:\n\n```\n  # Jar dependencies\n  s.requirements << \"jar 'org.elasticsearch:elasticsearch', '5.0.0'\"\n  s.add_runtime_dependency 'jar-dependencies'\n```\n\nWith these both defined, the install process will search for the required jar file at [http://mvnrepository.com](http://mvnrepository.com) and download the specified version."
            },
            "Document your plugin [_document_your_plugin]": "Documentation is an important part of your plugin. All plugin documentation is rendered and placed in the [Logstash Reference](/reference/index.md) and the [Versioned plugin docs](logstash-docs-md://vpr/integration-plugins.md).\n\nSee [Document your plugin](/extend/plugin-doc.md) for tips and guidelines.",
            "Add Tests [_add_tests]": "Logstash loves tests. Lots of tests. If you’re using your new input plugin in a production environment, you’ll want to have some tests to ensure you are not breaking any existing functionality.\n\n::::{note}\nA full exposition on RSpec is outside the scope of this document. Learn more about RSpec at [http://rspec.info](http://rspec.info)\n::::\n\nFor help learning about tests and testing, look in the `spec/inputs/` directory of several other similar plugins.",
            "Clone and test! [_clone_and_test]": "Now let’s start with a fresh clone of the plugin, build it and run the tests.\n\n['**Clone your plugin into a temporary location** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``input-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash-``input-MYPLUGINNAME.git`'], '`cd logstash-input-MYPLUGINNAME`']]\n\nThen, you’ll need to install your plugins dependencies with bundler:\n\n```\nbundle install\n```\n\n::::{important}\nIf your plugin has an external file dependency described in `vendor.json`, you must download that dependency before running or testing.  You can do this by running:\n\n```\nrake vendor\n```\n\n::::\n\nAnd finally, run the tests:\n\n```\nbundle exec rspec\n```\n\nYou should see a success message, which looks something like this:\n\n```\nFinished in 0.034 seconds\n1 example, 0 failures\n```\n\nHooray! You’re almost there! (Unless you saw failures…​ you should fix those first).",
            "Building and Testing [_building_and_testing]": {
              "Build [_build]": "You already have all the necessary ingredients, so let’s go ahead and run the build command:\n\n```\ngem build logstash-input-example.gemspec\n```\n\nThat’s it!  Your gem should be built and be in the same path with the name\n\n```\nlogstash-input-mypluginname-0.1.0.gem\n```\n\nThe `s.version` number from your gemspec file will provide the gem version, in this case, `0.1.0`.",
              "Test installation [_test_installation]": "You should test install your plugin into a clean installation of Logstash. Download the latest version from the [Logstash downloads page](https://www.elastic.co/downloads/logstash/).\n\n['Untar and cd in to the directory:', '```\\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\\ntar xzvf logstash-9.0.0.tar.gz\\ncd logstash-9.0.0\\n```', 'Using the plugin tool, we can install the gem we just built.', ['Replace `/my/logstash/plugins` with  the correct path to the gem for your environment, and `0.1.0` with the correct version number from the gemspec file.', '```\\nbin/logstash-plugin install /my/logstash/plugins/logstash-input-example/logstash-input-example-0.1.0.gem\\n```', 'After running this, you should see feedback from Logstash that it was successfully installed:', \"```\\nvalidating /my/logstash/plugins/logstash-input-example/logstash-input-example-0.1.0.gem >= 0\\nValid logstash plugin. Continuing...\\nSuccessfully installed 'logstash-input-example' with version '0.1.0'\\n```\", '::::{tip}\\nYou can also use the Logstash plugin tool to determine which plugins are currently available:', '```\\nbin/logstash-plugin list\\n```', 'Depending on what you have installed, you might see a short or long list of plugins: inputs, codecs, filters and outputs.', '::::'], 'Now try running Logstash with a simple configuration passed in via the command-line, using the `-e` flag.', '::::{note}\\nYour results will depend on what your input plugin is designed to do.\\n::::']\n\n```\nbin/logstash -e 'input { example{} } output {stdout { codec => rubydebug }}'\n```\n\nThe example input plugin will send the contents of `message` (with a default message of \"Hello World!\") every second.\n\n```\n{\n       \"message\" => \"Hello World!\",\n      \"@version\" => \"1\",\n    \"@timestamp\" => \"2015-01-27T19:17:18.932Z\",\n          \"host\" => \"cadenza\"\n}\n```\n\nFeel free to experiment and test this by changing the `message` and `interval` parameters:\n\n```\nbin/logstash -e 'input { example{ message => \"A different message\" interval => 5 } } output {stdout { codec => rubydebug }}'\n```\n\nCongratulations! You’ve built, deployed and successfully run a Logstash input."
            },
            "Submitting your plugin to [RubyGems.org](http://rubygems.org) and [logstash-plugins](https://github.com/logstash-plugins) [_submitting_your_plugin_to_rubygems_orghttprubygems_org_and_logstash_pluginshttpsgithub_comlogstash_plugins]": {
              "Licensing [_licensing]": "Logstash and all its plugins are licensed under [Apache License, version 2 (\"ALv2\")](https://github.com/elasticsearch/logstash/blob/main/LICENSE). If you make your plugin publicly available via [RubyGems.org](http://rubygems.org), please make sure to have this line in your gemspec:\n\n[\"`s.licenses = ['Apache License (2.0)']`\"]",
              "Publishing to [RubyGems.org](http://rubygems.org) [_publishing_to_rubygems_orghttprubygems_org]": "To begin, you’ll need an account on RubyGems.org\n\n['[Sign-up for a RubyGems account](https://rubygems.org/sign_up).']\n\nAfter creating an account, [obtain](http://guides.rubygems.org/rubygems-org-api/#api-authorization) an API key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials` to store your API key. These credentials will be used to publish the gem. Replace `username` and `password` with the credentials you created at RubyGems.org:\n\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\nBefore proceeding, make sure you have the right version in your gemspec file and commit your changes.\n\n[\"`s.version = '0.1.0'`\"]\n\nTo publish version 0.1.0 of your new logstash gem:\n\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n::::{note}\nExecuting `rake publish_gem`:\n\n[\"Reads the version from the gemspec file (`s.version = '0.1.0'`)\", 'Checks in your local repository if a tag exists for that version. If the tag already exists, it aborts the process. Otherwise, it creates a new version tag in your local repository.', 'Builds the gem', 'Publishes the gem to RubyGems.org']\n\n::::\n\nThat’s it! Your plugin is published! Logstash users can now install your plugin by running:\n\n```\nbin/logstash-plugin install logstash-input-mypluginname\n```"
            },
            "Contributing your source code to [logstash-plugins](https://github.com/logstash-plugins) [_contributing_your_source_code_to_logstash_pluginshttpsgithub_comlogstash_plugins]": {
              "Benefits [_benefits]": "Some of the many benefits of having your plugin in the logstash-plugins repository are:\n\n['**Discovery.** Your plugin will appear in the [Logstash Reference](/reference/index.md), where Logstash users look first for plugins and documentation.', '**Documentation.** Your plugin documentation will automatically be added to the [Logstash Reference](/reference/index.md).', '**Testing.** With our testing infrastructure, your plugin will be continuously tested against current and future releases of Logstash.  As a result, users will have the assurance that if incompatibilities arise, they will be quickly discovered and corrected.']",
              "Acceptance Guidelines [_acceptance_guidelines]": [
                "**Code Review.** Your plugin must be reviewed by members of the community for coherence, quality, readability, stability and security.",
                "**Tests.** Your plugin must contain tests to be accepted.  These tests are also subject to code review for scope and completeness.  It’s ok if you don’t know how to write tests — we will guide you. We are working on publishing a guide to creating tests for Logstash which will make it easier.  In the meantime, you can refer to [http://betterspecs.org/](http://betterspecs.org/) for examples."
              ]
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/input-new-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 7]"
        },
        {
          "title": "How to write a Java codec plugin [java-codec-plugin]",
          "description": null,
          "content": {
            "Set up your environment [_set_up_your_environment_2]": {
              "Copy the example repo [_copy_the_example_repo_2]": "Start by copying the [example codec plugin](https://github.com/logstash-plugins/logstash-codec-java_codec_example). The plugin API is currently part of the Logstash codebase so you must have a local copy of that available. You can obtain a copy of the Logstash codebase with the following `git` command:\n\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\nThe `branch_name` should correspond to the version of Logstash containing the preferred revision of the Java plugin API.\n\n::::{note}\nThe GA version of the Java plugin API is available in the `7.2` and later branches of the Logstash codebase.\n::::\n\nSpecify the `target_folder` for your local copy of the Logstash codebase. If you do not specify `target_folder`, it defaults to a new folder called `logstash` under your current folder.",
              "Generate the .jar file [_generate_the_jar_file_2]": "After you have obtained a copy of the appropriate revision of the Logstash codebase, you need to compile it to generate the .jar file containing the Java plugin API. From the root directory of your Logstash codebase ($LS_HOME), you can compile it with `./gradlew assemble` (or `gradlew.bat assemble` if you’re running on Windows). This should produce the `$LS_HOME/logstash-core/build/libs/logstash-core-x.y.z.jar` where `x`, `y`, and `z` refer to the version of Logstash.\n\nAfter you have successfully compiled Logstash, you need to tell your Java plugin where to find the `logstash-core-x.y.z.jar` file. Create a new file named `gradle.properties` in the root folder of your plugin project. That file should have a single line:\n\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\nwhere `target_folder` is the root folder of your local copy of the Logstash codebase."
            },
            "Code the plugin [_code_the_plugin_2]": {
              "Class declaration [_class_declaration_6]": {
                "Plugin settings [_plugin_settings_2]": "The snippet below contains both the setting definition and the method referencing it:\n\n```\npublic static final PluginConfigSpec<String> DELIMITER_CONFIG =\n        PluginConfigSpec.stringSetting(\"delimiter\", \",\");\n\n@Override\npublic Collection<PluginConfigSpec<?>> configSchema() {\n    return Collections.singletonList(DELIMITER_CONFIG);\n}\n```\n\nThe `PluginConfigSpec` class allows developers to specify the settings that a plugin supports complete with setting  name, data type, deprecation status, required status, and default value. In this example, the `delimiter` setting defines the delimiter on which the codec will split events. It is not a required setting and if it is not explicitly  set, its default value will be `,`.\n\nThe `configSchema` method must return a list of all settings that the plugin supports. The Logstash execution engine  will validate that all required settings are present and that no unsupported settings are present.",
                "Constructor and initialization [_constructor_and_initialization_2]": "```\nprivate final String id;\nprivate final String delimiter;\n\npublic JavaCodecExample(final Configuration config, final Context context) {\n    this(config.get(DELIMITER_CONFIG));\n}\n\nprivate JavaCodecExample(String delimiter) {\n    this.id = UUID.randomUUID().toString();\n    this.delimiter = delimiter;\n}\n```\n\nAll Java codec plugins must have a constructor taking a `Configuration` and `Context` argument. This is the  constructor that will be used to instantiate them at runtime. The retrieval and validation of all plugin settings  should occur in this constructor. In this example, the delimiter to be used for delimiting events is retrieved from  its setting and stored in a local variable so that it can be used later in the `decode` and `encode` methods. The codec’s ID is initialized to a random UUID (as should be done for most codecs), and a local `encoder` variable is initialized to encode and decode with a specified character set.\n\nAny additional initialization may occur in the constructor as well. If there are any unrecoverable errors encountered in the configuration or initialization of the codec plugin, a descriptive exception should be thrown. The exception will be logged and will prevent Logstash from starting."
              },
              "Codec methods [_codec_methods]": "```\n@Override\npublic void decode(ByteBuffer byteBuffer, Consumer<Map<String, Object>> consumer) {\n    // a not-production-grade delimiter decoder\n    byte[] byteInput = new byte[byteBuffer.remaining()];\n    byteBuffer.get(byteInput);\n    if (byteInput.length > 0) {\n        String input = new String(byteInput);\n        String[] split = input.split(delimiter);\n        for (String s : split) {\n            Map<String, Object> map = new HashMap<>();\n            map.put(\"message\", s);\n            consumer.accept(map);\n        }\n    }\n}\n\n@Override\npublic void flush(ByteBuffer byteBuffer, Consumer<Map<String, Object>> consumer) {\n    // if the codec maintains any internal state such as partially-decoded input, this\n    // method should flush that state along with any additional input supplied in\n    // the ByteBuffer\n\n    decode(byteBuffer, consumer); // this is a simplistic implementation\n}\n\n@Override\npublic void encode(Event event, OutputStream outputStream) throws IOException {\n    outputStream.write((event.toString() + delimiter).getBytes(Charset.defaultCharset()));\n}\n```\n\nThe `decode`, `flush`, and `encode` methods provide the core functionality of the codec. Codecs may be used by inputs to decode a sequence or stream of bytes into events or by outputs to encode events into a sequence of bytes.\n\nThe `decode` method decodes events from the specified `ByteBuffer` and passes them to the provided `Consumer`. The  input must provide a `ByteBuffer` that is ready for reading with `byteBuffer.position()` indicating the next position to read and `byteBuffer.limit()` indicating the first byte in the buffer that is not safe to read. Codecs must ensure that `byteBuffer.position()` reflects the last-read position before returning control to the input. The input is then responsible for returning the buffer to write mode via either `byteBuffer.clear()` or `byteBuffer.compact()` before resuming writes. In the example above, the `decode` method simply splits the incoming byte stream on the specified delimiter. A production-grade codec such as [`java-line`](https://github.com/elastic/logstash/blob/main/logstash-core/src/main/java/org/logstash/plugins/codecs/Line.java) would not make the simplifying assumption that the end of the supplied byte stream corresponded with the end of an event.\n\nEvents should be constructed as instances of `Map<String, Object>` and pushed into the event pipeline via the `Consumer<Map<String, Object>>.accept()` method. To reduce allocations and GC pressure, codecs may reuse the same map instance by modifying its fields between calls to `Consumer<Map<String, Object>>.accept()` because the event pipeline will create events based on a copy of the map’s data.\n\nThe `flush` method works in coordination with the `decode` method to decode all remaining events from the specified  `ByteBuffer` along with any internal state that may remain after previous calls to the `decode` method. As an example of internal state that a codec might maintain, consider an input stream of bytes `event1/event2/event3` with a  delimiter of `/`. Due to buffering or other reasons, the input might supply a partial stream of bytes such as  `event1/eve` to the codec’s `decode` method. In this case, the codec could save the beginning three characters `eve`  of the second event rather than assuming that the supplied byte stream ends on an event boundary. If the next call to `decode` supplied the `nt2/ev` bytes, the codec would prepend the saved `eve` bytes to produce the full `event2` event and then save the remaining `ev` bytes for decoding when the remainder of the bytes for that event were supplied. A call to `flush` signals the codec that the supplied bytes represent the end of an event stream and all remaining bytes should be decoded to events. The `flush` example above is a simplistic implementation that does not maintain any state about partially-supplied byte streams across calls to `decode`.\n\nThe `encode` method encodes an event into a sequence of bytes and writes it into the specified `OutputStream`. Because a single codec instance is shared across all pipeline workers in the output stage of the Logstash pipeline, codecs should *not* retain state across calls to their `encode` methods.",
              "cloneCodec method [_clonecodec_method]": "```\n@Override\npublic Codec cloneCodec() {\n    return new JavaCodecExample(this.delimiter);\n}\n```\n\nThe `cloneCodec` method should return an identical instance of the codec with the exception of its ID. Because codecs may be stateful across calls to their `decode` methods, input plugins that are multi-threaded should use a separate instance of each codec via the `cloneCodec` method for each of their threads. Because a single codec instance is shared across all pipeline workers in the output stage of the Logstash pipeline, codecs should *not* retain state across calls to their `encode` methods. In the example above, the codec is cloned with the same delimiter but a different ID.",
              "getId method [_getid_method_2]": "```\n@Override\npublic String getId() {\n    return id;\n}\n```\n\nFor codec plugins, the `getId` method should always return the id that was set at instantiation time. This is typically an UUID.",
              "Unit tests [_unit_tests_2]": "Lastly, but certainly not least importantly, unit tests are strongly encouraged. The example codec plugin includes an [example unit test](https://github.com/logstash-plugins/logstash-codec-java_codec_example/blob/main/src/test/java/org/logstashplugins/JavaCodecExampleTest.java) that you can use as a template for your own."
            },
            "Package and deploy [_package_and_deploy_2]": {
              "Configuring the Gradle packaging task [_configuring_the_gradle_packaging_task_2]": "The following section appears near the top of the `build.gradle` file supplied with the example Java plugins:\n\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \\$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\nYou should configure the values above for your plugin.\n\n['The `version` value will be automatically read from the `VERSION` file in the root of your plugin’s codebase.', '`pluginInfo.pluginType` should be set to one of `input`, `filter`, `codec`, or `output`.', '`pluginInfo.pluginName` must match the name specified on the `@LogstashPlugin` annotation on the main plugin class. The Gradle packaging task will validate that and return an error if they do not match.']",
              "Running the Gradle packaging task [_running_the_gradle_packaging_task_2]": "Several Ruby source files along with a `gemspec` file and a `Gemfile` are required to package the plugin as a Ruby gem. These Ruby files are used only for defining the Ruby gem structure or at Logstash startup time to register the Java plugin. They are not used during runtime event processing. The Gradle packaging task automatically generates all of these files based on the values configured in the section above.\n\nYou run the Gradle packaging task with the following command:\n\n```\n./gradlew gem\n```\n\nFor Windows platforms: Substitute `gradlew.bat` for `./gradlew` as appropriate in the command.\n\nThat task will produce a gem file in the root directory of your plugin’s codebase with the name `logstash-{{plugintype}}-<pluginName>-<version>.gem`",
              "Installing the Java plugin in Logstash [_installing_the_java_plugin_in_logstash_2]": "After you have packaged your Java plugin as a Ruby gem, you can install it in Logstash with this command:\n\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\nFor Windows platforms: Substitute backslashes for forward slashes as appropriate in the command."
            },
            "Run Logstash with the Java codec plugin [_run_logstash_with_the_java_codec_plugin]": "To test the plugin, start Logstash with:\n\n```\necho \"foo,bar\" | bin/logstash -e 'input { java_stdin { codec => java_codec_example } }'\n```\n\nThe expected Logstash output (excluding initialization) with the configuration above is:\n\n```\n{\n      \"@version\" => \"1\",\n       \"message\" => \"foo\",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ,\n          \"host\" => \"<yourHostName>\"\n}\n{\n      \"@version\" => \"1\",\n       \"message\" => \"bar\\n\",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ,\n          \"host\" => \"<yourHostName>\"\n}\n```",
            "Feedback [_feedback_2]": "If you have any feedback on Java plugin support in Logstash, please comment on our [main Github issue](https://github.com/elastic/logstash/issues/9215) or post in the [Logstash forum](https://discuss.elastic.co/c/logstash)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/java-codec-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 8]"
        },
        {
          "title": "How to write a Java filter plugin [java-filter-plugin]",
          "description": null,
          "content": {
            "Set up your environment [_set_up_your_environment_3]": {
              "Copy the example repo [_copy_the_example_repo_3]": "Start by copying the [example filter plugin](https://github.com/logstash-plugins/logstash-filter-java_filter_example). The plugin API is currently part of the Logstash codebase so you must have a local copy of that available. You can obtain a copy of the Logstash codebase with the following `git` command:\n\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\nThe `branch_name` should correspond to the version of Logstash containing the preferred revision of the Java plugin API.\n\n::::{note}\nThe GA version of the Java plugin API is available in the `7.2` and later branches of the Logstash codebase.\n::::\n\nSpecify the `target_folder` for your local copy of the Logstash codebase. If you do not specify `target_folder`, it defaults to a new folder called `logstash` under your current folder.",
              "Generate the .jar file [_generate_the_jar_file_3]": "After you have obtained a copy of the appropriate revision of the Logstash codebase, you need to compile it to generate the .jar file containing the Java plugin API. From the root directory of your Logstash codebase ($LS_HOME), you can compile it with `./gradlew assemble` (or `gradlew.bat assemble` if you’re running on Windows). This should produce the `$LS_HOME/logstash-core/build/libs/logstash-core-x.y.z.jar` where `x`, `y`, and `z` refer to the version of Logstash.\n\nAfter you have successfully compiled Logstash, you need to tell your Java plugin where to find the `logstash-core-x.y.z.jar` file. Create a new file named `gradle.properties` in the root folder of your plugin project. That file should have a single line:\n\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\nwhere `target_folder` is the root folder of your local copy of the Logstash codebase."
            },
            "Code the plugin [_code_the_plugin_3]": {
              "Class declaration [_class_declaration_7]": "```\n@LogstashPlugin(name = \"java_filter_example\")\npublic class JavaFilterExample implements Filter {\n```\n\nNotes about the class declaration:\n\n['All Java plugins must be annotated with the `@LogstashPlugin` annotation. Additionally:', ['The `name` property of the annotation must be supplied and defines the name of the plugin as it will be used in the Logstash pipeline definition. For example, this filter would be referenced in the filter section of the Logstash pipeline defintion as `filter { java_filter_example => { .... } }`', 'The value of the `name` property must match the name of the class excluding casing and underscores.'], 'The class must implement the `co.elastic.logstash.api.Filter` interface.', 'Java plugins may not be created in the `org.logstash` or `co.elastic.logstash` packages to prevent potential clashes with classes in Logstash itself.']",
              "Plugin settings [_plugin_settings_3]": "The snippet below contains both the setting definition and the method referencing it:\n\n```\npublic static final PluginConfigSpec<String> SOURCE_CONFIG =\n        PluginConfigSpec.stringSetting(\"source\", \"message\");\n\n@Override\npublic Collection<PluginConfigSpec<?>> configSchema() {\n    return Collections.singletonList(SOURCE_CONFIG);\n}\n```\n\nThe `PluginConfigSpec` class allows developers to specify the settings that a plugin supports complete with setting name, data type, deprecation status, required status, and default value. In this example, the `source` setting defines the name of the field in each event that will be reversed. It is not a required setting and if it is not explicitly set, its default value will be `message`.\n\nThe `configSchema` method must return a list of all settings that the plugin supports. In a future phase of the Java plugin project, the Logstash execution engine will validate that all required settings are present and that no unsupported settings are present.",
              "Constructor and initialization [_constructor_and_initialization_3]": "```\nprivate String id;\nprivate String sourceField;\n\npublic JavaFilterExample(String id, Configuration config, Context context) {\n    this.id = id;\n    this.sourceField = config.get(SOURCE_CONFIG);\n}\n```\n\nAll Java filter plugins must have a constructor taking a `String` id and a `Configuration` and `Context` argument.  This is the constructor that will be used to instantiate them at runtime. The retrieval and validation of all plugin settings should occur in this constructor. In this example, the name of the field to be reversed in each event is  retrieved from its setting and stored in a local variable so that it can be used later in the `filter` method.\n\nAny additional initialization may occur in the constructor as well. If there are any unrecoverable errors encountered in the configuration or initialization of the filter plugin, a descriptive exception should be thrown. The exception will be logged and will prevent Logstash from starting.",
              "Filter method [_filter_method_2]": "```\n@Override\npublic Collection<Event> filter(Collection<Event> events, FilterMatchListener matchListener) {\n    for (Event e : events) {\n        Object f = e.getField(sourceField);\n        if (f instanceof String) {\n            e.setField(sourceField, StringUtils.reverse((String)f));\n            matchListener.filterMatched(e);\n        }\n    }\n    return events;\n```\n\nFinally, we come to the `filter` method that is invoked by the Logstash execution engine on batches of events as they flow through the event processing pipeline. The events to be filtered are supplied in the `events` argument and the method should return a collection of filtered events. Filters may perform a variety of actions on events as they flow through the pipeline including:\n\n['Mutation\\u2009—\\u2009Fields in events may be added, removed, or changed by a filter. This is the most common scenario for  filters that perform various kinds of enrichment on events. In this scenario, the incoming `events` collection may be returned unmodified since the events in the collection are mutated in place.', 'Deletion\\u2009—\\u2009Events may be removed from the event pipeline by a filter so that subsequent filters and outputs  do not receive them. In this scenario, the events to be deleted must be removed from the collection of filtered events before it is returned.', 'Creation\\u2009—\\u2009A filter may insert new events into the event pipeline that will be seen only by subsequent filters and outputs. In this scenario, the new events must be added to the collection of filtered events before it is returned.', 'Observation\\u2009—\\u2009Events may pass unchanged by a filter through the event pipeline. This may be useful in scenarios where a filter performs external actions (e.g., updating an external cache) based on the events observed in the event pipeline. In this scenario, the incoming `events` collection may be returned unmodified since no changes were made.']\n\nIn the example above, the value of the `source` field is retrieved from each event and reversed if it is a string value. Because each event is mutated in place, the incoming `events` collection can be returned.\n\nThe `matchListener` is the mechanism by which filters indicate which events \"match\". The common actions for filters  such as `add_field` and `add_tag` are applied only to events that are designated as \"matching\". Some filters such as the [grok filter](logstash-docs-md://lsr/plugins-filters-grok.md) have a clear definition  for what constitutes a matching event and will notify the listener only for matching events. Other filters such as the [UUID filter](logstash-docs-md://lsr/plugins-filters-uuid.md) have no specific match  criteria and should notify the listener for every event filtered. In this example, the filter notifies the match listener for any event that had a `String` value in its `source` field and was therefore able to be reversed.",
              "getId method [_getid_method_3]": "```\n@Override\npublic String getId() {\n    return id;\n}\n```\n\nFor filter plugins, the `getId` method should always return the id that was provided to the plugin through its constructor at instantiation time.",
              "close method [_close_method]": "```\n@Override\npublic void close() {\n    // shutdown a resource that was instantiated during the filter initialization phase.\n    this.sourceField = null;\n    return;\n}\n```\n\nFilter plugins can use additional resources to perform operations, such as creating new database connections. Implementing the `close` method will allow the plugins to free up those resources when shutting down the pipeline.",
              "Unit tests [_unit_tests_3]": "Lastly, but certainly not least importantly, unit tests are strongly encouraged. The example filter plugin includes an [example unit test](https://github.com/logstash-plugins/logstash-filter-java_filter_example/blob/main/src/test/java/org/logstashplugins/JavaFilterExampleTest.java) that you can use as a template for your own."
            },
            "Package and deploy [_package_and_deploy_3]": {
              "Configuring the Gradle packaging task [_configuring_the_gradle_packaging_task_3]": "The following section appears near the top of the `build.gradle` file supplied with the example Java plugins:\n\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \\$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\nYou should configure the values above for your plugin.\n\n['The `version` value will be automatically read from the `VERSION` file in the root of your plugin’s codebase.', '`pluginInfo.pluginType` should be set to one of `input`, `filter`, `codec`, or `output`.', '`pluginInfo.pluginName` must match the name specified on the `@LogstashPlugin` annotation on the main plugin class. The Gradle packaging task will validate that and return an error if they do not match.']",
              "Running the Gradle packaging task [_running_the_gradle_packaging_task_3]": "Several Ruby source files along with a `gemspec` file and a `Gemfile` are required to package the plugin as a Ruby gem. These Ruby files are used only for defining the Ruby gem structure or at Logstash startup time to register the Java plugin. They are not used during runtime event processing. The Gradle packaging task automatically generates all of these files based on the values configured in the section above.\n\nYou run the Gradle packaging task with the following command:\n\n```\n./gradlew gem\n```\n\nFor Windows platforms: Substitute `gradlew.bat` for `./gradlew` as appropriate in the command.\n\nThat task will produce a gem file in the root directory of your plugin’s codebase with the name `logstash-{{plugintype}}-<pluginName>-<version>.gem`",
              "Installing the Java plugin in Logstash [_installing_the_java_plugin_in_logstash_3]": "After you have packaged your Java plugin as a Ruby gem, you can install it in Logstash with this command:\n\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\nFor Windows platforms: Substitute backslashes for forward slashes as appropriate in the command."
            },
            "Run Logstash with the Java filter plugin [_run_logstash_with_the_java_filter_plugin]": "The following is a minimal Logstash configuration that can be used to test that the Java filter plugin is correctly installed and functioning.\n\n```\ninput {\n  generator { message => \"Hello world!\" count => 1 }\n}\nfilter {\n  java_filter_example {}\n}\noutput {\n  stdout { codec => rubydebug }\n}\n```\n\nCopy the above Logstash configuration to a file such as `java_filter.conf`. Start Logstash with:\n\n```\nbin/logstash -f /path/to/java_filter.conf\n```\n\nThe expected Logstash output (excluding initialization) with the configuration above is:\n\n```\n{\n      \"sequence\" => 0,\n      \"@version\" => \"1\",\n       \"message\" => \"!dlrow olleH\",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ,\n          \"host\" => \"<yourHostName>\"\n}\n```",
            "Feedback [_feedback_3]": "If you have any feedback on Java plugin support in Logstash, please comment on our [main Github issue](https://github.com/elastic/logstash/issues/9215) or post in the [Logstash forum](https://discuss.elastic.co/c/logstash)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/java-filter-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 9]"
        },
        {
          "title": "How to write a Java input plugin [java-input-plugin]",
          "description": null,
          "content": {
            "Set up your environment [_set_up_your_environment]": {
              "Copy the example repo [_copy_the_example_repo]": "Start by copying the [example input plugin](https://github.com/logstash-plugins/logstash-input-java_input_example). The plugin API is currently part of the Logstash codebase so you must have a local copy of that available. You can obtain a copy of the Logstash codebase with the following `git` command:\n\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\nThe `branch_name` should correspond to the version of Logstash containing the preferred revision of the Java plugin API.\n\n::::{note}\nThe GA version of the Java plugin API is available in the `7.2` and later branches of the Logstash codebase.\n::::\n\nSpecify the `target_folder` for your local copy of the Logstash codebase. If you do not specify `target_folder`, it defaults to a new folder called `logstash` under your current folder.",
              "Generate the .jar file [_generate_the_jar_file]": "After you have obtained a copy of the appropriate revision of the Logstash codebase, you need to compile it to generate the .jar file containing the Java plugin API. From the root directory of your Logstash codebase ($LS_HOME), you can compile it with `./gradlew assemble` (or `gradlew.bat assemble` if you’re running on Windows). This should produce the `$LS_HOME/logstash-core/build/libs/logstash-core-x.y.z.jar` where `x`, `y`, and `z` refer to the version of Logstash.\n\nAfter you have successfully compiled Logstash, you need to tell your Java plugin where to find the `logstash-core-x.y.z.jar` file. Create a new file named `gradle.properties` in the root folder of your plugin project. That file should have a single line:\n\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\nwhere `target_folder` is the root folder of your local copy of the Logstash codebase."
            },
            "Code the plugin [_code_the_plugin]": {
              "Class declaration [_class_declaration_5]": "```\n@LogstashPlugin(name=\"java_input_example\")\npublic class JavaInputExample implements Input {\n```\n\nNotes about the class declaration:\n\n['All Java plugins must be annotated with the `@LogstashPlugin` annotation. Additionally:', ['The `name` property of the annotation must be supplied and defines the name of the plugin as it will be used in the Logstash pipeline definition. For example, this input would be referenced in the input section of the Logstash pipeline defintion as `input { java_input_example => { .... } }`', 'The value of the `name` property must match the name of the class excluding casing and underscores.'], 'The class must implement the `co.elastic.logstash.api.Input` interface.', 'Java plugins may not be created in the `org.logstash` or `co.elastic.logstash` packages to prevent potential clashes with classes in Logstash itself.']",
              "Plugin settings [_plugin_settings]": "The snippet below contains both the setting definition and the method referencing it.\n\n```\npublic static final PluginConfigSpec<Long> EVENT_COUNT_CONFIG =\n        PluginConfigSpec.numSetting(\"count\", 3);\n\npublic static final PluginConfigSpec<String> PREFIX_CONFIG =\n        PluginConfigSpec.stringSetting(\"prefix\", \"message\");\n\n@Override\npublic Collection<PluginConfigSpec<?>> configSchema() {\n    return Arrays.asList(EVENT_COUNT_CONFIG, PREFIX_CONFIG);\n}\n```\n\nThe `PluginConfigSpec` class allows developers to specify the settings that a plugin supports complete with setting  name, data type, deprecation status, required status, and default value. In this example, the `count` setting defines the number of events that will be generated and the `prefix` setting defines an optional prefix to include in the event field. Neither setting is required and if it is not explicitly set, the settings default to `3` and  `message`, respectively.\n\nThe `configSchema` method must return a list of all settings that the plugin supports. In a future phase of the Java plugin project, the Logstash execution engine will validate that all required settings are present and that no unsupported settings are present.",
              "Constructor and initialization [_constructor_and_initialization]": "```\nprivate String id;\nprivate long count;\nprivate String prefix;\n\npublic JavaInputExample(String id, Configuration config, Context context) {\n    this.id = id;\n    count = config.get(EVENT_COUNT_CONFIG);\n    prefix = config.get(PREFIX_CONFIG);\n}\n```\n\nAll Java input plugins must have a constructor taking a `String` id and `Configuration` and `Context` argument. This is the constructor that will be used to instantiate them at runtime. The retrieval and validation of all plugin settings should occur in this constructor. In this example, the values of the two plugin settings are retrieved and stored in local variables for later use in the `start` method.\n\nAny additional initialization may occur in the constructor as well. If there are any unrecoverable errors encountered in the configuration or initialization of the input plugin, a descriptive exception should be thrown. The exception will be logged and will prevent Logstash from starting.",
              "Start method [_start_method]": "```\n@Override\npublic void start(Consumer<Map<String, Object>> consumer) {\n    int eventCount = 0;\n    try {\n        while (!stopped && eventCount < count) {\n            eventCount++;\n            consumer.accept.push(Collections.singletonMap(\"message\",\n                    prefix + \" \" + StringUtils.center(eventCount + \" of \" + count, 20)));\n        }\n    } finally {\n        stopped = true;\n        done.countDown();\n    }\n}\n```\n\nThe `start` method begins the event-producing loop in an input. Inputs are flexible and may produce events through many different mechanisms including:\n\n['a pull mechanism such as periodic queries of external database', 'a push mechanism such as events sent from clients to a local network port', 'a timed computation such as a heartbeat', 'any other mechanism that produces a useful stream of events. Event streams may be either finite or infinite. If the input produces an infinite stream of events, this method should loop until a stop request is made through the `stop` method. If the input produces a finite stream of events, this method should terminate when the last event in the stream is produced or a stop request is made, whichever comes first.']\n\nEvents should be constructed as instances of `Map<String, Object>` and pushed into the event pipeline via the `Consumer<Map<String, Object>>.accept()` method. To reduce allocations and GC pressure, inputs may reuse the same map instance by modifying its fields between calls to `Consumer<Map<String, Object>>.accept()` because the event pipeline will create events based on a copy of the map’s data.",
              "Stop and awaitStop methods [_stop_and_awaitstop_methods]": "```\nprivate final CountDownLatch done = new CountDownLatch(1);\nprivate volatile boolean stopped;\n\n@Override\npublic void stop() {\n    stopped = true; // set flag to request cooperative stop of input\n}\n\n@Override\npublic void awaitStop() throws InterruptedException {\n    done.await(); // blocks until input has stopped\n}\n```\n\nThe `stop` method notifies the input to stop producing events. The stop mechanism may be implemented in any way that honors the API contract though a `volatile boolean` flag works well for many use cases.\n\nInputs stop both asynchronously and cooperatively. Use the `awaitStop` method to block until the input has  completed the stop process. Note that this method should **not** signal the input to stop as the `stop` method does. The awaitStop mechanism may be implemented in any way that honors the API contract though a `CountDownLatch` works well for many use cases.",
              "getId method [_getid_method]": "```\n@Override\npublic String getId() {\n    return id;\n}\n```\n\nFor input plugins, the `getId` method should always return the id that was provided to the plugin through its constructor at instantiation time.",
              "Unit tests [_unit_tests]": "Lastly, but certainly not least importantly, unit tests are strongly encouraged. The example input plugin includes an [example unit test](https://github.com/logstash-plugins/logstash-input-java_input_example/blob/main/src/test/java/org/logstashplugins/JavaInputExampleTest.java) that you can use as a template for your own."
            },
            "Package and deploy [_package_and_deploy]": {
              "Configuring the Gradle packaging task [_configuring_the_gradle_packaging_task]": "The following section appears near the top of the `build.gradle` file supplied with the example Java plugins:\n\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \\$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\nYou should configure the values above for your plugin.\n\n['The `version` value will be automatically read from the `VERSION` file in the root of your plugin’s codebase.', '`pluginInfo.pluginType` should be set to one of `input`, `filter`, `codec`, or `output`.', '`pluginInfo.pluginName` must match the name specified on the `@LogstashPlugin` annotation on the main plugin class. The Gradle packaging task will validate that and return an error if they do not match.']",
              "Running the Gradle packaging task [_running_the_gradle_packaging_task]": "Several Ruby source files along with a `gemspec` file and a `Gemfile` are required to package the plugin as a Ruby gem. These Ruby files are used only for defining the Ruby gem structure or at Logstash startup time to register the Java plugin. They are not used during runtime event processing. The Gradle packaging task automatically generates all of these files based on the values configured in the section above.\n\nYou run the Gradle packaging task with the following command:\n\n```\n./gradlew gem\n```\n\nFor Windows platforms: Substitute `gradlew.bat` for `./gradlew` as appropriate in the command.\n\nThat task will produce a gem file in the root directory of your plugin’s codebase with the name `logstash-{{plugintype}}-<pluginName>-<version>.gem`",
              "Installing the Java plugin in Logstash [_installing_the_java_plugin_in_logstash]": "After you have packaged your Java plugin as a Ruby gem, you can install it in Logstash with this command:\n\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\nFor Windows platforms: Substitute backslashes for forward slashes as appropriate in the command."
            },
            "Running Logstash with the Java input plugin [_running_logstash_with_the_java_input_plugin]": "The following is a minimal Logstash configuration that can be used to test that the Java input plugin is correctly installed and functioning.\n\n```\ninput {\n  java_input_example {}\n}\noutput {\n  stdout { codec => rubydebug }\n}\n```\n\nCopy the above Logstash configuration to a file such as `java_input.conf`. Start {{ls}} with:\n\n```\nbin/logstash -f /path/to/java_input.conf\n```\n\nThe expected Logstash output (excluding initialization) with the configuration above is:\n\n```\n{\n      \"@version\" => \"1\",\n       \"message\" => \"message        1 of 3       \",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ\n}\n{\n      \"@version\" => \"1\",\n       \"message\" => \"message        2 of 3       \",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ\n}\n{\n      \"@version\" => \"1\",\n       \"message\" => \"message        3 of 3       \",\n    \"@timestamp\" => yyyy-MM-ddThh:mm:ss.SSSZ\n}\n```",
            "Feedback [_feedback]": "If you have any feedback on Java plugin support in Logstash, please comment on our [main Github issue](https://github.com/elastic/logstash/issues/9215) or post in the [Logstash forum](https://discuss.elastic.co/c/logstash)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/java-input-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 10]"
        },
        {
          "title": "How to write a Java output plugin [java-output-plugin]",
          "description": null,
          "content": {
            "Set up your environment [_set_up_your_environment_4]": {
              "Copy the example repo [_copy_the_example_repo_4]": "Start by copying the [example output plugin](https://github.com/logstash-plugins/logstash-output-java_output_example). The plugin API is currently part of the Logstash codebase so you must have a local copy of that available. You can obtain a copy of the Logstash codebase with the following `git` command:\n\n```\ngit clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>\n```\n\nThe `branch_name` should correspond to the version of Logstash containing the preferred revision of the Java plugin API.\n\n::::{note}\nThe GA version of the Java plugin API is available in the `7.2` and later branches of the Logstash codebase.\n::::\n\nSpecify the `target_folder` for your local copy of the Logstash codebase. If you do not specify `target_folder`, it defaults to a new folder called `logstash` under your current folder.",
              "Generate the .jar file [_generate_the_jar_file_4]": "After you have obtained a copy of the appropriate revision of the Logstash codebase, you need to compile it to generate the .jar file containing the Java plugin API. From the root directory of your Logstash codebase ($LS_HOME), you can compile it with `./gradlew assemble` (or `gradlew.bat assemble` if you’re running on Windows). This should produce the `$LS_HOME/logstash-core/build/libs/logstash-core-x.y.z.jar` where `x`, `y`, and `z` refer to the version of Logstash.\n\nAfter you have successfully compiled Logstash, you need to tell your Java plugin where to find the `logstash-core-x.y.z.jar` file. Create a new file named `gradle.properties` in the root folder of your plugin project. That file should have a single line:\n\n```\nLOGSTASH_CORE_PATH=<target_folder>/logstash-core\n```\n\nwhere `target_folder` is the root folder of your local copy of the Logstash codebase."
            },
            "Code the plugin [_code_the_plugin_4]": {
              "Class declaration [_class_declaration_8]": "```\n@LogstashPlugin(name=\"java_output_example\")\npublic class JavaOutputExample implements Output {\n```\n\nNotes about the class declaration:\n\n['All Java plugins must be annotated with the `@LogstashPlugin` annotation. Additionally:', ['The `name` property of the annotation must be supplied and defines the name of the plugin as it will be used in the Logstash pipeline definition. For example, this output would be referenced in the output section of the Logstash pipeline definition as `output { java_output_example => { .... } }`', 'The value of the `name` property must match the name of the class excluding casing and underscores.'], 'The class must implement the `co.elastic.logstash.api.Output` interface.', 'Java plugins may not be created in the `org.logstash` or `co.elastic.logstash` packages to prevent potential clashes with classes in Logstash itself.']",
              "Plugin settings [_plugin_settings_4]": "The snippet below contains both the setting definition and the method referencing it:\n\n```\npublic static final PluginConfigSpec<String> PREFIX_CONFIG =\n        PluginConfigSpec.stringSetting(\"prefix\", \"\");\n\n@Override\npublic Collection<PluginConfigSpec<?>> configSchema() {\n    return Collections.singletonList(PREFIX_CONFIG);\n}\n```\n\nThe `PluginConfigSpec` class allows developers to specify the settings that a plugin supports complete with setting  name, data type, deprecation status, required status, and default value. In this example, the `prefix` setting defines an optional prefix to include in the output of the event. The setting is not required and if it is not explicitly set, it defaults to the empty string.\n\nThe `configSchema` method must return a list of all settings that the plugin supports. In a future phase of the Java plugin project, the Logstash execution engine will validate that all required settings are present and that no unsupported settings are present.",
              "Constructor and initialization [_constructor_and_initialization_4]": "```\nprivate final String id;\nprivate String prefix;\nprivate PrintStream printer;\n\npublic JavaOutputExample(final String id, final Configuration configuration, final Context context) {\n    this(configuration, context, System.out);\n}\n\nJavaOutputExample(final String id, final Configuration config, final Context context, OutputStream targetStream) {\n    this.id = id;\n    prefix = config.get(PREFIX_CONFIG);\n    printer = new PrintStream(targetStream);\n}\n```\n\nAll Java output plugins must have a constructor taking a `String` id and a `Configuration` and `Context` argument. This is the constructor that will be used to instantiate them at runtime. The retrieval and validation of all plugin settings should occur in this constructor. In this example, the values of the `prefix` setting is retrieved and stored in a local variable for later use in the `output` method. In this example, a second, pacakge private constructor is defined that is useful for unit testing with a `Stream` other than `System.out`.\n\nAny additional initialization may occur in the constructor as well. If there are any unrecoverable errors encountered in the configuration or initialization of the output plugin, a descriptive exception should be thrown. The exception will be logged and will prevent Logstash from starting.",
              "Output method [_output_method]": "```\n@Override\npublic void output(final Collection<Event> events) {\n    Iterator<Event> z = events.iterator();\n    while (z.hasNext() && !stopped) {\n        String s = prefix + z.next();\n        printer.println(s);\n    }\n}\n```\n\nOutputs may send events to local sinks such as the console or a file or to remote systems such as Elasticsearch or other external systems. In this example, the events are printed to the local console.",
              "Stop and awaitStop methods [_stop_and_awaitstop_methods_2]": "```\nprivate final CountDownLatch done = new CountDownLatch(1);\nprivate volatile boolean stopped;\n\n@Override\npublic void stop() {\n    stopped = true;\n    done.countDown();\n}\n\n@Override\npublic void awaitStop() throws InterruptedException {\n    done.await();\n}\n```\n\nThe `stop` method notifies the output to stop sending events. The stop mechanism may be implemented in any way that honors the API contract though a `volatile boolean` flag works well for many use cases. Because this output example is so simple, its `output` method does not check for the stop flag.\n\nOutputs stop both asynchronously and cooperatively. Use the `awaitStop` method to block until the output has  completed the stop process. Note that this method should **not** signal the output to stop as the `stop` method  does. The awaitStop mechanism may be implemented in any way that honors the API contract though a `CountDownLatch` works well for many use cases.",
              "getId method [_getid_method_4]": "```\n@Override\npublic String getId() {\n    return id;\n}\n```\n\nFor output plugins, the `getId` method should always return the id that was provided to the plugin through its constructor at instantiation time.",
              "Unit tests [_unit_tests_4]": "Lastly, but certainly not least importantly, unit tests are strongly encouraged. The example output plugin includes an [example unit test](https://github.com/logstash-plugins/logstash-output-java_output_example/blob/main/src/test/java/org/logstashplugins/JavaOutputExampleTest.java) that you can use as a template for your own."
            },
            "Package and deploy [_package_and_deploy_4]": {
              "Configuring the Gradle packaging task [_configuring_the_gradle_packaging_task_4]": "The following section appears near the top of the `build.gradle` file supplied with the example Java plugins:\n\n```\n// ===========================================================================\n// plugin info\n// ===========================================================================\ngroup                      'org.logstashplugins' // must match the package of the main plugin class\nversion                    \"${file(\"VERSION\").text.trim()}\" // read from required VERSION file\ndescription                = \"Example Java filter implementation\"\npluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs\npluginInfo.longDescription = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \\$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\npluginInfo.authors         = ['Elasticsearch']\npluginInfo.email           = ['info@elastic.co']\npluginInfo.homepage        = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\npluginInfo.pluginType      = \"filter\"\npluginInfo.pluginClass     = \"JavaFilterExample\"\npluginInfo.pluginName      = \"java_filter_example\"\n// ===========================================================================\n```\n\nYou should configure the values above for your plugin.\n\n['The `version` value will be automatically read from the `VERSION` file in the root of your plugin’s codebase.', '`pluginInfo.pluginType` should be set to one of `input`, `filter`, `codec`, or `output`.', '`pluginInfo.pluginName` must match the name specified on the `@LogstashPlugin` annotation on the main plugin class. The Gradle packaging task will validate that and return an error if they do not match.']",
              "Running the Gradle packaging task [_running_the_gradle_packaging_task_4]": "Several Ruby source files along with a `gemspec` file and a `Gemfile` are required to package the plugin as a Ruby gem. These Ruby files are used only for defining the Ruby gem structure or at Logstash startup time to register the Java plugin. They are not used during runtime event processing. The Gradle packaging task automatically generates all of these files based on the values configured in the section above.\n\nYou run the Gradle packaging task with the following command:\n\n```\n./gradlew gem\n```\n\nFor Windows platforms: Substitute `gradlew.bat` for `./gradlew` as appropriate in the command.\n\nThat task will produce a gem file in the root directory of your plugin’s codebase with the name `logstash-{{plugintype}}-<pluginName>-<version>.gem`",
              "Installing the Java plugin in Logstash [_installing_the_java_plugin_in_logstash_4]": "After you have packaged your Java plugin as a Ruby gem, you can install it in Logstash with this command:\n\n```\nbin/logstash-plugin install --no-verify --local /path/to/javaPlugin.gem\n```\n\nFor Windows platforms: Substitute backslashes for forward slashes as appropriate in the command."
            },
            "Running Logstash with the Java output plugin [_running_logstash_with_the_java_output_plugin]": "The following is a minimal Logstash configuration that can be used to test that the Java output plugin is correctly installed and functioning.\n\n```\ninput {\n  generator { message => \"Hello world!\" count => 1 }\n}\noutput {\n  java_output_example {}\n}\n```\n\nCopy the above Logstash configuration to a file such as `java_output.conf`. Logstash should then be started with:\n\n```\nbin/logstash -f /path/to/java_output.conf\n```\n\nThe expected Logstash output (excluding initialization) with the configuration above is:\n\n```\n{\"@timestamp\":\"yyyy-MM-ddTHH:mm:ss.SSSZ\",\"message\":\"Hello world!\",\"@version\":\"1\",\"host\":\"<yourHostname>\",\"sequence\":0}\n```",
            "Feedback [_feedback_4]": "If you have any feedback on Java plugin support in Logstash, please comment on our [main Github issue](https://github.com/elastic/logstash/issues/9215) or post in the [Logstash forum](https://discuss.elastic.co/c/logstash)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/java-output-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 11]"
        },
        {
          "title": "How to write a Logstash output plugin [output-new-plugin]",
          "description": null,
          "content": {
            "Get started [_get_started_4]": {
              "Create a GitHub repo for your new plugin [_create_a_github_repo_for_your_new_plugin_4]": "Each Logstash plugin lives in its own GitHub repository. To create a new repository for your plugin:\n\n['Log in to GitHub.', 'Click the **Repositories** tab. You’ll see a list of other repositories you’ve forked or contributed to.', 'Click the green **New** button in the upper right.', 'Specify the following settings for your new repo:', ['**Repository name**\\u2009—\\u2009a unique name of the form `logstash-output-pluginname`.', '**Public or Private**\\u2009—\\u2009your choice, but the repository must be Public if you want to submit it as an official plugin.', '**Initialize this repository with a README**\\u2009—\\u2009enables you to immediately clone the repository to your computer.'], 'Click **Create Repository**.']",
              "Use the plugin generator tool [_use_the_plugin_generator_tool_4]": "You can create your own Logstash plugin in seconds! The `generate` subcommand of `bin/logstash-plugin` creates the foundation for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you can start adding custom code to process data with Logstash.\n\nFor more information, see [Generating plugins](/reference/plugin-generator.md)",
              "Copy the output code [_copy_the_output_code]": "Alternatively, you can use the examples repo we host on github.com\n\n['**Clone your plugin.** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``output-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash``-output-MYPLUGINNAME.git`'], '`cd logstash-output-MYPLUGINNAME`'], '**Clone the output plugin example and copy it to your plugin branch.**', 'You don’t want to include the example .git directory or its contents, so delete it before you copy the example.', ['`cd /tmp`', '`git clone https://github.com/logstash-plugins/logstash``-output-example.git`', '`cd logstash-output-example`', '`rm -rf .git`', '`cp -R * /path/to/logstash-output-mypluginname/`'], '**Rename the following files to match the name of your plugin.**', ['`logstash-output-example.gemspec`', '`example.rb`', '`example_spec.rb`', '```\\ncd /path/to/logstash-output-mypluginname\\nmv logstash-output-example.gemspec logstash-output-mypluginname.gemspec\\nmv lib/logstash/outputs/example.rb lib/logstash/outputs/mypluginname.rb\\nmv spec/outputs/example_spec.rb spec/outputs/mypluginname_spec.rb\\n```']]\n\nYour file structure should look like this:\n\n```\n$ tree logstash-output-mypluginname\n├── Gemfile\n├── LICENSE\n├── README.md\n├── Rakefile\n├── lib\n│   └── logstash\n│       └── outputs\n│           └── mypluginname.rb\n├── logstash-output-mypluginname.gemspec\n└── spec\n    └── outputs\n        └── mypluginname_spec.rb\n```\n\nFor more information about the Ruby gem file structure and an excellent walkthrough of the Ruby gem creation process, see [http://timelessrepo.com/making-ruby-gems](http://timelessrepo.com/making-ruby-gems)",
              "See what your plugin looks like [_see_what_your_plugin_looks_like_4]": "Before we dive into the details, open up the plugin file in your favorite text editor and take a look.\n\n```\nrequire \"logstash/outputs/base\"\nrequire \"logstash/namespace\"\n\n# Add any asciidoc formatted documentation here\n# An example output that does nothing.\nclass LogStash::Outputs::Example < LogStash::Outputs::Base\n  config_name \"example\"\n\n  # This sets the concurrency behavior of this plugin. By default it is :legacy, which was the standard\n  # way concurrency worked before Logstash 2.4\n  #\n  # You should explicitly set it to either :single or :shared as :legacy will be removed in Logstash 6.0\n  #\n  # When configured as :single a single instance of the Output will be shared among the\n  # pipeline worker threads. Access to the `#multi_receive/#multi_receive_encoded/#receive` method will be synchronized\n  # i.e. only one thread will be active at a time making threadsafety much simpler.\n  #\n  # You can set this to :shared if your output is threadsafe. This will maximize\n  # concurrency but you will need to make appropriate uses of mutexes in `#multi_receive/#receive`.\n  #\n  # Only the `#multi_receive/#multi_receive_encoded` methods need to actually be threadsafe, the other methods\n  # will only be executed in a single thread\n  concurrency :single\n\n  public\n  def register\n  end # def register\n\n  public\n  # Takes an array of events\n  # Must be threadsafe if `concurrency :shared` is set\n  def multi_receive(events)\n  end # def multi_receive\nend # class LogStash::Outputs::Example\n```"
            },
            "Coding output plugins [_coding_output_plugins]": {
              "`require` Statements [_require_statements_4]": "Logstash output plugins require parent classes defined in `logstash/outputs/base` and logstash/namespace:\n\n```\nrequire \"logstash/outputs/base\"\nrequire \"logstash/namespace\"\n```\n\nOf course, the plugin you build may depend on other code, or even gems. Just put them here along with these Logstash dependencies."
            },
            "Plugin Body [_plugin_body_4]": {
              "`class` Declaration [_class_declaration_4]": "The output plugin class should be a subclass of `LogStash::Outputs::Base`:\n\n```\nclass LogStash::Outputs::Example < LogStash::Outputs::Base\n```\n\nThe class name should closely mirror the plugin name, for example:\n\n```\nLogStash::Outputs::Example\n```",
              "`config_name` [_config_name_4]": "```\n  config_name \"example\"\n```\n\nThis is the name your plugin will call inside the output configuration block.\n\nIf you set `config_name \"example\"` in your plugin code, the corresponding Logstash configuration block would need to look like this:"
            },
            "Configuration Parameters [_configuration_parameters_4]": "```\n  config :variable_name, :validate => :variable_type, :default => \"Default value\", :required => boolean, :deprecated => boolean, :obsolete => string\n```\n\nThe configuration, or `config` section allows you to define as many (or as few) parameters as are needed to enable Logstash to process events.\n\nThere are several configuration attributes:\n\n['`:validate` - allows you to enforce passing a particular data type to Logstash for this configuration option, such as `:string`, `:password`, `:boolean`, `:number`, `:array`, `:hash`, `:path` (a file-system path), `uri`, `:codec` (since 1.2.0), `:bytes`.  Note that this also works as a coercion in that if I specify \"true\" for boolean (even though technically a string), it will become a valid boolean in the config.  This coercion works for the `:number` type as well where \"1.2\" becomes a float and \"22\" is an integer.', '`:default` - lets you specify a default value for a parameter', '`:required` - whether or not this parameter is mandatory (a Boolean `true` or', '`:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable. `false`)', '`:deprecated` - informational (also a Boolean `true` or `false`)', '`:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.']",
            "Plugin Methods [_plugin_methods_4]": {
              "`register` Method [_register_method_4]": "```\n  public\n  def register\n  end # def register\n```\n\nThe Logstash `register` method is like an `initialize` method. It was originally created to enforce having `super` called, preventing headaches for newbies. (Note: It may go away in favor of `initialize`, in conjunction with some enforced testing to ensure `super` is called.)\n\n`public` means the method can be called anywhere, not just within the class. This is the default behavior for methods in Ruby, but it is specified explicitly here anyway.\n\nYou can also assign instance variables here (variables prepended by `@`). Configuration variables are now in scope as instance variables, like `@message`"
            },
            "Building the Plugin [_building_the_plugin_4]": {
              "External dependencies [_external_dependencies_4]": "A `require` statement in Ruby is used to include necessary code. In some cases your plugin may require additional files.  For example, the collectd plugin [uses](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/lib/logstash/codecs/collectd.rb#L148) the `types.db` file provided by collectd.  In the main directory of your plugin, a file called `vendor.json` is where these files are described.\n\nThe `vendor.json` file contains an array of JSON objects, each describing a file dependency. This example comes from the [collectd](https://github.com/logstash-plugins/logstash-codec-collectd/blob/main/vendor.json) codec plugin:\n\n```\n[{\n        \"sha1\": \"a90fe6cc53b76b7bdd56dc57950d90787cb9c96e\",\n        \"url\": \"http://collectd.org/files/collectd-5.4.0.tar.gz\",\n        \"files\": [ \"/src/types.db\" ]\n}]\n```\n\n['`sha1` is the sha1 signature used to verify the integrity of the file referenced by `url`.', '`url` is the address from where Logstash will download the file.', '`files` is an optional array of files to extract from the downloaded file. Note that while tar archives can use absolute or relative paths, treat them as absolute in this array.  If `files` is not present, all files will be uncompressed and extracted into the vendor directory.']\n\nAnother example of the `vendor.json` file is the [`geoip` filter](https://github.com/logstash-plugins/logstash-filter-geoip/blob/main/vendor.json)\n\nThe process used to download these dependencies is to call `rake vendor`.  This will be discussed further in the testing section of this document.\n\nAnother kind of external dependency is on jar files.  This will be described in the \"Add a `gemspec` file\" section.",
              "Deprecated features [_deprecated_features_4]": "As a plugin evolves, an option or feature may no longer serve the intended purpose, and the developer may want to *deprecate* its usage. Deprecation warns users about the option’s status, so they aren’t caught by surprise when it is removed in a later release.\n\n{{ls}} 7.6 introduced a *deprecation logger* to make handling those situations easier. You can use the [adapter](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support) to ensure that your plugin can use the deprecation logger while still supporting older versions of {{ls}}. See the [readme](https://github.com/logstash-plugins/logstash-mixin-deprecation_logger_support/blob/main/README.md) for more information and for instructions on using the adapter.\n\nDeprecations are noted in the `logstash-deprecation.log` file in the `log` directory.",
              "Add a Gemfile [_add_a_gemfile_4]": "Gemfiles allow Ruby’s Bundler to maintain the dependencies for your plugin. Currently, all we’ll need is the Logstash gem, for testing, but if you require other gems, you should add them in here.\n\n::::{tip}\nSee [Bundler’s Gemfile page](http://bundler.io/gemfile.html) for more details.\n::::\n\n```\nsource 'https://rubygems.org'\ngemspec\ngem \"logstash\", :github => \"elastic/logstash\", :branch => \"master\"\n```"
            },
            "Add a `gemspec` file [_add_a_gemspec_file_4]": {
              "Runtime and Development Dependencies [_runtime_and_development_dependencies_4]": "At the bottom of the `gemspec` file is a section with a comment: `Gem dependencies`.  This is where any other needed gems must be mentioned. If a gem is necessary for your plugin to function, it is a runtime dependency. If a gem are only used for testing, then it would be a development dependency.\n\n::::{note}\nYou can also have versioning requirements for your dependencies—​including other Logstash plugins:\n\n```\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core-plugin-api\", \">= 1.60\", \"<= 2.99\"\n  s.add_development_dependency 'logstash-devutils'\n```\n\nThis gemspec has a runtime dependency on the logstash-core-plugin-api and requires that it have a version number greater than or equal to version 1.60 and less than or equal to version 2.99.\n\n::::\n\n::::{important}\nAll plugins have a runtime dependency on the `logstash-core-plugin-api` gem, and a development dependency on `logstash-devutils`.\n::::",
              "Jar dependencies [_jar_dependencies_4]": "In some cases, such as the [Elasticsearch output plugin](https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/main/logstash-output-elasticsearch.gemspec#L22-L23), your code may depend on a jar file.  In cases such as this, the dependency is added in the gemspec file in this manner:\n\n```\n  # Jar dependencies\n  s.requirements << \"jar 'org.elasticsearch:elasticsearch', '5.0.0'\"\n  s.add_runtime_dependency 'jar-dependencies'\n```\n\nWith these both defined, the install process will search for the required jar file at [http://mvnrepository.com](http://mvnrepository.com) and download the specified version."
            },
            "Document your plugin [_document_your_plugin_4]": "Documentation is an important part of your plugin. All plugin documentation is rendered and placed in the [Logstash Reference](/reference/index.md) and the [Versioned plugin docs](logstash-docs-md://vpr/integration-plugins.md).\n\nSee [Document your plugin](/extend/plugin-doc.md) for tips and guidelines.",
            "Add Tests [_add_tests_4]": "Logstash loves tests. Lots of tests. If you’re using your new output plugin in a production environment, you’ll want to have some tests to ensure you are not breaking any existing functionality.\n\n::::{note}\nA full exposition on RSpec is outside the scope of this document. Learn more about RSpec at [http://rspec.info](http://rspec.info)\n::::\n\nFor help learning about tests and testing, look in the `spec/outputs/` directory of several other similar plugins.",
            "Clone and test! [_clone_and_test_4]": "Now let’s start with a fresh clone of the plugin, build it and run the tests.\n\n['**Clone your plugin into a temporary location** Replace `GITUSERNAME` with your github username, and `MYPLUGINNAME` with your plugin name.', ['`git clone https://github.com/GITUSERNAME/logstash-``output-MYPLUGINNAME.git`', ['alternately, via ssh: `git clone git@github.com:GITUSERNAME/logstash-``output-MYPLUGINNAME.git`'], '`cd logstash-output-MYPLUGINNAME`']]\n\nThen, you’ll need to install your plugins dependencies with bundler:\n\n```\nbundle install\n```\n\n::::{important}\nIf your plugin has an external file dependency described in `vendor.json`, you must download that dependency before running or testing.  You can do this by running:\n\n```\nrake vendor\n```\n\n::::\n\nAnd finally, run the tests:\n\n```\nbundle exec rspec\n```\n\nYou should see a success message, which looks something like this:\n\n```\nFinished in 0.034 seconds\n1 example, 0 failures\n```\n\nHooray! You’re almost there! (Unless you saw failures…​ you should fix those first).",
            "Building and Testing [_building_and_testing_4]": {
              "Build [_build_4]": "You already have all the necessary ingredients, so let’s go ahead and run the build command:\n\n```\ngem build logstash-output-example.gemspec\n```\n\nThat’s it!  Your gem should be built and be in the same path with the name\n\n```\nlogstash-output-mypluginname-0.1.0.gem\n```\n\nThe `s.version` number from your gemspec file will provide the gem version, in this case, `0.1.0`.",
              "Test installation [_test_installation_4]": "You should test install your plugin into a clean installation of Logstash. Download the latest version from the [Logstash downloads page](https://www.elastic.co/downloads/logstash/).\n\n['Untar and cd in to the directory:', '```\\ncurl -O https://download.elastic.co/logstash/logstash/logstash-9.0.0.tar.gz\\ntar xzvf logstash-9.0.0.tar.gz\\ncd logstash-9.0.0\\n```', 'Using the plugin tool, we can install the gem we just built.', ['Replace `/my/logstash/plugins` with  the correct path to the gem for your environment, and `0.1.0` with the correct version number from the gemspec file.', '```\\nbin/logstash-plugin install /my/logstash/plugins/logstash-output-example/logstash-output-example-0.1.0.gem\\n```', 'After running this, you should see feedback from Logstash that it was successfully installed:', \"```\\nvalidating /my/logstash/plugins/logstash-output-example/logstash-output-example-0.1.0.gem >= 0\\nValid logstash plugin. Continuing...\\nSuccessfully installed 'logstash-output-example' with version '0.1.0'\\n```\", '::::{tip}\\nYou can also use the Logstash plugin tool to determine which plugins are currently available:', '```\\nbin/logstash-plugin list\\n```', 'Depending on what you have installed, you might see a short or long list of plugins: inputs, codecs, filters and outputs.', '::::'], 'Now try running Logstash with a simple configuration passed in via the command-line, using the `-e` flag.', '::::{note}\\nYour results will depend on what your output plugin is designed to do.\\n::::']\n\nCongratulations! You’ve built, deployed and successfully run a Logstash output."
            },
            "Submitting your plugin to [RubyGems.org](http://rubygems.org) and [logstash-plugins](https://github.com/logstash-plugins) [_submitting_your_plugin_to_rubygems_orghttprubygems_org_and_logstash_pluginshttpsgithub_comlogstash_plugins_4]": {
              "Licensing [_licensing_4]": "Logstash and all its plugins are licensed under [Apache License, version 2 (\"ALv2\")](https://github.com/elasticsearch/logstash/blob/main/LICENSE). If you make your plugin publicly available via [RubyGems.org](http://rubygems.org), please make sure to have this line in your gemspec:\n\n[\"`s.licenses = ['Apache License (2.0)']`\"]",
              "Publishing to [RubyGems.org](http://rubygems.org) [_publishing_to_rubygems_orghttprubygems_org_4]": "To begin, you’ll need an account on RubyGems.org\n\n['[Sign-up for a RubyGems account](https://rubygems.org/sign_up).']\n\nAfter creating an account, [obtain](http://guides.rubygems.org/rubygems-org-api/#api-authorization) an API key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials` to store your API key. These credentials will be used to publish the gem. Replace `username` and `password` with the credentials you created at RubyGems.org:\n\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\nBefore proceeding, make sure you have the right version in your gemspec file and commit your changes.\n\n[\"`s.version = '0.1.0'`\"]\n\nTo publish version 0.1.0 of your new logstash gem:\n\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n::::{note}\nExecuting `rake publish_gem`:\n\n[\"Reads the version from the gemspec file (`s.version = '0.1.0'`)\", 'Checks in your local repository if a tag exists for that version. If the tag already exists, it aborts the process. Otherwise, it creates a new version tag in your local repository.', 'Builds the gem', 'Publishes the gem to RubyGems.org']\n\n::::\n\nThat’s it! Your plugin is published! Logstash users can now install your plugin by running:\n\n```\nbin/logstash-plugin install logstash-output-mypluginname\n```"
            },
            "Contributing your source code to [logstash-plugins](https://github.com/logstash-plugins) [_contributing_your_source_code_to_logstash_pluginshttpsgithub_comlogstash_plugins_4]": {
              "Benefits [_benefits_4]": "Some of the many benefits of having your plugin in the logstash-plugins repository are:\n\n['**Discovery.** Your plugin will appear in the [Logstash Reference](/reference/index.md), where Logstash users look first for plugins and documentation.', '**Documentation.** Your plugin documentation will automatically be added to the [Logstash Reference](/reference/index.md).', '**Testing.** With our testing infrastructure, your plugin will be continuously tested against current and future releases of Logstash.  As a result, users will have the assurance that if incompatibilities arise, they will be quickly discovered and corrected.']",
              "Acceptance Guidelines [_acceptance_guidelines_4]": [
                "**Code Review.** Your plugin must be reviewed by members of the community for coherence, quality, readability, stability and security.",
                "**Tests.** Your plugin must contain tests to be accepted.  These tests are also subject to code review for scope and completeness.  It’s ok if you don’t know how to write tests — we will guide you. We are working on publishing a guide to creating tests for Logstash which will make it easier.  In the meantime, you can refer to [http://betterspecs.org/](http://betterspecs.org/) for examples."
              ]
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/output-new-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 12]"
        },
        {
          "title": "Document your plugin [plugin-doc]",
          "description": null,
          "content": {
            "Documentation file [plugin-doc-file]": "Documentation belongs in a single file called *docs/index.asciidoc*. It belongs in a single file called *docs/index.asciidoc*. The [plugin generation utility](/reference/plugin-generator.md) creates a starter file for you.",
            "Heading IDs [heading-ids]": "Format heading anchors with variables that can support generated IDs. This approach creates unique IDs when the [Logstash Versioned Plugin Reference](logstash-docs-md://vpr/integration-plugins.md) is built. Unique heading IDs are required to avoid duplication over multiple versions of a plugin.\n\n**Example**\n\nDon’t hardcode a plugin heading ID like this: `[[config_models]]`\n\nInstead, use variables to define it:\n\n```\n[id=\"plugins-{type}s-{plugin}-config_models\"]\n==== Configuration models\n```\n\nIf you hardcode an ID, the [Logstash Versioned Plugin Reference](logstash-docs-md://vpr/integration-plugins.md) builds correctly the first time. The second time the doc build runs, the ID is flagged as a duplicate, and the build fails.",
            "Link formats [link-format]": {
              "Link to content in the same file [_link_to_content_in_the_same_file]": "Use angle brackets to format links to content in the same asciidoc file.\n\n**Example**\n\nThis link:\n\n```\n<<plugins-{type}s-{plugin}-config_models>>\n```\n\nPoints to this heading in the same file:\n\n```\n[id=\"plugins-{type}s-{plugin}-config_models\"]\n==== Configuration models\n```",
              "Link to content in the Logstash Reference Guide [_link_to_content_in_the_logstash_reference_guide]": "Use external link syntax for links that point to documentation for other plugins or content in the Logstash Reference Guide.\n\n**Examples**\n\n```\n{logstash-ref}/plugins-codecs-multiline.html[Multiline codec plugin]\n```\n\n```\n{logstash-ref}/getting-started-with-logstash.html\n```",
              "Link text [_link_text]": "If you don’t specify link text, the URL is used as the link text.\n\n**Examples**\n\nIf you want your link to display as {{logstash-ref}}/getting-started-with-logstash.html, use this format:\n\n```\n{logstash-ref}/getting-started-with-logstash.html\n```\n\nIf you want your link to display as [Getting Started with Logstash](/reference/getting-started-with-logstash.md), use this format:\n\n```\n{logstash-ref}/getting-started-with-logstash.html[Getting Started with Logstash]\n```",
              "Link to data type descriptions [_link_to_data_type_descriptions]": "We make an exception for links that point to data type descriptions, such as `<<boolean,boolean>>`, because they are used so frequently. We have a cleanup step in the conversion script that converts the links to the correct syntax."
            },
            "Code samples [format-code]": "We all love code samples. Asciidoc supports code blocks and config examples. To include Ruby code, use the asciidoc `[source,ruby]` directive.\n\nNote that the hashmarks (#) are present to make the example render correctly. Don’t include the hashmarks in your asciidoc file.\n\n```\n# [source,ruby]\n# -----\n# match => {\n#  \"field1\" => \"value1\"\n#  \"field2\" => \"value2\"\n#  ...\n# }\n# -----\n```\n\nThe sample above (with hashmarks removed) renders in the documentation like this:\n\n```\nmatch => {\n  \"field1\" => \"value1\"\n  \"field2\" => \"value2\"\n  ...\n}\n```",
            "Where’s my doc? [_wheres_my_doc]": {
              "Documentation or plugin updates [_documentation_or_plugin_updates]": "When you make updates to your plugin or the documentation, consider bumping the version number in the changelog and gemspec (or version file). The version change triggers the doc build to pick up your changes for publishing."
            },
            "Resources [_resources]": "For more asciidoc formatting tips, see the excellent reference at [https://github.com/elastic/docs#asciidoc-guide](https://github.com/elastic/docs#asciidoc-guide).\n\nFor tips on contributing and changelog guidelines, see [CONTRIBUTING.md](https://github.com/elastic/logstash/blob/main/CONTRIBUTING.md#logstash-plugin-changelog-guidelines).\n\nFor general information about contributing, see [Contributing to Logstash](/extend/index.md)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/plugin-doc.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 13]"
        },
        {
          "title": "Plugin Listing",
          "description": null,
          "content": {
            "List your plugin [plugin-listing]": "The [Logstash Reference](/reference/index.md) is the first place {{ls}} users look for plugins and documentation. If your plugin meets the [quality and acceptance guidelines](/extend/index.md#plugin-acceptance), we may be able to list it in the guide.\n\nThe plugin source and documentation will continue to live in your repo, and we will direct users there.\n\nIf you would like to have your plugin included in the [Logstash Reference](/reference/index.md), create a new [issue](https://github.com/elasticsearch/logstash/issues) in the Logstash repository with the following information:\n\n['Title: `PluginListing: <your-plugin-name>`', 'Body:', ['Brief description of the plugin (what it is and what it does).', 'Link to the plugin repository.', 'Link to the README.md or docs/index.asciidoc.', 'Describe how your plugin meets our [quality and acceptance guidelines](/extend/index.md#plugin-acceptance).'], 'Labels: `docs`, `new-plugin`']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/plugin-listing.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 14]"
        },
        {
          "title": "Publish your plugin to RubyGems.org [publish-plugin]",
          "description": null,
          "content": {
            "Licensing [_licensing_5]": "Logstash and all its plugins are licensed under [Apache License, version 2 (\"ALv2\")](https://github.com/elasticsearch/logstash/blob/main/LICENSE). If you make your plugin publicly available via [RubyGems.org](http://rubygems.org), please make sure to have this line in your gemspec:\n\n[\"`s.licenses = ['Apache License (2.0)']`\"]",
            "Publish to [RubyGems.org](http://rubygems.org) [_publish_to_rubygems_orghttprubygems_org]": "You’ll need an account on RubyGems.org\n\n['[Sign-up for a RubyGems account](https://rubygems.org/sign_up).']\n\nAfter creating an account, [obtain](http://guides.rubygems.org/rubygems-org-api/#api-authorization) an API key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials` to store your API key. These credentials will be used to publish the gem. Replace `username` and `password` with the credentials you created at RubyGems.org:\n\n```\ncurl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials\nchmod 0600 ~/.gem/credentials\n```\n\nBefore proceeding, make sure you have the right version in your gemspec file and commit your changes.\n\n[\"`s.version = '0.1.0'`\"]\n\nTo publish version 0.1.0 of your new logstash gem:\n\n```\nbundle install\nbundle exec rake vendor\nbundle exec rspec\nbundle exec rake publish_gem\n```\n\n::::{note}\nExecute `rake publish_gem`:\n\n[\"Reads the version from the gemspec file (`s.version = '0.1.0'`)\", 'Checks in your local repository if a tag exists for that version. If the tag already exists, it aborts the process. Otherwise, it creates a new version tag in your local repository.', 'Builds the gem', 'Publishes the gem to RubyGems.org']\n\n::::\n\nThat’s it! Your plugin is published! Logstash users can now install your plugin by running:\n\n```\nbin/plugin install logstash-output-mypluginname\n```\n\nWhere <plugintype> is `input`, `output`, `filter`, or `codec`, and <mypluginname> is the name of your new plugin."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/publish-plugin.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 0, \"subpages\", 15]"
        }
      ],
      "path": "[\"subpages\", 0]"
    },
    {
      "title": "Reference",
      "description": "Documentation section: reference",
      "content": {},
      "metadata": {
        "type": "directory",
        "path": "/home/anhnh/CodeWiki-Benchmarking-System/data/logstash/original/docs/reference"
      },
      "subpages": [
        {
          "title": "Advanced Logstash Configurations",
          "description": null,
          "content": {
            "Advanced Logstash configurations [configuration-advanced]": "You can take {{ls}} beyond basic configuration to handle more advanced requirements, such as multiple pipelines, communication between {{ls}} pipelines, and multiple line events."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/configuration-advanced.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 0]"
        },
        {
          "title": "Parsing Logs with Logstash [advanced-pipeline]",
          "description": null,
          "content": {
            "Configuring Filebeat to Send Log Lines to Logstash [configuring-filebeat]": "Before you create the Logstash pipeline, you’ll configure Filebeat to send log lines to Logstash. The [Filebeat](https://github.com/elastic/beats/tree/main/filebeat) client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your Logstash instance for processing. Filebeat is designed for reliability and low latency. Filebeat has a light resource footprint on the host machine, and the [`Beats input`](logstash-docs-md://lsr/plugins-inputs-beats.md) plugin minimizes the resource demands on the Logstash instance.\n\n::::{note}\nIn a typical use case, Filebeat runs on a separate machine from the machine running your Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the same machine.\n::::\n\nThe default Logstash installation includes the [`Beats input`](logstash-docs-md://lsr/plugins-inputs-beats.md) plugin. The Beats input plugin enables Logstash to receive events from the Elastic Beats framework, which means that any Beat written to work with the Beats framework, such as Packetbeat and Metricbeat, can also send event data to Logstash.\n\nTo install Filebeat on your data source machine, download the appropriate package from the Filebeat [product page](https://www.elastic.co/downloads/beats/filebeat). You can also refer to [Filebeat quick start](beats://reference/filebeat/filebeat-installation-configuration.md) for additional installation instructions.\n\nAfter installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation directory, and replace the contents with the following lines. Make sure `paths` points to the example Apache log file, `logstash-tutorial.log`, that you downloaded earlier:\n\n```\nfilebeat.inputs:\n- type: log\n  paths:\n    - /path/to/file/logstash-tutorial.log <1>\noutput.logstash:\n  hosts: [\"localhost:5044\"]\n```\n\n['Absolute path to the file or files that Filebeat processes.']\n\nSave your changes.\n\nTo keep the configuration simple, you won’t specify TLS/SSL settings as you would in a real world scenario.\n\nAt the data source machine, run Filebeat with the following command:\n\n```\nsudo ./filebeat -e -c filebeat.yml -d \"publish\"\n```\n\n::::{note}\nIf you run Filebeat as root, you need to change ownership of the configuration file (see [Config File Ownership and Permissions](beats://reference/libbeat/config-file-permissions.md) in the *Beats Platform Reference*).\n::::\n\nFilebeat will attempt to connect on port 5044. Until Logstash starts with an active Beats plugin, there won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.",
            "Configuring Logstash for Filebeat Input [_configuring_logstash_for_filebeat_input]": {
              "Parsing Web Logs with the Grok Filter Plugin [configuring-grok-filter]": "Now you have a working pipeline that reads log lines from Filebeat. However you’ll notice that the format of the log messages is not ideal. You want to parse the log messages to create specific, named fields from the logs. To do this, you’ll use the `grok` filter plugin.\n\nThe [`grok`](logstash-docs-md://lsr/plugins-filters-grok.md) filter plugin is one of several plugins that are available by default in Logstash. For details on how to manage Logstash plugins, see the [reference documentation](/reference/working-with-plugins.md) for the plugin manager.\n\nThe `grok` filter plugin enables you to parse the unstructured log data into something structured and queryable.\n\nBecause the `grok` filter plugin looks for patterns in the incoming log data, configuring the plugin requires you to make decisions about how to identify the patterns that are of interest to your use case. A representative line from the web server log sample looks like this:\n\n```\n83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png\nHTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel\nMac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\n```\n\nThe IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. To parse the data, you can use the `%{{COMBINEDAPACHELOG}}` grok pattern, which structures lines from the Apache log using the following schema:\n\n**Information**\n:   **Field Name**\n\nIP Address\n:   `clientip`\n\nUser ID\n:   `ident`\n\nUser Authentication\n:   `auth`\n\ntimestamp\n:   `timestamp`\n\nHTTP Verb\n:   `verb`\n\nRequest body\n:   `request`\n\nHTTP Version\n:   `httpversion`\n\nHTTP Status Code\n:   `response`\n\nBytes served\n:   `bytes`\n\nReferrer URL\n:   `referrer`\n\nUser agent\n:   `agent`\n\n::::{tip}\nIf you need help building grok patterns, try out the [Grok Debugger](docs-content://explore-analyze/query-filter/tools/grok-debugger.md). The Grok Debugger is an {{xpack}} feature under the Basic License and is therefore **free to use**.\n::::\n\nEdit the `first-pipeline.conf` file and replace the entire `filter` section with the following text:\n\n```\nfilter {\n    grok {\n        match => { \"message\" => \"%{COMBINEDAPACHELOG}\"}\n    }\n}\n```\n\nWhen you’re done, the contents of `first-pipeline.conf` should look like this:\n\n```\ninput {\n    beats {\n        port => \"5044\"\n    }\n}\nfilter {\n    grok {\n        match => { \"message\" => \"%{COMBINEDAPACHELOG}\"}\n    }\n}\noutput {\n    stdout { codec => rubydebug }\n}\n```\n\nSave your changes. Because you’ve enabled automatic config reloading, you don’t have to restart Logstash to pick up your changes. However, you do need to force Filebeat to read the log file from scratch. To do this, go to the terminal window where Filebeat is running and press Ctrl+C to shut down Filebeat. Then delete the Filebeat registry file. For example, run:\n\n```\nsudo rm data/registry\n```\n\nSince Filebeat stores the state of each file it harvests in the registry, deleting the registry file forces Filebeat to read all the files it’s harvesting from scratch.\n\nNext, restart Filebeat with the following command:\n\n```\nsudo ./filebeat -e -c filebeat.yml -d \"publish\"\n```\n\nThere might be a slight delay before Filebeat begins processing events if it needs to wait for Logstash to reload the config file.\n\nAfter Logstash applies the grok pattern, the events will have the following JSON representation:\n\n```\n{\n        \"request\" => \"/presentations/logstash-monitorama-2013/images/kibana-search.png\",\n          \"agent\" => \"\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n         \"offset\" => 325,\n           \"auth\" => \"-\",\n          \"ident\" => \"-\",\n           \"verb\" => \"GET\",\n     \"prospector\" => {\n        \"type\" => \"log\"\n    },\n     \"input\" => {\n        \"type\" => \"log\"\n    },\n         \"source\" => \"/path/to/file/logstash-tutorial.log\",\n        \"message\" => \"83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \\\"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\\\" 200 203023 \\\"http://semicomplete.com/presentations/logstash-monitorama-2013/\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n           \"tags\" => [\n        [0] \"beats_input_codec_plain_applied\"\n    ],\n       \"referrer\" => \"\\\"http://semicomplete.com/presentations/logstash-monitorama-2013/\\\"\",\n     \"@timestamp\" => 2017-11-09T02:51:12.416Z,\n       \"response\" => \"200\",\n          \"bytes\" => \"203023\",\n       \"clientip\" => \"83.149.9.216\",\n       \"@version\" => \"1\",\n           \"beat\" => {\n            \"name\" => \"My-MacBook-Pro.local\",\n        \"hostname\" => \"My-MacBook-Pro.local\",\n         \"version\" => \"6.0.0\"\n    },\n           \"host\" => \"My-MacBook-Pro.local\",\n    \"httpversion\" => \"1.1\",\n      \"timestamp\" => \"04/Jan/2015:05:13:42 +0000\"\n}\n```\n\nNotice that the event includes the original message, but the log message is also broken down into specific fields.",
              "Enhancing Your Data with the Geoip Filter Plugin [configuring-geoip-plugin]": "In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing data. As an example, the [`geoip`](logstash-docs-md://lsr/plugins-filters-geoip.md) plugin looks up IP addresses, derives geographic location information from the addresses, and adds that location information to the logs.\n\nConfigure your Logstash instance to use the `geoip` filter plugin by adding the following lines to the `filter` section of the `first-pipeline.conf` file:\n\n```\n    geoip {\n        source => \"clientip\"\n    }\n```\n\nThe `geoip` plugin configuration requires you to specify the name of the source field that contains the IP address to look up. In this example, the `clientip` field contains the IP address.\n\nSince filters are evaluated in sequence, make sure that the `geoip` section is after the `grok` section of the configuration file and that both the `grok` and `geoip` sections are nested within the `filter` section.\n\nWhen you’re done, the contents of `first-pipeline.conf` should look like this:\n\n```\ninput {\n    beats {\n        port => \"5044\"\n    }\n}\n filter {\n    grok {\n        match => { \"message\" => \"%{COMBINEDAPACHELOG}\"}\n    }\n    geoip {\n        source => \"clientip\"\n    }\n}\noutput {\n    stdout { codec => rubydebug }\n}\n```\n\nSave your changes. To force Filebeat to read the log file from scratch, as you did earlier, shut down Filebeat (press Ctrl+C), delete the registry file, and then restart Filebeat with the following command:\n\n```\nsudo ./filebeat -e -c filebeat.yml -d \"publish\"\n```\n\nNotice that the event now contains geographic location information:\n\n```\n{\n        \"request\" => \"/presentations/logstash-monitorama-2013/images/kibana-search.png\",\n          \"agent\" => \"\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\\\"\",\n          \"geoip\" => {\n              \"timezone\" => \"Europe/Moscow\",\n                    \"ip\" => \"83.149.9.216\",\n              \"latitude\" => 55.7485,\n        \"continent_code\" => \"EU\",\n             \"city_name\" => \"Moscow\",\n          \"country_name\" => \"Russia\",\n         \"country_code2\" => \"RU\",\n         \"country_code3\" => \"RU\",\n           \"region_name\" => \"Moscow\",\n              \"location\" => {\n            \"lon\" => 37.6184,\n            \"lat\" => 55.7485\n        },\n           \"postal_code\" => \"101194\",\n           \"region_code\" => \"MOW\",\n             \"longitude\" => 37.6184\n    },\n    ...\n```",
              "Indexing your data into Elasticsearch [indexing-parsed-data-into-elasticsearch]": {
                "Testing Your Pipeline [testing-initial-pipeline]": "Now that the Logstash pipeline is configured to index the data into an Elasticsearch cluster, you can query Elasticsearch.\n\nTry a test query to Elasticsearch based on the fields created by the `grok` filter plugin. Replace $DATE with the current date, in YYYY.MM.DD format:\n\n```\ncurl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=response=200'\n```\n\n::::{note}\nThe date used in the index name is based on UTC, not the timezone where Logstash is running. If the query returns `index_not_found_exception`, make sure that `logstash-$DATE` reflects the actual name of the index. To see a list of available indexes, use this query: `curl 'localhost:9200/_cat/indices?v'`.\n::::\n\nYou should get multiple hits back. For example:\n\n```\n{\n  \"took\": 50,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 98,\n    \"max_score\": 2.793642,\n    \"hits\": [\n      {\n        \"_index\": \"logstash-2017.11.09\",\n        \"_type\": \"doc\",\n        \"_id\": \"3IzDnl8BW52sR0fx5wdV\",\n        \"_score\": 2.793642,\n        \"_source\": {\n          \"request\": \"/presentations/logstash-monitorama-2013/images/frontend-response-codes.png\",\n          \"agent\": \"\"\"\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\"\"\",\n          \"geoip\": {\n            \"timezone\": \"Europe/Moscow\",\n            \"ip\": \"83.149.9.216\",\n            \"latitude\": 55.7485,\n            \"continent_code\": \"EU\",\n            \"city_name\": \"Moscow\",\n            \"country_name\": \"Russia\",\n            \"country_code2\": \"RU\",\n            \"country_code3\": \"RU\",\n            \"region_name\": \"Moscow\",\n            \"location\": {\n              \"lon\": 37.6184,\n              \"lat\": 55.7485\n            },\n            \"postal_code\": \"101194\",\n            \"region_code\": \"MOW\",\n            \"longitude\": 37.6184\n          },\n          \"offset\": 2932,\n          \"auth\": \"-\",\n          \"ident\": \"-\",\n          \"verb\": \"GET\",\n          \"prospector\": {\n            \"type\": \"log\"\n          },\n          \"input\": {\n            \"type\": \"log\"\n          },\n          \"source\": \"/path/to/file/logstash-tutorial.log\",\n          \"message\": \"\"\"83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\"\"\",\n          \"tags\": [\n            \"beats_input_codec_plain_applied\"\n          ],\n          \"referrer\": \"\"\"\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"\"\"\",\n          \"@timestamp\": \"2017-11-09T03:11:35.304Z\",\n          \"response\": \"200\",\n          \"bytes\": \"52878\",\n          \"clientip\": \"83.149.9.216\",\n          \"@version\": \"1\",\n          \"beat\": {\n            \"name\": \"My-MacBook-Pro.local\",\n            \"hostname\": \"My-MacBook-Pro.local\",\n            \"version\": \"6.0.0\"\n          },\n          \"host\": \"My-MacBook-Pro.local\",\n          \"httpversion\": \"1.1\",\n          \"timestamp\": \"04/Jan/2015:05:13:45 +0000\"\n        }\n      },\n    ...\n```\n\nTry another search for the geographic information derived from the IP address. Replace $DATE with the current date, in YYYY.MM.DD format:\n\n```\ncurl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=geoip.city_name=Buffalo'\n```\n\nA few log entries come from Buffalo, so the query produces the following response:\n\n```\n{\n  \"took\": 9,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 2,\n    \"max_score\": 2.6390574,\n    \"hits\": [\n      {\n        \"_index\": \"logstash-2017.11.09\",\n        \"_type\": \"doc\",\n        \"_id\": \"L4zDnl8BW52sR0fx5whY\",\n        \"_score\": 2.6390574,\n        \"_source\": {\n          \"request\": \"/blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29\",\n          \"agent\": \"\"\"\"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\"\"\"\",\n          \"geoip\": {\n            \"timezone\": \"America/New_York\",\n            \"ip\": \"198.46.149.143\",\n            \"latitude\": 42.8864,\n            \"continent_code\": \"NA\",\n            \"city_name\": \"Buffalo\",\n            \"country_name\": \"United States\",\n            \"country_code2\": \"US\",\n            \"dma_code\": 514,\n            \"country_code3\": \"US\",\n            \"region_name\": \"New York\",\n            \"location\": {\n              \"lon\": -78.8781,\n              \"lat\": 42.8864\n            },\n            \"postal_code\": \"14202\",\n            \"region_code\": \"NY\",\n            \"longitude\": -78.8781\n          },\n          \"offset\": 22795,\n          \"auth\": \"-\",\n          \"ident\": \"-\",\n          \"verb\": \"GET\",\n          \"prospector\": {\n            \"type\": \"log\"\n          },\n          \"input\": {\n            \"type\": \"log\"\n          },\n          \"source\": \"/path/to/file/logstash-tutorial.log\",\n          \"message\": \"\"\"198.46.149.143 - - [04/Jan/2015:05:29:13 +0000] \"GET /blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1\" 200 9316 \"-\" \"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\"\"\"\",\n          \"tags\": [\n            \"beats_input_codec_plain_applied\"\n          ],\n          \"referrer\": \"\"\"\"-\"\"\"\",\n          \"@timestamp\": \"2017-11-09T03:11:35.321Z\",\n          \"response\": \"200\",\n          \"bytes\": \"9316\",\n          \"clientip\": \"198.46.149.143\",\n          \"@version\": \"1\",\n          \"beat\": {\n            \"name\": \"My-MacBook-Pro.local\",\n            \"hostname\": \"My-MacBook-Pro.local\",\n            \"version\": \"6.0.0\"\n          },\n          \"host\": \"My-MacBook-Pro.local\",\n          \"httpversion\": \"1.1\",\n          \"timestamp\": \"04/Jan/2015:05:29:13 +0000\"\n        }\n      },\n     ...\n```\n\nIf you are using Kibana to visualize your data, you can also explore the Filebeat data in Kibana:\n\n![Discovering Filebeat data in Kibana](images/kibana-filebeat-data.png)\n\nSee the [Filebeat quick start docs](beats://reference/filebeat/filebeat-installation-configuration.md) for info about loading the Kibana index pattern for Filebeat.\n\nYou’ve successfully created a pipeline that uses Filebeat to take Apache web logs as input, parses those logs to create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Next, you learn how to create a pipeline that uses multiple input and output plugins."
              }
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 1]"
        },
        {
          "title": "Logstash configuration examples [config-examples]",
          "description": null,
          "content": {
            "Configuring filters [filter-example]": "Filters are an in-line processing mechanism that provide the flexibility to slice and dice your data to fit your needs. Let’s take a look at some filters in action. The following configuration file sets up the `grok` and `date` filters.\n\n```\ninput { stdin { } }\n\nfilter {\n  grok {\n    match => { \"message\" => \"%{COMBINEDAPACHELOG}\" }\n  }\n  date {\n    match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```\n\nRun Logstash with this configuration:\n\n```\nbin/logstash -f logstash-filter.conf\n```\n\nNow, paste the following line into your terminal and press Enter so it will be processed by the stdin input:\n\n```\n127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"\n```\n\nYou should see something returned to stdout that looks like this:\n\n```\n{\n        \"message\" => \"127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \\\"GET /xampp/status.php HTTP/1.1\\\" 200 3891 \\\"http://cadenza/xampp/navi.php\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\\\"\",\n     \"@timestamp\" => \"2013-12-11T08:01:45.000Z\",\n       \"@version\" => \"1\",\n           \"host\" => \"cadenza\",\n       \"clientip\" => \"127.0.0.1\",\n          \"ident\" => \"-\",\n           \"auth\" => \"-\",\n      \"timestamp\" => \"11/Dec/2013:00:01:45 -0800\",\n           \"verb\" => \"GET\",\n        \"request\" => \"/xampp/status.php\",\n    \"httpversion\" => \"1.1\",\n       \"response\" => \"200\",\n          \"bytes\" => \"3891\",\n       \"referrer\" => \"\\\"http://cadenza/xampp/navi.php\\\"\",\n          \"agent\" => \"\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\\\"\"\n}\n```\n\nAs you can see, Logstash (with help from the `grok` filter) was able to parse the log line (which happens to be in Apache \"combined log\" format) and break it up into many different discrete bits of information. This is extremely useful once you start querying and analyzing our log data. For example, you’ll be able to easily run reports on HTTP response codes, IP addresses, referrers, and so on. There are quite a few grok patterns included with Logstash out-of-the-box, so it’s quite likely if you need to parse a common log format, someone has already done the work for you. For more information, see the list of [Logstash grok patterns](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns) on GitHub.\n\nThe other filter used in this example is the `date` filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you’re ingesting the log data). You’ll notice that the `@timestamp` field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs. It gives you the ability to tell Logstash \"use this value as the timestamp for this event\".",
            "Processing Apache logs [_processing_apache_logs]": "Let’s do something that’s actually **useful**: process apache2 access log files! We are going to read the input from a file on the localhost, and use a [conditional](/reference/event-dependent-configuration.md#conditionals) to process the event according to our needs. First, create a file called something like *logstash-apache.conf* with the following contents (you can change the log’s file path to suit your needs):\n\n```\ninput {\n  file {\n    path => \"/tmp/access_log\"\n    start_position => \"beginning\"\n  }\n}\n\nfilter {\n  if [path] =~ \"access\" {\n    mutate { replace => { \"type\" => \"apache_access\" } }\n    grok {\n      match => { \"message\" => \"%{COMBINEDAPACHELOG}\" }\n    }\n  }\n  date {\n    match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"localhost:9200\"]\n  }\n  stdout { codec => rubydebug }\n}\n```\n\nThen, create the input file you configured above (in this example, \"/tmp/access_log\") with the following log entries (or use some from your own webserver):\n\n```\n71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] \"GET /admin HTTP/1.1\" 301 566 \"-\" \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3\"\n134.39.72.245 - - [18/May/2011:12:40:18 -0700] \"GET /favicon.ico HTTP/1.1\" 200 1189 \"-\" \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)\"\n98.83.179.51 - - [18/May/2011:19:35:08 -0700] \"GET /css/main.css HTTP/1.1\" 200 1837 \"http://www.safesand.com/information.htm\" \"Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\"\n```\n\nNow, run Logstash with the -f flag to pass in the configuration file:\n\n```\nbin/logstash -f logstash-apache.conf\n```\n\nNow you should see your apache log data in Elasticsearch! Logstash opened and read the specified input file, processing each event it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events, and stored in Elasticsearch. As an added bonus, they are stashed with the field \"type\" set to \"apache_access\" (this is done by the type ⇒ \"apache_access\" line in the input configuration).\n\nIn this configuration, Logstash is only watching the apache access_log, but it’s easy enough to watch both the access_log and the error_log (actually, any file matching `*log`), by changing one line in the above configuration:\n\n```\ninput {\n  file {\n    path => \"/tmp/*_log\"\n...\n```\n\nWhen you restart Logstash, it will process both the error and access logs. However, if you inspect your data (using elasticsearch-kopf, perhaps), you’ll see that the access_log is broken up into discrete fields, but the error_log isn’t. That’s because we used a `grok` filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn’t it be nice **if** we could control how a line was parsed, based on its format? Well, we can…​\n\nNote that Logstash did not reprocess the events that were already seen in the access_log file. When reading from a file, Logstash saves its position and only processes new lines as they are added. Neat!",
            "Using conditionals [using-conditionals]": "You use conditionals to control what events are processed by a filter or output. For example, you could label each event according to which file it appeared in (access_log, error_log, and other random files that end with \"log\").\n\n```\ninput {\n  file {\n    path => \"/tmp/*_log\"\n  }\n}\n\nfilter {\n  if [path] =~ \"access\" {\n    mutate { replace => { type => \"apache_access\" } }\n    grok {\n      match => { \"message\" => \"%{COMBINEDAPACHELOG}\" }\n    }\n    date {\n      match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n    }\n  } else if [path] =~ \"error\" {\n    mutate { replace => { type => \"apache_error\" } }\n  } else {\n    mutate { replace => { type => \"random_logs\" } }\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```\n\nThis example labels all events using the `type` field, but doesn’t actually parse the `error` or `random` files. There are so many types of error logs that how they should be labeled really depends on what logs you’re working with.\n\nSimilarly, you can use conditionals to direct events to particular outputs. For example, you could:\n\n['alert nagios of any apache events with status 5xx', 'record any 4xx status to Elasticsearch', 'record all status code hits via statsd']\n\nTo tell nagios about any http event that has a 5xx status code, you first need to check the value of the `type` field. If it’s apache, then you can check to see if the `status` field contains a 5xx error. If it is, send it to nagios. If it isn’t a 5xx error, check to see if the `status` field contains a 4xx error. If so, send it to Elasticsearch. Finally, send all apache status codes to statsd no matter what the `status` field contains:\n\n```\noutput {\n  if [type] == \"apache\" {\n    if [status] =~ /^5\\d\\d/ {\n      nagios { ...  }\n    } else if [status] =~ /^4\\d\\d/ {\n      elasticsearch { ... }\n    }\n    statsd { increment => \"apache.%{status}\" }\n  }\n}\n```",
            "Processing Syslog messages [_processing_syslog_messages]": "Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won’t need a functioning syslog instance; we’ll fake it from the command line so you can get a feel for what happens.\n\nFirst, let’s make a simple configuration file for Logstash + syslog, called *logstash-syslog.conf*.\n\n```\ninput {\n  tcp {\n    port => 5000\n    type => syslog\n  }\n  udp {\n    port => 5000\n    type => syslog\n  }\n}\n\nfilter {\n  if [type] == \"syslog\" {\n    grok {\n      match => { \"message\" => \"%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\\[%{POSINT:syslog_pid}\\])?: %{GREEDYDATA:syslog_message}\" }\n      add_field => [ \"received_at\", \"%{@timestamp}\" ]\n      add_field => [ \"received_from\", \"%{host}\" ]\n    }\n    date {\n      match => [ \"syslog_timestamp\", \"MMM  d HH:mm:ss\", \"MMM dd HH:mm:ss\" ]\n    }\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```\n\nRun Logstash with this new configuration:\n\n```\nbin/logstash -f logstash-syslog.conf\n```\n\nNormally, a client machine would connect to the Logstash instance on port 5000 and send its message. For this example, we’ll just telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). Open another shell window to interact with the Logstash syslog input and enter the following command:\n\n```\ntelnet localhost 5000\n```\n\nCopy and paste the following lines as samples. (Feel free to try some of your own, but keep in mind they might not parse if the `grok` filter is not correct for your data).\n\n```\nDec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]\nDec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied\nDec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)\nDec 22 18:28:06 louis rsyslogd: [origin software=\"rsyslogd\" swVersion=\"4.2.0\" x-pid=\"2253\" x-info=\"http://www.rsyslog.com\"] rsyslogd was HUPed, type 'lightweight'.\n```\n\nNow you should see the output of Logstash in your original shell as it processes and parses messages!\n\n```\n{\n                 \"message\" => \"Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)\",\n              \"@timestamp\" => \"2013-12-23T22:30:01.000Z\",\n                \"@version\" => \"1\",\n                    \"type\" => \"syslog\",\n                    \"host\" => \"0:0:0:0:0:0:0:1:52617\",\n        \"syslog_timestamp\" => \"Dec 23 14:30:01\",\n         \"syslog_hostname\" => \"louis\",\n          \"syslog_program\" => \"CRON\",\n              \"syslog_pid\" => \"619\",\n          \"syslog_message\" => \"(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)\",\n             \"received_at\" => \"2013-12-23 22:49:22 UTC\",\n           \"received_from\" => \"0:0:0:0:0:0:0:1:52617\",\n    \"syslog_severity_code\" => 5,\n    \"syslog_facility_code\" => 1,\n         \"syslog_facility\" => \"user-level\",\n         \"syslog_severity\" => \"notice\"\n}\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/config-examples.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 2]"
        },
        {
          "title": "Logstash Configuration Files [config-setting-files]",
          "description": null,
          "content": {
            "Pipeline Configuration Files [pipeline-config-files]": "You create pipeline configuration files when you define the stages of your Logstash processing pipeline. On deb and rpm, you place the pipeline configuration files in the `/etc/logstash/conf.d` directory. Logstash tries to load only files with `.conf` extension in the `/etc/logstash/conf.d directory` and ignores all other files.\n\nSee [*Creating a {{ls}} pipeline*](/reference/creating-logstash-pipeline.md) for more info.",
            "Settings Files [settings-files]": "The settings files are already defined in the Logstash installation. Logstash includes the following settings files:\n\n**`logstash.yml`**\n:   Contains Logstash configuration flags. You can set flags in this file instead of passing the flags at the command line. Any flags that you set at the command line override the corresponding settings in the `logstash.yml` file. See [logstash.yml](/reference/logstash-settings-file.md) for more info.\n\n**`pipelines.yml`**\n:   Contains the framework and instructions for running multiple pipelines in a single Logstash instance. See [Multiple Pipelines](/reference/multiple-pipelines.md) for more info.\n\n**`jvm.options`**\n:   Contains JVM configuration flags. Use this file to set initial and maximum values for total heap space. You can also use this file to set the locale for Logstash. Specify each flag on a separate line. All other settings in this file are considered expert settings.\n\n**`log4j2.properties`**\n:   Contains default settings for `log4j 2` library. See [Log4j2 configuration](/reference/logging.md#log4j2) for more info."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/config-setting-files.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 3]"
        },
        {
          "title": "Structure of a pipeline [configuration-file-structure]",
          "description": null,
          "content": {
            "Plugin configuration [plugin_configuration]": "A plugin configuration consists of the plugin name followed by a block of settings for that plugin. For example, this input section configures two file inputs:\n\n```\ninput {\n  http {\n    port => 3333\n    tags => gateway\n  }\n  http {\n    port => 4444\n    tags => billing\n  }\n}\n```\n\nIn this example, two settings are configured for each of the file inputs: *port* and *tags*.\n\nThe settings you can configure vary according to the plugin type. For information about each plugin, see [Input Plugins](logstash-docs-md://lsr/input-plugins.md), [Output Plugins](logstash-docs-md://lsr/output-plugins.md), [Filter Plugins](logstash-docs-md://lsr/filter-plugins.md), and [Codec Plugins](logstash-docs-md://lsr/codec-plugins.md).",
            "Value types [plugin-value-types]": "A plugin can require that the value for a setting be a certain type, such as boolean, list, or hash. The following value types are supported.",
            "Array [array]": {
              "Lists [list]": "Not a type in and of itself, but a property types can have. This makes it possible to type check multiple values. Plugin authors can enable list checking by specifying `:list => true` when declaring an argument.\n\nExample:\n\n```\n  path => [ \"/var/log/messages\", \"/var/log/*.log\" ]\n  uris => [ \"http://elastic.co\", \"http://example.net\" ]\n```\n\nThis example configures `path`, which is a `string` to be a list that contains an element for each of the three strings. It also will configure the `uris` parameter to be a list of URIs, failing if any of the URIs provided are not valid.",
              "Boolean [boolean]": "A boolean must be either `true` or `false`. Note that the `true` and `false` keywords are not enclosed in quotes.\n\nExample:\n\n```\n  ssl_enable => true\n```",
              "Bytes [bytes]": "A bytes field is a string field that represents a valid unit of bytes. It is a convenient way to declare specific sizes in your plugin options. Both SI (k M G T P E Z Y) and Binary (Ki Mi Gi Ti Pi Ei Zi Yi) units are supported. Binary units are in base-1024 and SI units are in base-1000. This field is case-insensitive and accepts space between the value and the unit. If no unit is specified, the integer string represents the number of bytes.\n\nExamples:\n\n```\n  my_bytes => \"1113\"   # 1113 bytes\n  my_bytes => \"10MiB\"  # 10485760 bytes\n  my_bytes => \"100kib\" # 102400 bytes\n  my_bytes => \"180 mb\" # 180000000 bytes\n```",
              "Codec [codec]": "A codec is the name of Logstash codec used to represent the data. Codecs can be used in both inputs and outputs.\n\nInput codecs provide a convenient way to decode your data before it enters the input. Output codecs provide a convenient way to encode your data before it leaves the output. Using an input or output codec eliminates the need for a separate filter in your Logstash pipeline.\n\nA list of available codecs can be found at the [Codec Plugins](logstash-docs-md://lsr/codec-plugins.md) page.\n\nExample:\n\n```\n  codec => \"json\"\n```",
              "Hash [hash]": "A hash is a collection of key value pairs specified in the format `\"field1\" => \"value1\"`. Note that multiple key value entries are separated by spaces rather than commas.\n\nExample:\n\n```\nmatch => {\n  \"field1\" => \"value1\"\n  \"field2\" => \"value2\"\n  ...\n}\n# or as a single line. No commas between entries:\nmatch => { \"field1\" => \"value1\" \"field2\" => \"value2\" }\n```",
              "Number [number]": "Numbers must be valid numeric values (floating point or integer).\n\nExample:\n\n```\n  port => 33\n```",
              "Password [password]": "A password is a string with a single value that is not logged or printed.\n\nExample:\n\n```\n  my_password => \"password\"\n```",
              "URI [uri]": "A URI can be anything from a full URL like *http://elastic.co/* to a simple identifier like *foobar*. If the URI contains a password such as *http://user:pass@example.net* the password portion of the URI will not be logged or printed.\n\nExample:\n\n```\n  my_uri => \"http://foo:bar@example.net\"\n```",
              "Path [path]": "A path is a string that represents a valid operating system path.\n\nExample:\n\n```\n  my_path => \"/tmp/logstash\"\n```",
              "String [string]": "A string must be a single character sequence. Note that string values are enclosed in quotes, either double or single.",
              "Escape sequences [_escape_sequences]": "By default, escape sequences are not enabled. If you wish to use escape sequences in quoted strings, you will need to set `config.support_escapes: true` in your `logstash.yml`. When `true`, quoted strings (double and single) will have this transformation:\n\n|     |     |\n| --- | --- |\n| Text | Result |\n| \\r | carriage return (ASCII 13) |\n| \\n | new line (ASCII 10) |\n| \\t | tab (ASCII 9) |\n| \\\\ | backslash (ASCII 92) |\n| \\\" | double quote (ASCII 34) |\n| \\' | single quote (ASCII 39) |\n\nExample:\n\n```\n  name => \"Hello world\"\n  name => 'It\\'s a beautiful day'\n```",
              "Field reference [field-reference]": "A Field Reference is a special [String](#string) value representing the path to a field in an event, such as `@timestamp` or `[@timestamp]` to reference a top-level field, or `[client][ip]` to access a nested field. The [*Field References Deep Dive*](https://www.elastic.co/guide/en/logstash/current/field-references-deepdive.html) provides detailed information about the structure of Field References. When provided as a configuration option, Field References need to be quoted and special characters must be escaped following the same rules as [String](#string)."
            },
            "Comments [comments]": "Comments are the same as in perl, ruby, and python. A comment starts with a *#* character, and does not need to be at the beginning of a line. For example:\n\n```\n# this is a comment\n\ninput { # comments can appear at the end of a line, too\n  # ...\n}\n```\n\n::::{note}\nComments containing environment variable `${var}` references in `config.string` are still evaluated. Remove the `$` sign to avoid pipeline loading failures.\n::::"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 4]"
        },
        {
          "title": "Configure Centralized Pipeline Management [configuring-centralized-pipelines]",
          "description": null,
          "content": {
            "Configuration Management Settings in Logstash [configuration-management-settings]": "You can set the following `xpack.management` settings in `logstash.yml` to enable [centralized pipeline management](/reference/logstash-centralized-pipeline-management.md). For more information about configuring Logstash, see [logstash.yml](/reference/logstash-settings-file.md).\n\nThe following example shows basic settings that assume {{es}} and {{kib}} are installed on the localhost with basic AUTH enabled, but no SSL. If you’re using SSL, you need to specify additional SSL settings.\n\n```\nxpack.management.enabled: true\nxpack.management.elasticsearch.hosts: \"http://localhost:9200/\"\nxpack.management.elasticsearch.username: logstash_admin_user\nxpack.management.elasticsearch.password: t0p.s3cr3t\nxpack.management.logstash.poll_interval: 5s\nxpack.management.pipeline.id: [\"apache\", \"cloudwatch_logs\"]\n```\n\n`xpack.management.enabled`\n:   Set to `true` to enable {{xpack}} centralized configuration management for Logstash.\n\n`xpack.management.logstash.poll_interval`\n:   How often the Logstash instance polls for pipeline changes from Elasticsearch. The default is 5s.\n\n`xpack.management.pipeline.id`\n:   Specify a comma-separated list of pipeline IDs to register for centralized pipeline management. After changing this setting, you need to restart Logstash to pick up changes. Pipeline IDs support `*` as a [wildcard](#wildcard-in-pipeline-id) for matching multiple IDs\n\n`xpack.management.elasticsearch.hosts`\n:   The {{es}} instance that will store the Logstash pipeline configurations and metadata. This might be the same {{es}} instance specified in the `outputs` section in your Logstash configuration, or a different one. Defaults to `http://localhost:9200`.\n\n`xpack.management.elasticsearch.username` and `xpack.management.elasticsearch.password`\n:   If your {{es}} cluster is protected with basic authentication, these settings provide the username and password that the Logstash instance uses to authenticate for accessing the configuration data. The username you specify here should have the built-in `logstash_admin` and `logstash_system` roles. These roles provide access to system indices for managing configurations.\n\n::::{note}\nStarting with Elasticsearch version 7.10.0, the `logstash_admin` role inherits the `manage_logstash_pipelines` cluster privilege for centralized pipeline management. If a user has created their own roles and granted them access to the .logstash index, those roles will continue to work in 7.x but will need to be updated for 8.0.\n::::\n\n`xpack.management.elasticsearch.proxy`\n:   Optional setting that allows you to specify a proxy URL if Logstash needs to use a proxy to reach your Elasticsearch cluster.\n\n`xpack.management.elasticsearch.ssl.ca_trusted_fingerprint`\n:   Optional setting that enables you to specify the hex-encoded SHA-256 fingerprint of the certificate authority for your {{es}} instance.\n\n::::{note}\nA self-secured Elasticsearch cluster will provide the fingerprint of its CA to the console during setup.\n\nYou can also get the SHA256 fingerprint of an Elasticsearch’s CA using the `openssl` command-line utility on the Elasticsearch host:\n\n```\nopenssl x509 -fingerprint -sha256 -in $ES_HOME/config/certs/http_ca.crt\n```\n\n::::\n\n`xpack.management.elasticsearch.ssl.certificate_authority`\n:   Optional setting that enables you to specify a path to the `.pem` file for the certificate authority for your {{es}} instance.\n\n`xpack.management.elasticsearch.ssl.truststore.path`\n:   Optional setting that provides the path to the Java keystore (JKS) to validate the server’s certificate.\n\n::::{note}\nYou cannot use this setting and `xpack.management.elasticsearch.ssl.certificate_authority` at the same time.\n::::\n\n`xpack.management.elasticsearch.ssl.truststore.password`\n:   Optional setting that provides the password to the truststore.\n\n`xpack.management.elasticsearch.ssl.keystore.path`\n:   Optional setting that provides the path to the Java keystore (JKS) to validate the client’s certificate.\n\n::::{note}\nYou cannot use this setting and `xpack.management.elasticsearch.ssl.keystore.certificate` at the same time.\n::::\n\n`xpack.management.elasticsearch.ssl.keystore.password`\n:   Optional setting that provides the password to the keystore.\n\n`xpack.management.elasticsearch.ssl.certificate`\n:   Optional setting that provides the path to an SSL certificate to use to authenticate the client. This certificate should be an OpenSSL-style X.509 certificate file.\n\n::::{note}\nThis setting can be used only if `xpack.management.elasticsearch.ssl.key` is set.\n::::\n\n`xpack.management.elasticsearch.ssl.key`\n:   Optional setting that provides the path to an OpenSSL-style RSA private key that corresponds to the `xpack.management.elasticsearch.ssl.certificate`.\n\n::::{note}\nThis setting can be used only if `xpack.management.elasticsearch.ssl.certificate` is set.\n::::\n\n`xpack.management.elasticsearch.ssl.verification_mode`\n:   Option to validate the server’s certificate. Defaults to `full`. To disable, set to `none`. Disabling this severely compromises security.\n\n`xpack.management.elasticsearch.ssl.cipher_suites`\n:   Optional setting that provides the list of cipher suites to use, listed by priorities. Supported cipher suites vary depending on the Java and protocol versions.\n\n`xpack.management.elasticsearch.cloud_id`\n:   If you’re using {{es}} in {{ecloud}}, you should specify the identifier here. This setting is an alternative to `xpack.management.elasticsearch.hosts`. If `cloud_id` is configured, `xpack.management.elasticsearch.hosts` should not be used. This {{es}} instance will store the Logstash pipeline configurations and metadata.\n\n`xpack.management.elasticsearch.cloud_auth`\n:   If you’re using {{es}} in {{ecloud}}, you can set your auth credentials here. This setting is an alternative to both `xpack.management.elasticsearch.username` and `xpack.management.elasticsearch.password`. If `cloud_auth` is configured, those settings should not be used. The credentials you specify here should be for a user with the `logstash_admin` and `logstash_system` roles, which provide access to system indices for managing configurations.\n\n`xpack.management.elasticsearch.api_key`\n:   Authenticate using an Elasticsearch API key. Note that this option also requires using SSL. The API key Format is `id:api_key` where `id` and `api_key` are as returned by the Elasticsearch [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key).",
            "Wildcard support in pipeline ID [wildcard-in-pipeline-id]": "Pipeline IDs must begin with a letter or underscore and contain only letters, underscores, dashes, and numbers. You can use `*` in `xpack.management.pipeline.id` to match any number of letters, underscores, dashes, and numbers.\n\n```\nxpack.management.pipeline.id: [\"*logs\", \"*apache*\", \"tomcat_log\"]\n```\n\nIn this example, `\"*logs\"` matches all IDs ending in `logs`. `\"*apache*\"` matches any IDs with `apache` in the name.\n\nWildcard in pipeline IDs is available starting with Elasticsearch 7.10. Logstash can pick up new pipeline without a restart if the new pipeline ID matches the wildcard pattern."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/configuring-centralized-pipelines.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 5]"
        },
        {
          "title": "Configure GeoIP Database Management [configuring-geoip-database-management]",
          "description": null,
          "content": {
            "GeoIP database Management settings in {{ls}} [geoip-database-management-settings]": "You can set the following `xpack.geoip` settings in `logstash.yml` to configure the [geoip database manager](/reference/logstash-geoip-database-management.md). For more information about configuring Logstash, see [logstash.yml](/reference/logstash-settings-file.md).\n\n`xpack.geoip.downloader.enabled`\n:   (Boolean) If `true`, Logstash automatically downloads and manages updates for GeoIP2 databases from the `xpack.geoip.downloader.endpoint`. If `false`, Logstash does not manage GeoIP2 databases and plugins that need a GeoIP2 database must be configured to provide their own.\n\n`xpack.geoip.downloader.endpoint`\n:   (String) Endpoint URL used to download updates for GeoIP2 databases. For example, `https://example.com/overview.json`. Defaults to `https://geoip.elastic.co/v1/database`. Note that Logstash will periodically make a GET request to `${xpack.geoip.downloader.endpoint}?elastic_geoip_service_tos=agree`, expecting the list of metadata about databases typically found in `overview.json`.\n\n`xpack.geoip.downloader.poll.interval`\n:   (Time Value) How often Logstash checks for GeoIP2 database updates at the `xpack.geoip.downloader.endpoint`. For example, `6h` to check every six hours. Defaults to `24h` (24 hours).",
            "Offline and air-gapped environments [configuring-geoip-database-management-offline]": {
              "Using an HTTP proxy [_using_an_http_proxy]": "If you can’t connect directly to the Elastic GeoIP endpoint, consider setting up an HTTP proxy server. You can then specify the proxy with `http_proxy` environment variable.\n\n```\nexport http_proxy=\"http://PROXY_IP:PROXY_PORT\"\n```",
              "Using a custom endpoint [_using_a_custom_endpoint]": "If you work in an air-gapped environment and can’t update your databases from the Elastic endpoint, You can then download databases from MaxMind and bootstrap the service.\n\n['Download both `GeoLite2-ASN.mmdb` and `GeoLite2-City.mmdb` database files from the [MaxMind site](http://dev.maxmind.com/geoip/geoip2/geolite2).', 'Copy both database files to a single directory.', '[Download {{es}}](https://www.elastic.co/downloads/elasticsearch).', 'From your {{es}} directory, run:', '```\\n./bin/elasticsearch-geoip -s my/database/dir\\n```', 'Serve the static database files from your directory. For example, you can use Docker to serve the files from nginx server:', '```\\ndocker run -p 8080:80 -v my/database/dir:/usr/share/nginx/html:ro nginx\\n```', 'Specify the service’s endpoint URL in Logstash using the `xpack.geoip.download.endpoint=http://localhost:8080/overview.json` setting in `logstash.yml`.']\n\nLogstash gets automatic updates from this service."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/configuring-geoip-database-management.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 6]"
        },
        {
          "title": "Sending data to {{ech}} [connecting-to-cloud]",
          "description": null,
          "content": {
            "Cloud ID [cloud-id]": "{{ls}} uses the Cloud ID, found in the Elastic Cloud web console, to build the Elasticsearch and Kibana hosts settings. It is a base64 encoded text value of about 120 characters made up of upper and lower case letters and numbers. If you have several Cloud IDs, you can add a label, which is ignored internally, to help you tell them apart. To add a label, prefix your Cloud ID with a label and a `:` separator in this format \"<label>:<cloud-id>\".",
            "Cloud Auth [cloud-auth]": "Cloud Auth is optional. Construct this value by following this format \"<username>:<password>\". Use your Cloud username for the first part. Use your Cloud password for the second part, which is given once in the Cloud UI when you create a cluster. If you change your Cloud password in the Cloud UI, remember to change it here, too.",
            "Using Cloud ID and Cloud Auth with plugins [cloud-id-plugins]": "The Elasticsearch input, output, and filter plugins support cloud_id and cloud_auth in their configurations.\n\n['[Elasticsearch input plugin](logstash-docs-md://lsr/plugins-inputs-elasticsearch.md#plugins-inputs-elasticsearch-cloud_id)', '[Elasticsearch filter plugin](logstash-docs-md://lsr/plugins-filters-elasticsearch.md#plugins-filters-elasticsearch-cloud_id)', '[Elasticsearch output plugin](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-cloud_id)']",
            "Sending {{ls}} management data to {{ech}} [cloud-id-mgmt]": "These settings in the `logstash.yml` config file can help you get set up to send management data to Elastic Cloud:\n\n['`xpack.management.elasticsearch.cloud_id`', '`xpack.management.elasticsearch.cloud_auth`']\n\nYou can use the `xpack.management.elasticsearch.cloud_id` setting as an alternative to `xpack.management.elasticsearch.hosts`.\n\nYou can use the `xpack.management.elasticsearch.cloud_auth` setting as an alternative to both `xpack.management.elasticsearch.username` and `xpack.management.elasticsearch.password`. The credentials you specify here should be for a user with the logstash_admin role, which provides access to .logstash-* indices for managing configurations."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/connecting-to-cloud.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 7]"
        },
        {
          "title": "Core Operations",
          "description": null,
          "content": {
            "Performing Core Operations [core-operations]": "The plugins described in this section are useful for core operations, such as mutating and dropping events.\n\n[date filter](logstash-docs-md://lsr/plugins-filters-date.md)\n:   Parses dates from fields to use as Logstash timestamps for events.\n\nThe following config parses a field called `logdate` to set the Logstash timestamp:\n\n```json\nfilter {\n  date {\n    match => [ \"logdate\", \"MMM dd yyyy HH:mm:ss\" ]\n  }\n}\n```\n\n\n\n[drop filter](logstash-docs-md://lsr/plugins-filters-drop.md)\n:   Drops events. This filter is typically used in combination with conditionals.\n\nThe following config drops `debug` level log messages:\n\n```json\nfilter {\n  if [loglevel] == \"debug\" {\n    drop { }\n  }\n}\n```\n\n\n\n[fingerprint filter](logstash-docs-md://lsr/plugins-filters-fingerprint.md)\n:   Fingerprints fields by applying a consistent hash.\n\nThe following config fingerprints the `IP`, `@timestamp`, and `message` fields and adds the hash to a metadata field called `generated_id`:\n\n```json\nfilter {\n  fingerprint {\n    source => [\"IP\", \"@timestamp\", \"message\"]\n    method => \"SHA1\"\n    key => \"0123\"\n    target => \"[@metadata][generated_id]\"\n  }\n}\n```\n\n\n\n[mutate filter](logstash-docs-md://lsr/plugins-filters-mutate.md)\n:   Performs general mutations on fields. You can rename, remove, replace, and modify fields in your events.\n\nThe following config renames the `HOSTORIP` field to `client_ip`:\n\n```json\nfilter {\n  mutate {\n    rename => { \"HOSTORIP\" => \"client_ip\" }\n  }\n}\n```\n\nThe following config strips leading and trailing whitespace from the specified fields:\n\n```json\nfilter {\n  mutate {\n    strip => [\"field1\", \"field2\"]\n  }\n}\n```\n\n\n\n[ruby filter](logstash-docs-md://lsr/plugins-filters-ruby.md)\n:   Executes Ruby code.\n\nThe following config executes Ruby code that cancels 90% of the events:\n\n```json\nfilter {\n  ruby {\n    code => \"event.cancel if rand <= 0.90\"\n  }\n}\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/core-operations.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 8]"
        },
        {
          "title": "Creating Logstash Pipeline",
          "description": null,
          "content": {
            "Creating a Logstash Pipeline [configuration]": "You can create a pipeline to process data by using several plugins together, like [inputs](logstash-docs-md://lsr/input-plugins.md), [outputs](logstash-docs-md://lsr/output-plugins.md), [filters](logstash-docs-md://lsr/filter-plugins.md), and [codecs](logstash-docs-md://lsr/codec-plugins.md). To build a Logstash pipeline, create a configuration file to specify which plugins you want to use and the settings for each plugin.\n\nThe minimum components of a pipeline are one input and one output. Most pipelines include at least one filter plugin because that’s where the processing part of the extract, transform, load (ETL) happens. You can reference event fields in a pipeline and use conditionals to process events when they meet certain criteria.\n\nLet’s step through creating a simple pipeline config on your local machine and then using it to run Logstash. Create a file named \"logstash-simple.conf\" and save it in the same directory as Logstash. For example:\n\n```\ninput { stdin { } }\noutput {\n  elasticsearch { cloud_id => \"<cloud id>\" api_key => \"<api key>\" }\n  stdout { codec => rubydebug }\n}\n```\n\nThen, run {{ls}} and specify the configuration file with the `-f` flag.\n\n```\nbin/logstash -f logstash-simple.conf\n```\n\nLogstash now reads the specified configuration file and outputs to both Elasticsearch and stdout. Before you move on to [more complex examples](/reference/config-examples.md), take a look at what’s in a pipeline config file."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/configuration.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 9]"
        },
        {
          "title": "Collect {{ls}} monitoring data for dashboards [dashboard-monitoring-with-elastic-agent]",
          "description": null,
          "content": {
            "Install and configure {{agent}} [install-and-configure-db]": {
              "Add the {{agent}} {{ls}} integration to monitor host logs and metrics [add-logstash-integration-ead]": [
                "Go to the {{kib}} home page, and click **Add integrations**.",
                "% TO DO: Use `:class: screenshot`\n![{{kib}} home page](images/kibana-home.png)",
                "In the query bar, search for **{{ls}}** and select the integration to see more details.",
                "Click **Add {{ls}}**.",
                "Configure the integration name and add a description (optional).",
                "Configure the integration to collect logs.",
                [
                  "Make sure that **Logs** is turned on if you want to collect logs from your {{ls}} instance. Be sure that the required settings are correctly configured.",
                  "Under **Logs**, modify the log paths to match your {{ls}} environment."
                ],
                "Configure the integration to collect metrics.",
                "::::{tip}\nFor the best experience with the {{ls}} dashboards, we recommend collecting all of the metrics. Turning off metrics will result in incomplete or missing visualizations.\n::::",
                [
                  "Make sure that **Metrics (Elastic Agent)** is turned on (default), and **Metrics (Stack Monitoring)** is turned off.",
                  "Under **Metrics (Elastic Agent)**, make sure the {{ls}} URL setting points to your {{ls}} instance URLs.<br> By default, the integration collects {{ls}} monitoring metrics from `https://localhost:9600`. If that host and port number are not correct, update the `Logstash URL` setting. If you configured {{ls}} to use encrypted communications and/or a username and password, you must access it via HTTPS, and expand the **Advanced Settings** options, and fill in with the appropriate values for your {{ls}} instance."
                ],
                "Click **Save and continue**.<br> This step takes a minute or two to complete. When it’s done, you’ll have an agent policy that contains a system integration policy for the configuration you just specified.",
                "In the popup, click **Add {{agent}} to your hosts** to open the **Add agent** flyout.",
                "::::{tip}\nIf you accidentally close the popup, go to **{{fleet}} > Agents** and click **Add agent**.\n::::"
              ]
            },
            "Install and run an {{agent}} on your machine [add-agent-to-fleet-ead]": "The **Add agent** flyout has two options: **Enroll in {{fleet}}** and **Run standalone**. Enrolling agents in {{fleet}} (default) provides a centralized management tool in {{kib}}, reducing management overhead.\n\n:::::::{tab-set}\n\n::::::{tab-item} Fleet-managed\n\n['When the **Add Agent flyout** appears, stay on the **Enroll in fleet** tab.', 'Skip the **Select enrollment token** step. The enrollment token you need is already selected.', '::::{note}\\nThe enrollment token is specific to the {{agent}} policy that you just created. When you run the command to enroll the agent in {{fleet}}, you will pass in the enrollment token.\\n::::', 'Download, install, and enroll the {{agent}} on your host by selecting your host operating system and following the **Install {{agent}} on your host** step.']\n\nIt takes about a minute for {{agent}} to enroll in {{fleet}}, download the configuration specified in the policy you just created, and start collecting data.\n::::::\n\n::::::{tab-item} Run standalone\n\n['When the **Add Agent flyout** appears, navigate to the **Run standalone** tab.', 'Configure the agent. Follow the instructions in **Install Elastic Agent on your host**.', 'After unpacking the binary, replace the `elastic-agent.yml` file with that supplied in the Add Agent flyout on the \"Run standalone\" tab, replacing the values of `ES_USERNAME` and `ES_PASSWORD` appropriately.', 'Run `sudo ./elastic-agent install`\\n::::::']\n\n:::::::",
            "View assets [view-assets-ead]": "After you have confirmed enrollment and data is coming in,  click **View assets** to access dashboards related to the {{ls}} integration.\n\nFor traditional Stack Monitoring UI, the dashboards marked **[Logs {{ls}}]** are used to visualize the logs produced by your {{ls}} instances, with those marked **[Metrics {{ls}}]** for metrics dashboards. These are populated with data only if you selected the **Metrics (Elastic Agent)** checkbox.\n\n% TO DO: Use `:class: screenshot`\n![Integration assets](images/integration-assets-dashboards.png)\n\nA number of dashboards are included to view {{ls}} as a whole, and dashboards that allow you to drill-down into how {{ls}} is performing on a node, pipeline and plugin basis.",
            "Monitor {{ls}} logs and metrics [view-data-dashboard]": "From the list of assets, open the **[Metrics {{ls}}] {{ls}} overview** dashboard to view overall performance. Then follow the navigation panel to further drill down into {{ls}} performance.\n\n% TO DO: Use `:class: screenshot`\n![The {{ls}} Overview dashboard in {{kib}} with various metrics from your monitored {{ls}}](images/integration-dashboard-overview.png)\n\nYou can hover over any visualization to adjust its settings, or click the **Edit** button to make changes to the dashboard. To learn more, refer to [Dashboard and visualizations](docs-content://explore-analyze/dashboards.md)."
          },
          "metadata": {
            "navigation_title": "Collect monitoring data for dashboards",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/dashboard-monitoring-with-elastic-agent.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 10]"
        },
        {
          "title": "Data Deserialization",
          "description": null,
          "content": {
            "Deserializing Data [data-deserialization]": "The plugins described in this section are useful for deserializing data into Logstash events.\n\n[avro codec](logstash-docs-md://lsr/plugins-codecs-avro.md)\n:   Reads serialized Avro records as Logstash events. This plugin deserializes individual Avro records. It is not for reading Avro files. Avro files have a unique format that must be handled upon input.\n\nThe following config deserializes input from Kafka:\n\n```json\ninput {\n  kafka {\n    codec => {\n      avro => {\n        schema_uri => \"/tmp/schema.avsc\"\n      }\n    }\n  }\n}\n...\n```\n\n\n\n[csv filter](logstash-docs-md://lsr/plugins-filters-csv.md)\n:   Parses comma-separated value data into individual fields. By default, the filter autogenerates field names (column1, column2, and so on), or you can specify a list of names. You can also change the column separator.\n\nThe following config parses CSV data into the field names specified in the `columns` field:\n\n```json\nfilter {\n  csv {\n    separator => \",\"\n    columns => [ \"Transaction Number\", \"Date\", \"Description\", \"Amount Debit\", \"Amount Credit\", \"Balance\" ]\n  }\n}\n```\n\n\n\n[fluent codec](logstash-docs-md://lsr/plugins-codecs-fluent.md)\n:   Reads the Fluentd `msgpack` schema.\n\nThe following config decodes logs received from `fluent-logger-ruby`:\n\n```json\ninput {\n  tcp {\n    codec => fluent\n    port => 4000\n  }\n}\n```\n\n\n\n[json codec](logstash-docs-md://lsr/plugins-codecs-json.md)\n:   Decodes (via inputs) and encodes (via outputs) JSON formatted content, creating one event per element in a JSON array.\n\nThe following config decodes the JSON formatted content in a file:\n\n```json\ninput {\n  file {\n    path => \"/path/to/myfile.json\"\n    codec =>\"json\"\n}\n```\n\n\n\n[protobuf codec](logstash-docs-md://lsr/plugins-codecs-protobuf.md)\n:   Reads protobuf encoded messages and converts them to Logstash events. Requires the protobuf definitions to be compiled as Ruby files. You can compile them by using the [ruby-protoc compiler](https://github.com/codekitchen/ruby-protocol-buffers).\n\nThe following config decodes events from a Kafka stream:\n\n```json\ninput\n  kafka {\n    zk_connect => \"127.0.0.1\"\n    topic_id => \"your_topic_goes_here\"\n    codec => protobuf {\n      class_name => \"Animal::Unicorn\"\n      include_path => ['/path/to/protobuf/definitions/UnicornProtobuf.pb.rb']\n    }\n  }\n}\n```\n\n\n\n[xml filter](logstash-docs-md://lsr/plugins-filters-xml.md)\n:   Parses XML into fields.\n\nThe following config parses the whole XML document stored in the `message` field:\n\n```json\nfilter {\n  xml {\n    source => \"message\"\n  }\n}\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/data-deserialization.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 11]"
        },
        {
          "title": "Dead letter queues (DLQ) [dead-letter-queues]",
          "description": null,
          "content": {
            "How the dead letter queue works [dead-letter-how]": "By default, when Logstash encounters an event that it cannot process because the data contains a mapping error or some other issue, the Logstash pipeline either hangs or drops the unsuccessful event. In order to protect against data loss in this situation, you can [configure Logstash](#configuring-dlq) to write unsuccessful events to a dead letter queue instead of dropping them.\n\n::::{note}\nThe dead letter queue is currently supported only for the [{{es}} output](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) and [conditional statements evaluation](/reference/event-dependent-configuration.md#conditionals). The dead letter queue is used for documents with response codes of 400 or 404, both of which indicate an event that cannot be retried. It’s also used when a conditional evaluation encounter an error.\n::::\n\nEach event written to the dead letter queue includes the original event, metadata that describes the reason the event could not be processed, information about the plugin that wrote the event, and the timestamp when the event entered the dead letter queue.\n\nTo process events in the dead letter queue, create a Logstash pipeline configuration that uses the [`dead_letter_queue` input plugin](logstash-docs-md://lsr/plugins-inputs-dead_letter_queue.md) to read from the queue. See [Processing events in the dead letter queue](#processing-dlq-events) for more information.\n\n![Diagram showing pipeline reading from the dead letter queue](images/dead_letter_queue.png)",
            "{{es}} processing and the dead letter queue [es-proc-dlq]": "**HTTP request failure.** If the HTTP request fails (because {{es}} is unreachable or because it returned an HTTP error code), the {{es}} output retries the entire request indefinitely. In these scenarios, the dead letter queue has no opportunity to intercept.\n\n**HTTP request success.** The [{{es}} Bulk API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-bulk) can perform multiple actions using the same request. If the Bulk API request is successful, it returns `200 OK`, even if some documents in the batch have [failed](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-bulk#bulk-failures-ex). In this situation, the `errors` flag for the request will be `true`.\n\nThe response body can include metadata indicating that one or more specific actions in the bulk request could not be performed, along with an HTTP-style status code per entry to indicate why the action could not be performed. If the DLQ is configured, individual indexing failures are routed there.\n\nEven if you regularly process events, events remain in the dead letter queue. The dead letter queue requires [manual intervention](#dlq-clear) to clear it.",
            "Conditional statements and the dead letter queue [conditionals-dlq]": "When a conditional statement reaches an error in processing an event, such as comparing string and integer values, the event, as it is at the time of evaluation, is inserted into the dead letter queue.",
            "Configuring {{ls}} to use dead letter queues [configuring-dlq]": {
              "File rotation [file-rotation]": "Dead letter queues have a built-in file rotation policy that manages the file size of the queue. When the file size reaches a preconfigured threshold, a new file is created automatically.",
              "Size management [size-management]": {
                "Age policy [age-policy]": "You can use the age policy to automatically control the volume of events in the dead letter queue. Use the `dead_letter_queue.retain.age` setting (in `logstash.yml` or `pipelines.yml`) to have {{ls}} remove events that are older than a value you define. Available time units are `d`, `h`, `m`, `s` respectively for days, hours, minutes and seconds. There is no default time unit, so you need to specify it.\n\n```\ndead_letter_queue.retain.age: 2d\n```\n\nThe age policy is verified and applied on event writes and during pipeline shutdown. For that reason, your dead-letter-queue folder may store expired events for longer than specified, and the reader pipeline could possibly encounter outdated events."
              },
              "Automatic cleaning of consumed events [auto-clean]": "By default, the dead letter queue input plugin does not remove the events that it consumes. Instead, it commits a reference to avoid re-processing events. Use the `clean_consumed` setting in the dead letter queue input plugin in order to remove segments that have been fully consumed, freeing space while processing.\n\n```\ninput {\n  dead_letter_queue {\n    path => \"/path/to/data/dead_letter_queue\"\n    pipeline_id => \"main\"\n    clean_consumed => true\n  }\n}\n```"
            },
            "Processing events in the dead letter queue [processing-dlq-events]": "When you are ready to process events in the dead letter queue, you create a pipeline that uses the [`dead_letter_queue` input plugin](logstash-docs-md://lsr/plugins-inputs-dead_letter_queue.md) to read from the dead letter queue. The pipeline configuration that you use depends, of course, on what you need to do. For example, if the dead letter queue contains events that resulted from a mapping error in Elasticsearch, you can create a pipeline that reads the \"dead\" events, removes the field that caused the mapping issue, and re-indexes the clean events into Elasticsearch.\n\nThe following example shows a simple pipeline that reads events from the dead letter queue and writes the events, including metadata, to standard output:\n\n```\ninput {\n  dead_letter_queue {\n    path => \"/path/to/data/dead_letter_queue\" <1>\n    commit_offsets => true <2>\n    pipeline_id => \"main\" <3>\n  }\n}\n\noutput {\n  stdout {\n    codec => rubydebug { metadata => true }\n  }\n}\n```\n\n['The path to the top-level directory containing the dead letter queue. This directory contains a separate folder for each pipeline that writes to the dead letter queue. To find the path to this directory, look at the `logstash.yml` [settings file](/reference/logstash-settings-file.md). By default, Logstash creates the `dead_letter_queue` directory under the location used for persistent storage (`path.data`), for example, `LOGSTASH_HOME/data/dead_letter_queue`. However, if `path.dead_letter_queue` is set, it uses that location instead.', 'When `true`, saves the offset. When the pipeline restarts, it will continue reading from the position where it left off rather than reprocessing all the items in the queue. You can set `commit_offsets` to `false` when you are exploring events in the dead letter queue and want to iterate over the events multiple times.', 'The ID of the pipeline that’s writing to the dead letter queue. The default is `\"main\"`.']\n\nFor another example, see [Example: Processing data that has mapping errors](#dlq-example).\n\nWhen the pipeline has finished processing all the events in the dead letter queue, it will continue to run and process new events as they stream into the queue. This means that you do not need to stop your production system to handle events in the dead letter queue.\n\n::::{note}\nEvents emitted from the [`dead_letter_queue` input plugin](logstash-docs-md://lsr/plugins-inputs-dead_letter_queue.md) plugin will not be resubmitted to the dead letter queue if they cannot be processed correctly.\n::::",
            "Reading from a timestamp [dlq-timestamp]": "When you read from the dead letter queue, you might not want to process all the events in the queue, especially if there are a lot of old events in the queue. You can start processing events at a specific point in the queue by using the `start_timestamp` option. This option configures the pipeline to start processing events based on the timestamp of when they entered the queue:\n\n```\ninput {\n  dead_letter_queue {\n    path => \"/path/to/data/dead_letter_queue\"\n    start_timestamp => \"2017-06-06T23:40:37\"\n    pipeline_id => \"main\"\n  }\n}\n```\n\nFor this example, the pipeline starts reading all events that were delivered to the dead letter queue on or after June 6, 2017, at 23:40:37.",
            "Example: Processing data that has mapping errors [dlq-example]": "In this example, the user attempts to index a document that includes geo_ip data, but the data cannot be processed because it contains a mapping error:\n\n```\n{\"geoip\":{\"location\":\"home\"}}\n```\n\nIndexing fails because the Logstash output plugin expects a `geo_point` object in the `location` field, but the value is a string. The failed event is written to the dead letter queue, along with metadata about the error that caused the failure:\n\n```\n{\n   \"@metadata\" => {\n    \"dead_letter_queue\" => {\n       \"entry_time\" => #<Java::OrgLogstash::Timestamp:0x5b5dacd5>,\n        \"plugin_id\" => \"fb80f1925088497215b8d037e622dec5819b503e-4\",\n      \"plugin_type\" => \"elasticsearch\",\n           \"reason\" => \"Could not index event to Elasticsearch. status: 400, action: [\\\"index\\\", {:_id=>nil, :_index=>\\\"logstash-2017.06.22\\\", :_type=>\\\"doc\\\", :_routing=>nil}, 2017-06-22T01:29:29.804Z My-MacBook-Pro-2.local {\\\"geoip\\\":{\\\"location\\\":\\\"home\\\"}}], response: {\\\"index\\\"=>{\\\"_index\\\"=>\\\"logstash-2017.06.22\\\", \\\"_type\\\"=>\\\"doc\\\", \\\"_id\\\"=>\\\"AVzNayPze1iR9yDdI2MD\\\", \\\"status\\\"=>400, \\\"error\\\"=>{\\\"type\\\"=>\\\"mapper_parsing_exception\\\", \\\"reason\\\"=>\\\"failed to parse\\\", \\\"caused_by\\\"=>{\\\"type\\\"=>\\\"illegal_argument_exception\\\", \\\"reason\\\"=>\\\"illegal latitude value [266.30859375] for geoip.location\\\"}}}}\"\n    }\n  },\n  \"@timestamp\" => 2017-06-22T01:29:29.804Z,\n    \"@version\" => \"1\",\n       \"geoip\" => {\n    \"location\" => \"home\"\n  },\n        \"host\" => \"My-MacBook-Pro-2.local\",\n     \"message\" => \"{\\\"geoip\\\":{\\\"location\\\":\\\"home\\\"}}\"\n}\n```\n\nTo process the failed event, you create the following pipeline that reads from the dead letter queue and removes the mapping problem:\n\n```\ninput {\n  dead_letter_queue {\n    path => \"/path/to/data/dead_letter_queue/\" <1>\n  }\n}\nfilter {\n  mutate {\n    remove_field => \"[geoip][location]\" <2>\n  }\n}\noutput {\n  elasticsearch{\n    hosts => [ \"localhost:9200\" ] <3>\n  }\n}\n```\n\n['The [`dead_letter_queue` input](logstash-docs-md://lsr/plugins-inputs-dead_letter_queue.md) reads from the dead letter queue.', 'The `mutate` filter removes the problem field called `location`.', 'The clean event is sent to Elasticsearch, where it can be indexed because the mapping issue is resolved.']",
            "Track dead letter queue size [dlq-size]": "Monitor the size of the dead letter queue before it becomes a problem. By checking it periodically, you can determine the maximum queue size that makes sense for each pipeline.\n\nThe size of the DLQ for each pipeline is available in the node stats API.\n\n```\npipelines.${pipeline_id}.dead_letter_queue.queue_size_in_bytes.\n```\n\nWhere `{{pipeline_id}}` is the name of a pipeline with DLQ enabled.",
            "Clear the dead letter queue [dlq-clear]": "The dead letter queue cannot be cleared with the upstream pipeline running.\n\nThe dead letter queue is a directory of pages. To clear it, stop the pipeline and delete location/<file-name>.\n\n```\n${path.data}/dead_letter_queue/${pipeline_id}\n```\n\nWhere `{{pipeline_id}}` is the name of a pipeline with DLQ enabled.\n\nThe pipeline creates a new dead letter queue when it starts again."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/dead-letter-queues.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 12]"
        },
        {
          "title": "Deploying and scaling Logstash [deploying-and-scaling]",
          "description": null,
          "content": {
            "Getting Started [deploying-getting-started]": {
              "Introducing Logstash [_introducing_logstash]": "What are the main benefits for integrating Logstash into your architecture?\n\n['Scale through ingestion spikes - Logstash has an adaptive disk-based buffering system that will absorb incoming throughput, therefore mitigating backpressure', 'Ingest from other data sources like databases, S3, or messaging queues', 'Emit data to multiple destinations like S3, HDFS, or write to a file', 'Compose more sophisticated processing pipelines with conditional dataflow logic']"
            },
            "Scaling Ingest [scaling-ingest]": {
              "Beats and Logstash [_beats_and_logstash]": "Beats run across thousands of edge host servers, collecting, tailing, and shipping logs to Logstash. Logstash serves as the centralized streaming engine for data unification and enrichment. The [Beats input plugin](logstash-docs-md://lsr/plugins-inputs-beats.md) exposes a secure, acknowledgement-based endpoint for Beats to send data to Logstash.\n\n![deploy2](images/deploy2.png)\n\n::::{note}\nEnabling persistent queues is strongly recommended, and these architecture characteristics assume that they are enabled. We encourage you to review the [Persistent queues (PQ)](/reference/persistent-queues.md) documentation for feature benefits and more details on resiliency.\n::::",
              "Scalability [_scalability]": "Logstash is horizontally scalable and can form groups of nodes running the same pipeline. Logstash’s adaptive buffering capabilities will facilitate smooth streaming even through variable throughput loads. If the Logstash layer becomes an ingestion bottleneck, simply add more nodes to scale out. Here are a few general recommendations:\n\n['Beats should [load balance](beats://reference/filebeat/elasticsearch-output.md#_loadbalance) across a group of Logstash nodes.', 'A minimum of two Logstash nodes are recommended for high availability.', 'It’s common to deploy just one Beats input per Logstash node, but multiple Beats inputs can also be deployed per Logstash node to expose independent endpoints for different data sources.']",
              "Resiliency [_resiliency]": "When using [Filebeat](https://www.elastic.co/products/beats/filebeat) or [Winlogbeat](https://www.elastic.co/products/beats/winlogbeat) for log collection within this ingest flow, **at-least-once delivery** is guaranteed. Both the communication protocols, from Filebeat or Winlogbeat to Logstash, and from Logstash to Elasticsearch, are synchronous and support acknowledgements. The other Beats don’t yet have support for acknowledgements.\n\nLogstash persistent queues provide protection across node failures. For disk-level resiliency in Logstash, it’s important to ensure disk redundancy. For on-premise deployments, it’s recommended that you configure RAID. When running in the cloud or a containerized environment, it’s recommended that you use persistent disks with replication strategies that reflect your data SLAs.\n\n::::{note}\nMake sure `queue.checkpoint.writes: 1` is set for at-least-once guarantees. For more details, see the [persistent queue durability](/reference/persistent-queues.md#durability-persistent-queues) documentation.\n::::",
              "Processing [_processing]": "Logstash will commonly extract fields with [grok](logstash-docs-md://lsr/plugins-filters-grok.md) or [dissect](logstash-docs-md://lsr/plugins-filters-dissect.md), augment [geographical](logstash-docs-md://lsr/plugins-filters-geoip.md) info, and can further enrich events with [file](logstash-docs-md://lsr/plugins-filters-translate.md), [database](logstash-docs-md://lsr/plugins-filters-jdbc_streaming.md), or [Elasticsearch](logstash-docs-md://lsr/plugins-filters-elasticsearch.md) lookup datasets. Be aware that processing complexity can affect overall throughput and CPU utilization. Make sure to check out the other [available filter plugins](logstash-docs-md://lsr/filter-plugins.md).",
              "Secure Transport [_secure_transport]": "Enterprise-grade security is available across the entire delivery chain.\n\n['Wire encryption is recommended for both the transport from [Beats to Logstash](beats://reference/filebeat/configuring-ssl-logstash.md) and from [Logstash to Elasticsearch](/reference/secure-connection.md).', 'There’s a wealth of security options when communicating with Elasticsearch including basic authentication, TLS, PKI, LDAP, AD, and other custom realms. To enable Elasticsearch security, see [Secure a cluster](docs-content://deploy-manage/security.md).']",
              "Monitoring [_monitoring]": "When running Logstash 5.2 or greater, the [Monitoring UI](https://www.elastic.co/products/x-pack/monitoring) provides deep visibility into your deployment metrics, helping observe performance and alleviate bottlenecks as you scale. Monitoring is an X-Pack feature under the Basic License and is therefore **free to use**. To get started, see [Monitoring Logstash](https://www.elastic.co/docs/api/doc/logstash/).\n\nIf external monitoring is preferred, there are [monitoring APIs](monitoring-logstash.md) that return point-in-time metrics snapshots."
            },
            "Adding Other Popular Sources [adding-other-sources]": {
              "TCP, UDP, and HTTP Protocols [_tcp_udp_and_http_protocols]": "The TCP, UDP, and HTTP protocols are common ways to feed data into Logstash. Logstash can expose endpoint listeners with the respective [TCP](logstash-docs-md://lsr/plugins-inputs-tcp.md), [UDP](logstash-docs-md://lsr/plugins-inputs-udp.md), and [HTTP](logstash-docs-md://lsr/plugins-inputs-http.md) input plugins. The data sources enumerated below are typically ingested through one of these three protocols.\n\n::::{note}\nThe TCP and UDP protocols do not support application-level acknowledgements, so connectivity issues may result in data loss.\n::::\n\nFor high availability scenarios, a third-party hardware or software load balancer, like HAProxy, should be added to fan out traffic to a group of Logstash nodes.",
              "Network and Security Data [_network_and_security_data]": "Although Beats may already satisfy your data ingest use case, network and security datasets come in a variety of forms. Let’s touch on a few other ingestion points.\n\n['Network wire data - collect and analyze network traffic with [Packetbeat](https://www.elastic.co/products/beats/packetbeat).', 'Netflow v5/v9/v10 - Logstash understands data from Netflow/IPFIX exporters with the [Netflow codec](logstash-docs-md://lsr/plugins-codecs-netflow.md).', 'Nmap - Logstash accepts and parses Nmap XML data with the [Nmap codec](logstash-docs-md://lsr/plugins-codecs-nmap.md).', 'SNMP trap - Logstash has a native [SNMP trap input](logstash-docs-md://lsr/plugins-inputs-snmptrap.md).', 'CEF - Logstash accepts and parses CEF data from systems like Arcsight SmartConnectors with the [CEF codec](logstash-docs-md://lsr/plugins-codecs-cef.md).']",
              "Centralized Syslog Servers [_centralized_syslog_servers]": "Existing syslog server technologies like rsyslog and syslog-ng generally send syslog over to Logstash TCP or UDP endpoints for extraction, processing, and persistence. If the data format conforms to RFC3164, it can be fed directly to the [Logstash syslog input](logstash-docs-md://lsr/plugins-inputs-syslog.md).",
              "Infrastructure & Application Data and IoT [_infrastructure_application_data_and_iot]": "Infrastructure and application metrics can be collected with [Metricbeat](https://www.elastic.co/products/beats/metricbeat), but applications can also send webhooks to a Logstash HTTP input or have metrics polled from an HTTP endpoint with the [HTTP poller input plugin](logstash-docs-md://lsr/plugins-inputs-http_poller.md).\n\nFor applications that log with log4j2, it’s recommended to use the SocketAppender to send JSON to the Logstash TCP input. Alternatively, log4j2 can also log to a file for collection with FIlebeat. Usage of the log4j1 SocketAppender is not recommended.\n\nIoT devices like Raspberry Pis, smartphones, and connected vehicles often send telemetry data through one of these protocols."
            },
            "Integrating with Messaging Queues [integrating-with-messaging-queues]": {
              "Resiliency and Recovery [_resiliency_and_recovery]": "When Logstash consumes from Kafka, persistent queues should be enabled and will add transport resiliency to mitigate the need for reprocessing during Logstash node failures. In this context, it’s recommended to use the default persistent queue disk allocation size `queue.max_bytes: 1GB`.\n\nIf Kafka is configured to retain data for an extended period of time, data can be reprocessed from Kafka in the case of disaster recovery and reconciliation.",
              "Other Messaging Queue Integrations [_other_messaging_queue_integrations]": "Although an additional queuing layer is not required, Logstash can consume from a myriad of other message queuing technologies like [RabbitMQ](logstash-docs-md://lsr/plugins-inputs-rabbitmq.md) and [Redis](logstash-docs-md://lsr/plugins-inputs-redis.md). It also supports ingestion from hosted queuing services like [Pub/Sub](logstash-docs-md://lsr/plugins-inputs-google_pubsub.md), [Kinesis](logstash-docs-md://lsr/plugins-inputs-kinesis.md), and [SQS](logstash-docs-md://lsr/plugins-inputs-sqs.md)."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 13]"
        },
        {
          "title": "Logstash Directory Layout [dir-layout]",
          "description": null,
          "content": {
            "Directory Layout of `.zip` and `.tar.gz` Archives [zip-targz-layout]": "The `.zip` and `.tar.gz` packages are entirely self-contained. All files and directories are, by default, contained within the home directory — the directory created when unpacking the archive.\n\nThis is very convenient because you don’t have to create any directories to start using Logstash, and uninstalling Logstash is as easy as removing the home directory.  However, it is advisable to change the default locations of the config and the logs directories so that you do not delete important data later on.\n\n| Type | Description | Default Location | Setting |\n| --- | --- | --- | --- |\n| home | Home directory of the Logstash installation. | `{extract.path}`- Directory created by unpacking the archive |  |\n| bin | Binary scripts, including `logstash` to start Logstash    and `logstash-plugin` to install plugins | `{extract.path}/bin` |  |\n| settings | Configuration files, including `logstash.yml` and `jvm.options` | `{extract.path}/config` | `path.settings` |\n| logs | Log files | `{extract.path}/logs` | `path.logs` |\n| plugins | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. | `{extract.path}/plugins` | `path.plugins` |\n| data | Data files used by logstash and its plugins for any persistence needs. | `{extract.path}/data` | `path.data` |",
            "Directory Layout of Debian and RPM Packages [deb-layout]": "The Debian package and the RPM package each place config files, logs, and the settings files in the appropriate locations for the system:\n\n| Type | Description | Default Location | Setting |\n| --- | --- | --- | --- |\n| home | Home directory of the Logstash installation. | `/usr/share/logstash` |  |\n| bin | Binary scripts including `logstash` to start Logstash    and `logstash-plugin` to install plugins | `/usr/share/logstash/bin` |  |\n| settings | Configuration files, including `logstash.yml` and `jvm.options` | `/etc/logstash` | `path.settings` |\n| conf | Logstash pipeline configuration files | `/etc/logstash/conf.d/*.conf` | See `/etc/logstash/pipelines.yml` |\n| logs | Log files | `/var/log/logstash` | `path.logs` |\n| plugins | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. | `/usr/share/logstash/plugins` | `path.plugins` |\n| data | Data files used by logstash and its plugins for any persistence needs. | `/var/lib/logstash` | `path.data` |",
            "Directory Layout of Docker Images [docker-layout]": "The Docker images are created from the `.tar.gz` packages, and follow a similar directory layout.\n\n| Type | Description | Default Location | Setting |\n| --- | --- | --- | --- |\n| home | Home directory of the Logstash installation. | `/usr/share/logstash` |  |\n| bin | Binary scripts, including `logstash` to start Logstash    and `logstash-plugin` to install plugins | `/usr/share/logstash/bin` |  |\n| settings | Configuration files, including `logstash.yml` and `jvm.options` | `/usr/share/logstash/config` | `path.settings` |\n| conf | Logstash pipeline configuration files | `/usr/share/logstash/pipeline` | `path.config` |\n| plugins | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. | `/usr/share/logstash/plugins` | `path.plugins` |\n| data | Data files used by logstash and its plugins for any persistence needs. | `/usr/share/logstash/data` | `path.data` |\n\n::::{note}\nLogstash Docker containers do not create log files by default. They log to standard output.\n::::"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/dir-layout.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 14]"
        },
        {
          "title": "Configuring Logstash for Docker [docker-config]",
          "description": null,
          "content": {
            "Pipeline Configuration [_pipeline_configuration]": "It is essential to place your pipeline configuration where it can be found by Logstash. By default, the container will look in `/usr/share/logstash/pipeline/` for pipeline configuration files.\n\nIn this example we use a bind-mounted volume to provide the configuration via the `docker run` command:\n\n```\ndocker run --rm -it -v ~/pipeline/:/usr/share/logstash/pipeline/ docker.elastic.co/logstash/logstash:9.0.0\n```\n\nEvery file in the host directory `~/pipeline/` will then be parsed by Logstash as pipeline configuration.\n\nIf you don’t provide configuration to Logstash, it will run with a minimal config that listens for messages from the [Beats input plugin](logstash-docs-md://lsr/plugins-inputs-beats.md) and echoes any that are received to `stdout`. In this case, the startup logs will be similar to the following:\n\n```\nSending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties.\n[2016-10-26T05:11:34,992][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=>\"0.0.0.0:5044\"}\n[2016-10-26T05:11:35,068][INFO ][logstash.pipeline        ] Starting pipeline {\"id\"=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>5, \"pipeline.max_inflight\"=>500}\n[2016-10-26T05:11:35,078][INFO ][org.logstash.beats.Server] Starting server on port: 5044\n[2016-10-26T05:11:35,078][INFO ][logstash.pipeline        ] Pipeline main started\n[2016-10-26T05:11:35,105][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}\n```\n\nThis is the default configuration for the image, defined in `/usr/share/logstash/pipeline/logstash.conf`.  If this is the behaviour that you are observing, ensure that your pipeline configuration is being picked up correctly, and that you are replacing either `logstash.conf` or the entire `pipeline` directory.",
            "Settings [_settings]": {
              "Bind-mounted settings files [docker-bind-mount-settings]": "Settings files can also be provided through bind-mounts. Logstash expects to find them at `/usr/share/logstash/config/`.\n\nIt’s possible to provide an entire directory containing all needed files:\n\n```\ndocker run --rm -it -v ~/settings/:/usr/share/logstash/config/ docker.elastic.co/logstash/logstash:9.0.0\n```\n\nAlternatively, a single file can be mounted:\n\n```\ndocker run --rm -it -v ~/settings/logstash.yml:/usr/share/logstash/config/logstash.yml docker.elastic.co/logstash/logstash:9.0.0\n```\n\n::::{note}\nBind-mounted configuration files will retain the same permissions and ownership within the container that they have on the host system. Be sure to set permissions such that the files will be readable and, ideally, not writeable by the container’s `logstash` user (UID 1000).\n::::",
              "Custom Images [_custom_images]": "Bind-mounted configuration is not the only option, naturally. If you prefer the *Immutable Infrastructure* approach, you can prepare a custom image containing your configuration by using a `Dockerfile` like this one:\n\n```\nFROM docker.elastic.co/logstash/logstash:9.0.0\nRUN rm -f /usr/share/logstash/pipeline/logstash.conf\nCOPY pipeline/ /usr/share/logstash/pipeline/\nCOPY config/ /usr/share/logstash/config/\n```\n\nBe sure to replace or delete `logstash.conf` in your custom image, so that you don’t retain the example config from the base image.",
              "Environment variable configuration [docker-env-config]": "Under Docker, Logstash settings can be configured via environment variables. When the container starts, a helper process checks the environment for variables that can be mapped to Logstash settings. Settings that are found in the environment override those in the `logstash.yml` as the container starts up.\n\nFor compatibility with container orchestration systems, these environment variables are written in all capitals, with underscores as word separators.\n\nSome example translations are shown here:\n\n**Environment Variable**\n:   **Logstash Setting**\n\n`PIPELINE_WORKERS`\n:   `pipeline.workers`\n\n`LOG_LEVEL`\n:   `log.level`\n\n`MONITORING_ENABLED`\n:   `monitoring.enabled`\n\nIn general, any setting listed in the [settings documentation](/reference/logstash-settings-file.md) can be configured with this technique.\n\n::::{note}\nDefining settings with environment variables causes `logstash.yml` to be modified in place. This behaviour is likely undesirable if `logstash.yml` was bind-mounted from the host system. Thus, it is not recommended to combine the bind-mount technique with the environment variable technique. It is best to choose a single method for defining Logstash settings.\n::::"
            },
            "Docker defaults [_docker_defaults]": "The following settings have different default values when using the Docker images:\n\n`api.http.host`\n:   `0.0.0.0`\n\n`monitoring.elasticsearch.hosts`\n:   `http://elasticsearch:9200`\n\n::::{note}\nThe setting `monitoring.elasticsearch.hosts` is not defined in the `-oss` image.\n::::\n\nThese settings are defined in the default `logstash.yml`. They can be overridden with a [custom `logstash.yml`](#docker-bind-mount-settings) or via [environment variables](#docker-env-config).\n\n::::{important}\nIf replacing `logstash.yml` with a custom version, be sure to copy the above defaults to the custom file if you want to retain them. If not, they will be \"masked\" by the new file.\n::::",
            "Logging Configuration [_logging_configuration]": "Under Docker, Logstash logs go to standard output by default. To change this behaviour, use any of the techniques above to replace the file at `/usr/share/logstash/config/log4j2.properties`."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/docker-config.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 15]"
        },
        {
          "title": "Running Logstash on Docker [docker]",
          "description": null,
          "content": {
            "Pulling the image [_pulling_the_image]": "Obtaining Logstash for Docker is as simple as issuing a `docker\npull` command against the Elastic Docker registry.\n\n```\ndocker pull {{docker-repo}}:{{version.stack}}\n```\n\nAlternatively, you can download other Docker images that contain only features\navailable under the Apache 2.0 license. To download the images, go to\n[www.docker.elastic.co](https://www.docker.elastic.co).",
            "Verifying the image [_verifying_the_image]": "Although it's optional, we highly recommend verifying the signatures included with your downloaded Docker images to ensure that the images are valid.\n\nElastic images are signed with [Cosign](https://docs.sigstore.dev/cosign/) which is part of the [Sigstore](https://www.sigstore.dev/) project.\nCosign supports container signing, verification, and storage in an OCI registry.\nInstall the appropriate Cosign application for your operating system.\n\nRun the following commands to verify the container image signature for {{ls}} v{{version.stack}}:\n\n```\nwget https://artifacts.elastic.co/cosign.pub <1>\ncosign verify --key cosign.pub {{docker-repo}}:{{version.stack}} <2>\n```\n\n['Download the Elastic public key to verify container signature', 'Verify the container against the Elastic public key']\n\nThe command prints the check results and the signature payload in JSON format, for example:\n\n```\nVerification for {{docker-repo}}:{{version.stack}} --\nThe following checks were performed on each of these signatures:\n  - The cosign claims were validated\n  - Existence of the claims in the transparency log was verified offline\n  - The signatures were verified against the specified public key\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/docker.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 16]"
        },
        {
          "title": "ECS in Logstash [ecs-ls]",
          "description": null,
          "content": {
            "ECS compatibility [ecs-compatibility]": {
              "Configuring ECS [ecs-configuration]": {
                "Specific plugin instance [_specific_plugin_instance]": "Use a plugin’s `ecs_compatibility` option to override the default value on the plugin instance.\n\nFor example, if you want a specific instance of the GeoIP Filter to behave without ECS compatibility, you can adjust its definition in your pipeline without affecting any other plugin instances.\n\n```\nfilter {\n  geoip {\n    source => \"[host][ip]\"\n    ecs_compatibility => disabled\n  }\n}\n```\n\nAlternatively, if you had a UDP input with a CEF codec, and wanted both to use an ECS mode while still running {{ls}} 7, you can adjust their definitions to specify the major version of ECS to use.\n\n```\ninput {\n  udp {\n    port => 1234\n    ecs_compatibility => v8\n    codec => cef {\n      ecs_compatibility => v8\n    }\n  }\n}\n```",
                "All plugins in a given pipeline [ecs-configuration-pipeline]": "If you wish to provide a specific default value for `ecs_compatibility` to *all* plugins in a pipeline, you can do so with the `pipeline.ecs_compatibility` setting in your pipeline definition in `config/pipelines.yml` or Central Management. This setting will be used unless overridden by a specific plugin instance. If unspecified for an individual pipeline, the global value will be used.\n\nFor example, setting `pipeline.ecs_compatibility: disabled` for a pipeline *locks in* that pipeline’s pre-{{ls}} 8 behavior.\n\n```\n- pipeline.id: my-legacy-pipeline\n  path.config: \"/etc/path/to/legacy-pipeline.config\"\n  pipeline.ecs_compatibility: disabled\n- pipeline.id: my-ecs-pipeline\n  path.config: \"/etc/path/to/ecs-pipeline.config\"\n  pipeline.ecs_compatibility: v8\n```",
                "All plugins in all pipelines [ecs-configuration-all]": "Similarly, you can set the default value for the whole {{ls}} process by setting the `pipeline.ecs_compatibility` value in `config/logstash.yml`.\n\n```\npipeline.ecs_compatibility: disabled\n```"
              }
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/ecs-ls.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 17]"
        },
        {
          "title": "Using environment variables [environment-variables]",
          "description": null,
          "content": {
            "Overview [_overview]": [
              "You can set environment variable references in the configuration for Logstash plugins by using `${var}`.",
              "At Logstash startup, each reference is replaced by the value of the environment variable.",
              "The replacement is case-sensitive.",
              "References to undefined variables raise a Logstash configuration error.",
              "You can give a default value by using the form `${var:default value}`. Logstash uses the default value if the environment variable is undefined.",
              "You can add environment variable references in any plugin option type: string, number, boolean, array, or hash.",
              "Environment variables for list-type URI parameters can support lists of space-delimited values. Currently, other non-URI based options do not support lists of values. See [Cross-plugin concepts and features](/reference/plugin-concepts.md)",
              "Environment variables are immutable. If you update the environment variable, you’ll have to restart Logstash to pick up the updated value.",
              "References to environment variables in `config.string` comments are evaluated during configuration parsing, and are therefore discouraged. Remove the `$` sign to avoid pipeline loading failures."
            ],
            "Examples [_examples]": {
              "Setting the TCP port [_setting_the_tcp_port]": "Here’s an example that uses an environment variable to set the TCP port:\n\n```\ninput {\n  tcp {\n    port => \"${TCP_PORT}\"\n  }\n}\n```\n\nNow let’s set the value of `TCP_PORT`:\n\n```\nexport TCP_PORT=12345\n```\n\nAt startup, Logstash uses this configuration:\n\n```\ninput {\n  tcp {\n    port => 12345\n  }\n}\n```\n\nIf the `TCP_PORT` environment variable is not set, Logstash returns a configuration error.\n\nYou can fix this problem by specifying a default value:\n\n```\ninput {\n  tcp {\n    port => \"${TCP_PORT:54321}\"\n  }\n}\n```\n\nNow, instead of returning a configuration error if the variable is undefined, Logstash uses the default:\n\n```\ninput {\n  tcp {\n    port => 54321\n  }\n}\n```\n\nIf the environment variable is defined, Logstash uses the value specified for the variable instead of the default.",
              "Setting the value of a tag [_setting_the_value_of_a_tag]": "Here’s an example that uses an environment variable to set the value of a tag:\n\n```\nfilter {\n  mutate {\n    add_tag => [ \"tag1\", \"${ENV_TAG}\" ]\n  }\n}\n```\n\nLet’s set the value of `ENV_TAG`:\n\n```\nexport ENV_TAG=\"tag2\"\n```\n\nAt startup, Logstash uses this configuration:\n\n```\nfilter {\n  mutate {\n    add_tag => [ \"tag1\", \"tag2\" ]\n  }\n}\n```",
              "Setting a file path [_setting_a_file_path]": "Here’s an example that uses an environment variable to set the path to a log file:\n\n```\nfilter {\n  mutate {\n    add_field => {\n      \"my_path\" => \"${HOME}/file.log\"\n    }\n  }\n}\n```\n\nLet’s set the value of `HOME`:\n\n```\nexport HOME=\"/path\"\n```\n\nAt startup, Logstash uses the following configuration:\n\n```\nfilter {\n  mutate {\n    add_field => {\n      \"my_path\" => \"/path/file.log\"\n    }\n  }\n}\n```"
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/environment-variables.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 18]"
        },
        {
          "title": "Event API [event-api]",
          "description": null,
          "content": {
            "Event Object [_event_object]": "Event is the main object that encapsulates data flow internally in Logstash and provides an API for the plugin developers to interact with the event’s content. Typically, this API is used in plugins and in a Ruby filter to retrieve data and use it for transformations. Event object contains the original data sent to Logstash and any additional fields created during Logstash’s filter stages.\n\nIn 5.0, we’ve re-implemented the Event class and its supporting classes in pure Java. Since Event is a critical component in data processing,  a rewrite in Java improves performance and provides efficient serialization when storing data on disk. For the most part, this change aims at keeping backward compatibility and is transparent to the users. To this extent we’ve updated and published most of the plugins in Logstash’s ecosystem to adhere to the new API changes. However, if you are maintaining a custom plugin, or have a Ruby filter, this change will affect you. The aim of this guide is to describe the new API and provide examples to migrate to the new changes.",
            "Event API [_event_api]": "Prior to version 5.0, developers could access and manipulate event data by directly using Ruby hash syntax. For example, `event[field] = foo`. While this is powerful, our goal is to abstract the internal implementation details and provide well-defined getter and setter APIs.\n\n**Get API**\n\nThe getter is a read-only access of field-based data in an Event.\n\n**Syntax:** `event.get(field)`\n\n**Returns:** Value for this field or nil if the field does not exist. Returned values could be a string, numeric or timestamp scalar value.\n\n`field` is a structured field sent to Logstash or created after the transformation process. `field` can also be a nested [field reference](https://www.elastic.co/guide/en/logstash/current/field-references-deepdive.html) such as `[field][bar]`.\n\nExamples:\n\n```\nevent.get(\"foo\" ) # => \"baz\"\nevent.get(\"[foo]\") # => \"zab\"\nevent.get(\"[foo][bar]\") # => 1\nevent.get(\"[foo][bar]\") # => 1.0\nevent.get(\"[foo][bar]\") # =>  [1, 2, 3]\nevent.get(\"[foo][bar]\") # => {\"a\" => 1, \"b\" => 2}\nevent.get(\"[foo][bar]\") # =>  {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]}\n```\n\nAccessing @metadata\n\n```\nevent.get(\"[@metadata][foo]\") # => \"baz\"\n```\n\n**Set API**\n\nThis API can be used to mutate data in an Event.\n\n**Syntax:** `event.set(field, value)`\n\n**Returns:**  The current Event  after the mutation, which can be used for chainable calls.\n\nExamples:\n\n```\nevent.set(\"foo\", \"baz\")\nevent.set(\"[foo]\", \"zab\")\nevent.set(\"[foo][bar]\", 1)\nevent.set(\"[foo][bar]\", 1.0)\nevent.set(\"[foo][bar]\", [1, 2, 3])\nevent.set(\"[foo][bar]\", {\"a\" => 1, \"b\" => 2})\nevent.set(\"[foo][bar]\", {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]})\nevent.set(\"[@metadata][foo]\", \"baz\")\n```\n\nMutating a collection after setting it in the Event has an undefined behaviour and is not allowed.\n\n```\nh = {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]}\nevent.set(\"[foo][bar]\", h)\n\nh[\"c\"] = [3, 4]\nevent.get(\"[foo][bar][c]\") # => undefined\n\nSuggested way of mutating collections:\n\nh = {\"a\" => 1, \"b\" => 2, \"c\" => [1, 2]}\nevent.set(\"[foo][bar]\", h)\n\nh[\"c\"] = [3, 4]\nevent.set(\"[foo][bar]\", h)\n\n# Alternatively,\nevent.set(\"[foo][bar][c]\", [3, 4])\n```",
            "Ruby Filter [_ruby_filter]": "The [Ruby Filter](logstash-docs-md://lsr/plugins-filters-ruby.md) can be used to execute any ruby code and manipulate event data using the API described above. For example, using the new API:\n\n```\nfilter {\n  ruby {\n    code => 'event.set(\"lowercase_field\", event.get(\"message\").downcase)'\n  }\n}\n```\n\nThis filter will lowercase the `message` field, and set it to a new field called `lowercase_field`"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/event-api.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 19]"
        },
        {
          "title": "Accessing event data and fields [event-dependent-configuration]",
          "description": null,
          "content": {
            "Field references [logstash-config-field-references]": {
              "Why use field references? [field-reference-deep-dive]": "You might find situations in which you need to refer to a field or collection of fields by name. \nYou can accomplish this goal using the Logstash field reference syntax.\n\nThe syntax to access a field specifies the entire path to the field, with each fragment wrapped in square brackets.\nWhen a field name contains square brackets, the brackets must be properly [ escaped](#formal-grammar-escape-sequences).\n\nField references can be expressed literally within [conditional statements](#conditionals) in your pipeline configurations,\nas string arguments to your pipeline plugins, or within sprintf statements that will be used by your pipeline plugins:\n\n```\nfilter {\n  #  +----literal----+     +----literal----+\n  #  |               |     |               |\n  if [@metadata][date] and [@metadata][time] {\n    mutate {\n      add_field {\n        \"[@metadata][timestamp]\" => \"%{[@metadata][date]} %{[@metadata][time]}\"\n      # |                      |    |  |               |    |               | |\n      # +----string-argument---+    |  +--field-ref----+    +--field-ref----+ |\n      #                             +-------- sprintf format string ----------+\n      }\n    }\n  }\n}\n```",
              "Formal grammar [formal-grammar]": {
                "Field Reference Literal [formal-grammar-field-reference-literal]": "A _Field Reference Literal_ is a sequence of one or more _Path Fragments_ that can be used directly in Logstash pipeline [conditionals](#conditionals)  without any additional quoting.\nExample: `[request]`, `[response][status]`).\n\n```\nfieldReferenceLiteral\n  : ( pathFragment )+\n  ;\n```",
                "Field Reference (Event APIs) [formal-grammar-field-reference]": "The Event API's methods for manipulating the fields of an event or using the sprintf syntax are more flexible than the pipeline grammar in what they accept as a Field Reference.\nTop-level fields can be referenced directly by their _Field Name_ without the square brackets, and there is some support for _Composite Field References_, simplifying use of programmatically-generated Field References.\n\nA _Field Reference_ for use with the Event API is therefore one of:\n\n['a single _Field Reference Literal_; OR', 'a single _Field Name_ (referencing a top-level field); OR', 'a single _Composite Field Reference_.']\n\n```\neventApiFieldReference\n  : fieldReferenceLiteral\n  | fieldName\n  | compositeFieldReference\n  ;\n```",
                "Path Fragment [formal-grammar-path-fragment]": "A _Path Fragment_ is a _Field Name_ wrapped in square brackets, such as `[request]`).\n\n```\npathFragment\n  : '[' fieldName ']'\n  ;\n```",
                "Field Name [formal-grammar-field-name]": "A _Field Name_ is a sequence of characters that are _not_ square brackets (`[` or `]`).\n\n```\nfieldName\n  : ( ~( '[' | ']' ) )+\n  ;\n```",
                "Composite Field Reference [formal-grammar-event-api-composite-field-reference]": {
                  "Canonical Representations of Composite Field References [canonical-representations]": "| Acceptable _composite field reference_ | Canonical _field reference_ representation | \n| ------------- | ------------- |\n| `[[deep][nesting]][field]`           | `[deep][nesting][field]`\n| `[foo][[bar]][bingo]`                | `[foo][bar][bingo]`\n| `[[ok]]`                             | `[ok]`\n\nA _Composite Field Reference_ is a sequence of one or more _Path Fragments_ or _Embedded Field References_.\n\n```\ncompositeFieldReference\n  : ( pathFragment | embeddedFieldReference )+\n  ;\n```\n\n_Composite Field References_ are supported by the Event API, but are _not_ supported as literals in the Pipeline Configuration."
                },
                "Embedded Field Reference [formal-grammar-event-api-embedded-field-reference]": "```\nembeddedFieldReference\n  : '[' fieldReference ']'\n  ;\n```\n\nAn _Embedded Field Reference_ is a _Field Reference_ that is itself wrapped in square brackets (`[` and `]`), and can be a component of a _Composite Field Reference_.\n::::"
              },
              "Escape sequences [formal-grammar-escape-sequences]": "For {{ls}} to reference a field whose name contains a character that has special meaning in the field reference grammar, the character must be escaped.\nLogstash can be globally configured to use one of two field reference escape modes:\n\n['`none` (default): no escape sequence processing is done. Fields containing literal square brackets cannot be referenced by the Event API.', '`percent`: URI-style percent encoding of UTF-8 bytes. The left square bracket (`[`) is expressed as `%5B`, and the right square bracket (`]`) is expressed as `%5D`.', '`ampersand`: HTML-style ampersand encoding (`&#` + decimal unicode codepoint + `;`). The left square bracket (`[`) is expressed as `&#91;`, and the right square bracket (`]`) is expressed as `&#93;`.']"
            },
            "sprintf format [sprintf]": "The field reference format is also used in what Logstash calls *sprintf format*. This format enables you to embed field values in other strings. For example, the statsd output has an *increment* setting that enables you to keep a count of apache logs by status code:\n\n```\noutput {\n  statsd {\n    increment => \"apache.%{[response][status]}\"\n  }\n}\n```\n\nSimilarly, you can convert the UTC timestamp in the `@timestamp` field into a string.\n\nInstead of specifying a field name inside the curly braces, use the `%{{FORMAT}}` syntax where `FORMAT` is a [java time format](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/time/format/DateTimeFormatter.html#patterns).\n\nFor example, if you want to use the file output to write logs based on the event’s UTC date and hour and the `type` field:\n\n```\noutput {\n  file {\n    path => \"/var/log/%{type}.%{{yyyy.MM.dd.HH}}\"\n  }\n}\n```\n\n::::{note}\n\n['The sprintf format continues to support [deprecated joda time format](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html) strings as well using the `%{+FORMAT}` syntax. These formats are not directly interchangeable, and we advise you to begin using the more modern Java Time format.', 'A Logstash timestamp represents an instant on the UTC-timeline, so using sprintf formatters will produce results that may not align with your machine-local timezone.\\n::::']\n\nYou can generate a fresh timestamp by using `%{{TIME_NOW}}` syntax instead of relying on the value in `@timestamp`. This is particularly useful when you need to estimate the time span of each plugin.\n\n```\ninput {\n  heartbeat {\n    add_field => { \"heartbeat_time\" => \"%{{TIME_NOW}}\" }\n  }\n}\nfilter {\n  mutate {\n    add_field => { \"mutate_time\" => \"%{{TIME_NOW}}\" }\n  }\n}\n```",
            "Conditionals [conditionals]": "Sometimes you want to filter or output an event only under certain conditions. For that, you can use a conditional.\n\nConditionals in Logstash look and act the same way they do in programming languages. Conditionals support `if`, `else if` and `else` statements and can be nested.\n\nThe conditional syntax is:\n\n```\nif EXPRESSION {\n  ...\n} else if EXPRESSION {\n  ...\n} else {\n  ...\n}\n```\n\nWhat’s an expression? Comparison tests, boolean logic, and so on!\n\nYou can use these comparison operators:\n\n['equality: `==`,  `!=`,  `<`,  `>`,  `<=`, `>=`', 'regexp: `=~`, `!~` (checks a pattern on the right against a string value on the left)', 'inclusion: `in`, `not in`']\n\nSupported boolean operators are:\n\n['`and`, `or`, `nand`, `xor`']\n\nSupported unary operators are:\n\n['`!`']\n\nExpressions can be long and complex. Expressions can contain other expressions, you can negate expressions with `!`, and you can group them with parentheses `(...)`.\n\nFor example, this conditional uses the mutate filter to remove the field `secret` if the field `action` has a value of `login`:\n\n```\nfilter {\n  if [action] == \"login\" {\n    mutate { remove_field => \"secret\" }\n  }\n}\n```\n\nIf an expression generates an error when it is evaluated, event processing stops and a warning message is written to the log. For example, comparing integer value `100` with string value `\"100\"` cannot be evaluated with certainty, and so processing stops and the error is logged.\n\nTo capture the full content of the message at the time the error occurs, set the log level to `debug`. Check out [Logging](/reference/logging.md) for more information about how to configure logging and available log levels.\n\nYou can specify multiple expressions in a single condition:\n\n```\noutput {\n  # Send production errors to pagerduty\n  if [loglevel] == \"ERROR\" and [deployment] == \"production\" {\n    pagerduty {\n    ...\n    }\n  }\n}\n```\n\nYou can use the `in` operator to test whether a field contains a specific string, key, or list element. Note that the semantic meaning of `in` can vary, based on the target type. For example, when applied to a string. `in` means \"is a substring of\". When applied to a collection type, `in` means \"collection contains the exact value\".\n\n```\nfilter {\n  if [foo] in [foobar] {\n    mutate { add_tag => \"field in field\" }\n  }\n  if [foo] in \"foo\" {\n    mutate { add_tag => \"field in string\" }\n  }\n  if \"hello\" in [greeting] {\n    mutate { add_tag => \"string in field\" }\n  }\n  if [foo] in [\"hello\", \"world\", \"foo\"] {\n    mutate { add_tag => \"field in list\" }\n  }\n  if [missing] in [alsomissing] {\n    mutate { add_tag => \"shouldnotexist\" }\n  }\n  if !(\"foo\" in [\"hello\", \"world\"]) {\n    mutate { add_tag => \"shouldexist\" }\n  }\n}\n```\n\nYou use the `not in` conditional the same way. For example, you could use `not in` to only route events to Elasticsearch when `grok` is successful:\n\n```\noutput {\n  if \"_grokparsefailure\" not in [tags] {\n    elasticsearch { ... }\n  }\n}\n```\n\nYou can check for the existence of a specific field, but there’s currently no way to differentiate between a field that doesn’t exist versus a field that’s simply false. \nThe expression `if [foo]` returns `false` when:\n\n['`[foo]` doesn’t exist in the event,', '`[foo]` exists in the event, but is false, or', '`[foo]` exists in the event, but is null']\n\nFor more complex examples, see [Using Conditionals](/reference/config-examples.md#using-conditionals).\n\n::::{note}\nSprintf date/time format in conditionals is not currently supported. \nA workaround using the `@metadata` field is available. \nSee [sprintf date/time format in conditionals](#date-time) for more details and an example.\n::::",
            "The @metadata field [metadata]": {
              "sprintf date/time format in conditionals [date-time]": "Sprintf date/time format in conditionals is not currently supported, but a workaround is available. Put the date calculation in a field so that you can use the field reference in a conditional.\n\n**Example**\n\nUsing sprintf time format directly to add a field based on ingestion time *will not work*:\n\n```\n----------\n# non-working example\nfilter{\n  if \"%{+HH}:%{+mm}\" < \"16:30\" {\n    mutate {\n      add_field => { \"string_compare\" => \"%{+HH}:%{+mm} is before 16:30\" }\n    }\n  }\n}\n----------\n```\n\nThis workaround gives you the intended results:\n\n```\nfilter {\n  mutate{\n     add_field => {\n      \"[@metadata][time]\" => \"%{+HH}:%{+mm}\"\n     }\n  }\n  if [@metadata][time] < \"16:30\" {\n    mutate {\n      add_field => {\n        \"string_compare\" => \"%{+HH}:%{+mm} is before 16:30\"\n      }\n    }\n  }\n}\n```"
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html",
              "https://www.elastic.co/guide/en/logstash/current/field-references-deepdive.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 20]"
        },
        {
          "title": "Execution Model",
          "description": null,
          "content": {
            "Execution Model [execution-model]": "The Logstash event processing pipeline coordinates the execution of inputs, filters, and outputs.\n\nEach input stage in the Logstash pipeline runs in its own thread. Inputs write events to a central queue that is either in memory (default) or on disk. Each pipeline worker thread takes a batch of events off this queue, runs the batch of events through the configured filters, and then runs the filtered events through any outputs. The size of the batch and number of pipeline worker threads are configurable (see [Tuning and profiling logstash pipeline performance](/reference/tuning-logstash.md)).\n\nBy default, Logstash uses in-memory bounded queues between pipeline stages (input → filter and filter → output) to buffer events. If Logstash terminates unsafely, any events that are stored in memory will be lost. To help prevent data loss, you can enable Logstash to persist in-flight events to disk. See [Persistent queues (PQ)](/reference/persistent-queues.md) for more information."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/execution-model.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 21]"
        },
        {
          "title": "Field Extraction",
          "description": null,
          "content": {
            "Extracting Fields and Wrangling Data [field-extraction]": "The plugins described in this section are useful for extracting fields and parsing unstructured data into fields.\n\n[dissect filter](logstash-docs-md://lsr/plugins-filters-dissect.md)\n:   Extracts unstructured event data into fields by using delimiters. The dissect filter does not use regular expressions and is very fast. However, if the structure of the data varies from line to line, the grok filter is more suitable.\n\nFor example, let’s say you have a log that contains the following message:\n\n```json\nApr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...\n```\n\nThe following config dissects the message:\n\n```json\nfilter {\n  dissect {\n    mapping => { \"message\" => \"%{ts} %{+ts} %{+ts} %{src} %{prog}[%{pid}]: %{msg}\" }\n  }\n}\n```\n\nAfter the dissect filter is applied, the event will be dissected into the following fields:\n\n```json\n{\n  \"msg\"        => \"Starting system activity accounting tool...\",\n  \"@timestamp\" => 2017-04-26T19:33:39.257Z,\n  \"src\"        => \"localhost\",\n  \"@version\"   => \"1\",\n  \"host\"       => \"localhost.localdomain\",\n  \"pid\"        => \"1\",\n  \"message\"    => \"Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...\",\n  \"type\"       => \"stdin\",\n  \"prog\"       => \"systemd\",\n  \"ts\"         => \"Apr 26 12:20:02\"\n}\n```\n\n\n\n[kv filter](logstash-docs-md://lsr/plugins-filters-kv.md)\n:   Parses key-value pairs.\n\nFor example, let’s say you have a log message that contains the following key-value pairs:\n\n```json\nip=1.2.3.4 error=REFUSED\n```\n\nThe following config parses the key-value pairs into fields:\n\n```json\nfilter {\n  kv { }\n}\n```\n\nAfter the filter is applied, the event in the example will have these fields:\n\n* `ip: 1.2.3.4`\n* `error: REFUSED`\n\n\n\n[grok filter](logstash-docs-md://lsr/plugins-filters-grok.md)\n:   Parses unstructured event data into fields. This tool is perfect for syslog logs, Apache and other webserver logs, MySQL logs, and in general, any log format that is generally written for humans and not computer consumption. Grok works by combining text patterns into something that matches your logs.\n\nFor example, let’s say you have an HTTP request log that contains the following message:\n\n```json\n55.3.244.1 GET /index.html 15824 0.043\n```\n\nThe following config parses the message into fields:\n\n```json\nfilter {\n  grok {\n    match => { \"message\" => \"%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\" }\n  }\n}\n```\n\nAfter the filter is applied, the event in the example will have these fields:\n\n* `client: 55.3.244.1`\n* `method: GET`\n* `request: /index.html`\n* `bytes: 15824`\n* `duration: 0.043`\n\n\n\n::::{tip}\nIf you need help building grok patterns, try the [Grok Debugger](docs-content://explore-analyze/query-filter/tools/grok-debugger.md). The Grok Debugger is an {{xpack}} feature under the Basic License and is therefore **free to use**.\n::::"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/field-extraction.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 22]"
        },
        {
          "title": "First Event",
          "description": null,
          "content": {
            "Stashing Your First Event [first-event]": "First, let’s test your Logstash installation by running the most basic *Logstash pipeline*.\n\nA Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write the data to a destination.\n\n![basic logstash pipeline](images/basic_logstash_pipeline.png)\n\nTo test your Logstash installation, run the most basic Logstash pipeline.\n\n**MacOS, Linux**\n\n```\ncd logstash-9.0.0\nbin/logstash -e 'input { stdin { } } output { stdout {} }'\n```\n\n**Windows**\n\n```\ncd logstash-9.0.0\n.\\bin\\logstash.bat -e \"input { stdin { } } output { stdout {} }\"\n```\n\nThe command might vary slightly, depending on the terminal or shell you are using.\n\n::::{note}\nThe location of the `bin` directory varies by platform. See [Directory layout](/reference/dir-layout.md) to find the location of `bin\\logstash` on your system.\n::::\n\n::::{admonition} macOS Gatekeeper warnings\n:class: important\n\nApple’s rollout of stricter notarization requirements affected the notarization of {{version.stack}} {{ls}} artifacts. If macOS Catalina displays a dialog when you first run {{ls}} that interrupts it, you will need to take an action to allow it to run. To prevent Gatekeeper checks on the {{ls}} files, run the following command on the downloaded `.tar.gz` archive or the directory to which was extracted:\n\n```\nxattr -d -r com.apple.quarantine <archive-or-directory>\n```\n\nFor example, if the `.tar.gz` file was extracted to the default logstash-{{version.stack}} directory, the command is:\n\n```\nxattr -d -r com.apple.quarantine logstash-{{version.stack}}\n```\n\nAlternatively, you can add a security override if a Gatekeeper popup appears by following the instructions in the *How to open an app that hasn’t been notarized or is from an unidentified developer* section of [Safely open apps on your Mac](https://support.apple.com/en-us/HT202491).\n\n::::\n\nThe `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the command line lets you quickly test configurations without having to edit a file between iterations. The pipeline in the example takes input from the standard input, `stdin`, and moves that input to the standard output, `stdout`, in a structured format.\n\nAfter starting Logstash, wait until you see \"Pipeline main started\" and then enter `hello world` at the command prompt:\n\n```\nhello world\n2013-11-21T01:22:14.405+0000 0.0.0.0 hello world\n```\n\nLogstash adds timestamp and IP address information to the message. Exit Logstash by issuing a **CTRL-D** command in the shell where Logstash is running.\n\nCongratulations! You’ve created and run a basic Logstash pipeline. Next, you learn how to create a more realistic pipeline."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/first-event.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 23]"
        },
        {
          "title": "Getting Started With Logstash",
          "description": null,
          "content": {
            "Getting started with Logstash [getting-started-with-logstash]": "This section guides you through the process of installing Logstash and verifying that everything is running properly. \nAfter learning how to stash your first event, you can go on to create a more advanced pipeline that takes Apache web logs as input, parses the logs, and writes the parsed data to an Elasticsearch cluster. \nThen you learn how to stitch together multiple input and output plugins to unify data from a variety of disparate sources.\n\nThis section includes these topics:\n\n['[Java (JVM) version](#ls-jvm)', '[Installing Logstash](/reference/installing-logstash.md)', '[Stashing Your First Event](/reference/first-event.md)', '[Parsing Logs with Logstash](/reference/advanced-pipeline.md)', '[Stitching Together Multiple Input and Output Plugins](/reference/multiple-input-output-plugins.md)']\n\nJava (JVM) version [ls-jvm]\n\n{{ls}} requires one of these versions:\n\n['Java 17 ', 'Java 21 (default).']\n\nUse the [official Oracle distribution](http://www.oracle.com/technetwork/java/javase/downloads/index.html) or an open-source distribution, such as [OpenJDK](http://openjdk.java.net/). The [Elastic Support Matrix](https://www.elastic.co/support/matrix#matrix_jvm) is the official word on supported versions across releases.\n\n::::{admonition} Bundled JDK\n:class: note\n\n:name: bundled-jdk\n\n{{ls}} offers architecture-specific [downloads](https://www.elastic.co/downloads/logstash) that include Adoptium Eclipse Temurin 21, a long term support (LTS) release of the JDK.\n\nUse the LS_JAVA_HOME environment variable if you want to use a JDK other than the version that is bundled. If you have the LS_JAVA_HOME environment variable set to use a custom JDK, Logstash will continue to use the JDK version you have specified, even after you upgrade.\n\n::::\n\nCheck your Java version [check-jvm]\n\nRun this command:\n\n```\njava -version\n```\n\nOn systems with Java installed, this command produces output similar to:\n\n```\nopenjdk version \"17.0.12\" 2024-07-16\nOpenJDK Runtime Environment Temurin-17.0.12+7 (build 17.0.12+7)\nOpenJDK 64-Bit Server VM Temurin-17.0.12+7 (build 17.0.12+7, mixed mode)\n```\n\n`LS_JAVA_HOME` [java-home]\n\n{{ls}} includes a bundled JDK which has been verified to work with each specific version of {{ls}}, and generally provides the best performance and reliability. If you need to use a JDK other than the bundled version, then set the `LS_JAVA_HOME` environment variable to the version you want to use.\n\nOn some Linux systems, you may need to have the `LS_JAVA_HOME` environment exported before installing {{ls}}, particularly if you installed Java from a tarball. {{ls}} uses Java during installation to automatically detect your environment and install the correct startup method (SysV init scripts, Upstart, or systemd). If {{ls}} is unable to find the `LS_JAVA_HOME` environment variable during package installation, you may get an error message, and {{ls}} will not start properly.\n\nUpdate JDK settings when upgrading from {{ls}} 7.11.x (or earlier)[jdk-upgrade]\n\n{{ls}} uses JDK 21 by default.\nIf you are upgrading from {{ls}} 7.11.x (or earlier), you need to update Java settings in `jvm.options` and `log4j2.properties`.\n\nUpdates to `jvm.options` [_updates_to_jvm_options]\n\nIn the `config/jvm.options` file, remove all CMS related flags:\n\n```\n## GC configuration\n-XX:+UseConcMarkSweepGC\n-XX:CMSInitiatingOccupancyFraction=75\n-XX:+UseCMSInitiatingOccupancyOnly\n```\n\nFor more information about how to use `jvm.options`, please refer to [JVM settings](/reference/jvm-settings.md).\n\nUpdates to `log4j2.properties` [_updates_to_log4j2_properties]\n\nIn the `config/log4j2.properties`:\n\n['Replace properties that start with `appender.rolling.avoid_pipelined_filter.*` with:', '```\\nappender.rolling.avoid_pipelined_filter.type = PipelineRoutingFilter\\n```', 'Replace properties that start with `appender.json_rolling.avoid_pipelined_filter.*` with:', '```\\nappender.json_rolling.avoid_pipelined_filter.type = PipelineRoutingFilter\\n```', 'Replace properties that start with `appender.routing.*` with:', '```\\nappender.routing.type = PipelineRouting\\nappender.routing.name = pipeline_routing_appender\\nappender.routing.pipeline.type = RollingFile\\nappender.routing.pipeline.name = appender-${ctx:pipeline.id}\\nappender.routing.pipeline.fileName = ${sys:ls.logs}/pipeline_${ctx:pipeline.id}.log\\nappender.routing.pipeline.filePattern = ${sys:ls.logs}/pipeline_${ctx:pipeline.id}.%i.log.gz\\nappender.routing.pipeline.layout.type = PatternLayout\\nappender.routing.pipeline.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n\\nappender.routing.pipeline.policy.type = SizeBasedTriggeringPolicy\\nappender.routing.pipeline.policy.size = 100MB\\nappender.routing.pipeline.strategy.type = DefaultRolloverStrategy\\nappender.routing.pipeline.strategy.max = 30\\n```']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 24]"
        },
        {
          "title": "Glob Pattern Support [glob-support]",
          "description": null,
          "content": {
            "Example Patterns [example-glob-patterns]": "Here are some common examples of glob patterns:\n\n`\"/path/to/*.conf\"`\n:   Matches config files ending in `.conf` in the specified path.\n\n`\"/var/log/*.log\"`\n:   Matches log files ending in `.log` in the specified path.\n\n`\"/var/log/**/*.log\"`\n:   Matches log files ending in `.log` in subdirectories under the specified path.\n\n`\"/path/to/logs/{app1,app2,app3}/data.log\"`\n:   Matches app log files in the `app1`, `app2`, and `app3` subdirectories under the specified path."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/glob-support.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 25]"
        },
        {
          "title": "How Logstash Works [pipeline]",
          "description": null,
          "content": {
            "Inputs [_inputs]": "You use inputs to get data into Logstash. Some of the more commonly-used inputs are:\n\n['**file**: reads from a file on the filesystem, much like the UNIX command `tail -0F`', '**syslog**: listens on the well-known port 514 for syslog messages and parses according to the RFC3164 format', '**redis**: reads from a redis server, using both redis channels and redis lists. Redis is often used as a \"broker\" in a centralized Logstash installation, which queues Logstash events from remote Logstash \"shippers\".', '**beats**: processes events sent by [Beats](https://www.elastic.co/downloads/beats).']\n\nFor more information about the available inputs, see [Input Plugins](logstash-docs-md://lsr/input-plugins.md).",
            "Filters [_filters]": "Filters are intermediary processing devices in the Logstash pipeline. You can combine filters with conditionals to perform an action on an event if it meets certain criteria. Some useful filters include:\n\n['**grok**: parse and structure arbitrary text. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns built-in to Logstash, it’s more than likely you’ll find one that meets your needs!', '**mutate**: perform general transformations on event fields. You can rename, remove, replace, and modify fields in your events.', '**drop**: drop an event completely, for example, *debug* events.', '**clone**: make a copy of an event, possibly adding or removing fields.', '**geoip**: add information about geographical location of IP addresses (also displays amazing charts in Kibana!)']\n\nFor more information about the available filters, see [Filter Plugins](logstash-docs-md://lsr/filter-plugins.md).",
            "Outputs [_outputs]": "Outputs are the final phase of the Logstash pipeline. An event can pass through multiple outputs, but once all output processing is complete, the event has finished its execution. Some commonly used outputs include:\n\n['**elasticsearch**: send event data to Elasticsearch. If you’re planning to save your data in an efficient, convenient, and easily queryable format… Elasticsearch is the way to go. Period. Yes, we’re biased :)', '**file**: write event data to a file on disk.', '**graphite**: send event data to graphite, a popular open source tool for storing and graphing metrics. [http://graphite.readthedocs.io/en/latest/](http://graphite.readthedocs.io/en/latest/)', '**statsd**: send event data to statsd, a service that \"listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services\". If you’re already using statsd, this could be useful for you!']\n\nFor more information about the available outputs, see [Output Plugins](logstash-docs-md://lsr/output-plugins.md).",
            "Codecs [_codecs]": "Codecs are basically stream filters that can operate as part of an input or output. Codecs enable you to easily separate the transport of your messages from the serialization process. Popular codecs include `json`, `msgpack`, and `plain` (text).\n\n['**json**: encode or decode data in the JSON format.', '**multiline**: merge multiple-line text events such as java exception and stacktrace messages into a single event.']\n\nFor more information about the available codecs, see [Codec Plugins](logstash-docs-md://lsr/codec-plugins.md)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/pipeline.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 26]"
        },
        {
          "title": "Index",
          "description": null,
          "content": {
            "Logstash [introduction]": "Logstash is an open source data collection engine with real-time pipelining capabilities.\nLogstash can dynamically unify data from disparate sources and normalize the data into destinations of your choice.\nCleanse and democratize all your data for diverse advanced downstream analytics and visualization use cases.\n\nWhile Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case.\nAny type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many native codecs further simplifying the ingestion process.\nLogstash accelerates your insights by harnessing a greater volume and variety of data.\n\n::::{admonition} {{ls}} to {{serverless-full}}\nYou’ll use the {{ls}} [{{es}} output plugin](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) to send data to {{serverless-full}}.\nNote these differences between {{es-serverless}} and both {{ech}} and self-managed {{es}}:\n\n['Use [**API keys**](/reference/secure-connection.md#ls-api-keys) to access {{serverless-full}} from {{ls}} as it does not support native user authentication.\\nAny user-based security settings in your [{{es}} output plugin](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) configuration are ignored and may cause errors.', '{{serverless-full}} uses **data streams** and [{{dlm}} ({{dlm-init}})](docs-content://manage-data/lifecycle/data-stream.md) instead of {{ilm}} ({{ilm-init}}). Any {{ilm-init}} settings in your [{{es}} output plugin](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) configuration are ignored and may cause errors.', '**{{ls}} monitoring** is available through the [{{ls}} Integration](https://github.com/elastic/integrations/blob/main/packages/logstash/_dev/build/docs/README.md) in [Elastic Observability](docs-content://solutions/observability.md) on {{serverless-full}}.']\n\n**Known issue for Logstash to Elasticsearch Serverless.**\nThe logstash-output-elasticsearch `hosts` setting defaults to port :9200.\nSet the value to port :443 instead.\n\n::::"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/index.html",
              "https://www.elastic.co/guide/en/logstash/current/introduction.html",
              "https://www.elastic.co/guide/en/serverless/current/elasticsearch-ingest-data-through-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 27]"
        },
        {
          "title": "Installing Logstash [installing-logstash]",
          "description": null,
          "content": {
            "Installing from a Downloaded Binary [installing-binary]": "The {{ls}} binaries are available from [https://www.elastic.co/downloads](https://www.elastic.co/downloads/logstash). Download the Logstash installation file for your host environment—TAR.GZ, DEB, ZIP, or RPM.\n\nUnpack the file. Do not install Logstash into a directory path that contains colon (:) characters.\n\n::::{note}\nThese packages are free to use under the Elastic license. They contain open source and free commercial features and access to paid commercial features. [Start a 30-day trial](docs-content://deploy-manage/license/manage-your-license-in-self-managed-cluster.md) to try out all of the paid commercial features. See the [Subscriptions](https://www.elastic.co/subscriptions) page for information about Elastic license levels.\n\nAlternatively, you can download an `oss` package, which contains only features that are available under the Apache 2.0 license.\n\n::::\n\nOn supported Linux operating systems, you can use a package manager to install Logstash.",
            "Installing from Package Repositories [package-repositories]": {
              "APT [_apt]": "Download and install the Public Signing Key:\n\n```\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elastic-keyring.gpg\n```\n\nYou may need to install the `apt-transport-https` package on Debian before proceeding:\n\n```\nsudo apt-get install apt-transport-https\n```\n\nSave the repository definition to  /etc/apt/sources.list.d/elastic-{{version.stack | M.x}}.list:\n\n```\necho \"deb [signed-by=/usr/share/keyrings/elastic-keyring.gpg] https://artifacts.elastic.co/packages/{{version.stack | M.x}}/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-{{version.stack | M.x}}.list\n```\n\n::::{warning}\nUse the `echo` method described above to add the Logstash repository.\nDo not use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not provide a source package.\nIf you have added the `deb-src` entry, you will see an error like the following:\n\n```\n    Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)\n```\n\nJust delete the `deb-src` entry from the `/etc/apt/sources.list` file and the\ninstallation should work as expected.\n::::\n\nRun `sudo apt-get update` and the repository is ready for use. You can install\nit with:\n\n```\nsudo apt-get update && sudo apt-get install logstash\n```\n\nCheck out [Running Logstash](running-logstash.md) for details about managing Logstash as a system service.",
              "YUM [_yum]": "Download and install the public signing key:\n\n```\nsudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch\n```\n\nAdd the following in your `/etc/yum.repos.d/` directory\nin a file with a `.repo` suffix, for example `logstash.repo`\n\n```\n[logstash-{{version.stack | M.x}}]\nname=Elastic repository for {{version.stack | M.x}} packages\nbaseurl=https://artifacts.elastic.co/packages/{{version.stack | M.x}}/yum\ngpgcheck=1\ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\nenabled=1\nautorefresh=1\ntype=rpm-md\n```\n\nAnd your repository is ready for use. You can install it with:\n\n```\nsudo yum install logstash\n```\n\n::::{warning}\nThe repositories do not work with older rpm based distributions that still use RPM v3, like CentOS5.\n::::\n\nCheck out [Running Logstash](running-logstash.md)  for managing Logstash as a system service.",
              "Docker [_docker]": "Images are available for running Logstash as a Docker container. They are available from the Elastic Docker registry.\n\nSee [Running Logstash on Docker](/reference/docker.md) for details on how to configure and run Logstash Docker containers."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/installing-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 28]"
        },
        {
          "title": "JVM settings [jvm-settings]",
          "description": null,
          "content": {
            "Setting the memory size [memory-size]": {
              "Setting the JVM heap size [heap-size]": "Here are some tips for adjusting the JVM heap size:\n\n['The recommended heap size for typical ingestion scenarios should be no less than 4GB and no more than 8GB.', 'CPU utilization can increase unnecessarily if the heap size is too low, resulting in the JVM constantly garbage collecting. You can check for this issue by doubling the heap size to see if performance improves.', 'Do not increase the heap size past the amount of physical memory. Some memory must be left to run the OS and other processes.  As a general guideline for most installations, don’t exceed 50-75% of physical memory. The more memory you have, the higher percentage you can use.', 'Set the minimum (Xms) and maximum (Xmx) heap allocation size to the same value to prevent the heap from resizing at runtime, which is a very costly process.', 'You can make more accurate measurements of the JVM heap by using either the `jmap` command line utility distributed with Java or by using VisualVM. For more info, see [Profiling the heap](/reference/tuning-logstash.md#profiling-the-heap).']",
              "Setting the off-heap size [off-heap-size]": "The operating system, persistent queue mmap pages, direct memory, and other processes require memory in addition to memory allocated to heap size.\n\nInternal JVM data structures, thread stacks, memory mapped files and direct memory for input/output (IO) operations are all parts of the off-heap JVM memory. Memory mapped files are not part of the Logstash’s process off-heap memory, but consume RAM when paging files from disk. These mapped files speed up the access to Persistent Queues pages, a performance improvement - or trade off - to reduce expensive disk operations such as read, write, and seek. Some network I/O operations also resort to in-process direct memory usage to avoid, for example, copying of buffers between network sockets. Input plugins such as Elastic Agent, Beats, TCP, and HTTP inputs, use direct memory. The zone for Thread stacks contains the list of stack frames for each Java thread created by the JVM; each frame keeps the local arguments passed during method calls. Read on [Setting the JVM stack size](#stacks-size) if the size needs to be adapted to the processing needs.\n\nPlugins, depending on their type (inputs, filters, and outputs), have different thread models. Every input plugin runs in its own thread and can potentially spawn others. For example, each JDBC input plugin launches a scheduler thread. Netty based plugins like TCP, Beats or HTTP input spawn a thread pool with 2 * number_of_cores threads. Output plugins may also start helper threads, such as a connection management thread for each {{es}} output instance. Every pipeline, also, has its own thread responsible to manage the pipeline lifecycle.\n\nTo summarize, we have 3 categories of memory usage, where 2 can be limited by the JVM and the other relies on available, free memory:\n\n| Memory Type | Configured using | Used by |\n| --- | --- | --- |\n| JVM Heap | -Xmx | any normal object allocation |\n| JVM direct memory | -XX:MaxDirectMemorySize | beats, tcp and http inputs |\n| Native memory | N/A | Persistent Queue Pages, Thread Stacks |\n\nKeep these memory requirements in mind as you calculate your ideal memory allocation.",
              "Buffer Allocation types [off-heap-buffers-allocation]": "Input plugins such as {{agent}}, {{beats}}, TCP, and HTTP allocate buffers in Java heap memory to read events from the network. Heap memory is the preferred allocation method, as it facilitates debugging memory usage problems (such as leaks and Out of Memory errors) through the analysis of heap dumps.\n\nBefore version 9.0.0, {{ls}} defaulted to direct memory instead of heap for this purpose. To re-enable the previous behavior {{ls}} provides a `pipeline.buffer.type` setting in [logstash.yml](/reference/logstash-settings-file.md) that lets you control where to allocate memory buffers for plugins that use them.\n\nPerformance should not be noticeably affected if you switch between `direct` and `heap`. While copying bytes from OS buffers to direct memory buffers is faster, {{ls}} Event objects produced by these plugins are allocated on the Java Heap, incurring the cost of copying from direct memory to heap memory, regardless of the setting.",
              "Memory sizing [memory-size-calculation]": "Total JVM memory allocation must be estimated and is controlled indirectly using Java heap and direct memory settings. By default, a JVM’s off-heap direct memory limit is the same as the heap size. Check out [beats input memory usage](logstash-docs-md://lsr/plugins-inputs-beats.md#plugins-inputs-beats-memory). Consider setting `-XX:MaxDirectMemorySize` to half of the heap size or any value that can accommodate the load you expect these plugins to handle.\n\nAs you make your capacity calculations, keep in mind that the JVM can’t consume the total amount of the host’s memory available, as the Operating System and other processes will require memory too.\n\nFor a {{ls}} instance with persistent queue (PQ) enabled on multiple pipelines, we could estimate memory consumption using:\n\n```\npipelines number * (pipeline threads * stack size + 2 * PQ page size) + direct memory + Java heap\n```\n\n::::{note}\nEach Persistent Queue requires that at least head and tail pages are present and accessible in memory. The default page size is 64 MB so each PQ requires at least 128 MB of heap memory, which can be a significant source of memory consumption per pipeline. Note that the size of memory mapped file can’t be limited with an upper bound.\n::::\n\n::::{note}\nStack size is a setting that depends on the JVM used, but could be customized with `-Xss` setting.\n::::\n\n::::{note}\nDirect memory space by default is big as much as Java heap, but can be customized with the `-XX:MaxDirectMemorySize` setting.\n::::\n\n**Example**\n\nConsider a {{ls}} instance running 10 pipelines, with simple input and output plugins that doesn’t start additional threads, it has 1 pipelines thread, 1 input plugin thread and 12 workers, summing up to 14. Keep in mind that, by default, JVM allocates direct memory equal to memory allocated for Java heap.\n\nThe calculation results in:\n\n['native memory: 1.4Gb  [derived from 10 * (14 * 1Mb + 128Mb)]', 'direct memory: 4Gb', 'Java heap: 4Gb']"
            },
            "Setting the JVM stack size [stacks-size]": "Large configurations may require additional JVM stack memory. If you see a stack overflow error, try increasing the JVM stack size. Add an entry similar to this one in the `jvm.options` [settings file](/reference/config-setting-files.md#settings-files):\n\n```\n-Xss4M\n```\n\nNote that the default stack size is different per platform and per OS flavor. You can find out what the default is by running:\n\n```\njava -XX:+PrintFlagsFinal -version | grep ThreadStackSize\n```\n\nDepending on the default stack size, start by multiplying by 4x, then 8x, and then 16x until the overflow error resolves.",
            "Using `LS_JAVA_OPTS` [ls-java-opts]": "The `LS_JAVA_OPTS` environment variable can also be used to override JVM settings in the `jvm.options` file [settings file](/reference/config-setting-files.md#settings-files). The content of this variable is additive to options configured in the `jvm.options` file, and will override any settings that exist in both places.\n\nFor example to set a different locale to launch {{ls}} instance:\n\n```\nLS_JAVA_OPTS=\"-Duser.country=DE -Duser.language=de\" bin/logstash -e 'input { stdin { codec => json } }'\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/jvm-settings.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 29]"
        },
        {
          "title": "Secrets keystore for secure settings [keystore]",
          "description": null,
          "content": {
            "Keystore password [keystore-password]": "You can protect access to the Logstash keystore by storing a password in an environment variable called `LOGSTASH_KEYSTORE_PASS`. If you create the Logstash keystore after setting this variable, the keystore will be password protected. This means that the environment variable needs to be accessible to the running instance of Logstash. This environment variable must also be correctly set for any users who need to issue keystore commands (add, list, remove, etc.).\n\nUsing a keystore password is recommended, but optional. The data will be encrypted even if you do not set a password. However, it is highly recommended to configure the keystore password and grant restrictive permissions to any files that may contain the environment variable value. If you choose not to set a password, then you can skip the rest of this section.\n\nFor example:\n\n```\nset +o history\nexport LOGSTASH_KEYSTORE_PASS=mypassword\nset -o history\nbin/logstash-keystore create\n```\n\nThis setup requires the user running Logstash to have the environment variable `LOGSTASH_KEYSTORE_PASS=mypassword` defined. If the environment variable is not defined, Logstash cannot access the keystore.\n\nWhen you run Logstash from an RPM or DEB package installation, the environment variables are sourced from `/etc/sysconfig/logstash`.\n\n::::{note}\nYou might need to create `/etc/sysconfig/logstash`. This file should be owned by `root` with `600` permissions. The expected format of `/etc/sysconfig/logstash` is `ENVIRONMENT_VARIABLE=VALUE`, with one entry per line.\n::::\n\nFor other distributions, such as Docker or ZIP, see the documentation for your runtime environment (Windows, Docker, etc) to learn how to set the environment variable for the user that runs Logstash. Ensure that the environment variable (and thus the password) is only accessible to that user.",
            "Keystore location [keystore-location]": "The keystore must be located in the Logstash `path.settings` directory. This is the same directory that contains the `logstash.yml` file. When performing any operation against the keystore, it is recommended to set `path.settings` for the keystore command. For example, to create a keystore on a RPM/DEB installation:\n\n```\nset +o history\nexport LOGSTASH_KEYSTORE_PASS=mypassword\nset -o history\nsudo -E /usr/share/logstash/bin/logstash-keystore --path.settings /etc/logstash create\n```\n\nSee [Logstash Directory Layout](/reference/dir-layout.md) for more about the default directory locations.\n\n::::{note}\nYou will see a warning if the `path.settings` is not pointed to the same directory as the `logstash.yml`.\n::::",
            "Create or overwrite a keystore [creating-keystore]": "The `create` command creates a new keystore or overwrites an existing keystore:\n\n```\nbin/logstash-keystore create\n```\n\nCreates the keystore in the directory defined in the `path.settings` setting.\n\n::::{important}\nIf a keystore already exists, the `create` command can overwrite it (after a Y/N prompt). Selecting `Y` clears all keys and secrets that were previously stored.\n::::\n\n::::{tip}\nSet a [keystore password](#keystore-password) when you create the keystore.\n::::",
            "Add keys [add-keys-to-keystore]": "To store sensitive values, such as authentication credentials for Elasticsearch, use the `add` command:\n\n```\nbin/logstash-keystore add ES_USER ES_PWD\n```\n\nWhen prompted, enter a value for each key.\n\n::::{note}\nKey values are limited to:\n\n['{applies_to}`stack: ga 9.0.1` ASCII letters (`a`-`z`, `A`-`Z`), numbers (`0`-`9`), underscores (`_`), and dots (`.`). Key values must be at least one character long and cannot begin with a number.', '{applies_to}`stack: ga 9.0.0` ASCII characters including digits, letters, and a few special symbols.\\n::::']",
            "List keys [list-settings]": "To list the keys defined in the keystore, use:\n\n```\nbin/logstash-keystore list\n```",
            "Remove keys [remove-settings]": "To remove keys from the keystore, use:\n\n```\nbin/logstash-keystore remove ES_USER ES_PWD\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/keystore.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 30]"
        },
        {
          "title": "Logging [logging]",
          "description": null,
          "content": {
            "Log4j2 configuration [log4j2]": {
              "Rollover settings [rollover]": "The `log4j2.properties` file has three appenders for writing to log files: one for plain text, one with json format, and one to split log lines on per pipeline basis when you set the `pipeline.separate_logs` value.\n\nThese appenders define:\n\n['**triggering policies** that determine *if* a rollover should be performed, and', '**rollover strategy**  to defines *how* the rollover should be done.']\n\nBy default, two triggering policies are defined—time and size.\n\n['The **time** policy creates one file per day.', 'The **size** policy forces the creation of a new file after the file size surpasses 100 MB.']\n\nThe default strategy also performs file rollovers based on a **maximum number of files**. When the limit of 30 files has been reached, the first (oldest) file is removed to create space for the new file. Subsequent files are renumbered accordingly.\n\nEach file has a date, and files older than 7 days (default) are removed during rollover.\n\n```\nappender.rolling.type = RollingFile <1>\nappender.rolling.name = plain_rolling\nappender.rolling.fileName = ${sys:ls.logs}/logstash-plain.log <2>\nappender.rolling.filePattern = ${sys:ls.logs}/logstash-plain-%d{yyyy-MM-dd}-%i.log.gz <3>\nappender.rolling.policies.type = Policies\nappender.rolling.policies.time.type = TimeBasedTriggeringPolicy <4>\nappender.rolling.policies.time.interval = 1\nappender.rolling.policies.time.modulate = true\nappender.rolling.layout.type = PatternLayout\nappender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c]%notEmpty{[%X{pipeline.id}]}%notEmpty{[%X{plugin.id}]} %m%n\nappender.rolling.policies.size.type = SizeBasedTriggeringPolicy <5>\nappender.rolling.policies.size.size = 100MB\nappender.rolling.strategy.type = DefaultRolloverStrategy\nappender.rolling.strategy.max = 30 <6>\nappender.rolling.strategy.action.type = Delete <7>\nappender.rolling.strategy.action.basepath = ${sys:ls.logs}\nappender.rolling.strategy.action.condition.type = IfFileName\nappender.rolling.strategy.action.condition.glob = logstash-plain-* <8>\nappender.rolling.strategy.action.condition.nested_condition.type = IfLastModified\nappender.rolling.strategy.action.condition.nested_condition.age = 7D <9>\n```\n\n['The appender type, which rolls older log files.', 'Name of the current log file.', 'Name’s format definition of the rolled files, in this case a date followed by an incremental number, up to 30 (by default).', 'Time policy to trigger a rollover at the end of the day.', 'Size policy to trigger a rollover once the plain text file reaches the size of 100 MB.', 'Rollover strategy defines a maximum of 30 files.', 'Action to execute during the rollover.', 'The file set to consider by the action.', 'Condition to execute the rollover action: older than 7 days.']\n\nThe rollover action can also enforce a disk usage limit, deleting older files to match the requested condition, as an example:\n\n```\nappender.rolling.type = RollingFile\n...\nappender.rolling.strategy.action.condition.glob = pipeline_${ctx:pipeline.id}.*.log.gz\nappender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize\nappender.rolling.strategy.action.condition.nested_condition.exceeds = 5MB <1>\n```\n\n['Deletes files if total accumulated compressed file size is over 5MB.']"
            },
            "Logging APIs [_logging_apis]": {
              "Retrieve list of logging configurations [_retrieve_list_of_logging_configurations]": "To retrieve a list of logging subsystems available at runtime, you can do a `GET` request to `_node/logging`\n\n```\ncurl -XGET 'localhost:9600/_node/logging?pretty'\n```\n\nExample response:\n\n```\n{\n...\n  \"loggers\" : {\n    \"logstash.agent\" : \"INFO\",\n    \"logstash.api.service\" : \"INFO\",\n    \"logstash.basepipeline\" : \"INFO\",\n    \"logstash.codecs.plain\" : \"INFO\",\n    \"logstash.codecs.rubydebug\" : \"INFO\",\n    \"logstash.filters.grok\" : \"INFO\",\n    \"logstash.inputs.beats\" : \"INFO\",\n    \"logstash.instrument.periodicpoller.jvm\" : \"INFO\",\n    \"logstash.instrument.periodicpoller.os\" : \"INFO\",\n    \"logstash.instrument.periodicpoller.persistentqueue\" : \"INFO\",\n    \"logstash.outputs.stdout\" : \"INFO\",\n    \"logstash.pipeline\" : \"INFO\",\n    \"logstash.plugins.registry\" : \"INFO\",\n    \"logstash.runner\" : \"INFO\",\n    \"logstash.shutdownwatcher\" : \"INFO\",\n    \"org.logstash.Event\" : \"INFO\",\n    \"slowlog.logstash.codecs.plain\" : \"TRACE\",\n    \"slowlog.logstash.codecs.rubydebug\" : \"TRACE\",\n    \"slowlog.logstash.filters.grok\" : \"TRACE\",\n    \"slowlog.logstash.inputs.beats\" : \"TRACE\",\n    \"slowlog.logstash.outputs.stdout\" : \"TRACE\"\n  }\n}\n```",
              "Update logging levels [_update_logging_levels]": "Prepend the name of the subsystem, module, or plugin with `logger.`.\n\nHere is an example using `outputs.elasticsearch`:\n\n```\ncurl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"logger.logstash.outputs.elasticsearch\" : \"DEBUG\"\n}\n'\n```\n\nWhile this setting is in effect, Logstash emits DEBUG-level logs for *all* the Elasticsearch outputs specified in your configuration. Please note this new setting is transient and will not survive a restart.\n\n::::{note}\nIf you want logging changes to persist after a restart, add them to `log4j2.properties` instead.\n::::",
              "Reset dynamic logging levels [_reset_dynamic_logging_levels]": "To reset any logging levels that may have been dynamically changed via the logging API, send a `PUT` request to `_node/logging/reset`. All logging levels will revert to the values specified in the `log4j2.properties` file.\n\n```\ncurl -XPUT 'localhost:9600/_node/logging/reset?pretty'\n```"
            },
            "Log file location [_log_file_location]": "You can specify the log file location using `--path.logs` setting.",
            "Slowlog [_slowlog]": {
              "Enable slowlog [_enable_slowlog]": "The `slowlog.threshold` fields use a time-value format which enables a wide range of trigger intervals. You can specify ranges using the following time units: `nanos` (nanoseconds), `micros` (microseconds), `ms` (milliseconds), `s` (second), `m` (minute), `h` (hour), `d` (day).\n\nSlowlog becomes more sensitive and logs more events as you raise the log level.\n\nExample:\n\n```\nslowlog.threshold.warn: 2s\nslowlog.threshold.info: 1s\nslowlog.threshold.debug: 500ms\nslowlog.threshold.trace: 100ms\n```\n\nIn this example:\n\n['If the log level is set to `warn`, the log shows events that took longer than 2s to process.', 'If the log level is set to `info`, the log shows events that took longer than 1s to process.', 'If the log level is set to `debug`, the log shows events that took longer than 500ms to process.', 'If the log level is set to `trace`, the log shows events that took longer than 100ms to process.']\n\nThe logs include the full event and filter configuration that are responsible for the slowness."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/logging.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 31]"
        },
        {
          "title": "Centralized Pipeline Management [logstash-centralized-pipeline-management]",
          "description": null,
          "content": {
            "Manage pipelines [_manage_pipelines]": {
              "Pipeline behavior [_pipeline_behavior]": [
                "The pipeline configurations and metadata are stored in Elasticsearch. Any changes that you make to a pipeline definition are picked up and loaded automatically by all Logstash instances registered to use the pipeline. The changes are applied immediately. If Logstash is registered to use the pipeline, you do not have to restart Logstash to pick up the changes.",
                "The pipeline runs on all Logstash instances that are registered to use the pipeline.  {{kib}} saves the new configuration, and Logstash will attempt to load it. There is no validation done at the UI level.",
                "You need to check the local Logstash logs for configuration errors. If you’re using the Logstash monitoring feature in {{kib}}, use the Monitoring tab to check the status of your Logstash nodes.",
                "You can specify multiple pipeline configurations that run in parallel on the same Logstash node.",
                "If you edit and save a pipeline configuration, Logstash reloads the configuration in the background and continues processing events.",
                "If you try to delete a pipeline that is running (for example, `apache`) in {{kib}}, Logstash will attempt to stop the pipeline. Logstash waits until all events have been fully processed by the pipeline. Before you delete a pipeline, make sure you understand your data sources. Stopping a pipeline may lead to data loss."
              ]
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/logstash-centralized-pipeline-management.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 32]"
        },
        {
          "title": "GeoIP Database Management [logstash-geoip-database-management]",
          "description": null,
          "content": {
            "Database Metrics [logstash-geoip-database-management-metrics]": "You can monitor the managed database’s status through the [Node Stats API](https://www.elastic.co/docs/api/doc/logstash/operation/operation-nodestats).\n\nThe following request returns a JSON document containing database manager stats, including:\n\n['database status and freshness', ['`geoip_download_manager.database.*.status`', ['`init` : initial CC database status', '`up_to_date` : using up-to-date EULA database', '`to_be_expired` : 25 days without calling service', '`expired` : 30 days without calling service'], '`fail_check_in_days` : number of days Logstash fails to call service since the last success'], 'info about download successes and failures', ['`geoip_download_manager.download_stats.successes` number of successful checks and downloads', '`geoip_download_manager.download_stats.failures` number of failed check or download', '`geoip_download_manager.download_stats.status`', ['`updating` : check and download at the moment', '`succeeded` : last download succeed', '`failed` : last download failed']]]\n\n```\ncurl -XGET 'localhost:9600/_node/stats/geoip_download_manager?pretty'\n```\n\nExample response:\n\n```\n{\n  \"geoip_download_manager\" : {\n    \"database\" : {\n      \"ASN\" : {\n        \"status\" : \"up_to_date\",\n        \"fail_check_in_days\" : 0,\n        \"last_updated_at\": \"2021-06-21T16:06:54+02:00\"\n      },\n      \"City\" : {\n        \"status\" : \"up_to_date\",\n        \"fail_check_in_days\" : 0,\n        \"last_updated_at\": \"2021-06-21T16:06:54+02:00\"\n      }\n    },\n    \"download_stats\" : {\n      \"successes\" : 15,\n      \"failures\" : 1,\n      \"last_checked_at\" : \"2021-06-21T16:07:03+02:00\",\n      \"status\" : \"succeeded\"\n    }\n  }\n}\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/logstash-geoip-database-management.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 33]"
        },
        {
          "title": "Logstash Monitoring Ui",
          "description": null,
          "content": {
            "Monitoring UI [logstash-monitoring-ui]": "Use the {{stack}} {{monitor-features}} to view metrics and gain insight into how your {{ls}} deployment is running. In the overview dashboard, you can see all events received and sent by Logstash, plus info about memory usage and uptime:\n\n![Logstash monitoring overview dashboard in Kibana](images/overviewstats.png)\n\nThen you can drill down to see stats about a specific node:\n\n![Logstash monitoring node stats dashboard in Kibana](images/nodestats.png)\n\n::::{note}\nA {{ls}} node is considered unique based on its persistent UUID, which is written to the [`path.data`](/reference/logstash-settings-file.md) directory when the node starts.\n::::\n\nBefore you can use the monitoring UI, [configure Logstash monitoring](/reference/monitoring-logstash-legacy.md).\n\nFor information about using the Monitoring UI, see [{{monitoring}} in {{kib}}](docs-content://deploy-manage/monitor/monitoring-data/visualizing-monitoring-data.md)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/logstash-monitoring-ui.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 34]"
        },
        {
          "title": "Pipeline Viewer UI [logstash-pipeline-viewer]",
          "description": null,
          "content": {
            "Prerequisites [_prerequisites]": "Before using the pipeline viewer:\n\n['[Configure Logstash monitoring](monitoring-logstash.md).', 'Start the Logstash pipeline that you want to monitor.']\n\nLogstash begins shipping metrics to the monitoring cluster.",
            "View the pipeline [_view_the_pipeline]": "To view the pipeline:\n\n['Kibana → Monitoring → Logstash → Pipelines']\n\nEach pipeline is identified by a pipeline ID (`main` by default). For each pipeline, you see the pipeline’s throughput and the number of nodes on which the pipeline is running during the selected time range.\n\nMany elements in the tree are clickable. For example, you can click the plugin name to expand the detail view.\n\n% TO DO: Use `:class: screenshot`\n![Pipeline Input Detail](images/pipeline-input-detail.png)\n\nClick the arrow beside a branch name to collapse or expand it.",
            "Notes and best practices [_notes_and_best_practices]": "**Use semantic IDs.** Specify semantic IDs when you configure the stages in your Logstash pipeline. Otherwise, Logstash generates them for you. Semantic IDs help you identify configurations that are causing bottlenecks. For example, you may have several grok filters running in your pipeline. If you have specified semantic IDs, you can tell at a glance which filters are slow. Semantic IDs, such as `apacheParsingGrok` and `cloudwatchGrok`, point you to the grok filters that are causing bottlenecks.\n\n**Outliers.** Values and stats that are anomalously slow or otherwise out of line are highlighted. This doesn’t necessarily indicate a problem, but it highlights potential bottle necks so that you can find them quickly.\n\nSome plugins are slower than others due to the nature of the work they do. For instance, you may find that a grok filter that uses a complicated regexp runs a lot slower than a mutate filter that simply adds a field. The grok filter might be highlighted in this case, though it may not be possible to further optimize its work.\n\n**Versioning.** Version information is available from the dropdown list beside the pipeline ID. Logstash generates a new version each time you modify a pipeline, and stores multiple versions of the pipeline stats. Use this information to see how changes over time affect throughput and other metrics. Logstash does not store multiple versions of the pipeline configurations."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/logstash-pipeline-viewer.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 35]"
        },
        {
          "title": "Logstash Settings File",
          "description": null,
          "content": {
            "logstash.yml [logstash-settings-file]": "You can set options in the Logstash settings file, `logstash.yml`, to control Logstash execution. For example, you can specify pipeline settings, the location of configuration files, logging options, and other settings. Most of the settings in the `logstash.yml` file are also available as [command-line flags](/reference/running-logstash-command-line.md#command-line-flags) when you run Logstash. Any flags that you set at the command line override the corresponding settings in the `logstash.yml` file.\n\nThe `logstash.yml` file is written in [YAML](http://yaml.org/). Its location varies by platform (see [Logstash Directory Layout](/reference/dir-layout.md)). You can specify settings in hierarchical form or use flat keys. For example, to use hierarchical form to set the pipeline batch size and batch delay, you specify:\n\n```\npipeline:\n  batch:\n    size: 125\n    delay: 50\n```\n\nTo express the same values as flat keys, you specify:\n\n```\npipeline.batch.size: 125\npipeline.batch.delay: 50\n```\n\nThe `logstash.yml` file also supports bash-style interpolation of environment variables and keystore secrets in setting values.\n\n```\npipeline:\n  batch:\n    size: ${BATCH_SIZE}\n    delay: ${BATCH_DELAY:50}\nnode:\n  name: \"node_${LS_NODE_NAME}\"\npath:\n   queue: \"/tmp/${QUEUE_DIR:queue}\"\n```\n\nNote that the `${VAR_NAME:default_value}` notation is supported, setting a default batch delay of `50` and a default `path.queue` of `/tmp/queue` in the above example.\n\nThe `logstash.yml` file includes these settings.\n\n| Setting | Description | Default value |\n| --- | --- | --- |\n| `node.name` | A descriptive name for the node. | Machine’s hostname |\n| `path.data` | The directory that Logstash and its plugins use for any persistent needs. | `LOGSTASH_HOME/data` |\n| `pipeline.id` | The ID of the pipeline. | `main` |\n| `pipeline.workers` | The number of workers that will, in parallel, execute the filter and outputstages of the pipeline. This setting uses the[`java.lang.Runtime.getRuntime.availableProcessors`](https://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.md#availableProcessors())value as a default if not overridden by `pipeline.workers` in `pipelines.yml` or`pipeline.workers` from `logstash.yml`.  If you have modified this setting andsee that events are backing up, or that the CPU is not saturated, considerincreasing this number to better utilize machine processing power. | Number of the host’s CPU cores |\n| `pipeline.batch.size` | The maximum number of events an individual worker thread will collect from inputs  before attempting to execute its filters and outputs.  Larger batch sizes are generally more efficient, but come at the cost of increased memory  overhead. You may need to increase JVM heap space in the `jvm.options` config file.  See [Logstash Configuration Files](/reference/config-setting-files.md) for more info. | `125` |\n| `pipeline.batch.delay` | When creating pipeline event batches, how long in milliseconds to wait for  each event before dispatching an undersized batch to pipeline workers. | `50` |\n| `pipeline.unsafe_shutdown` | When set to `true`, forces Logstash to exit during shutdown even if there are still inflight events  in memory. By default, Logstash will refuse to quit until all received events  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown. | `false` |\n| `pipeline.plugin_classloaders` | (Beta) Load Java plugins in independent classloaders to isolate their dependencies. | `false` |\n| `pipeline.ordered` | Set the pipeline event ordering. Valid options are:<br><br>* `auto`. Automatically enables ordering if the `pipeline.workers` setting is `1`, and disables otherwise.<br>* `true`. Enforces ordering on the pipeline and prevents Logstash from starting if there are multiple workers.<br>* `false`. Disables the processing required to preserve order. Ordering will not be guaranteed, but you save the processing cost of preserving order.<br> | `auto` |\n| `pipeline.ecs_compatibility` | Sets the pipeline’s default value for `ecs_compatibility`, a setting that is available to plugins that implement an ECS compatibility mode for use with the Elastic Common Schema. Possible values are:<br><br>* `disabled`<br>* `v1`<br>* `v8`<br><br>This option allows the [early opt-in (or preemptive opt-out) of ECS compatibility](/reference/ecs-ls.md) modes in plugins, which is scheduled to be on-by-default in a future major release of {{ls}}.<br><br>Values other than `disabled` are currently considered BETA, and may produce unintended consequences when upgrading {{ls}}.<br> | `disabled` |\n| `path.config` | The path to the Logstash config for the main pipeline. If you specify a directory or wildcard,  config files are read from the directory in alphabetical order. | Platform-specific. See [Logstash Directory Layout](/reference/dir-layout.md). |\n| `config.string` | A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as  the config file. | *N/A* |\n| `config.test_and_exit` | When set to `true`, checks that the configuration is valid and then exits. Note that grok patterns are not checked for  correctness with this setting. Logstash can read multiple config files from a directory. If you combine this  setting with `log.level: debug`, Logstash will log the combined config file, annotating  each config block with the source file it came from. | `false` |\n| `config.reload.automatic` | When set to `true`, periodically checks if the configuration has changed and reloads the configuration whenever it is changed.  This can also be triggered manually through the SIGHUP signal. | `false` |\n| `config.reload.interval` | How often in seconds Logstash checks the config files for changes. Note that the unit qualifier (`s`) is required. | `3s` |\n| `config.debug` | When set to `true`, shows the fully compiled configuration as a debug log message. You must also set `log.level: debug`.  WARNING: The log message will include any *password* options passed to plugin configs as plaintext, and may result  in plaintext passwords appearing in your logs! | `false` |\n| `config.support_escapes` | When set to `true`, quoted strings will process the following escape sequences: `\\n` becomes a literal newline (ASCII 10). `\\r` becomes a literal carriage return (ASCII 13). `\\t` becomes a literal tab (ASCII 9). `\\\\` becomes a literal backslash `\\`. `\\\"` becomes a literal double quotation mark. `\\'` becomes a literal quotation mark. | `false` |\n| `config.field_reference.escape_style` | Provides a way to reference fields that contain [field reference special characters](https://www.elastic.co/guide/en/logstash/current/field-references-deepdive.html#formal-grammar-escape-sequences) `[` and `]`.<br><br>Note: This feature is in **technical preview** and may change in the future.<br><br>Current options are:<br><br>* `percent`: URI-style `%`+`HH` hexadecimal encoding of UTF-8 bytes (`[` → `%5B`; `]` → `%5D`)<br>* `ampersand`: HTML-style `&#`+`DD`+`;` encoding of decimal Unicode code-points (`[` → `&#91;`; `]` → `&#93;`)<br>* `none`: field names containing special characters *cannot* be referenced.<br> | `none` |\n| `queue.type` | The internal queuing model to use for event buffering. Specify `memory` for legacy in-memory based queuing, or `persisted` for disk-based ACKed queueing ([persistent queues](/reference/persistent-queues.md)). | `memory` |\n| `path.queue` | The directory path where the data files will be stored when persistent queues are enabled (`queue.type: persisted`). | `path.data/queue` |\n| `queue.page_capacity` | The size of the page data files used when persistent queues are enabled (`queue.type: persisted`). The queue data consists of append-only data files separated into pages. | 64mb |\n| `queue.max_events` | The maximum number of unread events in the queue when persistent queues are enabled (`queue.type: persisted`). | 0 (unlimited) |\n| `queue.max_bytes` | The total capacity of the queue (`queue.type: persisted`) in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both `queue.max_events` and `queue.max_bytes` are specified, Logstash uses whichever criteria is reached first. | 1024mb (1g) |\n| `queue.checkpoint.acks` | The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.acks: 0` to set this value to unlimited. | 1024 |\n| `queue.checkpoint.writes` | The maximum number of written events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.writes: 0` to set this value to unlimited. | 1024 |\n| `queue.checkpoint.retry` | When enabled, Logstash will retry four times per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on Windows platform, filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. (`queue.type: persisted`) | `true` |\n| `queue.drain` | When enabled, Logstash waits until the persistent queue (`queue.type: persisted`) is drained before shutting down. | `false` |\n| `dead_letter_queue.enable` | Flag to instruct Logstash to enable the DLQ feature supported by plugins. | `false` |\n| `dead_letter_queue.max_bytes` | The maximum size of each dead letter queue. Entries will be dropped if they  would increase the size of the dead letter queue beyond this setting. | `1024mb` |\n| `dead_letter_queue.storage_policy` | Defines the action to take when the dead_letter_queue.max_bytes setting is reached: `drop_newer` stops accepting new values that would push the file size over the limit, and `drop_older` removes the oldest events to make space for new ones. | `drop_newer` |\n| `path.dead_letter_queue` | The directory path where the data files will be stored for the dead-letter queue. | `path.data/dead_letter_queue` |\n| `api.enabled` | The HTTP API is enabled by default. It can be disabled, but features that rely on it will not work as intended. | `true` |\n| `api.environment` | The API returns the provided string as a part of its response. Setting your environment may help to disambiguate between similarly-named nodes in production vs test environments. | `production` |\n| `api.http.host` | The bind address for the HTTP API endpoint.  By default, the {{ls}} HTTP API binds only to the local loopback interface.  When configured securely (`api.ssl.enabled: true` and `api.auth.type: basic`), the HTTP API binds to *all* available interfaces. | `\"127.0.0.1\"` |\n| `api.http.port` | The bind port for the HTTP API endpoint. | `9600-9700` |\n| `api.ssl.enabled` | Set to `true` to enable SSL on the HTTP API.  Doing so requires both `api.ssl.keystore.path` and `api.ssl.keystore.password` to be set. | `false` |\n| `api.ssl.keystore.path` | The path to a valid JKS or PKCS12 keystore for use in securing the {{ls}} API.  The keystore must be password-protected, and must contain a single certificate chain and a private key.  This setting is ignored unless `api.ssl.enabled` is set to `true`. | *N/A* |\n| `api.ssl.keystore.password` | The password to the keystore provided with `api.ssl.keystore.path`.  This setting is ignored unless `api.ssl.enabled` is set to `true`. | *N/A* |\n| `api.ssl.supported_protocols` | List of allowed SSL/TLS versions to use when establishing a secure connection. The availability of protocols depends on the JVM version. Certain protocols are disabled by default and need to be enabled manually by changing `jdk.tls.disabledAlgorithms` in the **$JDK_HOME/conf/security/java.security** configuration file. Possible values are:<br><br>* `TLSv1`<br>* `TLSv1.1`<br>* `TLSv1.2`<br>* `TLSv1.3`<br> | *N/A* |\n| `api.auth.type` | Set to `basic` to require HTTP Basic auth on the API using the credentials supplied with `api.auth.basic.username` and `api.auth.basic.password`. | `none` |\n| `api.auth.basic.username` | The username to require for HTTP Basic auth  Ignored unless `api.auth.type` is set to `basic`. | *N/A* |\n| `api.auth.basic.password` | The password to require for HTTP Basic auth. Ignored unless `api.auth.type` is set to `basic`. It should meet default password policy which requires non-empty minimum 8 char string that includes a digit, upper case letter and lower case letter. The default password policy can be customized by following options:<br><br>* Set `api.auth.basic.password_policy.include.digit` `REQUIRED` (default) to accept only passwords that include at least one digit or `OPTIONAL` to exclude from requirement.<br>* Set `api.auth.basic.password_policy.include.upper` `REQUIRED` (default) to accept only passwords that include at least one upper case letter or `OPTIONAL` to exclude from requirement.<br>* Set `api.auth.basic.password_policy.include.lower` `REQUIRED` (default) to accept only passwords that include at least one lower case letter or `OPTIONAL` to exclude from requirement.<br>* Set `api.auth.basic.password_policy.include.symbol` `REQUIRED` to accept only passwords that include at least one special character or `OPTIONAL` (default) to exclude from requirement.<br>* Set `api.auth.basic.password_policy.length.minimum` to a value from 9 to 1024 if you want to require more than the 8 character default setting for passwords.<br> | *N/A* |\n| `api.auth.basic.password_policy.mode` | Raises either `WARN` or `ERROR` message when password requirements are not met.Ignored unless `api.auth.type` is set to `basic`. | `WARN` |\n| `log.level` | The log level. Valid options are:<br><br>* `fatal`<br>* `error`<br>* `warn`<br>* `info`<br>* `debug`<br>* `trace`<br> | `info` |\n| `log.format` | The log format. Set to `json` to log in JSON format, or `plain` to use `Object#.inspect`. | `plain` |\n| `log.format.json.fix_duplicate_message_fields` | When the log format is `json` avoid collision of field names in log lines. | `true` |\n| `path.logs` | The directory where Logstash will write its log to. | `LOGSTASH_HOME/logs` |\n| `pipeline.separate_logs` | This a boolean setting to enable separation of logs per pipeline in different log files. If enabled Logstash will create a different log file for each pipeline,using the pipeline.id as name of the file. The destination directory is taken from the `path.log`s setting. When there are many pipelines configured in Logstash,separating each log lines per pipeline could be helpful in case you need to troubleshoot what’s happening in a single pipeline, without interference of the other ones. | `false` |\n| `path.plugins` | Where to find custom plugins. You can specify this setting multiple times to include  multiple paths. Plugins are expected to be in a specific directory hierarchy:  `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`,  and `NAME` is the name of the plugin. | Platform-specific. See [Logstash Directory Layout](/reference/dir-layout.md). |\n| `allow_superuser` | Setting to `true` to allow or `false` to block running Logstash as a superuser. | `false` |\n| `pipeline.buffer.type` | Determine where to allocate memory buffers, for plugins that leverage them.Defaults to `heap` but can be switched to `direct` to instruct Logstash to prefer allocation of buffers in direct memory. | `heap` Check out [Buffer Allocation types](/reference/jvm-settings.md#off-heap-buffers-allocation) for more info. |"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 36]"
        },
        {
          "title": "Logstash To Logstash Communications",
          "description": null,
          "content": {
            "Logstash-to-Logstash communications [ls-to-ls]": "{{ls}}-to-{{ls}} communication is available if you need to have one {{ls}} instance communicate with another {{ls}} instance. Implementing Logstash-to-Logstash communication can add complexity to your environment, but you may need it if the data path crosses network or firewall boundaries. However, we suggest you don’t implement unless it is strictly required.\n\n::::{note}\nIf you are looking for information on connecting multiple pipelines within one Logstash instance, see [Pipeline-to-pipeline communication](/reference/pipeline-to-pipeline.md).\n::::\n\nLogstash-to-Logstash communication can be achieved in one of two ways:\n\n['[Logstash output to Logstash Input](#native-considerations)', '[Lumberjack output to Beats input](#lumberjack-considerations)']\n\n$$$native-considerations$$$**Logstash to Logstash considerations**\n\nThis is the preferred method to implement Logstash-to-Logstash. It replaces [Logstash-to-Logstash: HTTP output to HTTP input](/reference/ls-to-ls-http.md) and has these considerations:\n\n['It relies on HTTP as the communication protocol between the Input and Output.', 'It supports multiple hosts, providing high availability by load balancing equally amongst the multiple destination hosts.', 'No connection information is added to events.']\n\nReady to see more configuration details? See [Logstash-to-Logstash: Output to Input](/reference/ls-to-ls-native.md).\n\n$$$lumberjack-considerations$$$**Lumberjack-Beats considerations**\n\nLumberjack output to Beats input has been our standard approach for {{ls}}-to-{{ls}} communication, but our recommended approach is now [Logstash-to-Logstash: Output to Input](/reference/ls-to-ls-native.md). Before you implement the Lumberjack to Beats configuration, keep these points in mind:\n\n['Lumberjack to Beats provides high availability, but does not provide load balancing. The Lumberjack output plugin allows defining multiple output hosts for high availability, but instead of load-balancing between all output hosts, it falls back to one host on the list in the case of failure.', 'If you need a proxy between the Logstash instances, TCP proxy is the only option.', 'There’s no explicit way to exert back pressure back to the beats input.']\n\nReady to see more configuration details? See [Logstash-to-Logstash: Lumberjack output to Beats input](/reference/ls-to-ls-lumberjack.md)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/ls-to-ls.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 37]"
        },
        {
          "title": "Enriching Data with Lookups [lookup-enrichment]",
          "description": null,
          "content": {
            "Lookup plugins [lookup-plugins]": "$$$dns-def$$$dns filter\n:   The [dns filter plugin](logstash-docs-md://lsr/plugins-filters-dns.md) performs a standard or reverse DNS lookup.\n\nThe following config performs a reverse lookup on the address in the `source_host` field and replaces it with the domain name:\n\n```json\nfilter {\n  dns {\n    reverse => [ \"source_host\" ]\n    action => \"replace\"\n  }\n}\n```\n\n\n\n$$$es-def$$$elasticsearch filter\n:   The [elasticsearch filter](logstash-docs-md://lsr/plugins-filters-elasticsearch.md) copies fields from previous log events in Elasticsearch to current events.\n\nThe following config shows a complete example of how this filter might be used.  Whenever Logstash receives an \"end\" event, it uses this Elasticsearch filter to find the matching \"start\" event based on some operation identifier. Then it copies the `@timestamp` field from the \"start\" event into a new field on the \"end\" event.  Finally, using a combination of the date filter and the ruby filter, the code in the example calculates the time duration in hours between the two events.\n\n```json\n      if [type] == \"end\" {\n         elasticsearch {\n            hosts => [\"es-server\"]\n            query => \"type:start AND operation:%{[opid]}\"\n            fields => { \"@timestamp\" => \"started\" }\n         }\n         date {\n            match => [\"[started]\", \"ISO8601\"]\n            target => \"[started]\"\n         }\n         ruby {\n            code => 'event.set(\"duration_hrs\", (event.get(\"@timestamp\") - event.get(\"started\")) / 3600) rescue nil'\n        }\n      }\n```\n\n\n\n$$$geoip-def$$$geoip filter\n:   The [geoip filter](logstash-docs-md://lsr/plugins-filters-geoip.md) adds geographical information about the location of IP addresses. For example:\n\n```json\nfilter {\n  geoip {\n    source => \"clientip\"\n  }\n}\n```\n\nAfter the geoip filter is applied, the event will be enriched with geoip fields. For example:\n\n```json\nfilter {\n  geoip {\n    source => \"clientip\"\n  }\n}\n```\n\n\n\n$$$http-def$$$http filter\n:   The [http filter](logstash-docs-md://lsr/plugins-filters-http.md) integrates with external web services/REST APIs, and enables lookup enrichment against any HTTP service or endpoint. This plugin is well suited for many enrichment use cases, such as social APIs, sentiment APIs, security feed APIs, and business service APIs.\n\n$$$jdbc-static-def$$$jdbc_static filter\n:   The [jdbc_static filter](logstash-docs-md://lsr/plugins-filters-jdbc_static.md) enriches events with data pre-loaded from a remote database.\n\nThe following example fetches data from a remote database, caches it in a local database, and uses lookups to enrich events with data cached in the local database.\n\n```json\nfilter {\n  jdbc_static {\n    loaders => [ <1>\n      {\n        id => \"remote-servers\"\n        query => \"select ip, descr from ref.local_ips order by ip\"\n        local_table => \"servers\"\n      },\n      {\n        id => \"remote-users\"\n        query => \"select firstname, lastname, userid from ref.local_users order by userid\"\n        local_table => \"users\"\n      }\n    ]\n    local_db_objects => [ <2>\n      {\n        name => \"servers\"\n        index_columns => [\"ip\"]\n        columns => [\n          [\"ip\", \"varchar(15)\"],\n          [\"descr\", \"varchar(255)\"]\n        ]\n      },\n      {\n        name => \"users\"\n        index_columns => [\"userid\"]\n        columns => [\n          [\"firstname\", \"varchar(255)\"],\n          [\"lastname\", \"varchar(255)\"],\n          [\"userid\", \"int\"]\n        ]\n      }\n    ]\n    local_lookups => [ <3>\n      {\n        id => \"local-servers\"\n        query => \"select descr as description from servers WHERE ip = :ip\"\n        parameters => {ip => \"[from_ip]\"}\n        target => \"server\"\n      },\n      {\n        id => \"local-users\"\n        query => \"select firstname, lastname from users WHERE userid = :id\"\n        parameters => {id => \"[loggedin_userid]\"}\n        target => \"user\" <4>\n      }\n    ]\n    # using add_field here to add & rename values to the event root\n    add_field => { server_name => \"%{[server][0][description]}\" }\n    add_field => { user_firstname => \"%{[user][0][firstname]}\" } <5>\n    add_field => { user_lastname => \"%{[user][0][lastname]}\" }\n    remove_field => [\"server\", \"user\"]\n    jdbc_user => \"logstash\"\n    jdbc_password => \"example\"\n    jdbc_driver_class => \"org.postgresql.Driver\"\n    jdbc_driver_library => \"/tmp/logstash/vendor/postgresql-42.1.4.jar\"\n    jdbc_connection_string => \"jdbc:postgresql://remotedb:5432/ls_test_2\"\n  }\n}\n```\n\n1. Queries an external database to fetch the dataset that will be cached locally.\n2. Defines the columns, types, and indexes used to build the local database structure. The column names and types should match the external database.\n3. Performs lookup queries on the local database to enrich the events.\n4. Specifies the event field that will store the looked-up data. If the lookup returns multiple columns, the data is stored as a JSON object within the field.\n5. Takes data from the JSON object and stores it in top-level event fields for easier analysis in Kibana.\n\n\n\n$$$jdbc-stream-def$$$jdbc_streaming filter\n:   The [jdbc_streaming filter](logstash-docs-md://lsr/plugins-filters-jdbc_streaming.md) enriches events with database data.\n\nThe following example executes a SQL query and stores the result set in a field called `country_details`:\n\n```json\nfilter {\n  jdbc_streaming {\n    jdbc_driver_library => \"/path/to/mysql-connector-java-5.1.34-bin.jar\"\n    jdbc_driver_class => \"com.mysql.jdbc.Driver\"\n    jdbc_connection_string => \"jdbc:mysql://localhost:3306/mydatabase\"\n    jdbc_user => \"me\"\n    jdbc_password => \"secret\"\n    statement => \"select * from WORLD.COUNTRY WHERE Code = :code\"\n    parameters => { \"code\" => \"country_code\"}\n    target => \"country_details\"\n  }\n}\n```\n\n\n\n$$$memcached-def$$$memcached filter\n:   The [memcached filter](logstash-docs-md://lsr/plugins-filters-memcached.md) enables key/value lookup enrichment against a Memcached object caching system. It supports both read (GET) and write (SET) operations. It is a notable addition for security analytics use cases.\n\n$$$translate-def$$$translate filter\n:   The [translate filter](logstash-docs-md://lsr/plugins-filters-translate.md) replaces field contents based on replacement values specified in a hash or file. Currently supports these file types: YAML, JSON, and CSV.\n\nThe following example takes the value of the `response_code` field, translates it to a description based on the values specified in the dictionary, and then removes the `response_code` field from the event:\n\n```json\nfilter {\n  translate {\n    field => \"response_code\"\n    destination => \"http_response\"\n    dictionary => {\n      \"200\" => \"OK\"\n      \"403\" => \"Forbidden\"\n      \"404\" => \"Not Found\"\n      \"408\" => \"Request Timeout\"\n    }\n    remove_field => \"response_code\"\n  }\n}\n```\n\n\n\n$$$useragent-def$$$useragent filter\n:   The [useragent filter](logstash-docs-md://lsr/plugins-filters-useragent.md) parses user agent strings into fields.\n\nThe following example takes the user agent string in the `agent` field, parses it into user agent fields, and adds the user agent fields to a new field called `user_agent`. It also removes the original `agent` field:\n\n```json\nfilter {\n  useragent {\n    source => \"agent\"\n    target => \"user_agent\"\n    remove_field => \"agent\"\n  }\n}\n```\n\nAfter the filter is applied, the event will be enriched with user agent fields. For example:\n\n```json\n        \"user_agent\": {\n          \"os\": \"Mac OS X 10.12\",\n          \"major\": \"50\",\n          \"minor\": \"0\",\n          \"os_minor\": \"12\",\n          \"os_major\": \"10\",\n          \"name\": \"Firefox\",\n          \"os_name\": \"Mac OS X\",\n          \"device\": \"Other\"\n        }\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 38]"
        },
        {
          "title": "Logstash-to-Logstash: HTTP output to HTTP input [ls-to-ls-http]",
          "description": null,
          "content": {
            "Configuration overview [overview-http-http]": {
              "Configure the downstream Logstash to use HTTP input [configure-downstream-logstash-http-input]": "Configure the HTTP input on the downstream (receiving) Logstash to receive connections. The minimum configuration requires these options:\n\n['`port` - To set a custom port.', '`additional_codecs` - To set `application/json` to be `json_lines`.']\n\n```\ninput {\n    http {\n        port => 8080\n        additional_codecs => { \"application/json\" => \"json_lines\" }\n    }\n}\n```",
              "Configure the upstream Logstash to use HTTP output [configure-upstream-logstash-http-output]": "In order to obtain the best performance when sending data from one Logstash to another, the data needs to be batched and compressed. As such, the upstream Logstash (the sending Logstash) needs to be configured with these options:\n\n['`url` - The receiving Logstash.', '`http_method` - Set to `post`.', '`retry_non_idempotent` - Set to `true`, in order to retry failed events.', '`format` - Set to `json_batch` to batch the data.', '`http_compression` - Set to `true` to ensure the data is compressed.']\n\n```\noutput {\n    http {\n        url => '<protocol>://<downstream-logstash>:<port>'\n        http_method => post\n        retry_non_idempotent => true\n        format => json_batch\n        http_compression => true\n    }\n}\n```",
              "Secure Logstash to Logstash [securing-logstash-to-logstash-http]": "It is important that you secure the communication between Logstash instances. Use SSL/TLS mutual authentication in order to ensure that the upstream Logstash instance sends encrypted data to a trusted downstream Logstash instance, and vice versa.\n\n['Create a certificate authority (CA) in order to sign the certificates that you plan to use between Logstash instances. Creating a correct SSL/TLS infrastructure is outside the scope of this document.', '::::{tip}\\nWe recommend you use the [elasticsearch-certutil](elasticsearch://reference/elasticsearch/command-line-tools/certutil.md) tool to generate your certificates.\\n::::', 'Configure the downstream (receiving) Logstash to use SSL. Add these settings to the HTTP Input configuration:', ['`ssl`: When set to `true`, it enables Logstash use of SSL/TLS', '`ssl_key`: Specifies the key that Logstash uses to authenticate with the client.', '`ssl_certificate`: Specifies the certificate that Logstash uses to authenticate with the client.', '`ssl_certificate_authorities`: Configures Logstash to trust any certificates signed by the specified CA.', '`ssl_verify_mode`:  Specifies whether Logstash server verifies the client certificate against the CA.'], 'For example:', '```\\ninput {\\n  http {\\n    ...\\n\\n    ssl => true\\n    ssl_key => \"server.key.pk8\"\\n    ssl_certificate => \"server.crt\"\\n    ssl_certificate_authorities => \"ca.crt\"\\n    ssl_verify_mode => force_peer\\n  }\\n}\\n```', 'Configure the upstream (sending) Logstash to use SSL. Add these settings to the HTTP output configuration:', ['`cacert`: Configures the Logstash client to trust any certificates signed by the specified CA.', '`client_key`: Specifies the key the Logstash client uses to authenticate with the Logstash server.', '`client_cert`: Specifies the certificate that the Logstash client uses to authenticate to the Logstash server.'], 'For example:', '```\\noutput {\\n  http {\\n    ...\\n\\n    cacert => \"ca.crt\"\\n    client_key => \"client.key.pk8\"\\n    client_cert => \"client.crt\"\\n  }\\n}\\n```', 'If you would like an additional authentication step, you can also use basic user/password authentication in both Logstash instances:', ['`user`: Sets the username to use for authentication.', '`password`: Sets the password to use for authentication.'], 'For example, you would need to add the following to both Logstash instances:', '```\\n...\\n  http {\\n    ...\\n\\n    user => \"your-user\"\\n    password => \"your-secret\"\\n  }\\n...\\n```']"
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/ls-to-ls-http.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 39]"
        },
        {
          "title": "Logstash-to-Logstash: Lumberjack output to Beats input [ls-to-ls-lumberjack]",
          "description": null,
          "content": {
            "Configuration overview [_configuration_overview]": {
              "Generate a self-signed SSL certificate and key [generate-self-signed-cert]": "Use the `openssl req` command to generate a self-signed certificate and key. The `openssl req` command is available with some operating systems. You may need to install the openssl command line program for others.\n\nRun the following command:\n\n```\nopenssl req -x509 -batch -nodes -newkey rsa:2048 -keyout lumberjack.key -out lumberjack.cert -subj /CN=localhost\n```\n\nwhere:\n\n['`lumberjack.key` is the name of the SSL key to be created', '`lumberjack.cert` is the name of the SSL certificate to be created', '`localhost` is the name of the upstream Logstash computer']\n\nThis command produces output similar to the following:\n\n```\nGenerating a 2048 bit RSA private key\n.................................+++\n....................+++\nwriting new private key to 'lumberjack.key'\n```",
              "Copy the SSL certificate and key [copy-cert-key]": "Copy the SSL certificate to the upstream Logstash machine.\n\nCopy the SSL certificate and key to the downstream Logstash machine.",
              "Start the upstream Logstash instance [save-cert-ls1]": "Start Logstash and generate test events:\n\n```\nbin/logstash -e 'input { generator { count => 5 } } output { lumberjack { codec => json hosts => \"mydownstreamhost\" ssl_certificate => \"lumberjack.cert\" port => 5000 } }'\n```\n\nThis sample command sends five events to mydownstreamhost:5000 using the SSL certificate provided.",
              "Start the downstream Logstash instance [save-cert-ls2]": "Start the downstream instance of Logstash:\n\n```\nbin/logstash -e 'input { beats { codec => json port => 5000 ssl_enabled => true ssl_certificate => \"lumberjack.cert\" ssl_key => \"lumberjack.key\"} }'\n```\n\nThis sample command sets port 5000 to listen for incoming Beats input.",
              "Verify the communication [test-ls-to-ls]": "Watch the downstream Logstash machine for the incoming events. You should see five incrementing events similar to the following:\n\n```\n{\n  \"@timestamp\" => 2018-02-07T12:16:39.415Z,\n  \"sequence\"   => 0\n  \"tags\"       => [\n    [0] \"beats_input_codec_json_applied\"\n  ],\n  \"message\"    => \"Hello world\",\n  \"@version\"   => \"1\",\n  \"host\"       => \"ls1.semicomplete.com\"\n}\n```\n\nIf you see all five events with consistent fields and formatting, incrementing by one, then your configuration is correct."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/ls-to-ls-lumberjack.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 40]"
        },
        {
          "title": "Logstash-to-Logstash: Output to Input [ls-to-ls-native]",
          "description": null,
          "content": {
            "Configuration overview [overview-ls-ls]": {
              "Configure the downstream Logstash to use Logstash input [configure-downstream-logstash-input]": "Configure the Logstash input on the downstream (receiving) Logstash to receive connections. The minimum configuration requires this option:\n\n['`port` - To set a custom port. The default is 9800 if none is provided.']\n\n```\ninput {\n    logstash {\n        port => 9800\n    }\n}\n```",
              "Configure the upstream Logstash to use Logstash output [configure-upstream-logstash-output]": "In order to obtain the best performance when sending data from one Logstash to another, the data is batched and compressed. As such, the upstream Logstash (the sending Logstash) only needs to be concerned about configuring the receiving endpoint with these options:\n\n['`hosts` - The receiving one or more Logstash host and port pairs. If no port specified, 9800 will be used.']\n\n::::{note}\n{{ls}} load balances batched events to *all* of its configured downstream hosts. Any failures caused by network issues, back-pressure or other conditions, will result in the downstream host being isolated from load balancing for at least 60 seconds.\n::::\n\n```\noutput {\n    logstash {\n        hosts => [\"10.0.0.123\", \"10.0.1.123:9800\"]\n    }\n}\n```",
              "Secure Logstash to Logstash [securing-logstash-to-logstash]": "It is important that you secure the communication between Logstash instances. Use SSL/TLS mutual authentication in order to ensure that the upstream Logstash instance sends encrypted data to a trusted downstream Logstash instance, and vice versa.\n\n['Create a certificate authority (CA) in order to sign the certificates that you plan to use between Logstash instances. Creating a correct SSL/TLS infrastructure is outside the scope of this document.', '::::{tip}\\nWe recommend you use the [elasticsearch-certutil](elasticsearch://reference/elasticsearch/command-line-tools/certutil.md) tool to generate your certificates.\\n::::', 'Configure the downstream (receiving) Logstash to use SSL. Add these settings to the Logstash input configuration:', ['`ssl_enabled`: When set to `true`, it enables Logstash use of SSL/TLS', '`ssl_key`: Specifies the key that Logstash uses to authenticate with the client.', '`ssl_certificate`: Specifies the certificate that Logstash uses to authenticate with the client.', '`ssl_certificate_authorities`: Configures Logstash to trust any certificates signed by the specified CA.', '`ssl_client_authentication`: Specifies whether Logstash server verifies the client certificate against the CA.'], 'For example:', '```\\ninput {\\n  logstash {\\n    ...\\n\\n    ssl_enabled => true\\n    ssl_key => \"server.pkcs8.key\"\\n    ssl_certificate => \"server.crt\"\\n    ssl_certificate_authorities => \"ca.crt\"\\n    ssl_client_authentication => required\\n  }\\n}\\n```', 'Configure the upstream (sending) Logstash to use SSL. Add these settings to the Logstash output configuration:', ['`ssl_key`: Specifies the key the Logstash client uses to authenticate with the Logstash server.', '`ssl_certificate`: Specifies the certificate that the Logstash client uses to authenticate to the Logstash server.', '`ssl_certificate_authorities`: Configures the Logstash client to trust any certificates signed by the specified CA.'], 'For example:', '```\\noutput {\\n  logstash {\\n    ...\\n\\n    ssl_enabled => true\\n    ssl_key => \"client.pkcs8.key\"\\n    ssl_certificate => \"client.crt\"\\n    ssl_certificate_authorities => \"ca.crt\"\\n  }\\n}\\n```', 'If you would like an additional authentication step, you can also use basic user/password authentication in both Logstash instances:', ['`username`: Sets the username to use for authentication.', '`password`: Sets the password to use for authentication.'], 'For example, you would need to add the following to both Logstash instances:', '```\\n...\\n  logstash {\\n    ...\\n\\n    username => \"your-user\"\\n    password => \"your-secret\"\\n  }\\n...\\n```']"
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/ls-to-ls-native.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 41]"
        },
        {
          "title": "Managing Geoip Databases",
          "description": null,
          "content": {
            "Managing GeoIP databases [geoip-database-management]": "Logstash provides GeoIP database management features to make it easier for you to use plugins that require an up-to-date database to enrich events with geographic data.\n\n['[Feature Overview](/reference/logstash-geoip-database-management.md)', '[Configuration Guide](/reference/configuring-geoip-database-management.md)']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/geoip-database-management.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 42]"
        },
        {
          "title": "Managing Logstash",
          "description": null,
          "content": {
            "Managing Logstash [config-management]": "Logstash provides configuration management features to make it easier for you to manage updates to your configuration over time.\n\nThe topics in this section describe Logstash configuration management features only. For information about other config management tools, such as Puppet and Chef, see the documentation for those projects."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/config-management.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 43]"
        },
        {
          "title": "Memory queue [memory-queue]",
          "description": null,
          "content": {
            "Benefits of memory queues [mem-queue-benefits]": "The memory queue might be a good choice if you value throughput over data resiliency.\n\n['Easier configuration', 'Easier management and administration', 'Faster throughput']",
            "Limitations of memory queues [mem-queue-limitations]": [
              "Can lose data in abnormal termination",
              "Don’t do well handling sudden bursts of data, where extra capacity in needed for {{ls}} to catch up"
            ],
            "Memory queue size [sizing-mem-queue]": {
              "Settings that affect queue size [mq-settings]": "These values can be configured in `logstash.yml` and `pipelines.yml`.\n\npipeline.batch.size\n:   Number events to retrieve from inputs before sending to filters+workers The default is 125.\n\npipelines.workers\n:   Number of workers that will, in parallel, execute the filters+outputs stage of the pipeline. This value defaults to the number of the host’s CPU cores."
            },
            "Back pressure [backpressure-mem-queue]": "When the queue is full, Logstash puts back pressure on the inputs to stall data flowing into Logstash. This mechanism helps Logstash control the rate of data flow at the input stage without overwhelming outputs like Elasticsearch.\n\nEach input handles back pressure independently."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/memory-queue.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 44]"
        },
        {
          "title": "Collect {{ls}} monitoring data using legacy collectors [monitoring-internal-collection-legacy]",
          "description": null,
          "content": {
            "Components for legacy collection [_components_for_legacy_collection]": {
              "Output [logstash-monitoring-output-legacy]": {
                "Default Configuration [logstash-monitoring-default-legacy]": "If a Logstash node does not explicitly define a monitoring output setting, the following default configuration is used:\n\n```\nxpack.monitoring.elasticsearch.hosts: [ \"http://localhost:9200\" ]\n```\n\nAll data produced by monitoring for Logstash is indexed in the monitoring cluster by using the `.monitoring-logstash` template, which is managed by the [exporters](docs-content://deploy-manage/monitor/stack-monitoring/es-monitoring-exporters.md) within {{es}}.\n\nIf you are working with a cluster that has {{security}} enabled, extra steps are necessary to properly configure Logstash. For more information, see [*Monitoring {{ls}} (legacy)*](/reference/monitoring-logstash-legacy.md).\n\n::::{important}\nWhen discussing security relative to the `elasticsearch` output, it is critical to remember that all users are managed on the production cluster, which is identified in the `xpack.monitoring.elasticsearch.hosts` setting. This is particularly important to remember when you move from development environments to production environments, where you often have dedicated monitoring clusters.\n::::\n\nFor more information about the configuration options for the output, see [Monitoring Settings](#monitoring-settings-legacy)."
              }
            },
            "Configure {{ls}} monitoring with legacy collectors [configure-internal-collectors-legacy]": "To monitor Logstash nodes:\n\n['Specify where to send monitoring data. This cluster is often referred to as the *production cluster*. For examples of typical monitoring architectures, see [How monitoring works](docs-content://deploy-manage/monitor/stack-monitoring.md).', '::::{important}\\nTo visualize Logstash as part of the Elastic Stack (as shown in Step 6), send metrics to your *production* cluster. Sending metrics to a dedicated monitoring cluster will show the Logstash metrics under the *monitoring* cluster.\\n::::', 'Verify that the `xpack.monitoring.allow_legacy_collection` and `xpack.monitoring.collection.enabled` settings are `true` on the production cluster. If that setting is `false`, the collection of monitoring data is disabled in {{es}} and data is ignored from all other sources.', 'Configure your Logstash nodes to send metrics by setting `xpack.monitoring.enabled` to `true` and specifying the destination {{es}} node(s) as `xpack.monitoring.elasticsearch.hosts` in `logstash.yml`. If {{security-features}} are enabled, you also need to specify the credentials for the [built-in `logstash_system` user](docs-content://deploy-manage/users-roles/cluster-or-deployment-auth/built-in-users.md). For more information about these settings, see [Monitoring Settings](#monitoring-settings-legacy).', '```\\nxpack.monitoring.allow_legacy_collection: true\\nxpack.monitoring.enabled: true\\nxpack.monitoring.elasticsearch.hosts: [\"http://es-prod-node-1:9200\", \"http://es-prod-node-2:9200\"] <1>\\nxpack.monitoring.elasticsearch.username: \"logstash_system\"\\nxpack.monitoring.elasticsearch.password: \"changeme\"\\n```', ['If SSL/TLS is enabled on the production cluster, you must connect through HTTPS. As of v5.2.1, you can specify multiple Elasticsearch hosts as an array as well as specifying a single host as a string. If multiple URLs are specified, Logstash can round-robin requests to these production nodes.'], 'If SSL/TLS is enabled on the production {{es}} cluster, specify the trusted CA certificates that will be used to verify the identity of the nodes in the cluster.', 'To add a CA certificate to a Logstash node’s trusted certificates, you can specify the location of the PEM encoded certificate with the `certificate_authority` setting:', '```\\nxpack.monitoring.elasticsearch.ssl.certificate_authority: /path/to/ca.crt\\n```', 'To add a CA without having it loaded on disk, you can specify a hex-encoded SHA 256 fingerprint of the DER-formatted CA with the `ca_trusted_fingerprint` setting:', '```\\nxpack.monitoring.elasticsearch.ssl.ca_trusted_fingerprint: 2cfe62e474fb381cc7773c84044c28c9785ac5d1940325f942a3d736508de640\\n```', '::::{note}\\nA self-secured Elasticsearch cluster will provide the fingerprint of its CA to the console during setup.', 'You can also get the SHA256 fingerprint of an Elasticsearch’s CA using the `openssl` command-line utility on the Elasticsearch host:', '```\\nopenssl x509 -fingerprint -sha256 -in $ES_HOME/config/certs/http_ca.crt\\n```', '::::']\n\nAlternatively, you can configure trusted certificates using a truststore (a Java Keystore file that contains the certificates):\n\n```yaml\nxpack.monitoring.elasticsearch.ssl.truststore.path: /path/to/file\nxpack.monitoring.elasticsearch.ssl.truststore.password: password\n```\n\nAlso, optionally, you can set up client certificate using a keystore (a Java Keystore file that contains the certificate) or using a certificate and key file pair:\n\n```yaml\nxpack.monitoring.elasticsearch.ssl.keystore.path: /path/to/file\nxpack.monitoring.elasticsearch.ssl.keystore.password: password\n```\n\n```yaml\nxpack.monitoring.elasticsearch.ssl.certificate: /path/to/certificate\nxpack.monitoring.elasticsearch.ssl.key: /path/to/key\n```\n\nSet sniffing to `true` to enable discovery of other nodes of the {{es}} cluster. It defaults to `false`.\n\n```yaml\nxpack.monitoring.elasticsearch.sniffing: false\n```\n\n\n['Restart your Logstash nodes.', 'To verify your monitoring configuration, point your web browser at your {{kib}} host, and select **Stack Monitoring** from the side navigation. If this is an initial setup, select **set up with self monitoring** and click **Turn on monitoring**. Metrics reported from your Logstash nodes should be visible in the Logstash section. When security is enabled, to view the monitoring dashboards you must log in to {{kib}} as a user who has the `kibana_user` and `monitoring_user` roles.', '![Monitoring](images/monitoring-ui.png)']",
            "Monitoring settings for legacy collection [monitoring-settings-legacy]": {
              "General monitoring settings [monitoring-general-settings-legacy]": "`xpack.monitoring.enabled`\n:   Monitoring is disabled by default. Set to `true` to enable {{xpack}} monitoring.\n\n`xpack.monitoring.elasticsearch.hosts`\n:   The {{es}} instances that you want to ship your Logstash metrics to. This might be the same {{es}} instance specified in the `outputs` section in your Logstash configuration, or a different one. This is **not** the URL of your dedicated monitoring cluster. Even if you are using a dedicated monitoring cluster, the Logstash metrics must be routed through your production cluster. You can specify a single host as a string, or specify multiple hosts as an array. Defaults to `http://localhost:9200`.\n\n::::{note}\nIf your Elasticsearch cluster is configured with dedicated master-eligible nodes, Logstash metrics should *not* be routed to these nodes, as doing so can create resource contention and impact the stability of the Elasticsearch cluster. Therefore, do not include such nodes in `xpack.monitoring.elasticsearch.hosts`.\n::::\n\n`xpack.monitoring.elasticsearch.proxy`\n:   The monitoring {{es}} instance and monitored Logstash can be separated by a proxy. To enable Logstash to connect to a proxied {{es}}, set this value to the URI of the intermediate proxy using the standard URI format, `<protocol>://<host>` for example `http://192.168.1.1`. An empty string is treated as if proxy was not set.\n\n`xpack.monitoring.elasticsearch.username` and `xpack.monitoring.elasticsearch.password`\n:   If your {{es}} is protected with basic authentication, these settings provide the username and password that the Logstash instance uses to authenticate for shipping monitoring data.",
              "Monitoring collection settings [monitoring-collection-settings-legacy]": "`xpack.monitoring.collection.interval`\n:   Controls how often data samples are collected and shipped on the Logstash side. Defaults to `10s`. If you modify the collection interval, set the `xpack.monitoring.min_interval_seconds` option in `kibana.yml` to the same value.",
              "Monitoring TLS/SSL settings [monitoring-ssl-settings-legacy]": "You can configure the following Transport Layer Security (TLS) or Secure Sockets Layer (SSL) settings. For more information, see [Configuring credentials for {{ls}} monitoring](/reference/secure-connection.md#ls-monitoring-user).\n\n`xpack.monitoring.elasticsearch.ssl.ca_trusted_fingerprint`\n:   Optional setting that enables you to specify the hex-encoded SHA-256 fingerprint of the certificate authority for your {{es}} instance.\n\n::::{note}\nA self-secured Elasticsearch cluster will provide the fingerprint of its CA to the console during setup.\n\nYou can also get the SHA256 fingerprint of an Elasticsearch’s CA using the `openssl` command-line utility on the Elasticsearch host:\n\n```\nopenssl x509 -fingerprint -sha256 -in $ES_HOME/config/certs/http_ca.crt\n```\n\n::::\n\n`xpack.monitoring.elasticsearch.ssl.certificate_authority`\n:   Optional setting that enables you to specify a path to the `.pem` file for the certificate authority for your {{es}} instance.\n\n`xpack.monitoring.elasticsearch.ssl.truststore.path`\n:   Optional settings that provide the paths to the Java keystore (JKS) to validate the server’s certificate.\n\n`xpack.monitoring.elasticsearch.ssl.truststore.password`\n:   Optional settings that provide the password to the truststore.\n\n`xpack.monitoring.elasticsearch.ssl.keystore.path`\n:   Optional settings that provide the paths to the Java keystore (JKS) to validate the client’s certificate.\n\n`xpack.monitoring.elasticsearch.ssl.keystore.password`\n:   Optional settings that provide the password to the keystore.\n\n`xpack.monitoring.elasticsearch.ssl.certificate`\n:   Optional setting that provides the path to an SSL certificate to use to authenticate the client. This certificate should be an OpenSSL-style X.509 certificate file.\n\n::::{note}\nThis setting can be used only if `xpack.monitoring.elasticsearch.ssl.key` is set.\n::::\n\n`xpack.monitoring.elasticsearch.ssl.key`\n:   Optional setting that provides the path to an OpenSSL-style RSA private key that corresponds to the `xpack.monitoring.elasticsearch.ssl.certificate`.\n\n::::{note}\nThis setting can be used only if `xpack.monitoring.elasticsearch.ssl.certificate` is set.\n::::\n\n`xpack.monitoring.elasticsearch.ssl.verification_mode`\n:   Option to validate the server’s certificate. Defaults to `full`. To disable, set to `none`. Disabling this severely compromises security.\n\n`xpack.monitoring.elasticsearch.ssl.cipher_suites`\n:   Optional setting that provides the list of cipher suites to use, listed by priorities. Supported cipher suites vary depending on the Java and protocol versions.",
              "Additional settings [monitoring-additional-settings-legacy]": "`xpack.monitoring.elasticsearch.cloud_id`\n:   If you’re using {{es}} in {{ecloud}}, you should specify the identifier here. This setting is an alternative to `xpack.monitoring.elasticsearch.hosts`. If `cloud_id` is configured, `xpack.monitoring.elasticsearch.hosts` should not be used. The {{es}} instances that you want to ship your Logstash metrics to. This might be the same {{es}} instance specified in the `outputs` section in your Logstash configuration, or a different one.\n\n`xpack.monitoring.elasticsearch.cloud_auth`\n:   If you’re using {{es}} in {{ecloud}}, you can set your auth credentials here. This setting is an alternative to both `xpack.monitoring.elasticsearch.username` and `xpack.monitoring.elasticsearch.password`. If `cloud_auth` is configured, those settings should not be used.\n\n`xpack.monitoring.elasticsearch.api_key`\n:   Authenticate using an Elasticsearch API key. Note that this option also requires using SSL.\n\nThe API key Format is `id:api_key` where `id` and `api_key` are as returned by the Elasticsearch [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key)."
            }
          },
          "metadata": {
            "navigation_title": "Legacy collection (deprecated)",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/monitoring-internal-collection-legacy.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 45]"
        },
        {
          "title": "Monitoring Logstash Legacy",
          "description": null,
          "content": {
            "Monitoring Logstash (Legacy) [configuring-logstash]": "Use the {{stack}} {{monitor-features}} to gain insight into the health of {{ls}} instances running in your environment. For an introduction to monitoring your Elastic stack, see [Monitoring a cluster](docs-content://deploy-manage/monitor.md) in the [Elasticsearch Reference](docs-content://get-started/index.md). Then, make sure that monitoring is enabled on your {{es}} cluster.\n\nThese options for collecting {{ls}} metrics for stack monitoring have been available for a while:\n\n['[{{metricbeat}} collection](/reference/monitoring-with-metricbeat.md). Metricbeat collects monitoring data from your {{ls}} instance and sends it directly to your monitoring cluster. The benefit of Metricbeat collection is that the monitoring agent remains active even if the {{ls}} instance does not.', '[Legacy collection (deprecated)](/reference/monitoring-internal-collection-legacy.md). Legacy collectors send monitoring data to your production cluster.']\n\nFor more features, dependability, and easier management, consider using:\n\n['[{{agent}} collection for Stack Monitoring](/reference/monitoring-with-elastic-agent.md). {{agent}} collects monitoring data from your {{ls}} instance and sends it directly to your monitoring cluster, and shows the data in {{ls}} Dashboards. The benefit of {{agent}} collection is that the monitoring agent remains active even if the {{ls}} instance does not, you can manage all your monitoring agents from a central location in {{fleet}}.']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/configuring-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 46]"
        },
        {
          "title": "Monitoring Logstash With Elastic Agent",
          "description": null,
          "content": {
            "Monitoring Logstash with Elastic Agent [monitoring-with-ea]": "You can use {{agent}} to collect data about {{ls}} and ship it to the monitoring cluster. When you use {{agent}} collection, the monitoring agent remains active even if the {{ls}} instance does not. Plus you have the option to manage all of your monitoring agents from a central location in {{fleet}}.\n\n{{agent}} gives you a single, unified way to add monitoring for logs, metrics, and other types of data to a host. Each agent has a single policy you can update to add integrations for new data sources, security protections, and more.\n\nYou can use {{agent}} to collect {{ls}} monitoring data on:\n\n['[{{ecloud}} or self-managed dashboards](/reference/dashboard-monitoring-with-elastic-agent.md).<br> {{agent}} collects monitoring data from your {{ls}} instance, sends it directly to your monitoring cluster, and shows the data in {{ls}} dashboards. {{ls}} dashboards include an extended range of metrics, including plugin drilldowns, and plugin specific dashboards for the dissect filter, the grok filter, and the elasticsearch output.', '[{{ecloud}} dashboards (serverless)](/reference/serverless-monitoring-with-elastic-agent.md).<br> {{agent}} collects monitoring data from your {{ls}} instance, sends it to [Elastic serverless](docs-content://deploy-manage/deploy/elastic-cloud/serverless.md), and shows the data in {{ls}} dashboards in [Elastic Observability](docs-content://solutions/observability.md). {{ls}} dashboards include an extended range of metrics, including plugin drilldowns, and plugin specific dashboards for the dissect filter, the grok filter, and the elasticsearch output.', '[{{stack}} monitoring](/reference/monitoring-with-elastic-agent.md).<br> Use the Elastic Stack monitoring features to gain insight into the health of {{ls}} instances running in your environment.']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/monitoring-with-ea.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 47]"
        },
        {
          "title": "Monitoring Logstash with APIs",
          "description": null,
          "content": {
            "APIs for monitoring Logstash [monitoring]": "Logstash provides monitoring APIs for retrieving runtime information about Logstash:\n\n['[Node info API](https://www.elastic.co/docs/api/doc/logstash/group/endpoint-node-info)', '[Plugins info API](https://www.elastic.co/docs/api/doc/logstash/group/endpoint-plugin-info)', '[Node stats API](https://www.elastic.co/docs/api/doc/logstash/group/endpoint-node-stats)', '[Hot threads API](https://www.elastic.co/docs/api/doc/logstash/group/endpoint-hot-threads)', '[Health report API](https://www.elastic.co/docs/api/doc/logstash/group/endpoint-health)']\n\nYou can use the root resource to retrieve general information about the Logstash instance, including\nthe host and version.\n\n```\ncurl -XGET 'localhost:9600/?pretty'\n```\n\nExample response:\n\n```\n{\n   \"host\": \"skywalker\",\n   \"version\": \"{logstash_version}\",\n   \"http_address\": \"127.0.0.1:9600\"\n}\n```\n\n:::{note}\nBy default, the monitoring API attempts to bind to `tcp:9600`.\nIf this port is already in use by another Logstash instance, you need to launch Logstash with the `--api.http.port` flag specified to bind to a different port. For more information, go to [](running-logstash-command-line.md#command-line-flags)  \n:::",
            "Securing the Logstash API [monitoring-api-security]": "The Logstash monitoring APIs are not secured by default, but you can configure Logstash to secure them in one of several ways to meet your organization's needs.\n\nYou can enable SSL for the Logstash API by setting `api.ssl.enabled: true` in the `logstash.yml`, and providing the relevant keystore settings `api.ssl.keystore.path` and `api.ssl.keystore.password`:\n\n```\napi.ssl.enabled: true\napi.ssl.keystore.path: /path/to/keystore.jks\napi.ssl.keystore.password: \"s3cUr3p4$$w0rd\"\n```\n\nThe keystore must be in either jks or p12 format, and must contain both a certificate and a private key.\nConnecting clients receive this certificate, allowing them to authenticate the Logstash endpoint.\n\nYou can also require HTTP Basic authentication by setting `api.auth.type: basic` in the `logstash.yml`, and providing the relevant credentials `api.auth.basic.username` and `api.auth.basic.password`:\n\n```\napi.auth.type: basic\napi.auth.basic.username: \"logstash\"\napi.auth.basic.password: \"s3cUreP4$$w0rD\"\n```\n\n:::{note}\nUsage of `Keystore` or `Environment` or variable replacements is encouraged for password-type fields to avoid storing them in plain text.\nFor example, specifying the value `\"${HTTP_PASS}\"` will resolve to the value stored in the [secure keystore's](keystore.md) `HTTP_PASS` variable if present or the same variable from the [environment](environment-variables.md).\n:::",
            "Common options [monitoring-common-options]": {
              "Pretty results": "When appending `?pretty=true` to any request made, the JSON returned will be pretty formatted (use it for debugging only!).",
              "Human-readable output": ":::{note}\nThe `human` option is supported for the hot threads API only.\nWhen you specify `human=true`, the results are returned in plain text instead of JSON format.\nThe default is `false`.\n:::\n\nStatistics are returned in a format suitable for humans (for example, `\"exists_time\": \"1h\"` or `\"size\": \"1kb\"`) and for computers (for example, `\"exists_time_in_millis\": 3600000` or `\"size_in_bytes\": 1024`). The human-readable values can be turned off by adding `?human=false` to the query string. This makes sense when the stats results are being consumed by a monitoring tool, rather than intended for human consumption.  The default for the `human` flag is `false`."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 48]"
        },
        {
          "title": "Troubleshooting monitoring in Logstash [monitoring-troubleshooting]",
          "description": null,
          "content": {
            "Logstash Monitoring Not Working After Upgrade [_logstash_monitoring_not_working_after_upgrade]": "When upgrading from older versions, the built-in `logstash_system` user is disabled for security reasons. To resume monitoring:\n\n['Change the `logstash_system` password:', '```\\nPUT _security/user/logstash_system/_password\\n{\\n  \"password\": \"newpassword\"\\n}\\n```', 'Re-enable the `logstash_system` user:', '```\\nPUT _security/user/logstash_system/_enable\\n```']"
          },
          "metadata": {
            "navigation_title": "Troubleshooting",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/monitoring-troubleshooting.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 49]"
        },
        {
          "title": "Collect {{ls}} monitoring data for stack monitoring [monitoring-with-elastic-agent]",
          "description": null,
          "content": {
            "Install and configure {{agent}} [install-and-configure-mon]": {
              "Add the {{agent}} {{ls}} integration [add-logstash-integration-ea]": [
                "Go to the {{kib}} home page, and click **Add integrations**.",
                "% TO DO: Use `:class: screenshot`\n![{{kib}} home page](images/kibana-home.png)",
                "In the query bar, search for **{{ls}}** and select the integration to see more details about it.",
                "Click **Add {{ls}}**.",
                "Configure the integration name and optionally add a description.",
                "Configure the integration to collect logs.",
                [
                  "Make sure that **Logs** is turned on if you want to collect logs from your {{ls}} instance, ensuring that the required settings are correctly configured:",
                  "Under **Logs**, modify the log paths to match your {{ls}} environment."
                ],
                "Configure the integration to collect metrics",
                "::::{tip}\nFor the best experience with Stack Monitoring, we recommend collecting both `node` and `node_stats`. Turning off either of these will result in incomplete or missing visualizations.\n::::",
                [
                  "Make sure that **Metrics (Stack Monitoring)** is turned on, and **Metrics (Elastic Agent)** is turned off, if you want to collect metrics from your {{ls}} instance.",
                  "Under **Metrics (Stack Monitoring)**, make sure the hosts setting points to your {{ls}} host URLs. By default, the integration collects {{ls}} monitoring metrics from `localhost:9600`. If that host and port number are not correct, update the `hosts` setting. If you configured {{ls}} to use encrypted communications, you must access it via HTTPS. For example, use a `hosts` setting like `https://localhost:9600`."
                ],
                "Choose where to add the integration policy.<br> Click **New hosts** to add it to new agent policy or **Existing hosts** to add it to an existing agent policy.",
                "In the popup, click **Add {{agent}} to your hosts** to open the **Add agent** flyout.",
                "::::{tip}\nIf you accidentally close the popup, go to **{{fleet}} > Agents**, then click **Add agent** to access the flyout.\n::::"
              ],
              "Install and run an {{agent}} on your machine [add-agent-to-fleet-ea]": "The **Add agent** flyout has two options: **Enroll in {{fleet}}** and **Run standalone**. Enrolling agents in {{fleet}} (default) provides a centralized management tool in {{kib}}, reducing management overhead.\n\n:::::::{tab-set}\n\n::::::{tab-item} Fleet-managed\n\n['When the **Add Agent flyout** appears, stay on the **Enroll in fleet** tab.', 'Skip the **Select enrollment token** step. The enrollment token you need is already selected.', '::::{note}\\nThe enrollment token is specific to the {{agent}} policy that you just created. When you run the command to enroll the agent in {{fleet}}, you will pass in the enrollment token.\\n::::', 'Download, install, and enroll the {{agent}} on your host by selecting your host operating system and following the **Install {{agent}} on your host** step.']\n\nIt takes about a minute for {{agent}} to enroll in {{fleet}}, download the configuration specified in the policy you just created, and start collecting data.\n::::::\n\n::::::{tab-item} Run standalone\n\n['When the **Add Agent flyout** appears, navigate to the **Run standalone** tab.', 'Configure the agent. Follow the instructions in **Install Elastic Agent on your host**.', 'After unpacking the binary, replace the `elastic-agent.yml` file with that supplied in the Add Agent flyout on the \"Run standalone\" tab, replacing the values of `ES_USERNAME` and `ES_PASSWORD` appropriately.', 'Run `sudo ./elastic-agent install`\\n::::::']\n\n:::::::"
            },
            "View assets [view-assets]": {
              "Monitor {{ls}} logs and metrics (Stack Monitoring) [view-data-stack]": "[View the monitoring data in {{kib}}](docs-content://deploy-manage/monitor/stack-monitoring/kibana-monitoring-data.md), and navigate to the [monitoring UI](/reference/logstash-monitoring-ui.md)."
            }
          },
          "metadata": {
            "navigation_title": "Collect monitoring data for stack monitoring",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/monitoring-with-elastic-agent.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 50]"
        },
        {
          "title": "Collect {{ls}} monitoring data with {{metricbeat}} [monitoring-with-metricbeat]",
          "description": null,
          "content": {
            "Disable default collection of {{ls}} monitoring metrics [disable-default]": "Set the `monitoring.enabled` to `false` in logstash.yml to disable to default monitoring:\n\n```\nmonitoring.enabled: false\n```",
            "Determine target Elasticsearch cluster [define-cluster__uuid]": "You will need to determine which Elasticsearch cluster that {{ls}} will bind metrics to in the Stack Monitoring UI by specifying the `cluster_uuid`. When pipelines contain [{{es}} output plugins](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md), the `cluster_uuid` is automatically calculated, and the metrics should be bound without any additional settings.\n\nTo override automatic values, or if your pipeline does not contain any [{{es}} output plugins](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md), you can bind the metrics of {{ls}} to a specific cluster, by defining the target cluster in the `monitoring.cluster_uuid` setting. in the configuration file (logstash.yml):\n\n```\nmonitoring.cluster_uuid: PRODUCTION_ES_CLUSTER_UUID\n```\n\nRefer to [{{es}} cluster stats page](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-cluster-stats) to figure out how to get your cluster `cluster_uuid`.",
            "Install and configure {{metricbeat}} [configure-metricbeat]": [
              "[Install {{metricbeat}}](beats://reference/metricbeat/metricbeat-installation-configuration.md) on the same server as {{ls}}.",
              "Enable the `logstash-xpack` module in {{metricbeat}}.<br>",
              "To enable the default configuration in the {{metricbeat}} `modules.d` directory, run:",
              "**deb or rpm:**<br>",
              "```\nmetricbeat modules enable logstash-xpack\n```",
              "**linux or mac:**",
              "```\n./metricbeat modules enable logstash-xpack\n```",
              "**win:**",
              "```\nPS > .\\metricbeat.exe modules enable logstash-xpack\n```",
              "For more information, see [Specify which modules to run](beats://reference/metricbeat/configuration-metricbeat.md) and [beat module](beats://reference/metricbeat/metricbeat-module-beat.md).",
              "Configure the `logstash-xpack` module in {{metricbeat}}.<br>",
              "The `modules.d/logstash-xpack.yml` file contains these settings:",
              "```\n  - module: logstash\n    metricsets:\n      - node\n      - node_stats\n    period: 10s\n    hosts: [\"localhost:9600\"]\n    #username: \"user\"\n    #password: \"secret\"\n    xpack.enabled: true\n```",
              "::::{tip}\nFor the best experience with Stack Monitoring, we recommend collecting both `node` and `node_stats` metricsets. Turning off either of these will result in incomplete or missing visualizations.\n::::",
              "Set the `hosts`, `username`, and `password` to authenticate with {{ls}}.",
              "By default, the module collects {{ls}} monitoring data from `localhost:9600`.",
              "To monitor multiple {{ls}} instances, specify a list of hosts, for example:",
              "```\nhosts: [\"http://localhost:9601\",\"http://localhost:9602\",\"http://localhost:9603\"]\n```",
              "**Elastic security.** The Elastic {{security-features}} are enabled by default. You must provide a user ID and password so that {{metricbeat}} can collect metrics successfully:",
              [
                "Create a user on the production cluster that has the `remote_monitoring_collector` [built-in role](elasticsearch://reference/elasticsearch/roles.md).",
                "Add the `username` and `password` settings to the module configuration file (`logstash-xpack.yml`)."
              ],
              "Optional: Disable the system module in the {{metricbeat}}.",
              "By default, the [system module](beats://reference/metricbeat/metricbeat-module-system.md) is enabled. The information it collects, however, is not shown on the **Stack Monitoring** page in {{kib}}. Unless you want to use that information for other purposes, run the following command:",
              "```\nmetricbeat modules disable system\n```",
              "Identify where to send the monitoring data.<br>",
              "::::{tip}\nIn production environments, we strongly recommend using a separate cluster (referred to as the *monitoring cluster*) to store the data. Using a separate monitoring cluster prevents production cluster outages from impacting your ability to access your monitoring data. It also prevents monitoring activities from impacting the performance of your production cluster.\n::::"
            ]
          },
          "metadata": {
            "navigation_title": "{{metricbeat}} collection",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/monitoring-with-metricbeat.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 51]"
        },
        {
          "title": "Managing Multiline Events [multiline]",
          "description": null,
          "content": {
            "Examples of Multiline Codec Configuration [_examples_of_multiline_codec_configuration]": {
              "Java Stack Traces [_java_stack_traces]": "Java stack traces consist of multiple lines, with each line after the initial line beginning with whitespace, as in this example:\n\n```\nException in thread \"main\" java.lang.NullPointerException\n        at com.example.myproject.Book.getTitle(Book.java:16)\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\n```\n\nTo consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:\n\n```\ninput {\n  stdin {\n    codec => multiline {\n      pattern => \"^\\s\"\n      what => \"previous\"\n    }\n  }\n}\n```\n\nThis configuration merges any line that begins with whitespace up to the previous line.",
              "Line Continuations [_line_continuations]": "Several programming languages use the `\\` character at the end of a line to denote that the line continues, as in this example:\n\n```\nprintf (\"%10.10ld  \\t %10.10ld \\t %s\\\n  %f\", w, x, y, z );\n```\n\nTo consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:\n\n```\ninput {\n  stdin {\n    codec => multiline {\n      pattern => \"\\\\$\"\n      what => \"next\"\n    }\n  }\n}\n```\n\nThis configuration merges any line that ends with the `\\` character with the following line.",
              "Timestamps [_timestamps]": "Activity logs from services such as Elasticsearch typically begin with a timestamp, followed by information on the specific activity, as in this example:\n\n```\n[2015-08-24 11:49:14,389][INFO ][env                      ] [Letha] using [1] data paths, mounts [[/\n(/dev/disk1)]], net usable_space [34.5gb], net total_space [118.9gb], types [hfs]\n```\n\nTo consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:\n\n```\ninput {\n  file {\n    path => \"/var/log/someapp.log\"\n    codec => multiline {\n      pattern => \"^%{TIMESTAMP_ISO8601} \"\n      negate => true\n      what => previous\n    }\n  }\n}\n```\n\nThis configuration uses the `negate` option to specify that any line that does not begin with a timestamp belongs to the previous line."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/multiline.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 52]"
        },
        {
          "title": "Stitching Together Multiple Input and Output Plugins [multiple-input-output-plugins]",
          "description": null,
          "content": {
            "Reading from a Twitter Feed [twitter-configuration]": "To add a Twitter feed, you use the [`twitter`](logstash-docs-md://lsr/plugins-inputs-twitter.md) input plugin. To configure the plugin, you need several pieces of information:\n\n['A *consumer key*, which uniquely identifies your Twitter app.', 'A *consumer secret*, which serves as the password for your Twitter app.', 'One or more *keywords* to search in the incoming feed. The example shows using \"cloud\" as a keyword, but you can use whatever you want.', 'An *oauth token*, which identifies the Twitter account using this app.', 'An *oauth token secret*, which serves as the password of the Twitter account.']\n\nVisit [https://dev.twitter.com/apps](https://dev.twitter.com/apps) to set up a Twitter account and generate your consumer key and secret, as well as your access token and secret. See the docs for the [`twitter`](logstash-docs-md://lsr/plugins-inputs-twitter.md) input plugin if you’re not sure how to generate these keys.\n\nLike you did earlier when you worked on [Parsing Logs with Logstash](/reference/advanced-pipeline.md), create a config file (called `second-pipeline.conf`) that contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make sure you pass in the correct config file name when you run Logstash.\n\nAdd the following lines to the `input` section of the `second-pipeline.conf` file, substituting your values for the placeholder values shown here:\n\n```\n    twitter {\n        consumer_key => \"enter_your_consumer_key_here\"\n        consumer_secret => \"enter_your_secret_here\"\n        keywords => [\"cloud\"]\n        oauth_token => \"enter_your_access_token_here\"\n        oauth_token_secret => \"enter_your_access_token_secret_here\"\n    }\n```",
            "Configuring Filebeat to Send Log Lines to Logstash [configuring-lsf]": "As you learned earlier in [Configuring Filebeat to Send Log Lines to Logstash](/reference/advanced-pipeline.md#configuring-filebeat), the [Filebeat](https://github.com/elastic/beats/tree/main/filebeat) client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your Logstash instance for processing.\n\nAfter installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation directory, and replace the contents with the following lines. Make sure `paths` points to your syslog:\n\n```\nfilebeat.inputs:\n- type: log\n  paths:\n    - /var/log/*.log <1>\n  fields:\n    type: syslog <2>\noutput.logstash:\n  hosts: [\"localhost:5044\"]\n```\n\n['Absolute path to the file or files that Filebeat processes.', 'Adds a field called `type` with the value `syslog` to the event.']\n\nSave your changes.\n\nTo keep the configuration simple, you won’t specify TLS/SSL settings as you would in a real world scenario.\n\nConfigure your Logstash instance to use the Filebeat input plugin by adding the following lines to the `input` section of the `second-pipeline.conf` file:\n\n```\n    beats {\n        port => \"5044\"\n    }\n```",
            "Writing Logstash Data to a File [logstash-file-output]": "You can configure your Logstash pipeline to write data directly to a file with the [`file`](logstash-docs-md://lsr/plugins-outputs-file.md) output plugin.\n\nConfigure your Logstash instance to use the `file` output plugin by adding the following lines to the `output` section of the `second-pipeline.conf` file:\n\n```\n    file {\n        path => \"/path/to/target/file\"\n    }\n```",
            "Writing to Multiple Elasticsearch Nodes [multiple-es-nodes]": {
              "Testing the Pipeline [testing-second-pipeline]": "At this point, your `second-pipeline.conf` file looks like this:\n\n```\ninput {\n    twitter {\n        consumer_key => \"enter_your_consumer_key_here\"\n        consumer_secret => \"enter_your_secret_here\"\n        keywords => [\"cloud\"]\n        oauth_token => \"enter_your_access_token_here\"\n        oauth_token_secret => \"enter_your_access_token_secret_here\"\n    }\n    beats {\n        port => \"5044\"\n    }\n}\noutput {\n    elasticsearch {\n        hosts => [\"IP Address 1:port1\", \"IP Address 2:port2\", \"IP Address 3\"]\n    }\n    file {\n        path => \"/path/to/target/file\"\n    }\n}\n```\n\nLogstash is consuming data from the Twitter feed you configured, receiving data from Filebeat, and indexing this information to three nodes in an Elasticsearch cluster as well as writing to a file.\n\nAt the data source machine, run Filebeat with the following command:\n\n```\nsudo ./filebeat -e -c filebeat.yml -d \"publish\"\n```\n\nFilebeat will attempt to connect on port 5044. Until Logstash starts with an active Beats plugin, there won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.\n\nTo verify your configuration, run the following command:\n\n```\nbin/logstash -f second-pipeline.conf --config.test_and_exit\n```\n\nThe `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file passes the configuration test, start Logstash with the following command:\n\n```\nbin/logstash -f second-pipeline.conf\n```\n\nUse the `grep` utility to search in the target file to verify that information is present:\n\n```\ngrep syslog /path/to/target/file\n```\n\nRun an Elasticsearch query to find the same information in the Elasticsearch cluster:\n\n```\ncurl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=fields.type:syslog'\n```\n\nReplace $DATE with the current date, in YYYY.MM.DD format.\n\nTo see data from the Twitter feed, try this query:\n\n```\ncurl -XGET 'http://localhost:9200/logstash-$DATE/_search?pretty&q=client:iphone'\n```\n\nAgain, remember to replace $DATE with the current date, in YYYY.MM.DD format."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/multiple-input-output-plugins.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 53]"
        },
        {
          "title": "Multiple Pipelines [multiple-pipelines]",
          "description": null,
          "content": {
            "Usage Considerations [multiple-pipeline-usage]": "Using multiple pipelines is especially useful if your current configuration has event flows that don’t share the same inputs/filters and outputs and are being separated from each other using tags and conditionals.\n\nHaving multiple pipelines in a single instance also allows these event flows to have different performance and durability parameters (for example, different settings for pipeline workers and persistent queues). This separation means that a blocked output in one pipeline won’t exert backpressure in the other.\n\nThat said, it’s important to take into account resource competition between the pipelines, given that the default values are tuned for a single pipeline. So, for example, consider reducing the number of pipeline workers used by each pipeline, because each pipeline will use 1 worker per CPU core by default.\n\nPersistent queues and dead letter queues are isolated per pipeline, with their locations namespaced by the `pipeline.id` value."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/multiple-pipelines.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 54]"
        },
        {
          "title": "Offline Plugin Management [offline-plugins]",
          "description": null,
          "content": {
            "Building Offline Plugin Packs [building-offline-packs]": "An *offline plugin pack* is a compressed file that contains all the plugins your offline Logstash installation requires, along with the dependencies for those plugins.\n\nTo build an offline plugin pack:\n\n['Make sure all the plugins that you want to package are installed on the staging server and that the staging server can access the Internet.', 'Run the `bin/logstash-plugin prepare-offline-pack` subcommand to package the plugins and dependencies:', '```\\nbin/logstash-plugin prepare-offline-pack --output OUTPUT --overwrite [PLUGINS]\\n```', 'where:', ['`OUTPUT` specifies the zip file where the compressed plugin pack will be written. The default file is `/LOGSTASH_HOME/logstash-offline-plugins-9.0.0.zip`. If you are using 5.2.x and 5.3.0, this location should be a zip file whose contents will be overwritten.', '`[PLUGINS]` specifies one or more plugins that you want to include in the pack.', '`--overwrite` specifies if you want to override an existing file at the location']]\n\nExamples:\n\n```\nbin/logstash-plugin prepare-offline-pack logstash-input-beats <1>\nbin/logstash-plugin prepare-offline-pack logstash-filter-* <2>\nbin/logstash-plugin prepare-offline-pack logstash-filter-* logstash-input-beats <3>\n```\n\n['Packages the Beats input plugin and any dependencies.', 'Uses a wildcard to package all filter plugins and any dependencies.', 'Packages all filter plugins, the Beats input plugin, and any dependencies.']\n\n::::{note}\nDownloading all dependencies for the specified plugins may take some time, depending on the plugins listed.\n::::",
            "Installing Offline Plugin Packs [installing-offline-packs]": "To install an offline plugin pack:\n\n['Move the compressed bundle to the machine where you want to install the plugins.', 'Run the `bin/logstash-plugin install` subcommand and pass in the file URI of the offline plugin pack.', '```\\nbin/logstash-plugin install file:///c:/path/to/logstash-offline-plugins-9.0.0.zip\\n```', '```\\nbin/logstash-plugin install file:///path/to/logstash-offline-plugins-9.0.0.zip\\n```', 'This command expects a file URI, so make sure you use forward slashes and specify the full path to the pack.']",
            "Updating Offline Plugins [updating-offline-packs]": "To update offline plugins, you update the plugins on the staging server and then use the same process that you followed to build and install the plugin pack:\n\n['On the staging server, run the `bin/logstash-plugin update` subcommand to update the plugins. See [Updating plugins](/reference/working-with-plugins.md#updating-plugins).', 'Create a new version of the plugin pack. See [Building Offline Plugin Packs](#building-offline-packs).', 'Install the new version of the plugin pack. See [Installing Offline Plugin Packs](#installing-offline-packs).']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/offline-plugins.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 55]"
        },
        {
          "title": "Performance troubleshooting [performance-troubleshooting]",
          "description": null,
          "content": {
            "Performance checklist [_performance_checklist]": [
              "**Check the performance of input sources and output destinations:**",
              [
                "Logstash is only as fast as the services it connects to. Logstash can only consume and produce data as fast as its input and output destinations can!"
              ],
              "**Check system statistics:**",
              [
                "CPU",
                [
                  "Note whether the CPU is being heavily used. On Linux/Unix, you can run `top -H` to see process statistics broken out by thread, as well as total CPU statistics.",
                  "If CPU usage is high, skip forward to the section about checking the JVM heap and then read the section about tuning Logstash worker settings."
                ],
                "Memory",
                [
                  "Be aware of the fact that Logstash runs on the Java VM. This means that Logstash will always use the maximum amount of memory you allocate to it.",
                  "Look for other applications that use large amounts of memory and may be causing Logstash to swap to disk. This can happen if the total memory used by applications exceeds physical memory."
                ],
                "I/O Utilization",
                [
                  "Monitor disk I/O to check for disk saturation.",
                  [
                    "Disk saturation can happen if you’re using Logstash plugins (such as the file output) that may saturate your storage.",
                    "Disk saturation can also happen if you’re encountering a lot of errors that force Logstash to generate large error logs.",
                    "On Linux, you can use iostat, dstat, or something similar to monitor disk I/O."
                  ],
                  "Monitor network I/O for network saturation.",
                  [
                    "Network saturation can happen if you’re using inputs/outputs that perform a lot of network operations.",
                    "On Linux, you can use a tool like dstat or iftop to monitor your network."
                  ]
                ]
              ],
              "**Check the JVM heap:**",
              [
                "The recommended heap size for typical ingestion scenarios should be no less than 4GB and no more than 8GB.",
                "CPU utilization can increase unnecessarily if the heap size is too low, resulting in the JVM constantly garbage collecting. You can check for this issue by doubling the heap size to see if performance improves.",
                "Do not increase the heap size past the amount of physical memory. Some memory must be left to run the OS and other processes.  As a general guideline for most installations, don’t exceed 50-75% of physical memory. The more memory you have, the higher percentage you can use.",
                "Set the minimum (Xms) and maximum (Xmx) heap allocation size to the same value to prevent the heap from resizing at runtime, which is a very costly process.",
                "You can make more accurate measurements of the JVM heap by using either the `jmap` command line utility distributed with Java or by using VisualVM. For more info, see [Profiling the heap](/reference/tuning-logstash.md#profiling-the-heap)."
              ],
              "**Tune Logstash pipeline settings:**",
              [
                "Continue on to [Tuning and profiling logstash pipeline performance](/reference/tuning-logstash.md) to learn about tuning individual pipelines."
              ]
            ]
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/performance-troubleshooting.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 56]"
        },
        {
          "title": "Performance Tuning",
          "description": null,
          "content": {
            "Performance tuning [performance-tuning]": "This section includes the following information about tuning Logstash performance:\n\n['[Performance troubleshooting](/reference/performance-troubleshooting.md)', '[Tuning and profiling logstash pipeline performance](/reference/tuning-logstash.md)']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/performance-tuning.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 57]"
        },
        {
          "title": "Persistent queues (PQ) [persistent-queues]",
          "description": null,
          "content": {
            "Benefits of persistent queues [persistent-queues-benefits]": "A persistent queue (PQ):\n\n['Helps protect against message loss during a normal shutdown and when Logstash is terminated abnormally. If Logstash is restarted while events are in-flight, Logstash attempts to deliver messages stored in the persistent queue until delivery succeeds at least once.', 'Can absorb bursts of events without needing an external buffering mechanism like Redis or Apache Kafka.']\n\n::::{note}\nPersistent queues are disabled by default. To enable them, check out [Configuring persistent queues](#configuring-persistent-queues).\n::::",
            "Limitations of persistent queues [persistent-queues-limitations]": "Persistent queues do not solve these problems:\n\n['Input plugins that do not use a request-response protocol cannot be protected from data loss. Tcp, udp, zeromq push+pull, and many other inputs do not have a mechanism to acknowledge receipt to the sender. (Plugins such as beats and http, which **do** have an acknowledgement capability, are well protected by this queue.)', 'Data may be lost if an abnormal shutdown occurs before the checkpoint file has been committed.', 'A persistent queue does not handle permanent machine failures such as disk corruption, disk failure, and machine loss. The data persisted to disk is not replicated.']\n\n::::{tip}\nUse the local filesystem for data integrity and performance. Network File System (NFS) is not supported.\n::::",
            "Configuring persistent queues [configuring-persistent-queues]": "To configure persistent queues, specify options in the Logstash [settings file](/reference/logstash-settings-file.md). Settings are applied to every pipeline.\n\nWhen you set values for capacity and sizing settings, remember that the value you set is applied *per pipeline* rather than a total to be shared among all pipelines.\n\n::::{tip}\nIf you want to define values for a specific pipeline, use [`pipelines.yml`](/reference/multiple-pipelines.md).\n::::\n\n`queue.type`\n:   Specify `persisted` to enable persistent queues. By default, persistent queues are disabled (default: `queue.type: memory`).\n\n`path.queue`\n:   The directory path where the data files will be stored. By default, the files are stored in `path.data/queue`.\n\n`queue.page_capacity`\n:   The queue data consists of append-only files called \"pages.\" This value sets the maximum size of a queue page in bytes. The default size of 64mb is a good value for most users, and changing this value is unlikely to have performance benefits. If you change the page capacity of an existing queue, the new size applies only to the new page.\n\n`queue.drain`\n:   Specify `true` if you want Logstash to wait until the persistent queue is drained before shutting down. The amount of time it takes to drain the queue depends on the number of events that have accumulated in the queue. Therefore, you should avoid using this setting unless the queue, even when full, is relatively small and can be drained quickly.\n\n`queue.max_events`\n:   The maximum number of events not yet read by the pipeline worker. The default is 0 (unlimited). We use this setting for internal testing. Users generally shouldn’t be changing this value.\n\n`queue.max_bytes`\n:   The total capacity of *each queue* in number of bytes. Unless overridden in `pipelines.yml` or central management, each persistent queue will be sized at the value of `queue.max_bytes` specified in `logstash.yml`. The default is 1024mb (1gb).\n\n::::{note}\nBe sure that your disk has sufficient capacity to handle the cumulative total of `queue.max_bytes` across all persistent queues. The total of `queue.max_bytes` for *all* queues should be lower than the capacity of your disk.\n::::\n\n\n::::{tip}\nIf you are using persistent queues to protect against data loss, but don’t require much buffering, you can set `queue.max_bytes` to a smaller value as long as it is not less than the value of `queue.page_capacity`. A smaller value produces smaller queues and improves queue performance.\n::::\n\n\n\n`queue.checkpoint.acks`\n:   Sets the number of acked events before forcing a checkpoint. Default is `1024`. Set to `0` for unlimited.\n\n`queue.checkpoint.writes`\n:   Sets the maximum number of written events before a forced checkpoint. Default is `1024`. Set to `0` for unlimited.\n\nTo avoid losing data in the persistent queue, you can set `queue.checkpoint.writes: 1` to force a checkpoint after each event is written. Keep in mind that disk writes have a resource cost. Setting this value to `1` ensures maximum durability, but can severely impact performance. See [Controlling durability](#durability-persistent-queues) to better understand the trade-offs.\n\n\n`queue.checkpoint.interval` {applies_to}`stack: deprecated 9.1`\n:   Sets the interval in milliseconds when a checkpoint is forced on the head page. Default is `1000`. Set to `0` to eliminate periodic checkpoints.",
            "Configuration notes [pq-config-notes]": {
              "Queue size [pq-size]": {
                "Queue size by data type [sizing-by-type]": "{{ls}} serializes the events it receives before they are stored in the queue. This process results in added overhead to the event inside {{ls}}. This overhead depends on the type and the size of the `Original Event Size`. As such, the `Multiplication Factor` changes depending on your use case. These tables show examples of overhead by event type and how that affects the multiplication factor.\n\n**Raw string message**\n\n| Plaintext size (bytes) | Serialized {{ls}} event size (bytes) | Overhead (bytes) | Overhead (%) | Multiplication Factor |\n| --- | --- | --- | --- | --- |\n| 11 | 213 | `202` | `1836%` | `19.4` |\n| 1212 | 1416 | `204` | `17%` | `1.17` |\n| 10240 | 10452 | `212` | `2%` | `1.02` |\n\n**JSON document**\n\n| JSON document size (bytes) | Serialized {{ls}} event size (bytes) | Overhead (bytes) | Overhead (%) | Multiplication Factor |\n| --- | --- | --- | --- | --- |\n| 947 | 1133 | `186` | `20%` | `1.20` |\n| 2707 | 3206 | `499` | `18%` | `1.18` |\n| 6751 | 7388 | `637` | `9%` | `1.09` |\n| 58901 | 59693 | `792` | `1%` | `1.01` |\n\n**Example**\n\nLet’s consider a {{ls}} instance that receives 1000 EPS and each event is 1KB, or 3.5GB every hour. In order to tolerate a downstream component being unavailable for 12h without {{ls}} exerting back-pressure upstream, the persistent queue’s `max_bytes` would have to be set to 3.6*12*1.10 = 47.25GB, or about 50GB."
              },
              "Smaller queue size [pq-lower-max_bytes]": "If you are using persistent queues to protect against data loss, but don’t require much buffering, you can set `queue.max_bytes` to a smaller value. A smaller value may produce smaller queues and improves queue performance.\n\n**Sample configuration**\n\n```\nqueue.type: persisted\nqueue.max_bytes: 10mb\n```",
              "Fewer checkpoints [pq-fewer-checkpoints]": "Setting `queue.checkpoint.writes` and `queue.checkpoint.acks` to `0` may yield maximum performance, but may have potential impact on durability.\n\nIn a situation where Logstash is terminated or there is a hardware-level failure, any data that has not been checkpointed, is lost. See [Controlling durability](#durability-persistent-queues) to better understand the trade-offs.",
              "PQs and pipeline-to-pipeline communication [pq-pline-pline]": {
                "Use case: PQs and output isolator pattern [uc-isolator]": "Here is a real world use case described by a Logstash user.\n\n\"*In our deployment, we use one pipeline per output, and each pipeline has a large PQ. This configuration allows a single output to stall without blocking the input (and thus all other outputs), until the operator can restore flow to the stalled output and let the queue drain.*\"\n\n\"*Our real-time outputs must be low-latency, and our bulk outputs must be consistent. We use PQs to protect against stalling the real-time outputs more so than to protect against data loss in the bulk outputs. (Although the protection is nice, too).*\""
              }
            },
            "Troubleshooting persistent queues [troubleshooting-pqs]": {
              "`pqcheck` utility [pqcheck]": "```\nthe `pqcheck` utility to identify which persistent queue--or queues--have been corrupted.\n```\n\nFrom LOGSTASH_HOME, run:\n\n```\nbin/pqcheck <queue_directory>\n```\n\nwhere `<queue_directory>` is the fully qualified path to the persistent queue location.\n\nThe `pqcheck utility` reads through the checkpoint files in the given directory and outputs information about the current state of those files. The utility outputs this information for each checkpoint file:\n\n['Checkpoint file name', 'Whether or not the page file has been fully acknowledged. A fully acknowledged page file indicates that all events have been read and processed.', 'Page file name that the checkpoint file is referencing', 'Size of the page file. A page file with a size of 0 results in the output `NOT FOUND`. In this case, run `pqrepair` against the specified queue directory.', 'Page number', 'First unacknowledged page number (only relevant in the head checkpoint)', 'First unacknowledged event sequence number in the page', 'First event sequence number in the page', 'Number of events in the page', 'Whether or not the page has been fully acknowledged']\n\n**Sample with healthy page file**\n\nThis sample represents a healthy queue with three page files. In this sample, Logstash is currently writing to `page.2` as referenced by `checkpoint.head`. Logstash is reading from `page.0` as referenced by `checkpoint.0`.\n\n```\nubuntu@bigger:/usr/share/logstash$ bin/pqcheck /var/lib/logstash/queue/main/\nUsing bundled JDK: /usr/share/logstash/jdk\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\nChecking queue dir: /var/lib/logstash/queue/main\ncheckpoint.1, fully-acked: NO, page.1 size: 67108864\n  pageNum=1, firstUnackedPageNum=0, firstUnackedSeqNum=239675, minSeqNum=239675,\n  elementCount=218241, isFullyAcked=no\ncheckpoint.head, fully-acked: NO, page.2 size: 67108864\n  pageNum=2, firstUnackedPageNum=0, firstUnackedSeqNum=457916, minSeqNum=457916, elementCount=11805, isFullyAcked=no\ncheckpoint.0, fully-acked: NO, page.0 size: 67108864  <1>\n  pageNum=0, firstUnackedPageNum=0, firstUnackedSeqNum=176126, minSeqNum=1,\n  elementCount=239674, isFullyAcked=no <2>\n```\n\n['Represents `checkpoint.0`, which refers to the page file `page.0`, and has a size of `67108864`.', 'Continuing for `checkpoint.0`, these lines indicate that the page number is `0`, the first unacknowledged event is number `176126`, there are `239674` events in the page file, the first event in this page file is event number `1`, and the page file has not been fully acknowledged. That is, there are still events left in the page file that need to be ingested.']\n\n**Sample with corrupted page file**\n\nIf Logstash doesn’t start and/or `pqcheck` shows an anomaly, such as `NOT_FOUND` for a page, run `pqrepair` on the queue directory.\n\n```\nbin/pqcheck /var/lib/logstash/queue/main/\nUsing bundled JDK: /usr/share/logstash/jdk\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\nChecking queue dir: /var/lib/logstash/queue/main\ncheckpoint.head, fully-acked: NO, page.2 size: NOT FOUND <1>\n  pageNum=2, firstUnackedPageNum=2, firstUnackedSeqNum=534041, minSeqNum=457916,\n  elementCount=76127, isFullyAcked=no\n```\n\n['`NOT FOUND` is an indication of a corrupted page file. Run `pqrepair` against the specified queue directory.']\n\n::::{note}\nIf the queue shows `fully-acked: YES` and 0 bytes, you can safely delete the file.\n::::",
              "`pqrepair` utility [pqrepair]": "The `pqrepair` utility tries to remove corrupt queue segments to bring the queue back into working order. It starts searching from the directory where is launched and looks for `data/queue/main`.\n\n::::{note}\nThe queue may lose some data in this operation.\n::::\n\nFrom LOGSTASH_HOME, run:\n\n```\nbin/pqrepair <queue_directory>\n```\n\nwhere `<queue_directory>` is the fully qualified path to the persistent queue location.\n\nThere is no output if the utility runs properly.\n\nThe `pqrepair` utility requires write access to the directory. Folder permissions may cause problems when Logstash is run as a service. In this situation, use `sudo`.\n\n```\n/usr/share/logstash$ sudo -u logstash bin/pqrepair /var/lib/logstash/queue/main/\n```\n\nAfter you run `pqrepair`, restart Logstash to verify that the repair operation was successful.",
              "Draining the queue [draining-pqs]": "You may encounter situations where you want to drain the queue. Examples include:\n\n['Pausing new ingestion. There may be situations where you want to stop new ingestion, but still keep a backlog of data.', 'PQ repair. You can drain the queue to route to a different PQ while repairing an old one.', 'Data or workflow migration. If you are moving off a disk/hardware and/or migrating to a new data flow, you may want to drain the existing queue.']\n\nTo drain the persistent queue:\n\n['In the `logstash.yml` file, set `queue.drain: true`.', 'Restart Logstash for this setting to take effect.', 'Shutdown Logstash (using CTRL+C or SIGTERM), and wait for the queue to empty.']"
            },
            "How persistent queues work [persistent-queues-architecture]": {
              "Handling back pressure [backpressure-persistent-queue]": "When the queue is full, Logstash puts back pressure on the inputs to stall data flowing into Logstash. This mechanism helps Logstash control the rate of data flow at the input stage without overwhelming outputs like Elasticsearch.\n\nUse `queue.max_bytes` setting to configure the total capacity of the queue on disk. The following example sets the total capacity of the queue to 8gb:\n\n```\nqueue.type: persisted\nqueue.max_bytes: 8gb\n```\n\nWith these settings specified, Logstash buffers events on disk until the size of the queue reaches 8gb. When the queue is full of unACKed events, and the size limit has been reached, Logstash no longer accepts new events.\n\nEach input handles back pressure independently. For example, when the [beats](logstash-docs-md://lsr/plugins-inputs-beats.md) input encounters back pressure, it no longer accepts new connections and waits until the persistent queue has space to accept more events. After the filter and output stages finish processing existing events in the queue and ACKs them, Logstash automatically starts accepting new events.",
              "Controlling durability [durability-persistent-queues]": "Durability is a property of storage writes that ensures data will be available after it’s written.\n\nWhen the persistent queue feature is enabled, Logstash stores events on disk. Logstash commits to disk in a mechanism called *checkpointing*.\n\nThe queue itself is a set of pages. There are two kinds of pages: head pages and tail pages. The head page is where new events are written. There is only one head page. When the head page is of a certain size (see `queue.page_capacity`), it becomes a tail page, and a new head page is created. Tail pages are immutable, and the head page is append-only. Second, the queue records details about itself (pages, acknowledgements, etc) in a separate file called a checkpoint file.\n\nWhen recording a checkpoint, Logstash:\n\n['Calls `fsync` on the head page.', 'Atomically writes to disk the current state of the queue.']\n\nThe process of checkpointing is atomic, which means any update to the file is saved if successful.\n\n::::{important}\nIf Logstash is terminated, or if there is a hardware-level failure, any data that is buffered in the persistent queue, but not yet checkpointed, is lost.\n::::\n\nYou can force Logstash to checkpoint more frequently by setting `queue.checkpoint.writes`. This setting specifies the maximum number of events that may be written to disk before forcing a checkpoint. The default is 1024. To ensure maximum durability and avoid data loss in the persistent queue, you can set `queue.checkpoint.writes: 1` to force a checkpoint after each event is written. Keep in mind that disk writes have a resource cost. Setting this value to `1` can severely impact performance.",
              "Disk garbage collection [garbage-collection]": "On disk, the queue is stored as a set of pages where each page is one file. Each page can be at most `queue.page_capacity` in size. Pages are deleted (garbage collected) after all events in that page have been ACKed. If an older page has at least one event that is not yet ACKed, that entire page will remain on disk until all events in that page are successfully processed. Each page containing unprocessed events will count against the `queue.max_bytes` byte size."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/persistent-queues.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 58]"
        },
        {
          "title": "Pipeline-to-pipeline communication [pipeline-to-pipeline]",
          "description": null,
          "content": {
            "Configuration overview [pipeline-to-pipeline-overview]": {
              "How it works [how-pipeline-to-pipeline-works]": "The `pipeline` input acts as a virtual server listening on a single virtual address in the local process. Only `pipeline` outputs running on the same local Logstash can send events to this address. Pipeline `outputs` can send events to a list of virtual addresses. A `pipeline` output will be blocked if the downstream pipeline is blocked or unavailable.\n\nWhen events are sent across pipelines, their data is fully copied. Modifications to an event in a downstream pipeline do not affect that event in any upstream pipelines.\n\nThe `pipeline` plugin may be the most efficient way to communicate between pipelines, but it still incurs a performance cost. Logstash must duplicate each event in full on the Java heap for each downstream pipeline. Using this feature may affect the heap memory utilization of Logstash.",
              "Delivery guarantees [delivery-guarantees]": "In its standard configuration the `pipeline` input/output has at-least-once delivery guarantees. The output will be blocked if the address is blocked or unavailable.\n\nBy default, the `ensure_delivery` option on the `pipeline` output is set to `true.` If you change the `ensure_delivery` flag to `false`, an *unavailable* downstream pipeline causes the sent message to be discarded. Note that a pipeline is considered unavailable only when it is starting up or reloading, not when any of the plugins it may contain are blocked. A *blocked* downstream pipeline blocks the sending output/pipeline regardless of the value of the `ensure_delivery` flag. Use `ensure_delivery => false` when you want the ability to temporarily disable a downstream pipeline without blocking any upstream pipelines sending to it.\n\nThese delivery guarantees also inform the shutdown behavior of this feature. When performing a pipeline reload, changes will be made immediately as the user requests, even if that means removing a downstream pipeline receiving events from an upstream pipeline. This will cause the upstream pipeline to block. You must restore the downstream pipeline to cleanly shut down Logstash. You may issue a force kill, but inflight events may be lost unless the persistent queue is enabled for that pipeline.",
              "Avoid cycles [avoid-cycles]": "When you connect pipelines, keep the data flowing in one direction. Looping data or connecting the pipelines into a cyclic graph can cause problems. Logstash waits for each pipeline’s work to complete before shutting down. Pipeline loops can prevent Logstash from shutting down cleanly."
            },
            "Architectural patterns [architectural-patterns]": {
              "The distributor pattern [distributor-pattern]": "You can use the distributor pattern in situations where there are multiple types of data coming through a single input, each with its own complex set of processing rules. With the distributor pattern one pipeline is used to route data to other pipelines based on type. Each type is routed to a pipeline with only the logic for handling that type. In this way each type’s logic can be isolated.\n\nAs an example, in many organizations a single beats input may be used to receive traffic from a variety of sources, each with its own processing logic. A common way to deal with this type of data is to have a number of `if` conditions separating the traffic and processing each type differently. This approach can quickly get messy when configs are long and complex.\n\nHere is an example distributor pattern configuration.\n\n```\n# config/pipelines.yml\n- pipeline.id: beats-server\n  config.string: |\n    input { beats { port => 5044 } }\n    output {\n        if [type] == apache {\n          pipeline { send_to => weblogs }\n        } else if [type] == system {\n          pipeline { send_to => syslog }\n        } else {\n          pipeline { send_to => fallback }\n        }\n    }\n- pipeline.id: weblog-processing\n  config.string: |\n    input { pipeline { address => weblogs } }\n    filter {\n       # Weblog filter statements here...\n    }\n    output {\n      elasticsearch { hosts => [es_cluster_a_host] }\n    }\n- pipeline.id: syslog-processing\n  config.string: |\n    input { pipeline { address => syslog } }\n    filter {\n       # Syslog filter statements here...\n    }\n    output {\n      elasticsearch { hosts => [es_cluster_b_host] }\n    }\n- pipeline.id: fallback-processing\n    config.string: |\n    input { pipeline { address => fallback } }\n    output { elasticsearch { hosts => [es_cluster_b_host] } }\n```\n\nNotice how following the flow of data is a simple due to the fact that each pipeline only works on a single specific task.",
              "The output isolator pattern [output-isolator-pattern]": "You can use the output isolator pattern to prevent Logstash from becoming blocked if one of multiple outputs experiences a temporary failure. Logstash, by default, is blocked when any single output is down. This behavior is important in guaranteeing at-least-once delivery of data.\n\nFor example, a server might be configured to send log data to both Elasticsearch and an HTTP endpoint. The HTTP endpoint might be frequently unavailable due to regular service or other reasons. In this scenario, data would be paused from sending to Elasticsearch any time the HTTP endpoint is down.\n\nUsing the output isolator pattern and persistent queues, we can continue sending to Elasticsearch, even when one output is down.\n\nHere is an example of this scenario using the output isolator pattern.\n\n```\n# config/pipelines.yml\n- pipeline.id: intake\n  config.string: |\n    input { beats { port => 5044 } }\n    output { pipeline { send_to => [es, http] } }\n- pipeline.id: buffered-es\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => es } }\n    output { elasticsearch { } }\n- pipeline.id: buffered-http\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => http } }\n    output { http { } }\n```\n\nIn this architecture, each output has its own queue with its own tuning and settings. Note that this approach uses up to twice as much disk space and incurs three times as much serialization/deserialization cost as a single pipeline.\n\nIf any of the persistent queues of the downstream pipelines (in the example above, `buffered-es` and `buffered-http`) become full, both outputs will stop.",
              "The forked path pattern [forked-path-pattern]": "You can use the forked path pattern for situations where a single event must be processed more than once according to different sets of rules. Before the `pipeline` input and output were available, this need was commonly addressed through creative use of the `clone` filter and `if/else` rules.\n\nLet’s imagine a use case where we receive data and index the full event in our own systems, but publish a redacted version of the data to a partner’s S3 bucket. We might use the output isolator pattern described above to decouple our writes to either system. The distinguishing feature of the forked path pattern is the existence of additional rules in the downstream pipelines.\n\nHere is an example of the forked path configuration.\n\n```\n# config/pipelines.yml\n- pipeline.id: intake\n  queue.type: persisted\n  config.string: |\n    input { beats { port => 5044 } }\n    output { pipeline { send_to => [\"internal-es\", \"partner-s3\"] } }\n- pipeline.id: buffered-es\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => \"internal-es\" } }\n    # Index the full event\n    output { elasticsearch { } }\n- pipeline.id: partner\n  queue.type: persisted\n  config.string: |\n    input { pipeline { address => \"partner-s3\" } }\n    filter {\n      # Remove the sensitive data\n      mutate { remove_field => 'sensitive-data' }\n    }\n    output { s3 { } } # Output to partner's bucket\n```",
              "The collector pattern [collector-pattern]": "You can use the collector pattern when you want to define a common set of outputs and pre-output filters that many disparate pipelines might use. This pattern is the opposite of the distributor pattern. In this pattern many pipelines flow in to a single pipeline where they share outputs and processing. This pattern simplifies configuration at the cost of reducing isolation, since all data is sent through a single pipeline.\n\nHere is an example of the collector pattern.\n\n```\n# config/pipelines.yml\n- pipeline.id: beats\n  config.string: |\n    input { beats { port => 5044 } }\n    output { pipeline { send_to => [commonOut] } }\n- pipeline.id: kafka\n  config.string: |\n    input { kafka { ... } }\n    output { pipeline { send_to => [commonOut] } }\n- pipeline.id: partner\n  # This common pipeline enforces the same logic whether data comes from Kafka or Beats\n  config.string: |\n    input { pipeline { address => commonOut } }\n    filter {\n      # Always remove sensitive data from all input sources\n      mutate { remove_field => 'sensitive-data' }\n    }\n    output { elasticsearch { } }\n```"
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/pipeline-to-pipeline.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 59]"
        },
        {
          "title": "Cross-plugin concepts and features [plugin-concepts]",
          "description": null,
          "content": {
            "Space-deliminated URIs in list-type params [space-delimited-uris-in-list-params]": "List-type URI parameters will automatically expand strings that contain multiple whitespace-delimited URIs into separate entries. This behaviour enables the expansion of an arbitrary list of URIs from a single Environment- or Keystore-variable.\n\nThese plugins and options support this functionality:\n\n['[Elasticsearch input plugin - `hosts`](logstash-docs-md://lsr/plugins-inputs-elasticsearch.md#plugins-inputs-elasticsearch-hosts)', '[Elasticsearch output plugin - `hosts`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-hosts)', '[Elasticsearch filter plugin - `hosts`](logstash-docs-md://lsr/plugins-filters-elasticsearch.md#plugins-filters-elasticsearch-hosts)']\n\nYou can use this functionality to define an environment variable with multiple whitespace-delimited URIs and use it for the options above.\n\n**Example**\n\n```\nES_HOSTS=\"es1.example.com es2.example.com:9201 es3.example.com:9201\"\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/plugin-concepts.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 60]"
        },
        {
          "title": "Plugin Generator",
          "description": null,
          "content": {
            "Generating plugins [plugin-generator]": "You can create your own Logstash plugin in seconds! The generate subcommand of `bin/logstash-plugin` creates the foundation for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you can start adding custom code to process data with Logstash.\n\n**Example Usage**\n\n```\nbin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins\n```\n\n['`--type`: Type of plugin - input, filter, output, or codec', '`--name`: Name for the new plugin', '`--path`: Directory path where the new plugin structure will be created. If you don’t specify a directory, the plugin is created in the current directory.']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/plugin-generator.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 61]"
        },
        {
          "title": "Private Gem Repositories [private-rubygem]",
          "description": null,
          "content": {
            "Editing the Gemfile [_editing_the_gemfile]": "The gemfile is a configuration file that specifies information required for plugin management. Each gem file has a `source` line that specifies a location for plugin content.\n\nBy default, the gemfile’s `source` line reads:\n\n```\n# This is a Logstash generated Gemfile.\n# If you modify this file manually all comments and formatting will be lost.\n\nsource \"https://rubygems.org\"\n```\n\nTo change the source, edit the `source` line to contain your preferred source, as in the following example:\n\n```\n# This is a Logstash generated Gemfile.\n# If you modify this file manually all comments and formatting will be lost.\n\nsource \"https://my.private.repository\"\n```\n\nAfter saving the new version of the gemfile, use [plugin management commands](/reference/working-with-plugins.md) normally.\n\nThe following links contain further material on setting up some commonly used repositories:\n\n['[Geminabox](https://github.com/geminabox/geminabox/blob/master/README.md)', '[Artifactory](https://www.jfrog.com/confluence/display/RTF/RubyGems+Repositories)', 'Running a [rubygems mirror](http://guides.rubygems.org/run-your-own-gem-server/)']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/private-rubygem.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 62]"
        },
        {
          "title": "Processing Details [processing]",
          "description": null,
          "content": {
            "Event ordering [event-ordering]": {
              "*pipeline.ordered* setting [order-setting]": "The `pipeline.ordered` setting in [logstash.yml](/reference/logstash-settings-file.md) gives you more control over event ordering for single worker pipelines.\n\n`auto` automatically enables ordering if the `pipeline.workers` setting is also set to `1`. `true` will enforce ordering on the pipeline and prevent logstash from starting if there are multiple workers. `false` will disable the processing required to preserve order. Ordering will not be guaranteed, but you save the processing cost required to preserve order."
            },
            "Java pipeline initialization time [pipeline-init-time]": "The Java pipeline initialization time appears in the startup logs at INFO level. Initialization time is the time it takes to compile the pipeline config and instantiate the compiled execution for all workers.",
            "Reserved fields in {{ls}} events [reserved-fields]": "Some fields in {{ls}} events are reserved, or are required to adhere to a certain shape. Using these fields can cause runtime exceptions when the event API or plugins encounter incompatible values.\n\n|  |  |\n| --- | --- |\n| [`@metadata`](/reference/event-dependent-configuration.md#metadata) | A key/value map.<br>Ruby-based Plugin API: value is an[org.jruby.RubyHash](https://javadoc.io/static/org.jruby/jruby-core/9.2.5.0/org/jruby/RubyHash.html).<br>Java-based Plugin API: value is an[org.logstash.ConvertedMap](https://github.com/elastic/logstash/blob/main/logstash-core/src/main/java/org/logstash/ConvertedMap.java).<br>In serialized form (such as JSON): a key/value map where the keys must bestrings and the values are not constrained to a particular type. |\n| `@timestamp` | An object holding representation of a specific moment in time.<br>Ruby-based Plugin API: value is an[org.jruby.RubyTime](https://javadoc.io/static/org.jruby/jruby-core/9.2.5.0/org/jruby/RubyTime.html).<br>Java-based Plugin API: value is a[java.time.Instant](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/time/Instant.html).<br>In serialized form (such as JSON) or when setting with Event#set: anISO8601-compliant String value is acceptable. |\n| `@version` | A string, holding an integer value. |\n| `tags` | An array of distinct strings |"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/processing.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 63]"
        },
        {
          "title": "Queues Data Resiliency",
          "description": null,
          "content": {
            "Queues and data resiliency [resiliency]": "By default, Logstash uses [in-memory bounded queues](/reference/memory-queue.md) between pipeline stages (inputs → pipeline workers) to buffer events.\n\nAs data flows through the event processing pipeline, Logstash may encounter situations that prevent it from delivering events to the configured output. For example, the data might contain unexpected data types, or Logstash might terminate abnormally.\n\nTo guard against data loss and ensure that events flow through the pipeline without interruption, Logstash provides data resiliency features.\n\n['[Persistent queues (PQ)](/reference/persistent-queues.md) protect against data loss by storing events in an internal queue on disk.', '[Dead letter queues (DLQ)](/reference/dead-letter-queues.md) provide on-disk storage for events that Logstash is unable to process so that you can evaluate them. You can easily reprocess events in the dead letter queue by using the `dead_letter_queue` input plugin.']\n\nThese resiliency features are disabled by default. To turn on these features, you must explicitly enable them in the Logstash [settings file](/reference/logstash-settings-file.md)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/resiliency.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 64]"
        },
        {
          "title": "Reloading the Config File [reloading-config]",
          "description": null,
          "content": {
            "Force reloading the config file [force-reload]": "If Logstash is already running without auto-reload enabled, you can force Logstash to reload the config file and restart the pipeline. Do this by sending a SIGHUP (signal hangup) to the process running Logstash. For example:\n\n```\nkill -SIGHUP 14175\n```\n\nWhere 14175 is the ID of the process running Logstash.\n\n::::{note}\nThis functionality is not supported on Windows OS.\n::::",
            "How automatic config reloading works [_how_automatic_config_reloading_works]": "When Logstash detects a change in a config file, it stops the current pipeline by stopping all inputs, and it attempts to create a new pipeline that uses the updated configuration. After validating the syntax of the new configuration, Logstash verifies that all inputs and outputs can be initialized (for example, that all required ports are open). If the checks are successful, Logstash swaps the existing pipeline with the new pipeline. If the checks fail, the old pipeline continues to function, and the errors are propagated to the console.\n\nDuring automatic config reloading, the JVM is not restarted. The creating and swapping of pipelines all happens within the same process.\n\nChanges to [grok](logstash-docs-md://lsr/plugins-filters-grok.md) pattern files are also reloaded, but only when a change in the config file triggers a reload (or the pipeline is restarted).\n\nIn general, Logstash is not watching or monitoring any configuration files used or referenced by inputs, filters or outputs.",
            "Plugins that prevent automatic reloading [plugins-block-reload]": "Input and output plugins usually interact with OS resources. In some circumstances those resources can’t be released without a restart. For this reason some plugins can’t be simply updated and this prevents pipeline reload.\n\nThe [stdin input](logstash-docs-md://lsr/plugins-inputs-stdin.md) plugin, for example, prevents reloading for these reasons."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/reloading-config.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 65]"
        },
        {
          "title": "Running Logstash from the Command Line [running-logstash-command-line]",
          "description": null,
          "content": {
            "Command-Line Flags [command-line-flags]": "Logstash has the following flags. You can use the `--help` flag to display this information.\n\n**`--node.name NAME`**\n:   Specify the name of this Logstash instance. If no value is given it will default to the current hostname.\n\n**`-f, --path.config CONFIG_PATH`**\n:   Load the Logstash config from a specific file or directory. If a directory is given, all files in that directory will be concatenated in lexicographical order and then parsed as a single config file. Specifying this flag multiple times is not supported. If you specify this flag multiple times, Logstash uses the last occurrence (for example, `-f foo -f bar` is the same as `-f bar`).\n\nYou can specify wildcards ([globs](/reference/glob-support.md)) and any matched files will be loaded in the order described above. For example, you can use the wildcard feature to load specific files by name:\n\n```shell\nbin/logstash --debug -f '/tmp/{one,two,three}'\n```\n\nWith this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/two`, and `/tmp/three`, and parses them into a single config.\n\n\n\n**`-e, --config.string CONFIG_STRING`**\n:   Use the given string as the configuration data. Same syntax as the config file. If no input is specified, then the following is used as the default input: `input { stdin { type => stdin } }` and if no output is specified, then the following is used as the default output: `output { stdout { codec => rubydebug } }`. If you wish to use both defaults, please use the empty string for the `-e` flag. The default is nil.\n\n**`--plugin-classloaders`**\n:   (Beta) Load Java plugins in independent classloaders to isolate their dependencies.\n\n**`--pipeline.id ID`**\n:   Sets the ID of pipeline. The default is `main`.\n\n**`-w, --pipeline.workers COUNT`**\n:   Sets the number of pipeline workers to run. This option sets the number of workers that will, in parallel, execute the filter and output stages of the pipeline. If you find that events are backing up, or that  the CPU is not saturated, consider increasing this number to better utilize machine processing power. The default is the number of the host’s CPU cores.\n\n**`--pipeline.ordered ORDERED`**\n:   Preserves events order. Possible values are `auto` (default), `true` and `false`. This setting will work only when also using a single worker for the pipeline. Note that when enabled, it may impact the performance of the filters and output processing. The `auto` option will automatically enable ordering if the `pipeline.workers` setting is set to `1`. Use `true` to enable ordering on the pipeline and prevent logstash from starting if there are multiple workers. Use `false` to disable any extra processing necessary for preserving ordering.\n\n**`-b, --pipeline.batch.size SIZE`**\n:   Size of batches the pipeline is to work in. This option defines the maximum number of events an individual worker thread will collect from inputs before attempting to execute its filters and outputs. The default is 125 events. Larger batch sizes are generally more efficient, but come at the cost of increased memory overhead. You may need to increase JVM heap space in the `jvm.options` config file. See [Logstash Configuration Files](/reference/config-setting-files.md) for more info.\n\n**`-u, --pipeline.batch.delay DELAY_IN_MS`**\n:   When creating pipeline batches, how long to wait while polling for the next event. This option defines how long in milliseconds to wait while polling for the next event before dispatching an undersized batch to filters and outputs. The default is 50ms.\n\n**`--pipeline.ecs_compatibility MODE`**\n:   Sets the process default value for  ECS compatibility mode. Can be an ECS version like `v1` or `v8`, or `disabled`. The default is `v8`. Pipelines defined before Logstash 8 operated without ECS in mind. To ensure a migrated pipeline continues to operate as it did in older releases of Logstash, opt-OUT of ECS for the individual pipeline by setting `pipeline.ecs_compatibility: disabled` in its `pipelines.yml` definition. Using the command-line flag will set the default for *all* pipelines, including new ones. See [ECS compatibility](/reference/ecs-ls.md#ecs-compatibility) for more info.\n\n**`--pipeline.unsafe_shutdown`**\n:   Force Logstash to exit during shutdown even if there are still inflight events in memory. By default, Logstash will refuse to quit until all received events have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.\n\n**`--path.data PATH`**\n:   This should point to a writable directory. Logstash will use this directory whenever it needs to store data. Plugins will also have access to this path. The default is the `data` directory under Logstash home.\n\n**`-p, --path.plugins PATH`**\n:   A path of where to find custom plugins. This flag can be given multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`, and `NAME` is the name of the plugin.\n\n**`-l, --path.logs PATH`**\n:   Directory to write Logstash internal logs to.\n\n**`--log.level LEVEL`**\n:   Set the log level for Logstash. Possible values are:\n\n* `fatal`: log very severe error messages that will usually be followed by the application aborting\n* `error`: log errors\n* `warn`: log warnings\n* `info`: log verbose info (this is the default)\n* `debug`: log debugging info (for developers)\n* `trace`: log finer-grained messages beyond debugging info\n\n\n\n**`--config.debug`**\n:   Show the fully compiled configuration as a debug log message (you must also have `--log.level=debug` enabled).\n\n:::{warning}\nThe log message will include any *password* options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs!\n:::\n\n\n**`-i, --interactive SHELL`**\n:   Drop to shell instead of running as normal. Valid shells are \"irb\" and \"pry\".\n\n**`-V, --version`**\n:   Emit the version of Logstash and its friends, then exit.\n\n**`-t, --config.test_and_exit`**\n:   Check configuration for valid syntax and then exit. Note that grok patterns are not checked for correctness with this flag. Logstash can read multiple config files from a directory. If you combine this flag with `--log.level=debug`, Logstash will log the combined config file, annotating each config block with the source file it came from.\n\n**`-r, --config.reload.automatic`**\n:   Monitor configuration changes and reload whenever the configuration is changed.\n\n:::{note}\nUse SIGHUP to manually reload the config. The default is false.\n:::\n\n\n**`--config.reload.interval RELOAD_INTERVAL`**\n:   How frequently to poll the configuration location for changes. The default value is \"3s\". Note that the unit qualifier (`s`) is required.\n\n**`--api.enabled ENABLED`**\n:   The HTTP API is enabled by default, but can be disabled by passing `false` to this option.\n\n**`--api.http.host HTTP_HOST`**\n:   Web API binding host. This option specifies the bind address for the metrics REST endpoint. The default is \"127.0.0.1\".\n\n**`--api.http.port HTTP_PORT`**\n:   Web API http port. This option specifies the bind port for the metrics REST endpoint. The default is 9600-9700. This setting accepts a range of the format 9600-9700. Logstash will pick up the first available port.\n\n**`--log.format FORMAT`**\n:   Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text (using Ruby’s Object#inspect). The default is \"plain\".\n\n**`--log.format.json.fix_duplicate_message_fields ENABLED`**\n:   Avoid `message` field collision using JSON log format. Possible values are `true` (default) and `false`.\n\n**`--path.settings SETTINGS_DIR`**\n:   Set the directory containing the `logstash.yml` [settings file](/reference/logstash-settings-file.md) as well as the log4j logging configuration. This can also be set through the LS_SETTINGS_DIR environment variable. The default is the `config` directory under Logstash home.\n\n**`--enable-local-plugin-development`**\n:   This flag enables developers to update their local Gemfile without running into issues caused by a frozen lockfile. This flag can be helpful when you are developing/testing plugins locally.\n\n::::{note}\nThis flag is for Logstash developers only. End users should not need it.\n::::\n\n**`-h, --help`**\n:   Print help"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/running-logstash-command-line.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 66]"
        },
        {
          "title": "Running Logstash Kubernetes",
          "description": null,
          "content": {
            "Running Logstash on Kubernetes [running-logstash-kubernetes]": "Check out the [QuickStart](docs-content://deploy-manage/deploy/cloud-on-k8s/install-using-yaml-manifest-quickstart.md) to install ECK and [Run {{ls}} on ECK](docs-content://deploy-manage/deploy/cloud-on-k8s/logstash.md) to deploy {{ls}} with ECK."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/running-logstash-kubernetes.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 67]"
        },
        {
          "title": "Running Logstash on Windows [running-logstash-windows]",
          "description": null,
          "content": {
            "Validating JVM prerequisites on Windows [running-logstash-windows-validation]": {
              "`Write-Host $env:LS_JAVA_HOME` [_write_host_envls_java_home]": [
                "The output should be pointed to where the JVM software is located, for example:",
                "```\nPS C:\\> Write-Host $env:LS_JAVA_HOME\nC:\\Program Files\\Java\\jdk-11.0.3\n```",
                "If `LS_JAVA_HOME` is not set, perform one of the following:",
                [
                  "Set using the GUI:",
                  [
                    "Navigate to the Windows [Environmental Variables](https://docs.microsoft.com/en-us/windows/win32/procthread/environment-variables) window",
                    "In the Environmental Variables window, edit LS_JAVA_HOME to point to where the JDK software is located, for example: `C:\\Program Files\\Java\\jdk-11.0.3`"
                  ],
                  "Set using PowerShell:",
                  [
                    "In an Administrative PowerShell session, execute the following [SETX](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/setx) commands:",
                    "```\nPS C:\\Windows\\system32> SETX /m LS_JAVA_HOME \"C:\\Program Files\\Java\\jdk-11.0.3\"\nPS C:\\Windows\\system32> SETX /m PATH \"$env:PATH;C:\\Program Files\\Java\\jdk-11.0.3\\bin;\"\n```",
                    "Exit PowerShell, then open a new PowerShell session and run `Write-Host $env:LS_JAVA_HOME` to verify"
                  ]
                ]
              ],
              "`Java -version` [_java_version]": [
                "This command produces output similar to the following:",
                "```\nPS C:\\> Java -version\njava version \"11.0.3\" 2019-04-16 LTS\nJava(TM) SE Runtime Environment 18.9 (build 11.0.3+12-LTS)\nJava HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.3+12-LTS, mixed mode)\n```"
              ]
            },
            "Running Logstash manually [running-logstash-windows-manual]": "Logstash can be run manually using [PowerShell](https://docs.microsoft.com/en-us/powershell/).  Open an Administrative [PowerShell](https://docs.microsoft.com/en-us/powershell/) session, then run the following commands:\n\n```\nPS C:\\Windows\\system32> cd C:\\logstash-9.0.0\\\nPS C:\\logstash-9.0.0> .\\bin\\logstash.bat -f .\\config\\syslog.conf\n```\n\n::::{note}\nIn a production environment, we recommend that you use [logstash.yml](/reference/logstash-settings-file.md) to control Logstash execution.\n::::\n\nWait for the following messages to appear, to confirm Logstash has started successfully:\n\n```\n[logstash.runner          ] Starting Logstash {\"logstash.version\"=>\"9.0.0\"}\n[logstash.inputs.udp      ] Starting UDP listener {:address=>\"0.0.0.0:514\"}\n[logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}\n```",
            "Running Logstash as a service with NSSM [running-logstash-windows-nssm]": "::::{note}\nIt is recommended to validate your configuration works by running Logstash manually before you proceed.\n::::\n\nDownload [NSSM](https://nssm.cc/), then extract `nssm.exe` from `nssm-<version.number>\\win64\\nssm.exe` to `C:\\logstash-9.0.0\\bin\\`. Then open an Administrative [PowerShell](https://docs.microsoft.com/en-us/powershell/) session, then run the following commands:\n\n```\nPS C:\\Windows\\system32> cd C:\\logstash-9.0.0\\\nPS C:\\logstash-9.0.0> .\\bin\\nssm.exe install logstash\n```\n\nOnce the `NSSM service installer` window appears, specify the following parameters in the `Application` tab:\n\n['In the `Application` tab:', ['Path: Path to `logstash.bat`: `C:\\\\logstash-9.0.0\\\\bin\\\\logstash.bat`', 'Startup Directory: Path to the `bin` directory: `C:\\\\logstash-9.0.0\\\\bin`', 'Arguments: For this example to start Logstash: `-f C:\\\\logstash-9.0.0\\\\config\\\\syslog.conf`', '::::{note}\\nIn a production environment, we recommend that you use [logstash.yml](/reference/logstash-settings-file.md) to control Logstash execution.\\n::::'], 'Review and make any changes necessary in the `Details` tab:', ['Ensure `Startup Type` is set appropriately', 'Set the `Display name` and `Description` fields to something relevant'], 'Review any other required settings (for the example we aren’t making any other changes)', ['Be sure to determine if you need to set the `Log on` user'], 'Validate the `Service name` is set appropriately', ['For this example, we will set ours to `logstash-syslog`'], 'Click `Install Service`', ['Click *OK* when the `Service \"logstash-syslog\" installed successfully!` window appears']]\n\nOnce the service has been installed with NSSM, validate and start the service following the [PowerShell Managing Services](https://docs.microsoft.com/en-us/powershell/scripting/samples/managing-services) documentation.",
            "Running Logstash with Task Scheduler [running-logstash-windows-scheduledtask]": "::::{note}\nIt is recommended to validate your configuration works by running Logstash manually before you proceed.\n::::\n\nOpen the Windows [Task Scheduler](https://docs.microsoft.com/en-us/windows/desktop/taskschd/task-scheduler-start-page), then click `Create Task` in the Actions window.  Specify the following parameters in the `Actions` tab:\n\n['In the `Actions` tab:', ['Click `New`, then specify the following:', 'Action: `Start a program`', 'Program/script: `C:\\\\logstash-9.0.0\\\\bin\\\\logstash.bat`', 'Add arguments: `-f C:\\\\logstash-9.0.0\\\\config\\\\syslog.conf`', 'Start in: `C:\\\\logstash-9.0.0\\\\bin\\\\`', '::::{note}\\nIn a production environment, we recommend that you use [logstash.yml](/reference/logstash-settings-file.md) to control Logstash execution.\\n::::'], 'Review and make any changes necessary in the `General`, `Triggers`, `Conditions`, and `Settings` tabs.', 'Click `OK` to finish creating the scheduled task.', 'Once the new task has been created, either wait for it to run on the schedule or select the service then click `Run` to start the task.']\n\n::::{note}\nLogstash can be stopped by selecting the service, then clicking `End` in the Task Scheduler window.\n::::",
            "Example Logstash Configuration [running-logstash-windows-example]": "We will configure Logstash to listen for syslog messages over port 514 with this configuration (file name is `syslog.conf`):\n\n```\n# Sample Logstash configuration for receiving\n# UDP syslog messages over port 514\n\ninput {\n  udp {\n    port => 514\n    type => \"syslog\"\n  }\n}\n\noutput {\n  elasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 68]"
        },
        {
          "title": "Running Logstash as a Service on Debian or RPM [running-logstash]",
          "description": null,
          "content": {
            "Running Logstash by Using Systemd [running-logstash-systemd]": "Distributions like Debian Jessie, Ubuntu 15.10+, and many of the SUSE derivatives use systemd and the `systemctl` command to start and stop services. Logstash places the systemd unit files in `/etc/systemd/system` for both deb and rpm. After installing the package, you can start up Logstash with:\n\n```\nsudo systemctl start logstash.service\n```"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/running-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 69]"
        },
        {
          "title": "Secure your connection to {{es}} [ls-security]",
          "description": null,
          "content": {
            "{{es}} security on by default [es-security-on]": {
              "Secure communication with an on-premise {{es}} cluster [es-security-onprem]": {
                "Copy and save the certificate [es-sec-copy-cert]": "By default an on-premise {{es}} cluster generates a self-signed CA and creates its own SSL certificates when it starts. Therefore {{ls}} needs its own copy of the self-signed CA from the {{es}} cluster in order for {{ls}} to validate the certificate presented by {{es}}.\n\nCopy the [self-signed CA certificate](docs-content://deploy-manage/deploy/self-managed/installing-elasticsearch.md#stack-security-certificates) from the {{es}} `config/certs` directory.\n\nSave it to a location that Logstash can access, such as `config/certs` on the {{ls}} instance.",
                "Configure the elasticsearch output [es-sec-plugin]": "Use the [`elasticsearch output`'s](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) [`ssl_certificate_authorities` option](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_certificate_authorities) to point to the certificate’s location.\n\n**Example**\n\n```\noutput {\n  elasticsearch {\n    hosts => [\"https://...] <1>\n    ssl_certificate_authorities => ['/etc/logstash/config/certs/ca.crt'] <2>\n  }\n}\n```\n\n['Note that the `hosts` url must begin with `https`', 'Path to the {{ls}} copy of the {{es}} certificate']\n\nFor more information about establishing secure communication with {{es}}, see [security is on by default](docs-content://deploy-manage/deploy/self-managed/installing-elasticsearch.md)."
              },
              "Configuring Logstash to use basic authentication [ls-http-auth-basic]": "Logstash needs to be able to manage index templates, create indices, and write and delete documents in the indices it creates.\n\nTo set up authentication credentials for Logstash:\n\n['Use the **Management > Roles** UI in {{kib}} or the `role` API to create a `logstash_writer` role. For **cluster** privileges, add `manage_index_templates` and `monitor`. For **indices** privileges, add `write`, `create`, and `create_index`.', 'Add `manage_ilm` for cluster and `manage` and `manage_ilm` for indices if you plan to use [index lifecycle management](docs-content://manage-data/lifecycle/index-lifecycle-management/tutorial-automate-rollover.md).', '```\\nPOST _security/role/logstash_writer\\n{\\n  \"cluster\": [\"manage_index_templates\", \"monitor\", \"manage_ilm\"], <1>\\n  \"indices\": [\\n    {\\n      \"names\": [ \"logstash-*\" ], <2>\\n      \"privileges\": [\"write\",\"create\",\"create_index\",\"manage\",\"manage_ilm\"]  <3>\\n    }\\n  ]\\n}\\n```', ['The cluster needs the `manage_ilm` privilege if [index lifecycle management](docs-content://manage-data/lifecycle/index-lifecycle-management/tutorial-automate-rollover.md) is enabled.', 'If you use a custom Logstash index pattern, specify your custom pattern instead of the default `logstash-*` pattern.', 'If [index lifecycle management](docs-content://manage-data/lifecycle/index-lifecycle-management/tutorial-automate-rollover.md) is enabled, the role requires the `manage` and `manage_ilm` privileges to load index lifecycle policies, create rollover aliases, and create and manage rollover indices.'], 'Create a `logstash_internal` user and assign it the `logstash_writer` role. You can create users from the **Management > Users** UI in {{kib}} or through the `user` API:', '```\\nPOST _security/user/logstash_internal\\n{\\n  \"password\" : \"x-pack-test-password\",\\n  \"roles\" : [ \"logstash_writer\"],\\n  \"full_name\" : \"Internal Logstash User\"\\n}\\n```', 'Configure Logstash to authenticate as the `logstash_internal` user you just created. You configure credentials separately for each of the {{es}} plugins in your Logstash `.conf` file. For example:', '```\\ninput {\\n  elasticsearch {\\n    ...\\n    user => logstash_internal\\n    password => x-pack-test-password\\n  }\\n}\\nfilter {\\n  elasticsearch {\\n    ...\\n    user => logstash_internal\\n    password => x-pack-test-password\\n  }\\n}\\noutput {\\n  elasticsearch {\\n    ...\\n    user => logstash_internal\\n    password => x-pack-test-password\\n  }\\n}\\n```']",
              "Granting access to the indices Logstash creates [ls-user-access]": "To access the indices Logstash creates, users need the `read` and `view_index_metadata` privileges:\n\n['Create a `logstash_reader` role that has the `read` and `view_index_metadata` privileges  for the Logstash indices. You can create roles from the **Management > Roles** UI in {{kib}} or through the `role` API:', '```\\nPOST _security/role/logstash_reader\\n{\\n  \"cluster\": [\"manage_logstash_pipelines\"],\\n  \"indices\": [\\n    {\\n      \"names\": [ \"logstash-*\" ],\\n      \"privileges\": [\"read\",\"view_index_metadata\"]\\n    }\\n  ]\\n}\\n```', 'Assign your Logstash users the `logstash_reader` role. If the Logstash user will be using [centralized pipeline management](/reference/logstash-centralized-pipeline-management.md), also assign the `logstash_system` role. You can create and manage users from the **Management > Users** UI in {{kib}} or through the `user` API:', '```\\nPOST _security/user/logstash_user\\n{\\n  \"password\" : \"x-pack-test-password\",\\n  \"roles\" : [ \"logstash_reader\", \"logstash_system\"], <1>\\n  \"full_name\" : \"Kibana User for Logstash\"\\n}\\n```', ['`logstash_system` is a built-in role that provides the necessary permissions to check the availability of the supported features of {{es}} cluster.']]",
              "Configuring Logstash to use TLS/SSL encryption [ls-http-ssl]": "If TLS encryption is enabled on an on premise {{es}} cluster, you need to configure the `ssl` and `cacert` options in your Logstash `.conf` file:\n\n```\noutput {\n  elasticsearch {\n    ...\n    ssl_enabled => true\n    ssl_certificate_authorities => '/path/to/cert.pem' <1>\n  }\n}\n```\n\n['The path to the local `.pem` file that contains the Certificate Authority’s certificate.']\n\n::::{note}\nHosted {{ess}} simplifies security. This configuration step is not necessary for hosted Elasticsearch Service on Elastic Cloud. {{ess-leadin-short}}\n::::",
              "Configuring the {{es}} output to use PKI authentication [ls-http-auth-pki]": "The `elasticsearch` output supports PKI authentication. To use an X.509 client-certificate for authentication, you configure the `keystore` and `keystore_password` options in your Logstash `.conf` file:\n\n```\noutput {\n  elasticsearch {\n    ...\n    ssl_keystore_path => /path/to/keystore.jks\n    ssl_keystore_password => realpassword\n    ssl_truststore_path =>  /path/to/truststore.jks <1>\n    ssl_truststore_password =>  realpassword\n  }\n}\n```\n\n['If you use a separate truststore, the truststore path and password are also required.']",
              "Configuring credentials for {{ls}} monitoring [ls-monitoring-user]": "If you want to monitor your Logstash instance with {{stack-monitor-features}}, and store the monitoring data in a secured {{es}} cluster, you must configure Logstash with a username and password for a user with the appropriate permissions.\n\nThe {{security-features}} come preconfigured with a [`logstash_system` built-in user](docs-content://deploy-manage/users-roles/cluster-or-deployment-auth/built-in-users.md) for this purpose. This user has the minimum permissions necessary for the monitoring function, and *should not* be used for any other purpose - it is specifically *not intended* for use within a Logstash pipeline.\n\nBy default, the `logstash_system` user does not have a password. The user will not be enabled until you set a password. See [Setting built-in user passwords](docs-content://deploy-manage/users-roles/cluster-or-deployment-auth/built-in-users.md#set-built-in-user-passwords).\n\nThen configure the user and password in the `logstash.yml` configuration file:\n\n```\nxpack.monitoring.elasticsearch.username: logstash_system\nxpack.monitoring.elasticsearch.password: t0p.s3cr3t\n```\n\nIf you initially installed an older version of {{xpack}} and then upgraded, the `logstash_system` user may have defaulted to `disabled` for security reasons. You can enable the user through the `user` API:\n\n```\nPUT _security/user/logstash_system/_enable\n```",
              "Configuring credentials for Centralized Pipeline Management [ls-pipeline-management-user]": "If you plan to use Logstash [centralized pipeline management](/reference/logstash-centralized-pipeline-management.md), you need to configure the username and password that Logstash uses for managing configurations.\n\nYou configure the user and password in the `logstash.yml` configuration file:\n\n```\nxpack.management.elasticsearch.username: logstash_admin_user <1>\nxpack.management.elasticsearch.password: t0p.s3cr3t\n```\n\n['The user you specify here must have the built-in `logstash_admin` role as well as the `logstash_writer` role that you created earlier.']",
              "Grant access using API keys [ls-api-keys]": {
                "Create an API key [ls-create-api-key]": {
                  "Create an API key for publishing [ls-api-key-publish]": "You’re in luck! The example we used in the [Create an API key](#ls-create-api-key) section creates an API key for publishing to {{es}} using the [Elasticsearch output plugin](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md).\n\nHere’s an example using the API key in your [Elasticsearch output plugin](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) configuration.\n\n```\noutput {\n  elasticsearch {\n    api_key => \"TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA\" <1>\n  }\n}\n```\n\n['The format of the value is `id:api_key`, where `id` and `api_key` are the values returned by the [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key)']",
                  "Create an API key for reading [ls-api-key-input]": "Creating an API key to use for reading data from {{es}} is similar to creating an API key for publishing described earlier. You can use the example in the [Create an API key](#ls-create-api-key) section, granting the appropriate privileges.\n\nHere’s an example using the API key in your [Elasticsearch inputs plugin](logstash-docs-md://lsr/plugins-inputs-elasticsearch.md) configuration.\n\n```\ninput {\n  elasticsearch {\n    \"api_key\" => \"TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA\" <1>\n  }\n}\n```\n\n['The format of the value is `id:api_key`, where `id` and `api_key` are the values returned by the [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key)']",
                  "Create an API key for filtering [ls-api-key-filter]": "Creating an API key to use for processing data from {{es}} is similar to creating an API key for publishing described earlier. You can use the example in the [Create an API key](#ls-create-api-key) section, granting the appropriate privileges.\n\nHere’s an example using the API key in your [Elasticsearch filter plugin](logstash-docs-md://lsr/plugins-filters-elasticsearch.md) configuration.\n\n```\nfilter {\n  elasticsearch {\n    api_key => \"TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA\" <1>\n  }\n}\n```\n\n['The format of the value is `id:api_key`, where `id` and `api_key` are the values returned by the [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key)']",
                  "Create an API key for monitoring [ls-api-key-monitor]": "To create an API key to use for sending monitoring data to {{es}}, use the [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key). For example:\n\n```\nPOST /_security/api_key\n{\n  \"name\": \"logstash_host001\", <1>\n  \"role_descriptors\": {\n    \"logstash_monitoring\": { <2>\n      \"cluster\": [\"monitor\"],\n      \"index\": [\n        {\n          \"names\": [\".monitoring-ls-*\"],\n          \"privileges\": [\"create_index\", \"create\"]\n        }\n      ]\n    }\n  }\n}\n```\n\n['Name of the API key', 'Granted privileges']\n\nThe return value should look similar to this:\n\n```\n{\n  \"id\":\"TiNAGG4BaaMdaH1tRfuU\", <1>\n  \"name\":\"logstash_host001\",\n  \"api_key\":\"KnR6yE41RrSowb0kQ0HWoA\" <2>\n}\n```\n\n['Unique id for this API key', 'Generated API key']\n\nNow you can use this API key in your logstash.yml configuration file:\n\n```\nxpack.monitoring.elasticsearch.api_key: TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA <1>\n```\n\n['The format of the value is `id:api_key`, where `id` and `api_key` are the values returned by the [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key)']",
                  "Create an API key for central management [ls-api-key-man]": "To create an API key to use for central management, use the [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key). For example:\n\n```\nPOST /_security/api_key\n{\n  \"name\": \"logstash_host001\", <1>\n  \"role_descriptors\": {\n    \"logstash_monitoring\": { <2>\n      \"cluster\": [\"monitor\", \"manage_logstash_pipelines\"]\n    }\n  }\n}\n```\n\n['Name of the API key', 'Granted privileges']\n\nThe return value should look similar to this:\n\n```\n{\n  \"id\":\"TiNAGG4BaaMdaH1tRfuU\", <1>\n  \"name\":\"logstash_host001\",\n  \"api_key\":\"KnR6yE41RrSowb0kQ0HWoA\" <2>\n}\n```\n\n['Unique id for this API key', 'Generated API key']\n\nNow you can use this API key in your logstash.yml configuration file:\n\n```\nxpack.management.elasticsearch.api_key: TiNAGG4BaaMdaH1tRfuU:KnR6yE41RrSowb0kQ0HWoA <1>\n```\n\n['The format of the value is `id:api_key`, where `id` and `api_key` are the values returned by the [Create API key API](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key)']"
                },
                "Learn more about API keys [learn-more-api-keys]": "See the {{es}} API key documentation for more information:\n\n['[Create API key](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-create-api-key)', '[Get API key information](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-get-api-key)', '[Invalidate API key](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-security-invalidate-api-key)']\n\nSee [API Keys](docs-content://deploy-manage/api-keys/elasticsearch-api-keys.md) for info on managing API keys through {{kib}}."
              }
            }
          },
          "metadata": {
            "navigation_title": "Secure your connection",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/ls-security.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 70]"
        },
        {
          "title": "Collect {{ls}} monitoring data for dashboards ({{serverless-short}}) [serverless-monitoring-with-elastic-agent]",
          "description": null,
          "content": {
            "Add and configure the {{ls}} integration [setup-project]": "**Add the {{ls}} integration**\n\n['Log in to your [cloud.elastic.co](https://cloud.elastic.co/) account and create an Observability serverless project.', 'Select **Get Started** from the main menu.', 'Select **Start exploring** (near the bottom of the page).', 'On the **Integrations** page, search for **{{ls}}** and select it to see details.', 'Click **Add {{ls}}**.', 'Follow the instructions to install {{agent}} and add the {{ls}} integration.']\n\nFor more info, check out the [Elastic Observability](docs-content://solutions/observability.md) docs.\n\n**Configure the integration to collect logs**\n\n['Make sure that **Logs** is ON if you want to collect logs from your {{ls}} instance. Check the settings to be sure that they are configured correctly.', 'Modify the log paths to match your {{ls}} environment.']\n\n**Configure the integration to collect metrics**\n\n::::{tip}\nFor the best experience with the Logstash dashboards, we recommend collecting all of the metrics. Turning off metrics will result in incomplete or missing visualizations.\n::::\n\n['Make sure that **Metrics (Elastic Agent)** is turned on (default), and **Metrics (Stack Monitoring)** is turned off.', 'Set the {{ls}} URL to point to your {{ls}} instance.<br> By default, the integration collects {{ls}} monitoring metrics from `https://localhost:9600`. If that host and port number are not correct, update the `Logstash URL` setting. If you configured {{ls}} to use encrypted communications and/or a username and password, you must access it using HTTPS. Expand the **Advanced Settings** options, and fill in the appropriate values for your {{ls}} instance.']",
            "View assets [view-assets-esvrless]": [
              "Go to **Project settings → Integrations** to see your **Installed integrations**.",
              "Select the {{ls}} integration, and then select **Assets** to access dashboards for the {{ls}} integration."
            ],
            "Monitor {{ls}} logs and metrics [view-data-svrless]": "From the list of assets, open the **[Metrics {{ls}}] {{ls}} overview** dashboard to view overall performance. Then follow the navigation panel to further drill down into {{ls}} performance.\n\n% TO DO: Use `:class: screenshot`\n![The {{ls}} Overview dashboard in {{kib}} with various metrics from your monitored {{ls}}](images/integration-dashboard-overview.png)\n\nYou can hover over any visualization to adjust its settings, or click the **Edit** button to make changes to the dashboard. To learn more, refer to [Dashboard and visualizations](docs-content://explore-analyze/dashboards.md)."
          },
          "metadata": {
            "navigation_title": "Collect monitoring data for dashboards ({{serverless-short}} )",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/serverless-monitoring-with-elastic-agent.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 71]"
        },
        {
          "title": "Setting Up Running Logstash",
          "description": null,
          "content": {
            "Setting up and running Logstash [setup-logstash]": "Before reading this section, see [Installing Logstash](/reference/installing-logstash.md) for basic installation instructions to get you started.\n\nThis section includes additional information on how to set up and run Logstash, including:\n\n['[Logstash Directory Layout](/reference/dir-layout.md)', '[Logstash Configuration Files](/reference/config-setting-files.md)', '[logstash.yml](/reference/logstash-settings-file.md)', '[Secrets keystore for secure settings](/reference/keystore.md)', '[Running Logstash from the Command Line](/reference/running-logstash-command-line.md)', '[Running Logstash as a Service on Debian or RPM](/reference/running-logstash.md)', '[Running Logstash on Docker](/reference/docker.md)', '[Configuring Logstash for Docker](/reference/docker-config.md)', '[Running Logstash on Kubernetes](/reference/running-logstash-kubernetes.md)', '[Running Logstash on Windows](/reference/running-logstash-windows.md)', '[Logging](/reference/logging.md)', '[Shutting Down Logstash](/reference/shutdown.md)']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/setup-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 72]"
        },
        {
          "title": "Shutting Down Logstash [shutdown]",
          "description": null,
          "content": {
            "What Happens During a Controlled Shutdown? [_what_happens_during_a_controlled_shutdown]": "When you attempt to shut down a running Logstash instance, Logstash performs several steps before it can safely shut down. It must:\n\n['Stop all input, filter and output plugins', 'Process all in-flight events', 'Terminate the Logstash process']\n\nThe following conditions affect the shutdown process:\n\n['An input plugin receiving data at a slow pace.', 'A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy query.', 'A disconnected output plugin that is waiting to reconnect to flush in-flight events.']\n\nThese situations make the duration and success of the shutdown process unpredictable.\n\nLogstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown. This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy worker threads.\n\nTo enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--pipeline.unsafe_shutdown` flag when you start Logstash.\n\n::::{warning}\nUnsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason may result in data loss (unless you’ve enabled Logstash to use [persistent queues](/reference/persistent-queues.md)). Shut down Logstash safely whenever possible.\n::::",
            "Stall Detection Example [shutdown-stall-example]": "In this example, slow filter execution prevents the pipeline from performing a clean shutdown. Because Logstash is started with the `--pipeline.unsafe_shutdown` flag, the shutdown results in the loss of 20 events.\n\n::::{admonition}\n\n```\nbin/logstash -e 'input { generator { } } filter { ruby { code => \"sleep 10000\" } }\n  output { stdout { codec => dots } }' -w 1 --pipeline.unsafe_shutdown\nPipeline main started\n^CSIGINT received. Shutting down the agent. {:level=>:warn}\nstopping pipeline {:id=>\"main\", :level=>:warn}\nReceived shutdown signal, but pipeline is still waiting for in-flight events\nto be processed. Sending another ^C will force quit Logstash, but this may cause\ndata loss. {:level=>:warn}\n{\"inflight_count\"=>125, \"stalling_thread_info\"=>{[\"LogStash::Filters::Ruby\",\n{\"code\"=>\"sleep 10000\"}]=>[{\"thread_id\"=>19, \"name\"=>\"[main]>worker0\",\n\"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}} {:level=>:warn}\nThe shutdown process appears to be stalled due to busy or blocked plugins.\nCheck the logs for more information. {:level=>:error}\n{\"inflight_count\"=>125, \"stalling_thread_info\"=>{[\"LogStash::Filters::Ruby\",\n{\"code\"=>\"sleep 10000\"}]=>[{\"thread_id\"=>19, \"name\"=>\"[main]>worker0\",\n\"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}} {:level=>:warn}\n{\"inflight_count\"=>125, \"stalling_thread_info\"=>{[\"LogStash::Filters::Ruby\",\n{\"code\"=>\"sleep 10000\"}]=>[{\"thread_id\"=>19, \"name\"=>\"[main]>worker0\",\n\"current_call\"=>\"(ruby filter code):1:in `sleep'\"}]}} {:level=>:warn}\nForcefully quitting logstash.. {:level=>:fatal}\n```\n\n::::\n\nWhen `--pipeline.unsafe_shutdown` isn’t enabled, Logstash continues to run and produce these reports periodically."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/shutdown.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 73]"
        },
        {
          "title": "Tips and best practices [tips]",
          "description": null,
          "content": {
            "Command line [tip-cli]": {
              "Shell commands on Windows OS [tip-windows-cli]": "Command line examples often show single quotes. On Windows systems, replace a single quote `'` with a double quote `\"`.\n\n**Example**\n\nInstead of:\n\n```\nbin/logstash -e 'input { stdin { } } output { stdout {} }'\n```\n\nUse this format on Windows systems:\n\n```\nbin\\logstash -e \"input { stdin { } } output { stdout {} }\"\n```"
            },
            "Pipelines [tip-pipelines]": {
              "Pipeline management [tip-pipeline-mgmt]": "You can manage pipelines in a {{ls}} instance using either local pipeline configurations or [centralized pipeline management](/reference/configuring-centralized-pipelines.md) in {{kib}}.\n\nAfter you configure Logstash to use centralized pipeline management, you can no longer specify local pipeline configurations. The `pipelines.yml` file and settings such as `path.config` and `config.string` are inactive when centralized pipeline management is enabled."
            },
            "Tips using filters [tip-filters]": {
              "Check to see if a boolean field exists [tip-check-field]": "You can use the mutate filter to see if a boolean field exists.\n\n{{ls}} supports [@metadata] fields—​fields that are not visible for output plugins and live only in the filtering state. You can use [@metadata] fields with the mutate filter to see if a field exists.\n\n```\nfilter {\n  mutate {\n    # we use a \"temporal\" field with a predefined arbitrary known value that\n    # lives only in filtering stage.\n    add_field => { \"[@metadata][test_field_check]\" => \"a null value\" }\n\n    # we copy the field of interest into that temporal field.\n    # If the field doesn't exist, copy is not executed.\n    copy => { \"test_field\" => \"[@metadata][test_field_check]\" }\n  }\n\n\n  # now we now if testField didn't exists, our field will have\n  # the initial arbitrary value\n  if [@metadata][test_field_check] == \"a null value\" {\n    # logic to execute when [test_field] did not exist\n    mutate { add_field => { \"field_did_not_exist\" => true }}\n  } else {\n    # logic to execute when [test_field] existed\n    mutate { add_field => { \"field_did_exist\" => true }}\n  }\n}\n```"
            },
            "Kafka [tip-kafka]": {
              "Kafka settings [tip-kafka-settings]": {
                "Partitions per topic [tip-kafka-partitions]": "\"How many partitions should I use per topic?\"\n\nAt least the number of {{ls}} nodes multiplied by consumer threads per node.\n\nBetter yet, use a multiple of the above number. Increasing the number of partitions for an existing topic is extremely complicated. Partitions have a very low overhead. Using 5 to 10 times the number of partitions suggested by the first point is generally fine, so long as the overall partition count does not exceed 2000.\n\nErr on the side of over-partitioning up to a total 1000 partitions overall. Try not to exceed 1000 partitions.",
                "Consumer threads [tip-kafka-threads]": "\"How many consumer threads should I configure?\"\n\nLower values tend to be more efficient and have less memory overhead. Try a value of `1` then iterate your way up. The value should in general be lower than the number of pipeline workers. Values larger than 4 rarely result in performance improvement."
              },
              "Kafka input and persistent queue (PQ) [tip-kafka-pq-persist]": {
                "Kafka offset commits [tip-kafka-offset-commit]": "\"Does Kafka Input commit offsets only after the event has been safely persisted to the PQ?\"\n\n\"Does Kafa Input commit offsets only for events that have passed the pipeline fully?\"\n\nNo, we can’t make that guarantee. Offsets are committed to Kafka periodically. If writes to the PQ are slow or blocked, offsets for events that haven’t safely reached the PQ can be committed."
              }
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/tips.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 74]"
        },
        {
          "title": "Transforming Data",
          "description": null,
          "content": {
            "Transforming data [transformation]": "With over 200 plugins in the Logstash plugin ecosystem, it’s sometimes challenging to choose the best plugin to meet your data processing needs. In this section, we’ve collected a list of popular plugins and organized them according to their processing capabilities:\n\n['[Performing Core Operations](/reference/core-operations.md)', '[Deserializing Data](/reference/data-deserialization.md)', '[Extracting Fields and Wrangling Data](/reference/field-extraction.md)', '[Enriching Data with Lookups](/reference/lookup-enrichment.md)']\n\nAlso see [*Filter plugins*](logstash-docs-md://lsr/filter-plugins.md) and [*Codec plugins*](logstash-docs-md://lsr/codec-plugins.md) for the full list of available data processing plugins."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/transformation.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 75]"
        },
        {
          "title": "Tuning and profiling logstash pipeline performance [tuning-logstash]",
          "description": null,
          "content": {
            "Worker utilization [tuning-logstash-worker-utilization]": "When a pipeline’s `worker_utilization` flow metric is consistently near 100, all of its workers are occupied processing the filters and outputs of the pipeline. We can see *which* plugins in the pipeline are consuming the available worker capacity by looking at the plugin-level `worker_utilization` and `worker_millis_per_event` flow metrics. Using this information, we can gain intuition about how to tune the pipeline’s settings to add resources, how to find and eliminate wasteful computation, or realize the need to scale up/out the capacity of downstream destinations.\n\nIn general, plugins fit into one of two categories:\n\n['**CPU-bound**: plugins that perform computation on the contents of events *without* the use of the network or disk IO tend to benefit from incrementally increasing `pipeline.workers` as long as the process has available CPU; once CPU is exhausted additional concurrency can result in *lower* throughput as the pipeline workers contend for resources and the amount of time spent in context-switching increases.', '**IO-bound**: plugins that use the network to either enrich events or transmit events tend to benefit from incrementally increasing `pipeline.workers` and/or tuning the `pipeline.batch.*` parameters described below. This allows them to make better use of network resources, as long as those external services are not exerting back-pressure (even if Logstash is using nearly all of its available CPU).']\n\nThe further a pipeline’s `worker_utilization` is from 100, the more time its workers are spending waiting for events to arrive in the queue. Because the volume of data in most pipelines is often inconsistent, the goal should be to tune the pipeline such that it has the resources to avoid propagating back-pressure to its inputs during peak periods.",
            "Queue back-pressure [tuning-logstash-queue-backpressure]": "When a pipeline receives events faster than it can process them, the inputs eventually experience back-pressure that prevents them from receiving additional events. Depending on the input plugin being used, back-pressure can either propagate upstream or lead to data loss.\n\nA pipeline’s `queue_backpressure` flow metric reflects how much time the inputs are spending attempting to push events into the queue. The metric isn’t precisely comparable across pipelines, but instead allows you to compare a single pipeline’s current behaviour to *itself* over time. When this metric is growing, look *downstream* at the pipeline’s filters and outputs to see if they are using resources effectively, have sufficient resources allocated, or are experiencing back-pressure of their own.\n\n::::{note}\nA persisted queue offers durability guarantees and can absorb back-pressure for longer than the default in-memory queue, but once it is full it too propagates back-pressure. The `queue_persisted_growth_events` flow metric is useful measure of how much back-pressure is being actively absorbed by the persisted queue, and should trend toward zero (or less) over the pipeline’s lifetime. Negative numbers indicate that the queue is *shrinking*, and that the workers are catching up on lag that had previously developed.\n::::",
            "Tuning-related settings [tuning-logstash-settings]": "The Logstash defaults are chosen to provide fast, safe performance for most users. However if you notice performance issues, you may need to modify some of the defaults. Logstash provides the following configurable options for tuning pipeline performance: `pipeline.workers`, `pipeline.batch.size`, and `pipeline.batch.delay`.\n\nFor more information about setting these options, see [logstash.yml](/reference/logstash-settings-file.md).\n\nMake sure you’ve read the [Performance troubleshooting](/reference/performance-troubleshooting.md) before modifying these options.\n\n['The `pipeline.workers` setting determines how many threads to run for filter and output processing. If you find that events are backing up, or that the CPU is not saturated, consider increasing the value of this parameter to make better use of available processing power. Good results can even be found increasing this number past the number of available processors as these threads may spend significant time in an I/O wait state when writing to external systems.', 'The `pipeline.batch.size` setting defines the maximum number of events an individual worker thread collects from the queue before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, attempts to send a single [bulk request](https://www.elastic.co/docs/api/doc/elasticsearch/operation/operation-bulk) for each batch received. Tuning the `pipeline.batch.size` setting adjusts the size of bulk requests sent to Elasticsearch.', 'The `pipeline.batch.delay` setting rarely needs to be tuned. This setting adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that a pipeline worker waits for each new event while its current batch is not yet full. After this time elapses without any more events becoming available, the worker begins to execute filters and outputs. The maximum time that the worker waits between receiving an event and processing that event in a filter is the product of the `pipeline.batch.delay` and `pipeline.batch.size` settings.']",
            "Notes on pipeline configuration and performance [_notes_on_pipeline_configuration_and_performance]": "If you plan to modify the default pipeline settings, take into account the following suggestions:\n\n['The total number of inflight events is determined by the product of the  `pipeline.workers` and `pipeline.batch.size` settings. This product is referred to as the *inflight count*. Keep the value of the inflight count in mind as you adjust the `pipeline.workers` and `pipeline.batch.size` settings. Pipelines that intermittently receive large events at irregular intervals require sufficient memory to handle these spikes. Set the JVM heap space accordingly in the `jvm.options` config file (See [Logstash Configuration Files](/reference/config-setting-files.md) for more info).', 'Measure each change to make sure it increases, rather than decreases, performance.', 'Ensure that you leave enough memory available to cope with a sudden increase in event size. For example, an application that generates exceptions that are represented as large blobs of text.', 'The number of workers may be set higher than the number of CPU cores since outputs often spend idle time in I/O wait conditions.', 'Threads in Java have names and you can use the `jstack`, `top`, and the VisualVM graphical tools to figure out which resources a given thread uses.', 'On Linux platforms, Logstash labels its threads with descriptive names. For example, inputs show up as `[base]<inputname`, and pipeline workers show up as `[base]>workerN`, where N is an integer. Where possible, other threads are also labeled to help you identify their purpose.']",
            "Profiling the heap [profiling-the-heap]": "When tuning Logstash you may have to adjust the heap size. You can use the [VisualVM](https://visualvm.github.io/) tool to profile the heap. The **Monitor** pane in particular is useful for checking whether your heap allocation is sufficient for the current workload. The screenshots below show sample **Monitor** panes. The first pane examines a Logstash instance configured with too many inflight events. The second pane examines a Logstash instance configured with an appropriate amount of inflight events. Note that the specific batch sizes used here are most likely not applicable to your specific workload, as the memory demands of Logstash vary in large part based on the type of messages you are sending.\n\n% TO DO: Use `:class: screenshot`\n![pipeline overload](images/pipeline_overload.png)\n\n% TO DO: Use `:class: screenshot`\n![pipeline correct load](images/pipeline_correct_load.png)\n\nIn the first example we see that the CPU isn’t being used very efficiently. In fact, the JVM is often times having to stop the VM for “full GCs”. Full garbage collections are a common symptom of excessive memory pressure. This is visible in the spiky pattern on the CPU chart. In the more efficiently configured example, the GC graph pattern is more smooth, and the CPU is used in a more uniform manner. You can also see that there is ample headroom between the allocated heap size, and the maximum allowed, giving the JVM GC a lot of room to work with.\n\nExamining the in-depth GC statistics with a tool similar to the excellent [VisualGC](https://visualvm.github.io/plugins.html) plugin shows that the over-allocated VM spends very little time in the efficient Eden GC, compared to the time spent in the more resource-intensive Old Gen “Full” GCs.\n\n::::{note}\nAs long as the GC pattern is acceptable, heap sizes that occasionally increase to the maximum are acceptable. Such heap size spikes happen in response to a burst of large events passing through the pipeline. In general practice, maintain a gap between the used amount of heap memory and the maximum. This document is not a comprehensive guide to JVM GC tuning. Read the official [Oracle guide](http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html) for more information on the topic. We also recommend reading [Debugging Java Performance](https://www.semicomplete.com/blog/geekery/debugging-java-performance/).\n::::"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/tuning-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 76]"
        },
        {
          "title": "Upgrading Logstash [upgrading-logstash]",
          "description": null,
          "content": {
            "Before you upgrade {{ls}}": [
              "Upgrade {{ls}} to 8.18 for an easier upgrade to 9.0.",
              "Check out the [breaking changes](/release-notes/breaking-changes.md) docs.",
              "Read the [*Release Notes*](/release-notes/index.md).",
              "If you’re upgrading other products in the stack, check out the [Elastic upgrade documentation](docs-content://deploy-manage/upgrade.md)."
            ],
            "Upgrade tips and considerations": [
              "Test upgrades in a development environment before upgrading your production cluster.",
              "If you use monitoring, re-use the data directory when you upgrade Logstash. Otherwise, the Logstash node is assigned a new persistent UUID and becomes a new node in the monitoring data.",
              "If you use the [elastic_integration filter](logstash-docs-md://lsr/plugins-filters-elastic_integration.md) plugin in Logstash pipelines:",
              [
                "Logstash and Elasticsearch must be on the same version.",
                "The recommended order for upgrading {{stack}} is {{es}}-{{ls}}-{{kib}} to ensure the best experience with {{agent}}-managed pipelines.\nNote that this is different from the typical {{stack}} upgrade order.\nSee [when `elastic_integration` is in {{ls}} pipeline](#upgrading-when-elastic_integration-in-pipeline) section for details."
              ]
            ],
            "Ways to upgrade": [
              "[Upgrading using package managers](/reference/upgrading-using-package-managers.md)",
              "[Upgrading using a direct download](/reference/upgrading-using-direct-download.md)"
            ],
            "When to upgrade [_when_to_upgrade]": "Fresh installations should start with the same version across the Elastic Stack.\n\nElasticsearch 9.0 does not require Logstash 9.0.\nAn Elasticsearch 9.0 cluster can receive data from earlier versions of Logstash through the default HTTP communication layer.\nThis provides some flexibility to decide when to upgrade Logstash relative to an Elasticsearch upgrade.\nIt may or may not be convenient for you to upgrade them together, and it is not required to be done at the same time as long as Elasticsearch is upgraded first. However, there are special plugin cases for example, if your pipeline includes [elastic_integration filter](logstash-docs-md://lsr/plugins-filters-elastic_integration.md) plugin. See [when `elastic_integration` is in {{ls}} pipeline](#upgrading-when-elastic_integration-in-pipeline) section for details.\n\nYou should upgrade in a timely manner to get the performance improvements that come with Logstash 9.0, but do so in the way that makes the most sense for your environment.",
            "When not to upgrade [_when_not_to_upgrade]": "If any Logstash plugin that you require is not compatible with Logstash 9.0, then you should wait until it is ready before upgrading.\n\nAlthough we make great efforts to ensure compatibility, Logstash 9.0 is not completely backwards compatible. As noted in the Elastic Stack upgrade guide, you should not upgrade Logstash 9.0 before you upgrade Elasticsearch 9.0. This is both practical and because some Logstash 9.0 plugins may attempt to use features of Elasticsearch 9.0 that did not exist in earlier versions.\n\nFor example, if you attempt to send the 8.x template to a cluster before Elasticsearch 9.0, then  all indexing likely fail. If you use your own custom template with Logstash, then this issue can be ignored.\n\nAnother example is when your pipeline utilizes the [`elastic_integration` filter](logstash-docs-md://lsr/plugins-filters-elastic_integration.md) plugin. In such cases, the plugin may encounter issues loading and executing deprecated integrations or features that have been removed in newer versions. This can lead to disruptions in your pipeline’s functionality, especially if your workflow relies on these outdated components. For a comprehensive understanding of how to handle such scenarios and ensure compatibility, refer to the [when `elastic_integration` is in {{ls}} pipeline](#upgrading-when-elastic_integration-in-pipeline) section in this documentation.",
            "When `elastic_integration` is in {{ls}} pipeline [upgrading-when-elastic_integration-in-pipeline]": "[elastic_integration filter](logstash-docs-md://lsr/plugins-filters-elastic_integration.md) plugin requires a special attention due to its dependencies on various components of the stack such as {{es}}, {{kib}} and {{ls}}. Any updates, deprecations, or changes in the stack products can directly impact the functionality of the plugin.\n\n**When upgrading {{es}}**\n\nThis plugin is compiled with a specific version of {{es}} and embeds {{es}} Ingest Node components that match the `major.minor` stack version. Therefore, we recommend using a plugin version that aligns with the `major.minor` version of your stack.\n\nIf the versions do not match, the plugin may encounter issues such as failing to load or execute pipelines. For example, if your {{es}} version is newer than the plugin, the plugin may not support new features introduced in the updated {{es}} version. Conversely, if your {{es}} version is older, the plugin may rely on features that have been deprecated or removed in your {{es}} version.\n\n**When upgrading {{kib}}**\n\nWhen you upgrade {{kib}}, {{kib}} downloads the latest version of the integrations through [Elastic Package Registry](docs-content://reference/fleet/index.md#package-registry-intro). As part of the upgrade process, you will also have the opportunity to review and upgrade your currently installed integrations to their latest versions. However, we strongly recommend upgrading the [elastic_integration filter](logstash-docs-md://lsr/plugins-filters-elastic_integration.md) plugin before upgrading {{kib}} and {{es}}. This is because [elastic_integration filter](logstash-docs-md://lsr/plugins-filters-elastic_integration.md) plugin pulls and processes the ingest pipelines associated with the installed integrations. These pipelines are then executed using the {{es}} Ingest Node components that the plugin was compiled with. If {{es}} or {{es}} is upgraded first, there is a risk of incompatibility between the plugin’s ingest componenets and the newer versions of {{es}}'s Ingest Node features or {{kib}}'s integration definitions.\n\n**When upgrading {{ls}}**\n\nThis plugin is by default embedded in {{ls}} core. When you upgrade {{ls}}, new version of the plugin is installed. The plugin is backward compatible accross {{ls}} 8.x versions. However, if you are considering to upgrade {{ls}} only (not the plugin), there are exceptions cases, such as JDK compatibility which require matching certain {{ls}} versions. We recommend visiting [elastic_integration plugin requirements](logstash-docs-md://lsr/plugins-filters-elastic_integration.md#plugins-filters-elastic_integration-requirements) guide considering the {{ls}} version you are upgrading to."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/upgrading-logstash.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 77]"
        },
        {
          "title": "Upgrading Minor Versions",
          "description": null,
          "content": {
            "Upgrading between minor versions [upgrading-minor-versions]": "As a general rule, you can upgrade between minor versions (for example, 9.x to 9.y, where x < y) by simply installing the new release and restarting {{ls}}. {{ls}} typically maintains backwards compatibility for configuration settings and exported fields. Please review the [release notes](/release-notes/index.md) for potential exceptions.\n\nUpgrading between non-consecutive major versions (7.x to 9.x, for example) is not supported."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/upgrading-minor-versions.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 78]"
        },
        {
          "title": "Upgrading Using Direct Download",
          "description": null,
          "content": {
            "Upgrading using a direct download [upgrading-using-direct-download]": "This procedure downloads the relevant Logstash binaries directly from Elastic.\n\n['Shut down your Logstash pipeline, including any inputs that send events to Logstash.', 'Download the [Logstash installation file](https://www.elastic.co/downloads/logstash) that matches your host environment.', 'Backup your `config/` and `data/` folders in a temporary space.', 'Delete your Logstash directory.', 'Unpack the installation file into the folder that contained the Logstash directory that you just deleted.', 'Restore the `config/` and `data/` folders that were previously saved, overwriting the folders created during the unpack operation.', 'Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for some Logstash plugins have changed.', 'Restart your Logstash pipeline after updating your configuration file.']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/upgrading-using-direct-download.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 79]"
        },
        {
          "title": "Upgrading Using Package Managers",
          "description": null,
          "content": {
            "Upgrading using package managers [upgrading-using-package-managers]": "This procedure uses [package managers](/reference/installing-logstash.md#package-repositories) to upgrade Logstash.\n\n['Shut down your Logstash pipeline, including any inputs that send events to Logstash.', 'Using the directions in the [Installing from Package Repositories](/reference/installing-logstash.md#package-repositories) section, update your repository links to point to the 9.x repositories.', 'Run the `apt-get upgrade logstash` or `yum update logstash` command as appropriate for your operating system.', 'Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for some Logstash plugins have changed in the 9.x release.', 'Restart your Logstash pipeline after you have updated your configuration file.']"
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/upgrading-using-package-managers.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 80]"
        },
        {
          "title": "Example: Set up Filebeat modules to work with Kafka and Logstash [use-filebeat-modules-kafka]",
          "description": null,
          "content": {
            "Set up and run {{filebeat}} [_set_up_and_run_filebeat]": [
              "If you haven’t already set up the {{filebeat}} index template and sample {{kib}} dashboards, run the {{filebeat}} `setup` command to do that now:",
              "```\nfilebeat -e setup\n```",
              "The `-e` flag is optional and sends output to standard error instead of syslog.",
              "A connection to {{es}} and {{kib}} is required for this one-time setup step because {{filebeat}} needs to create the index template in {{es}} and load the sample dashboards into {{kib}}. For more information about configuring the connection to {{es}}, see the Filebeat [quick start](beats://reference/filebeat/filebeat-installation-configuration.md).",
              "After the template and dashboards are loaded, you’ll see the message _INFO {{kib}} dashboards successfully loaded. Loaded dashboards_.",
              "Run the `modules enable` command to enable the modules that you want to run. For example:",
              "```\nfilebeat modules enable system\n```",
              "You can further configure the module by editing the config file under the {{filebeat}} `modules.d` directory. For example, if the log files are not in the location expected by the module, you can set the `var.paths` option.",
              "::::{note}\nYou must enable at least one fileset in the module. **Filesets are disabled by default.**\n::::",
              "Run the `setup` command with the `--pipelines` and `--modules` options specified to load ingest pipelines for the modules you’ve enabled. This step also requires a connection to {{es}}. If you want use a {{ls}} pipeline instead of ingest node to parse the data, skip this step.",
              "```\nfilebeat setup --pipelines --modules system\n```",
              "Configure {{filebeat}} to send log lines to Kafka. To do this, in the `filebeat.yml` config file, disable the {{es}} output by commenting it out, and enable the Kafka output. For example:",
              "```\n#output.elasticsearch:\n  #hosts: [\"localhost:9200\"]\noutput.kafka:\n  hosts: [\"kafka:9092\"]\n  topic: \"filebeat\"\n  codec.json:\n    pretty: false\n```",
              "Start {{filebeat}}. For example:",
              "```\nfilebeat -e\n```",
              "{{filebeat}} will attempt to send messages to {{ls}} and continue until {{ls}} is available to receive them.",
              "::::{note}\nDepending on how you’ve installed {{filebeat}}, you might see errors related to file ownership or permissions when you try to run {{filebeat}} modules. See [Config File Ownership and Permissions](beats://reference/libbeat/config-file-permissions.md) in the *Beats Platform Reference* if you encounter errors related to file ownership or permissions.\n::::"
            ],
            "Create and start the {{ls}} pipeline [_create_and_start_the_ls_pipeline]": [
              "On the system where {{ls}} is installed, create a {{ls}} pipeline configuration that reads from a Kafka input and sends events to an {{es}} output:",
              "```\ninput {\n  kafka {\n    bootstrap_servers => \"myhost:9092\"\n    topics => [\"filebeat\"]\n    codec => json\n  }\n}\n\noutput {\n  if [@metadata][pipeline] {\n    elasticsearch {\n      hosts => \"https://myEShost:9200\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\"\n      pipeline => \"%{[@metadata][pipeline]}\" <1>\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  } else {\n    elasticsearch {\n      hosts => \"https://myEShost:9200\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\"\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  }\n}\n```",
              [
                "Set the `pipeline` option to `%{[@metadata][pipeline]}`. This setting configures {{ls}} to select the correct ingest pipeline based on metadata passed in the event."
              ],
              "Start {{ls}}, passing in the pipeline configuration file you just defined. For example:",
              "```\nbin/logstash -f mypipeline.conf\n```",
              "{{ls}} should start a pipeline and begin receiving events from the Kafka input."
            ],
            "Visualize the data [_visualize_the_data]": "To visualize the data in {{kib}}, launch the {{kib}} web interface by pointing your browser to port 5601. For example, [http://127.0.0.1:5601](http://127.0.0.1:5601). Click **Dashboards** then view the {{filebeat}} dashboards."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/use-filebeat-modules-kafka.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 81]"
        },
        {
          "title": "Use Ingest Pipelines",
          "description": null,
          "content": {
            "Use ingest pipelines for parsing [use-ingest-pipelines]": "When you use {{filebeat}} modules with {{ls}}, you can use the ingest pipelines provided by {{filebeat}} to parse the data. You need to load the pipelines into {{es}} and configure {{ls}} to use them.\n\n**To load the ingest pipelines:**\n\nOn the system where {{filebeat}} is installed, run the `setup` command with the `--pipelines` option specified to load ingest pipelines for specific modules. For example, the following command loads ingest pipelines for the system and nginx modules:\n\n```\nfilebeat setup --pipelines --modules nginx,system\n```\n\nA connection to {{es}} is required for this setup step because {{filebeat}} needs to load the ingest pipelines into {{es}}. If necessary, you can temporarily disable your configured output and enable the {{es}} output before running the command.\n\n**To configure {{ls}} to use the pipelines:**\n\nOn the system where {{ls}} is installed, create a {{ls}} pipeline configuration that reads from a {{ls}} input, such as {{beats}} or Kafka, and sends events to an {{es}} output. Set the `pipeline` option in the {{es}} output to `%{[@metadata][pipeline]}` to use the ingest pipelines that you loaded previously.\n\nHere’s an example configuration that reads data from the Beats input and uses {{filebeat}} ingest pipelines to parse data collected by modules:\n\n```\ninput {\n  beats {\n    port => 5044\n  }\n}\n\noutput {\n  if [@metadata][pipeline] {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" <1>\n      action => \"create\" <2>\n      pipeline => \"%{[@metadata][pipeline]}\" <3>\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  } else {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" <1>\n      action => \"create\"\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  }\n}\n```\n\n['If data streams are disabled in your configuration, set the `index` option to `%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}`. Data streams are enabled by default.', 'If you are disabling the use of Data Streams on your configuration, you can remove this setting, or set it to a different value as appropriate.', 'Configures {{ls}} to select the correct ingest pipeline based on metadata passed in the event.']\n\nSee the {{filebeat}} [Modules](beats://reference/filebeat/filebeat-modules-overview.md) documentation for more information about setting up and running modules.\n\nFor a full example, see [Example: Set up {{filebeat}} modules to work with Kafka and {{ls}}](/reference/use-filebeat-modules-kafka.md)."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/use-ingest-pipelines.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 82]"
        },
        {
          "title": "Using Logstash with Elastic integrations [ea-integrations]",
          "description": null,
          "content": {
            "Elastic {{integrations}}: ingesting to visualizing [integrations-value]": "[Elastic {{integrations}}](integration-docs://reference/index.md) provide quick, end-to-end solutions for:\n\n['ingesting data from a variety of data sources,', 'ensuring compliance with the [Elastic Common Schema (ECS)](ecs://reference/index.md),', 'getting the data into the {{stack}}, and', 'visualizing it with purpose-built dashboards.']\n\n{{integrations}} are available for [popular services and platforms](integration-docs://reference/all_integrations.md), such as Nginx, AWS, and MongoDB, as well as many generic input types like log files. Each integration includes pre-packaged assets to help reduce the time between ingest and insights.\n\nTo see available integrations, go to the {{kib}} home page, and click **Add {{integrations}}**. You can use the query bar to search for integrations you may want to use. When you find an integration for your data source, the UI walks you through adding and configuring it.",
            "Extend {{integrations}} with {{ls}} [integrations-and-ls]": {
              "Using `filter-elastic_integration` with `output-elasticsearch` [es-tips]": "Elastic {{integrations}} are designed to work with [data streams](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-data-streams) and [ECS-compatible](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#_compatibility_with_the_elastic_common_schema_ecs) output. Be sure that these features are enabled in the [`output-elasticsearch`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) plugin.\n\n['Set [`data-stream`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-data_stream) to `true`.<br> (Check out [Data streams](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-data-streams) for additional data streams settings.)', 'Set [`ecs_compatibility`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ecs_compatibility) to `v1` or `v8`.']\n\nCheck out the [`output-elasticsearch` plugin](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md) docs for additional settings."
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/ea-integrations.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 83]"
        },
        {
          "title": "Working With Filebeat Modules",
          "description": null,
          "content": {
            "Working with Filebeat modules [filebeat-modules]": "{{filebeat}} comes packaged with pre-built [modules](beats://reference/filebeat/filebeat-modules.md) that contain the configurations needed to collect, parse, enrich, and visualize data from various log file formats. Each {{filebeat}} module consists of one or more filesets that contain ingest node pipelines, {{es}} templates, {{filebeat}} input configurations, and {{kib}} dashboards.\n\nYou can use {{filebeat}} modules with {{ls}}, but you need to do some extra setup. The simplest approach is to [set up and use the ingest pipelines](/reference/use-ingest-pipelines.md) provided by {{filebeat}}."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/filebeat-modules.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 84]"
        },
        {
          "title": "Working with plugins [working-with-plugins]",
          "description": null,
          "content": {
            "No internet connection? [pointer-to-offline]": {
              "Proxy configuration [http-proxy]": "Most plugin manager commands require access to the internet to reach [RubyGems.org](https://rubygems.org). If your organization is behind a firewall, you can set these environments variables to configure Logstash to use your proxy.\n\n```\nexport http_proxy=http://localhost:3128\nexport https_proxy=http://localhost:3128\n```"
            },
            "Listing plugins [listing-plugins]": "Logstash release packages bundle common plugins. To list the plugins currently available in your deployment:\n\n```\nbin/logstash-plugin list <1>\nbin/logstash-plugin list --verbose <2>\nbin/logstash-plugin list '*namefragment*' <3>\nbin/logstash-plugin list --group output <4>\n```\n\n['Lists all installed plugins', 'Lists installed plugins with version information', 'Lists all installed plugins containing a namefragment', 'Lists all installed plugins for a particular group (input, filter, codec, output)']",
            "Adding plugins to your deployment [installing-plugins]": "When you have access to internet, you can retrieve plugins hosted on the [RubyGems.org](https://rubygems.org/)public repository and install them on top of your Logstash installation.\n\n```\nbin/logstash-plugin install logstash-input-github\n```\n\nAfter a plugin is successfully installed, you can use it in your configuration file.",
            "Updating plugins [updating-plugins]": {
              "Major version plugin updates [updating-major]": "To avoid introducing breaking changes, the plugin manager updates only plugins for which newer *minor* or *patch* versions exist by default. If you wish to also include breaking changes, specify `--level=major`.\n\n```\nbin/logstash-plugin update --level=major <1>\nbin/logstash-plugin update --level=major logstash-input-github <2>\n```\n\n['updates all installed plugins to latest, including major versions with breaking changes', 'updates only the plugin you specify to latest, including major versions with breaking changes']"
            },
            "Removing plugins [removing-plugins]": {
              "Advanced: Adding a locally built plugin [installing-local-plugins]": "In some cases, you may want to install plugins which are not yet released and not hosted on RubyGems.org. Logstash provides you the option to install a locally built plugin which is packaged as a ruby gem. Using a file location:\n\n```\nbin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem\n```",
              "Advanced: Using `--path.plugins` [installing-local-plugins-path]": "Using the Logstash `--path.plugins` flag, you can load a plugin source code located on your file system. Typically this is used by developers who are iterating on a custom plugin and want to test it before creating a ruby gem.\n\nThe path needs to be in a  specific directory hierarchy: `PATH/logstash/TYPE/NAME.rb`, where TYPE is *inputs* *filters*, *outputs* or *codecs* and NAME is the name of the plugin.\n\n```\n# supposing the code is in /opt/shared/lib/logstash/inputs/my-custom-plugin-code.rb\nbin/logstash --path.plugins /opt/shared/lib\n```"
            }
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 85]"
        },
        {
          "title": "Working with Winlogbeat modules [winlogbeat-modules]",
          "description": null,
          "content": {
            "Use ingest pipelines for parsing [use-winlogbeat-ingest-pipelines]": "When you use {{winlogbeat}} modules with {{ls}}, you can use the ingest pipelines provided by {{winlogbeat}} to parse the data. You need to load the pipelines into {{es}} and configure {{ls}} to use them.\n\n**To load the ingest pipelines:**\n\nOn the system where {{winlogbeat}} is installed, run the `setup` command with the `--pipelines` option specified to load ingest pipelines for specific modules. For example, the following command loads ingest pipelines for the security and sysmon modules:\n\n```\nwinlogbeat setup --pipelines --modules security,sysmon\n```\n\nA connection to {{es}} is required for this setup step because {{winlogbeat}} needs to load the ingest pipelines into {{es}}. If necessary, you can temporarily disable your configured output and enable the {{es}} output before running the command.\n\n**To configure {{ls}} to use the pipelines:**\n\nOn the system where {{ls}} is installed, create a {{ls}} pipeline configuration that reads from a {{ls}} input, such as {{beats}} or Kafka, and sends events to an {{es}} output. Set the `pipeline` option in the {{es}} output to `%{[@metadata][pipeline]}` to use the ingest pipelines that you loaded previously.\n\nHere’s an example configuration that reads data from the Beats input and uses {{winlogbeat}} ingest pipelines to parse data collected by modules:\n\n```\ninput {\n  beats {\n    port => 5044\n  }\n}\n\noutput {\n  if [@metadata][pipeline] {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" <1>\n      action => \"create\" <2>\n      pipeline => \"%{[@metadata][pipeline]}\" <3>\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  } else {\n    elasticsearch {\n      hosts => \"https://061ab24010a2482e9d64729fdb0fd93a.us-east-1.aws.found.io:9243\"\n      manage_template => false\n      index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" <1>\n      action => \"create\"\n      user => \"elastic\"\n      password => \"secret\"\n    }\n  }\n}\n```\n\n['If data streams are disabled in your configuration, set the `index` option to `%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}`. Data streams are enabled by default.', 'If you are disabling the use of Data Streams on your configuration, you can remove this setting, or set it to a different value as appropriate.', 'Configures {{ls}} to select the correct ingest pipeline based on metadata passed in the event.']\n\nSee the {{winlogbeat}} [Modules](beats://reference/winlogbeat/winlogbeat-modules.md) documentation for more information about setting up and running modules."
          },
          "metadata": {
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/winlogbeat-modules.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 1, \"subpages\", 86]"
        }
      ],
      "path": "[\"subpages\", 1]"
    },
    {
      "title": "Release Notes",
      "description": "Documentation section: release-notes",
      "content": {},
      "metadata": {
        "type": "directory",
        "path": "/home/anhnh/CodeWiki-Benchmarking-System/data/logstash/original/docs/release-notes"
      },
      "subpages": [
        {
          "title": "Logstash breaking changes [logstash-breaking-changes]",
          "description": null,
          "content": {
            "9.0.0 [logstash-900-breaking-changes]": ":::::{dropdown} Changes to SSL settings in {{ls}} plugins\n:name: ssl-deprecations-9.0.0\n\nWe’ve removed deprecated SSL settings in some {{ls}} plugins, and have replaced them with updated settings. If your plugin configuration contains any of these obsolete options, the plugin may fail to start.\n\n::::{dropdown} logstash-input-beats\n:name: input-beats-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| cipher_suites | [`ssl_cipher_suites`](logstash-docs-md://lsr/plugins-inputs-beats.md#plugins-inputs-beats-ssl_cipher_suites) |\n| ssl | [`ssl_enabled`](logstash-docs-md://lsr/plugins-inputs-beats.md#plugins-inputs-beats-ssl_enabled) |\n| ssl_peer_metadata | `ssl_peer_metadata` option of [`enrich`](logstash-docs-md://lsr/plugins-inputs-beats.md#plugins-inputs-beats-enrich) |\n| ssl_verify_mode | [`ssl_client_authentication`](logstash-docs-md://lsr/plugins-inputs-beats.md#plugins-inputs-beats-ssl_client_authentication) |\n| tls_min_version | [`ssl_supported_protocols`](logstash-docs-md://lsr/plugins-inputs-beats.md#plugins-inputs-beats-ssl_supported_protocols) |\n| tls_max_version | [`ssl_supported_protocols`](logstash-docs-md://lsr/plugins-inputs-beats.md#plugins-inputs-beats-ssl_supported_protocols) |\n\n::::\n\n::::{dropdown} logstash-input-elastic_agent\n:name: input-elastic_agent-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| cipher_suites | [`ssl_cipher_suites`](logstash-docs-md://lsr/plugins-inputs-elastic_agent.md#plugins-inputs-elastic_agent-ssl_cipher_suites) |\n| ssl | [`ssl_enabled`](logstash-docs-md://lsr/plugins-inputs-elastic_agent.md#plugins-inputs-elastic_agent-ssl_enabled) |\n| ssl_peer_metadata | `ssl_peer_metadata` option of [`enrich`](logstash-docs-md://lsr/plugins-inputs-elastic_agent.md#plugins-inputs-elastic_agent-enrich) |\n| ssl_verify_mode | [`ssl_client_authentication`](logstash-docs-md://lsr/plugins-inputs-elastic_agent.md#plugins-inputs-elastic_agent-ssl_client_authentication) |\n| tls_min_version | [`ssl_supported_protocols`](logstash-docs-md://lsr/plugins-inputs-elastic_agent.md#plugins-inputs-elastic_agent-ssl_supported_protocols) |\n| tls_max_version | [`ssl_supported_protocols`](logstash-docs-md://lsr/plugins-inputs-elastic_agent.md#plugins-inputs-elastic_agent-ssl_supported_protocols) |\n\n::::\n\n::::{dropdown} logstash-input-elasticsearch\n:name: input-elasticsearch-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| ca_file | [`ssl_certificate_authorities`](logstash-docs-md://lsr/plugins-inputs-elasticsearch.md#plugins-inputs-elasticsearch-ssl_certificate_authorities) |\n| ssl | [`ssl_enabled`](logstash-docs-md://lsr/plugins-inputs-elasticsearch.md#plugins-inputs-elasticsearch-ssl_enabled) |\n| ssl_certificate_verification | [`ssl_verification_mode`](logstash-docs-md://lsr/plugins-inputs-elasticsearch.md#plugins-inputs-elasticsearch-ssl_verification_mode) |\n\n::::\n\n::::{dropdown} logstash-input-elastic_serverless_forwarder\n:name: input-elastic_serverless_forwarder-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| ssl | [`ssl_enabled`](logstash-docs-md://lsr/plugins-inputs-elastic_serverless_forwarder.md#plugins-inputs-elastic_serverless_forwarder-ssl_enabled) |\n\n::::\n\n::::{dropdown} logstash-input-http\n:name: input-http-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| cipher_suites | [`ssl_cipher_suites`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_cipher_suites) |\n| keystore | [`ssl_keystore_path`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_keystore_path) |\n| keystore_password | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_keystore_password) |\n| ssl | [`ssl_enabled`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_enabled) |\n| ssl_verify_mode | [`ssl_client_authentication`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_client_authentication) |\n| tls_max_version | [`ssl_supported_protocols`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_supported_protocols) |\n| tls_min_version | [`ssl_supported_protocols`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_supported_protocols) |\n| verify_mode | [`ssl_client_authentication`](logstash-docs-md://lsr/plugins-inputs-http.md#plugins-inputs-http-ssl_client_authentication) |\n\n::::\n\n::::{dropdown} logstash-input-http_poller\n:name: input-http_poller-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| cacert | [`ssl_certificate_authorities`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_certificate_authorities) |\n| client_cert | [`ssl_certificate`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_certificate) |\n| client_key | [`ssl_key`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_key) |\n| keystore | [`ssl_keystore_path`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_keystore_path) |\n| keystore_password | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_keystore_password) |\n| keystore_type | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_keystore_password) |\n| truststore | [`ssl_truststore_path`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_truststore_path) |\n| truststore_password | [`ssl_truststore_password`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_truststore_password) |\n| truststore_type | [`ssl_truststore_type`](logstash-docs-md://lsr/plugins-inputs-http_poller.md#plugins-inputs-http_poller-ssl_truststore_type) |\n\n::::\n\n::::{dropdown} logstash-input-tcp\n:name: input-tcp-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| ssl_cert | [`ssl_certificate`](logstash-docs-md://lsr/plugins-inputs-tcp.md#plugins-inputs-tcp-ssl_certificate) |\n| ssl_enable | [`ssl_enabled`](logstash-docs-md://lsr/plugins-inputs-tcp.md#plugins-inputs-tcp-ssl_enabled) |\n| ssl_verify | [`ssl_client_authentication`](logstash-docs-md://lsr/plugins-inputs-tcp.md#plugins-inputs-tcp-ssl_client_authentication) in `server` mode and [`ssl_verification_mode`](logstash-docs-md://lsr/plugins-inputs-tcp.md#plugins-inputs-tcp-ssl_verification_mode) in `client` mode |\n\n::::\n\n::::{dropdown} logstash-filter-elasticsearch\n:name: filter-elasticsearch-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| ca_file | [`ssl_certificate_authorities`](logstash-docs-md://lsr/plugins-filters-elasticsearch.md#plugins-filters-elasticsearch-ssl_certificate_authorities) |\n| keystore | [`ssl_keystore_path`](logstash-docs-md://lsr/plugins-filters-elasticsearch.md#plugins-filters-elasticsearch-ssl_keystore_path) |\n| keystore_password | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-filters-elasticsearch.md#plugins-filters-elasticsearch-ssl_keystore_password) |\n| ssl | [`ssl_enabled`](logstash-docs-md://lsr/plugins-filters-elasticsearch.md#plugins-filters-elasticsearch-ssl_enabled) |\n\n::::\n\n::::{dropdown} logstash-filter-http\n:name: filter-http-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| cacert | [`ssl_certificate_authorities`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_certificate_authorities) |\n| client_cert | [`ssl_certificate`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_certificate) |\n| client_key | [`ssl_key`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_key) |\n| keystore | [`ssl_keystore_path`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_keystore_path) |\n| keystore_password | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_keystore_password) |\n| keystore_type | [`ssl_keystore_type`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_keystore_type) |\n| truststore | [`ssl_truststore_path`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_truststore_path) |\n| truststore_password | [`ssl_truststore_password`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_truststore_password) |\n| truststore_type | [`ssl_truststore_type`](logstash-docs-md://lsr/plugins-filters-http.md#plugins-filters-http-ssl_truststore_type) |\n\n::::\n\n::::{dropdown} logstash-output-elasticsearch\n:name: output-elasticsearch-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| cacert | [`ssl_certificate_authorities`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_certificate_authorities) |\n| keystore | [`ssl_keystore_path`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_keystore_path) |\n| keystore_password | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_keystore_password) |\n| ssl | [`ssl_enabled`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_enabled) |\n| ssl_certificate_verification | [`ssl_verification_mode`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_verification_mode) |\n| truststore | [`ssl_truststore_path`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_truststore_path) |\n| truststore_password | [`ssl_truststore_password`](logstash-docs-md://lsr/plugins-outputs-elasticsearch.md#plugins-outputs-elasticsearch-ssl_truststore_password) |\n\n::::\n\n::::{dropdown} logstash-output-http\n:name: output-http-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| cacert | [`ssl_certificate_authorities`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_certificate_authorities) |\n| client_cert | [`ssl_certificate`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_certificate) |\n| client_key | [`ssl_key`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_key) |\n| keystore | [`ssl_keystore_path`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_keystore_path) |\n| keystore_password | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_keystore_password) |\n| keystore_type | [`ssl_keystore_password`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_keystore_password) |\n| truststore | [`ssl_truststore_path`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_truststore_path) |\n| truststore_password | [`ssl_truststore_password`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_truststore_password) |\n| truststore_type | [`ssl_truststore_type`](logstash-docs-md://lsr/plugins-outputs-http.md#plugins-outputs-http-ssl_truststore_type) |\n\n::::\n\n::::{dropdown} logstash-output-tcp\n:name: output-tcp-ssl-9.0\n\n| Setting | Replaced by |\n| --- | --- |\n| ssl_cacert | [`ssl_certificate_authorities`](logstash-docs-md://lsr/plugins-outputs-tcp.md#plugins-outputs-tcp-ssl_certificate_authorities) |\n| ssl_cert | [`ssl_certificate`](logstash-docs-md://lsr/plugins-outputs-tcp.md#plugins-outputs-tcp-ssl_certificate) |\n| ssl_enable | [`ssl_enabled`](logstash-docs-md://lsr/plugins-outputs-tcp.md#plugins-outputs-tcp-ssl_enabled) |\n| ssl_verify | [`ssl_client_authentication`](logstash-docs-md://lsr/plugins-outputs-tcp.md#plugins-outputs-tcp-ssl_client_authentication) in `server` mode and [`ssl_verification_mode`](logstash-docs-md://lsr/plugins-outputs-tcp.md#plugins-outputs-tcp-ssl_verification_mode) in `client` mode |\n::::\n:::::\n\n::::{dropdown} Pipeline buffer type defaults to `heap`\n:name: pipeline-buffer-type]\n\nWe've improved memory configuration for certain {{ls}} plugins.\nInput plugins such as `elastic_agent`, `beats`, `tcp`, and `http` allocate buffers in Java memory to read events from the network.\nThe default allocation method is `direct` memory rather than `heap` memory to simplify configuration, and to help facilitate debugging memory usage problems through the analysis of heap dumps.\nIf you need to re-enable the previous behavior, change the `pipeline.buffer.type` setting in [logstash.yml](/reference/logstash-settings-file.md).\nCheck out [off-heap-buffers-allocation](/reference/jvm-settings.md#off-heap-buffers-allocation) for details. [#16500](https://github.com/elastic/logstash/pull/16500)\n::::\n\n::::{dropdown} {{ls}} modules removed\n:name: removed-modules\n\nWe have removed the {{ls}} modules framework, and encourage users to try Elastic Integrations\nThis includes the netflow, azure and arcsight modules, and the modules framework as a whole. [#16794](https://github.com/elastic/logstash/pull/16794)\n::::\n\n::::{dropdown} Deprecated configuration settings removed\n:name:removed-params\n\nWe have removed support for previously deprecated configuration settings:\n\n['**`http.*` prefixed settings for the {{ls}} API.** Settings prefixed by `http.*` have been replaced by the equivalent settings prefixed with `api.*`. [#16552](https://github.com/elastic/logstash/pull/16552)', '**`event_api.tags.illegal`**\\nAny events that include field named tags automatically rename the field _tags to avoid any clash\\nwith the reserved {{ls}} tags field.\\nInstead, {{ls}} generates `_tagsparsefailure` in the event `tags` and the illegal value is written to the `_tags` field. [#16461](https://github.com/elastic/logstash/pull/16461)\\n::::']\n\n::::{dropdown} Ingest converter removed\n:name: removed-ingest-converter\n\nThe ingest converter, which was used to convert ingest pipelines to {{ls}} pipelines, has been removed. [#16453](https://github.com/elastic/logstash/pull/16453)\n\nThe `logstash-filter-elastic_integration` plugin offers similar functionality, and can help you use [Logstash to extend Elastic integrations](/reference/using-logstash-with-elastic-integrations.md).\n::::\n\n::::{dropdown} JDK11 not supported\n:name: jdk-11-support-drop\n\nJDK17 is the minimum version of the JDK required to run Logstash.\nFor the best experience, we still recommend running {{ls}} using the bundled-jdk.\nSee [Logstash JVM requirements](/reference/getting-started-with-logstash.md#ls-jvm)\nfor details. [#16443](https://github.com/elastic/logstash/pull/16443)\n::::\n\n::::{dropdown} Docker base image now UBI9 based\n:name: docker-base-image-change\n\nThe base image for {{ls}} on Docker has been changed from Ubuntu to UBI9.\nIf you create a Docker image based on the {{ls}} image and rely on it being Ubuntu based, you need to change your derived image to take account of this change.\nFor example, if your derived docker image installs additional packages using a package manager, UBI9 uses `microdnf`, rather than `apt`.\n[#16599](https://github.com/elastic/logstash/pull/16599)\n::::\n\n::::{dropdown} Cannot run {{ls}} as `superuser` by default\n:name: [disallow-superuser\n\nWe've changed the default behavior to prevent users from accidentally running {{ls}} as a superuser.\nIf you try to run {{ls}} as a superuser, it logs an error and fails to start, ensuring that users cannot run Logstash with elevated privileges by accident.\n\nYou can change the value of the `allow_superuser` setting to `true` in [logstash.yml](/reference/logstash-settings-file.md) if you want to restore the previous behavior and allow {{ls}} to run with superuser privileges. [#16558](https://github.com/elastic/logstash/pull/16558)\n::::\n\n::::{dropdown} New setting required to continue using legacy internal monitoring\n:name: allow-legacy-monitoring\n\nTo continue using deprecated internal collection to monitor {{ls}}, set `xpack.monitoring.allow_legacy_collection` to `true` in [logstash.yml](/reference/logstash-settings-file.md).\nWe encourage you to move to [agent-driven monitoring](/reference/monitoring-logstash-with-elastic-agent.md), the latest, supported way to monitor Logstash [#16586](https://github.com/elastic/logstash/pull/16586)\n::::\n\n::::{dropdown} Avoiding JSON log lines collision\n:name: avoid-collision-on-json-fields\n\nWe've improved the way we deal with duplicate `message` fields in `json` documents.\nSome code paths that log in `json` produce log events that include multiple instances of the  `message` field. (The JSON codec plugin is one example.)\nWhile this approach produces JSON that is technically valid, many clients do not parse this data correctly, and either crash or discard one of the fields.\n\nWe recently introduced the option to fix duplicates, and made it the default behavior for `9.0` and later.\nTo re-enable the previous behavior, change the `log.format.json.fix_duplicate_message_fields` setting in [logstash.yml](/reference/logstash-settings-file.md) to `false`.\n\nCheck out [Logging in json format can write duplicate message fields](docs-content://troubleshoot/ingest/logstash.md) for more details about the issue. [#16578](https://github.com/elastic/logstash/pull/16578)\n::::\n\n::::{dropdown} Enterprise_search integration plugin is removed from default Logstash install\n:name: enterprise_search-deprecated-9.0\n\nWe’ve removed the {{ls}} Enterprise_search integration plugin, and its component App Search and Workplace Search plugins from the default {{ls}} install.\nThese plugins will receive only security updates and critical fixes moving forward.\n\nWe recommend using our native {{es}} tools for your Search use cases. For more details, please visit the [Search solution and use case documentation](docs-content://solutions/search.md).\n::::"
          },
          "metadata": {
            "navigation_title": "Breaking changes"
          },
          "subpages": [],
          "path": "[\"subpages\", 2, \"subpages\", 0]"
        },
        {
          "title": "Deprecations",
          "description": null,
          "content": {
            "Logstash deprecations [logstash-deprecations]": "Over time, certain Elastic functionality becomes outdated and is replaced or removed. To help with the transition, Elastic deprecates functionality for a period before removal, giving you time to update your applications. \n\nReview the deprecated functionality for Logstash. \nWhile deprecations have no immediate impact, we strongly encourage you update your implementation after you upgrade.\n\n% ## Next version [logstash-versionnext-deprecations]\n\n% ::::{dropdown} Deprecation title\n% Description of the deprecation.\n% For more information, check [PR #](PR link).\n% **Impact**<br> Impact of deprecation. \n% **Action**<br> Steps for mitigating deprecation impact.\n% ::::\n\n% ## 9.0.0 [logstash-900-deprecations]\n\n% ::::{dropdown} Deprecation title\n% Description of the deprecation.\n% For more information, check [PR #](PR link).\n% **Impact**<br> Impact of deprecation. \n% **Action**<br> Steps for mitigating deprecation impact.\n% ::::\n\nNone at this time"
          },
          "metadata": {
            "navigation_title": "Deprecations"
          },
          "subpages": [],
          "path": "[\"subpages\", 2, \"subpages\", 1]"
        },
        {
          "title": "Logstash release notes [logstash-release-notes]",
          "description": null,
          "content": {
            "9.1.2 [logstash-9.1.2-release-notes]": {
              "Plugins [logstash-plugin-9.1.2-changes]": "No change to the plugins in this release."
            },
            "9.1.1 [logstash-9.1.1-release-notes]": {
              "Features and enhancements [logstash-9.1.1-features-enhancements]": "No user-facing changes in Logstash core.",
              "Plugins [logstash-plugin-9.1.1-changes]": "**Elastic_integration Filter - 9.1.1**\n\n['Add terminate processor support [#345](https://github.com/elastic/logstash-filter-elastic_integration/pull/345)']\n\n**Translate Filter - 3.4.3**\n\n[\"Allow YamlFile's Psych::Parser and Visitor instances to be garbage collected [#104](https://github.com/logstash-plugins/logstash-filter-translate/pull/104)\"]\n\n**Xml Filter - 4.3.2**\n\n['Update Nokogiri dependency version [#89](https://github.com/logstash-plugins/logstash-filter-xml/pull/89)']\n\n**Azure_event_hubs Input - 1.5.2**\n\n['Updated JWT dependency [#101](https://github.com/logstash-plugins/logstash-input-azure_event_hubs/pull/101)']\n\n**Snmp Integration - 4.0.7**\n\n['FIX: The `snmptrap` input now correctly enforces the user security level set by `security_level` config, and drops received events that do not match the configured value [#75](https://github.com/logstash-plugins/logstash-integration-snmp/pull/75)']\n\n**Elasticsearch Output - 12.0.6**\n\n['Add headers reporting uncompressed size and doc count for bulk requests [#1217](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1217)']"
            },
            "9.1.0 [logstash-9.1.0-release-notes]": {
              "Features and enhancements [logstash-9.1.0-features-enhancements]": {
                "Field Tracking Support in Elasticsearch Input (Technical Preview)": "The Elasticsearch Input now provides [support](https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/205) for field value tracking, persisted to disk on each `search_after` page. This is useful to track new data being written to an index or series of indices."
              },
              "Updates to dependencies [logstash-9.1.0-dependencies]": [
                "Update JDK to 21.0.7+6 [#17591](https://github.com/elastic/logstash/pull/17591)"
              ],
              "Plugins [logstash-plugin-9.1.0-changes]": "**Elastic Integration Filter - 9.1.0**\n\n['Introduces `proxy` param to support proxy [#316](https://github.com/elastic/logstash-filter-elastic_integration/pull/316)', 'Embeds Ingest Node components from Elasticsearch 9.1']\n\n**Elasticsearch Filter - 4.2.0**\n\n['Add `target` configuration option to store the result into it [#196](https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/196)']\n\n**Elasticsearch Input - 5.2.0**\n\n['Add \"cursor\"-like index tracking [#205](https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/205)', 'ES|QL support [#233](https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/233)']\n\n**Elasticsearch Output - 12.0.5**\n\n['Docs: update Cloud terminology [#1212](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1212)', 'Change connection log entry from `WARN` to `INFO` when connecting during register phase [#1211](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1211)']\n\n**JDBC Integration - 5.6.0**\n\n['Support other rufus scheduling options in JDBC Input [#183](https://github.com/logstash-plugins/logstash-integration-jdbc/pull/183)']\n\n**JMS Input - 3.3.0**\n\n['Added support for decoding multiple events from text or binary messages when using a codec that produces multiple events [#56](https://github.com/logstash-plugins/logstash-input-jms/pull/56)']\n\n**Kafka Integration - 11.6.3**\n\n['Update kafka client to `3.9.1` [#193](https://github.com/logstash-plugins/logstash-integration-kafka/pull/193)', 'Docs: fixed setting type reference for `sasl_iam_jar_paths` [#192](https://github.com/logstash-plugins/logstash-integration-kafka/pull/192)', 'Expose the SASL client callback class setting to the Logstash configuration [#177](https://github.com/logstash-plugins/logstash-integration-kafka/pull/177)', 'Adds a mechanism to load AWS IAM authentication as SASL client libraries at startup [#178](https://github.com/logstash-plugins/logstash-integration-kafka/pull/178)']\n\n**Xml Filter - 4.3.1**\n\n['Update Nokogiri dependency version [#88](https://github.com/logstash-plugins/logstash-filter-xml/pull/88)']\n\n**Tcp Output - 7.0.1**\n\n['Call connection check after connect [#61](https://github.com/logstash-plugins/logstash-output-tcp/pull/61)']"
            },
            "9.0.4 [logstash-9.0.4-release-notes]": {
              "Fixes [logstash-9.0.4-fixes]": [
                "Significantly improves write speeds to the persistent queue (PQ) when a pipeline's workers are caught up with already-written events [#17791](https://github.com/elastic/logstash/pull/17791)",
                "Eliminated log warning about unknown gauge metric type when using pipeline-to-pipeline. [#17721](https://github.com/elastic/logstash/pull/17721)"
              ],
              "Plugins [logstash-plugin-9.0.4-changes]": "**Elastic_integration Filter - 9.0.1**\n\n['Introduces `proxy` config to support proxy URI to connect to Elasticsearch. [#320](https://github.com/elastic/logstash-filter-elastic_integration/pull/320)']\n\n**Elasticsearch Output - 12.0.4**\n\n['Docs: update Cloud terminology [#1212](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1212)']"
            },
            "9.0.3 [logstash-9.0.3-release-notes]": {
              "Plugins [logstash-plugin-9.0.3-changes]": "**Kafka Integration - 11.6.3**\n\n['Update kafka client to `3.9.1` [#193](https://github.com/logstash-plugins/logstash-integration-kafka/pull/193)']"
            },
            "9.0.2 [logstash-9.0.2-release-notes]": {
              "Plugins [logstash-plugin-9.0.2-changes]": "**Kafka Integration - 11.6.2**\n\n['Docs: fixed setting type reference for `sasl_iam_jar_paths` [#192](https://github.com/logstash-plugins/logstash-integration-kafka/pull/192)', 'Expose the SASL client callback class setting to the Logstash configuration [#177](https://github.com/logstash-plugins/logstash-integration-kafka/pull/177)', 'Adds a mechanism to load AWS IAM authentication as SASL client libraries at startup [#178](https://github.com/logstash-plugins/logstash-integration-kafka/pull/178)']"
            },
            "9.0.1 [logstash-9.0.1-release-notes]": {
              "Features and enhancements [logstash-9.0.1-features-enhancements]": [
                "Enhanced keystore validation to prevent the creation of secrets in an invalid format [#17351](https://github.com/elastic/logstash/pull/17351)"
              ],
              "Updates to dependencies [logstash-9.0.1-dependencies]": [
                "Update JDK to 21.0.7+6 [#17591](https://github.com/elastic/logstash/pull/17591)"
              ],
              "Plugins [logstash-plugin-9.0.1-changes]": "**Xml Filter - 4.3.1**\n\n['Update Nokogiri dependency version [#88](https://github.com/logstash-plugins/logstash-filter-xml/pull/88)']\n\n**Elasticsearch Output - 12.0.3**\n\n['Change connection log entry from `WARN` to `INFO` when connecting during register phase [#1211](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1211)']\n\n**Tcp Output - 7.0.1**\n\n['Call connection check after connect [#61](https://github.com/logstash-plugins/logstash-output-tcp/pull/61)']"
            },
            "9.0.0 [logstash-900-release-notes]": {
              "Features and enhancements [logstash-900-features-enhancements]": [
                "Use UBI9 as base image [#17174](https://github.com/elastic/logstash/pull/17174)",
                "Improve plugins remove command to support multiple plugins [#17030](https://github.com/elastic/logstash/pull/17030)",
                "Allow concurrent Batch deserialization [#17050](https://github.com/elastic/logstash/pull/17050)"
              ],
              "Fixes [logstash-900-fixes]": [
                "Fix pqcheck and pqrepair on Windows [#17210](https://github.com/elastic/logstash/pull/17210)",
                "Fix empty node stats pipelines [#17185](https://github.com/elastic/logstash/pull/17185)"
              ],
              "Plugins [logstash-plugin-900-changes]": "**elastic_integration Filter - 9.0.0**\n\n['9.0 prerelease compatible plugin version [#265](https://github.com/elastic/logstash-filter-elastic_integration/pull/265)']\n\n**Elasticsearch Filter - 4.1.0**\n\n['Remove deprecated SSL settings [#183](https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/183)']\n\n**Http Filter - 2.0.0**\n\n['Remove deprecated SSL settings [#54](https://github.com/logstash-plugins/logstash-filter-http/pull/54)']\n\n**Beats Input - 7.0.1**\n\n['Remove deprecated SSL settings [#508](https://github.com/logstash-plugins/logstash-input-beats/pull/508)']\n\n**Elastic_serverless_forwarder Input - 2.0.0**\n\n['Remove deprecated SSL settings [#11](https://github.com/logstash-plugins/logstash-input-elastic_serverless_forwarder/pull/11)', 'Promote from technical preview to GA [#10](https://github.com/logstash-plugins/logstash-input-elastic_serverless_forwarder/pull/10)']\n\n**Elasticsearch Input - 5.0.0**\n\n['Remove deprecated SSL settings [#213](https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/213)']\n\n**Http Input - 4.1.0**\n\n['Remove deprecated SSL settings [#182](https://github.com/logstash-plugins/logstash-input-http/pull/182)']\n\n**Http_poller Input - 6.0.0**\n\n['Remove deprecated SSL settings [#149](https://github.com/logstash-plugins/logstash-input-http_poller/pull/149)']\n\n**Tcp Input - 7.0.0**\n\n['Remove deprecated SSL settings [#228](https://github.com/logstash-plugins/logstash-input-tcp/pull/228)']\n\n**Kafka Integration - 11.6.0**\n\n['Support additional `oauth` and `sasl` configuration options for configuring kafka client [#189](https://github.com/logstash-plugins/logstash-integration-kafka/pull/189)']\n\n**Snmp Integration - 4.0.6**\n\n['[DOC] Fix typo in snmptrap migration section [#74](https://github.com/logstash-plugins/logstash-integration-snmp/pull/74)']\n\n**Elasticsearch Output - 12.0.2**\n\n['Remove deprecated SSL settings [#1197](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1197)']\n\n**Http Output - 6.0.0**\n\n['Remove deprecated SSL settings [#147](https://github.com/logstash-plugins/logstash-output-http/pull/147)']\n\n**Tcp Output - 7.0.0**\n\n['Remove deprecated SSL settings [#58](https://github.com/logstash-plugins/logstash-output-tcp/pull/58)']"
            }
          },
          "metadata": {
            "navigation_title": "Logstash",
            "mapped_pages": [
              "https://www.elastic.co/guide/en/logstash/current/releasenotes.html",
              "https://www.elastic.co/guide/en/logstash/master/upgrading-logstash-9.0.html"
            ]
          },
          "subpages": [],
          "path": "[\"subpages\", 2, \"subpages\", 2]"
        },
        {
          "title": "Logstash known issues [logstash-known-issues]",
          "description": null,
          "content": {
            "9.0.0": "None at this time"
          },
          "metadata": {
            "navigation_title": "Known issues"
          },
          "subpages": [],
          "path": "[\"subpages\", 2, \"subpages\", 3]"
        }
      ],
      "path": "[\"subpages\", 2]"
    }
  ]
}