import json
import argparse
import os
import glob
from typing import List, Dict
import statistics
from collections import Counter
from time import sleep
import asyncio

import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import config
from utils import run_llm_natively

def parse_args():
    parser = argparse.ArgumentParser(description="Combine rubrics generated from multiple LLMs using semantic analysis")
    parser.add_argument("--repo-name", required=True, help="Name of the repository")
    parser.add_argument("--output-file", help="Output file name (default: combined_rubrics.json)")
    parser.add_argument("--temperature", type=float, default=0.1, help="Temperature for LLM inference (default: 0.1)")
    parser.add_argument("--max-retries", type=int, default=3, help="Maximum number of retries for API calls (default: 3)")
    return parser.parse_args()


async def semantic_combine_rubrics(all_rubrics: List[List[Dict]], llm_type: str = "anthropic", model: str = None, temperature: float = 0.1, max_retries: int = 3) -> List[Dict]:
    """Use Anthropic LLM to semantically combine rubrics from multiple sources"""
    
    if not all_rubrics:
        return []
    
    if len(all_rubrics) == 1:
        return all_rubrics[0]
    
    # Prepare the prompt for the LLM
    rubrics_data = {
        f"rubrics_set_{i+1}": rubrics for i, rubrics in enumerate(all_rubrics)
    }
    
    prompt = f"""You are an expert at combining and consolidating evaluation rubrics for code repositories. You have been given {len(all_rubrics)} different sets of rubrics that were generated by different AI models for the same repository.

Your task is to intelligently combine these rubrics into a single, comprehensive, and well-structured set that:
1. Eliminates redundancy while preserving important distinctions
2. Combines similar requirements into more comprehensive ones
3. Maintains appropriate granularity and specificity
4. Preserves the hierarchical structure with sub_tasks where appropriate

Here are the rubrics to combine:

{json.dumps(rubrics_data, indent=2)}

Please return ONLY a JSON object with the following structure:
{{
  "rubrics": [
        {{
            "requirements": "Top-level concept or component",
            "weight": 1/2/3,
            "sub_tasks": [
                {{
                    "requirements": "More specific concept or subcomponent",
                    "weight": 1/2/3,
                    "sub_tasks": [
                        {{
                            "requirements": "Detailed technical element or behavior",
                            "weight": 1/2/3,
                            "sub_tasks": [
                                {{
                                    "requirements": "Leaf-level functionality",
                                    "weight": 1/2/3
                                }},
                                {{
                                    "requirements": "More specific concept or subcomponent",
                                    "weight": 1/2/3,
                                    "sub_tasks": [
                                        {{
                                            "requirements": "Leaf-level functionality",
                                            "weight": 1/2/3,
                                            "sub_tasks": [...] # dive deeper into the functionality
                                        }}
                                    ]
                                }}
                            ]
                        }}
                    ]
                }},
                {{
                    "requirements": "Alternative aspect or feature",
                    "weight": 1/2/3
                }}
            ]
        }}
    ]
}}

Guidelines:
- Merge similar requirements that overlap significantly (>70% semantic similarity)
- Keep distinct requirements separate even if they're related
- Sub_tasks should be specific, measurable criteria that contribute to the parent requirement
""".strip()

    for attempt in range(max_retries):
        try:
            print(f"Making API call to Anthropic (attempt {attempt + 1}/{max_retries})...")
            
            response_text = await run_llm_natively(model, prompt)
            
            
            # Try to parse the JSON response
            try:

                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1

                result = json.loads(response_text[json_start:json_end])
                if "rubrics" in result:
                    print("✓ Successfully combined rubrics using semantic analysis")
                    return result["rubrics"]
                else:
                    print("✗ Response missing 'rubrics' key, trying to extract...")
                    # Try to extract JSON if it's wrapped or formatted differently
                    if isinstance(result, list):
                        return result
                    else:
                        raise ValueError("Unexpected response format")
            except json.JSONDecodeError as e:
                print(f"✗ JSON parsing error: {e}")
                print(f"Response: {response_text[:500]}...")
                
                # Try to extract JSON from response if it's wrapped in markdown or other text
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        if "rubrics" in result:
                            return result["rubrics"]
                        elif isinstance(result, list):
                            return result
                    except json.JSONDecodeError:
                        pass
                
                if attempt == max_retries - 1:
                    raise ValueError(f"Failed to parse LLM response after {max_retries} attempts")
        
        except Exception as e:
            print(f"✗ API call failed: {e}")
            if attempt == max_retries - 1:
                print("Max retries reached. Falling back to simple merge...")
                return fallback_simple_merge(all_rubrics)
            else:
                print(f"Retrying in {2 ** attempt} seconds...")
                sleep(2 ** attempt)
    
    # Fallback if all retries failed
    return fallback_simple_merge(all_rubrics)

def fallback_simple_merge(all_rubrics: List[List[Dict]]) -> List[Dict]:
    """Simple fallback merge when LLM calls fail"""
    print("Using fallback simple merge method...")
    
    all_items = []
    for rubrics_set in all_rubrics:
        all_items.extend(rubrics_set)
    
    # Remove duplicates based on name similarity
    merged = []
    processed_names = set()
    
    for item in all_items:
        name = item.get("name", "").lower().strip()
        if name not in processed_names:
            merged.append(item)
            processed_names.add(name)
    
    return merged

def load_rubrics_files(repo_name: str) -> List[Dict]:
    """Load all rubrics files for the repository"""
    base_path = config.get_data_path(repo_name, "rubrics")
    
    if not os.path.exists(base_path):
        raise ValueError(f"Rubrics directory not found: {base_path}")
    
    file_pattern = os.path.join(base_path, "*.json")
    rubrics_files = [f for f in glob.glob(file_pattern) if "combined" not in os.path.basename(f) and "embeddings" not in os.path.basename(f)]
    
    if not rubrics_files:
        raise ValueError(f"No rubrics files found in: {base_path}")
    
    print(f"Found {len(rubrics_files)} rubrics files:")
    for file_path in rubrics_files:
        print(f"  - {os.path.basename(file_path)}")
    
    all_rubrics = []
    for file_path in rubrics_files:
        try:
            with open(file_path, "r") as f:
                rubrics = json.load(f)
                all_rubrics.append(rubrics)
                print(f"✓ Loaded: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"✗ Error loading {file_path}: {e}")
    
    return all_rubrics

def calculate_rubrics_statistics(rubrics: List[Dict]) -> Dict:
    """Calculate statistics about the combined rubrics"""
    def count_items(items, level=0):
        count = 0
        weights = []
        max_depth = level
        
        for item in items:
            count += 1
            weights.append(item.get("weight", 1))
            
            children = item.get("sub_tasks", item.get("children", []))
            if children:
                child_count, child_weights, child_depth = count_items(children, level + 1)
                count += child_count
                weights.extend(child_weights)
                max_depth = max(max_depth, child_depth)
        
        return count, weights, max_depth
    
    total_items, all_weights, max_depth = count_items(rubrics)
    
    return {
        "total_items": total_items,
        "top_level_items": len(rubrics),
        "max_depth": max_depth,
        "weight_distribution": dict(Counter(all_weights)),
        "average_weight": statistics.mean(all_weights) if all_weights else 0
    }

async def main():
    args = parse_args()

    # Save combined results
    base_path = config.get_data_path(args.repo_name, "rubrics")
    output_file = args.output_file or "combined_rubrics.json"
    output_path = os.path.join(base_path, output_file)

    # check if output file already exists
    if os.path.exists(output_path):
        print(f"Combined rubrics already exists: {output_path}")
        return
    
    # Load all rubrics files
    print("Loading rubrics files...")
    all_rubrics = load_rubrics_files(args.repo_name)
    
    if len(all_rubrics) < 2:
        print("Warning: Only one rubrics file found. Creating a copy as combined rubrics.")
        if all_rubrics:
            combined_rubrics = all_rubrics[0]
        else:
            print("Error: No rubrics files found")
            return
    else:
        print(f"Combining {len(all_rubrics)} rubrics using semantic analysis...")
        
        # Combine rubrics using semantic method
        combined_rubrics = await semantic_combine_rubrics(
            all_rubrics, 
            llm_type="anthropic", 
            model=config.MODEL, 
            temperature=args.temperature,
            max_retries=args.max_retries
        )
    
    # Calculate statistics
    stats = calculate_rubrics_statistics(combined_rubrics)
    
    # Add metadata about the combination
    combination_metadata = {
        "combination_method": "semantic_llm",
        "llm_model": config.MODEL,
        "temperature": args.temperature,
        "num_rubrics_combined": len(all_rubrics),
        "max_retries": args.max_retries,
        "statistics": stats
    }
    
    # Create the final result structure
    result = {
        "rubrics": combined_rubrics,
        "combination_metadata": combination_metadata
    }
    
    with open(output_path, "w") as f:
        json.dump(result, f, indent=2)
    
    print(f"Combined rubrics saved to: {output_path}")
    
    # Display summary statistics
    print("-" * 100)
    print("COMBINATION SUMMARY:")
    print(f"Method used: Semantic LLM Analysis ({config.MODEL})")
    print(f"Temperature: {args.temperature}")
    print(f"Number of rubrics combined: {len(all_rubrics)}")
    print(f"Total rubric items: {stats['total_items']}")
    print(f"Top-level items: {stats['top_level_items']}")
    print(f"Maximum depth: {stats['max_depth']}")
    print(f"Weight distribution: {stats['weight_distribution']}")
    print(f"Average weight: {stats['average_weight']:.2f}")
    print("-" * 100)

if __name__ == "__main__":
    asyncio.run(main()) 